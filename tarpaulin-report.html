<!doctype html>
<html>
<head>
    <meta charset="utf-8">
    <style>:root {
  --color: black;
  --bg: white;
  --head-bg: white;
  --link: #338;

  --blue: #ccf;
  --red: #fcc;
  --yellow: #ffc;
  --green: #cfc;
}

[data-theme='dark'] {
  --color: white;
  --bg: black;
  --head-bg: #333;
  --link: #aaf;

  --blue: #225;
  --red: #522;
  --yellow: #552;
  --green: #252;
}

html,
body {
  margin: 0;
  padding: 0;
  color: var(--color);
  background: var(--bg);
}

.app {
  margin: 10px;
  padding: 0;
}

.files-list {
  margin: 10px 0 0;
  width: 100%;
  border-collapse: collapse;
}
.files-list__head {
  border: 1px solid #999;
}
.files-list__head > tr > th {
  padding: 10px;
  border: 1px solid #999;
  text-align: left;
  font-weight: normal;
  background: var(--head-bg);
}
.files-list__body {
}
.files-list__file {
  cursor: pointer;
}
.files-list__file:hover {
  background: var(--blue);
}
.files-list__file > td {
  padding: 10px;
  border: 1px solid #999;
}
.files-list__file > td:first-child::before {
  content: '\01F4C4';
  margin-right: 1em;
}
.files-list__file_low {
  background: var(--red);
}
.files-list__file_medium {
  background: var(--yellow);
}
.files-list__file_high {
  background: var(--green);
}
.files-list__file_folder > td:first-child::before {
  content: '\01F4C1';
  margin-right: 1em;
}

.file-header {
  border: 1px solid #999;
  display: flex;
  justify-content: space-between;
  align-items: center;
  position: sticky;
  top: 0;
  background: var(--bg);
}

.file-header__back {
  margin: 10px;
  cursor: pointer;
  flex-shrink: 0;
  flex-grow: 0;
  text-decoration: underline;
  color: var(--link);
}

.file-header__name {
  margin: 10px;
  flex-shrink: 2;
  flex-grow: 2;
}

.file-header__stat {
  margin: 10px;
  flex-shrink: 0;
  flex-grow: 0;
}

.file-content {
  margin: 10px 0 0;
  border: 1px solid #999;
  padding: 10px;
  counter-reset: line;
  display: flex;
  flex-direction: column;
}

.code-line::before {
  content: counter(line);
  margin-right: 72px;
}
.code-line {
  margin: 0;
  height: 1em;
  counter-increment: line;

  position: absolute;
  padding: 0 0.3em 0.3em 0.3em;
  display: inherit;
  width: 100%;
}
.code-line_covered {
  background: var(--green);
}
.code-line_uncovered {
  background: var(--red);
}

.code-text-container {
  position: relative;
  height: 1em;
  padding: 0.3em 0;
}

.cover-indicator {
  display: flex;
  width: 100%;
  position: absolute;
  justify-content: end;
  height: 1em;
  align-items: center;
  padding: 0 0.3em 0.3em 0.3em;
}

.cover-indicator.check-cover::after {
  content: "\2713";
  font-weight: bold;
  background-color: var(--green);
  height: 1em;
}

.cover-indicator.no-cover::after {
  content: "\2716";
  font-weight: bold;
  background-color: var(--red);
  height: 1em;
}

.stat-line-hit {
  max-width: 48px;
  overflow: hidden;
  font-weight: bold;
  margin-right: 4px;
  background-color: var(--green);
  position: relative;
  top: 0.1em;
}

#theme-toggle-label {
  margin-left: 1ch;
}
</style>
</head>
<body>
    <div id="root"></div>
    <script>
        var data = {"files":[{"path":["/","home","victor","ML","machinelearne-rs","benchmarks","benches","linear_regression","predict.rs"],"content":"use benchmarks::data::CaliforniaHousingDataset;\nuse criterion::{black_box, criterion_group, criterion_main, BenchmarkId, Criterion};\nuse machinelearne_rs::{\n    backend::Tensor2D,\n    dataset::InMemoryDataset,\n    loss::MSELoss,\n    model::{linear::LinearRegression, InferenceModel},\n    optimizer::SGD,\n    regularizers::NoRegularizer,\n    trainer::Trainer,\n    CpuBackend,\n};\n\n/// Helper function to convert Vec\u003cVec\u003cf32\u003e\u003e to Tensor2D\u003cCpuBackend\u003e\nfn vec_to_tensor2d(data: \u0026[Vec\u003cf32\u003e]) -\u003e Tensor2D\u003cCpuBackend\u003e {\n    let flat: Vec\u003cf32\u003e = data.iter().flatten().copied().collect();\n    Tensor2D::new(flat, data.len(), data.first().map(|v| v.len()).unwrap_or(0))\n}\n\n/// Train a model once for prediction benchmarks\nfn train_model_for_prediction(\n) -\u003e machinelearne_rs::model::linear::LinearModel\u003cCpuBackend, machinelearne_rs::model::state::Fitted\u003e\n{\n    let dataset = CaliforniaHousingDataset::load(\"datasets/california_housing.csv\")\n        .expect(\"Failed to load dataset\");\n    let (train_dataset, _test_dataset) = dataset.split(0.8);\n    let in_memory = train_dataset\n        .to_in_memory_dataset()\n        .expect(\"Failed to create dataset\");\n\n    let model = LinearRegression::\u003cCpuBackend\u003e::new(8);\n    let optimizer = SGD::new(0.001);\n    let trainer = Trainer::builder(MSELoss, optimizer, NoRegularizer)\n        .batch_size(32)\n        .max_epochs(50)\n        .build();\n\n    trainer.fit(model, \u0026in_memory).expect(\"Failed to fit model\")\n}\n\nfn bench_predict_single(c: \u0026mut Criterion) {\n    let model = train_model_for_prediction();\n\n    c.bench_function(\"predict_single\", |b| {\n        let input: Vec\u003cf32\u003e = vec![5.0, 30.0, 6.0, 1.0, 500.0, 3.0, 37.8, -122.2];\n        let input_tensor = machinelearne_rs::backend::Tensor1D::new(input);\n        b.iter(|| {\n            let pred = model.predict(black_box(\u0026input_tensor));\n            black_box(pred);\n        });\n    });\n}\n\nfn bench_predict_batch(c: \u0026mut Criterion) {\n    let model = train_model_for_prediction();\n\n    // Test different batch sizes\n    for batch_size in [10, 100, 1000, 10000].iter() {\n        c.bench_with_input(\n            BenchmarkId::new(\"predict_batch\", batch_size),\n            batch_size,\n            |b, \u0026bs| {\n                // Create test data\n                let test_x: Vec\u003cVec\u003cf32\u003e\u003e = (0..bs)\n                    .map(|_| vec![5.0, 30.0, 6.0, 1.0, 500.0, 3.0, 37.8, -122.2])\n                    .collect();\n\n                b.iter(|| {\n                    let predictions = model.predict_batch(black_box(\u0026vec_to_tensor2d(\u0026test_x)));\n                    black_box(predictions);\n                });\n            },\n        );\n    }\n}\n\nfn bench_predict_throughput(c: \u0026mut Criterion) {\n    let model = train_model_for_prediction();\n\n    // Test different batch sizes for throughput\n    for batch_size in [100, 1000, 10000].iter() {\n        c.bench_with_input(\n            BenchmarkId::new(\"predict_throughput\", batch_size),\n            batch_size,\n            |b, \u0026bs| {\n                // Create test data\n                let test_x: Vec\u003cVec\u003cf32\u003e\u003e = (0..bs)\n                    .map(|_| vec![5.0, 30.0, 6.0, 1.0, 500.0, 3.0, 37.8, -122.2])\n                    .collect();\n\n                b.iter(|| {\n                    let predictions = model.predict_batch(black_box(\u0026vec_to_tensor2d(\u0026test_x)));\n                    // Throughput is calculated as batch_size / time\n                    black_box((predictions.len(), predictions));\n                });\n            },\n        );\n    }\n}\n\nfn bench_predict_warmup(c: \u0026mut Criterion) {\n    let model = train_model_for_prediction();\n\n    c.bench_function(\"predict_warmup\", |b| {\n        let input: Vec\u003cf32\u003e = vec![5.0, 30.0, 6.0, 1.0, 500.0, 3.0, 37.8, -122.2];\n        let input_tensor = machinelearne_rs::backend::Tensor1D::new(input);\n\n        // Warmup\n        for _ in 0..5 {\n            let _ = model.predict(\u0026input_tensor);\n        }\n\n        b.iter(|| {\n            let pred = model.predict(black_box(\u0026input_tensor));\n            black_box(pred);\n        });\n    });\n}\n\nfn bench_predict_latencies(c: \u0026mut Criterion) {\n    let model = train_model_for_prediction();\n\n    // Measure latency percentiles for single prediction\n    let mut group = c.benchmark_group(\"predict_latency\");\n    group.sample_size(10000);\n\n    group.bench_function(\"p50\", |b| {\n        let input: Vec\u003cf32\u003e = vec![5.0, 30.0, 6.0, 1.0, 500.0, 3.0, 37.8, -122.2];\n        let input_tensor = machinelearne_rs::backend::Tensor1D::new(input);\n        b.iter(|| {\n            let pred = model.predict(black_box(\u0026input_tensor));\n            black_box(pred);\n        });\n    });\n\n    group.finish();\n}\n\nfn bench_predict_batch_warmup(c: \u0026mut Criterion) {\n    let model = train_model_for_prediction();\n\n    for batch_size in [100, 1000].iter() {\n        c.bench_with_input(\n            BenchmarkId::new(\"predict_batch_warmup\", batch_size),\n            batch_size,\n            |b, \u0026bs| {\n                let test_x: Vec\u003cVec\u003cf32\u003e\u003e = (0..bs)\n                    .map(|_| vec![5.0, 30.0, 6.0, 1.0, 500.0, 3.0, 37.8, -122.2])\n                    .collect();\n\n                // Warmup\n                for _ in 0..5 {\n                    let _ = model.predict_batch(\u0026vec_to_tensor2d(\u0026test_x));\n                }\n\n                b.iter(|| {\n                    let predictions = model.predict_batch(black_box(\u0026vec_to_tensor2d(\u0026test_x)));\n                    black_box(predictions);\n                });\n            },\n        );\n    }\n}\n\nfn bench_predict_different_features(c: \u0026mut Criterion) {\n    // Train models with different feature counts\n    let dataset = CaliforniaHousingDataset::load(\"datasets/california_housing.csv\")\n        .expect(\"Failed to load dataset\");\n    let (train_dataset, _) = dataset.split(0.8);\n    let train_memory = train_dataset\n        .to_in_memory_dataset()\n        .expect(\"Failed to create dataset\");\n\n    // Train models with 1, 2, 4, and 8 features\n    let models = vec![\n        (1, {\n            let subset = dataset.select_features(\u0026[0]);\n            let (train, _) = subset.split(0.8);\n            let in_memory = train.to_in_memory_dataset().unwrap();\n            let model = LinearRegression::\u003cCpuBackend\u003e::new(1);\n            let optimizer = SGD::new(0.01);\n            let trainer = Trainer::builder(MSELoss, optimizer, NoRegularizer)\n                .batch_size(32)\n                .max_epochs(50)\n                .build();\n            trainer.fit(model, \u0026in_memory).expect(\"Failed to fit model\")\n        }),\n        (2, {\n            let subset = dataset.select_features(\u0026[0, 1]);\n            let (train, _) = subset.split(0.8);\n            let in_memory = train.to_in_memory_dataset().unwrap();\n            let model = LinearRegression::\u003cCpuBackend\u003e::new(2);\n            let optimizer = SGD::new(0.001);\n            let trainer = Trainer::builder(MSELoss, optimizer, NoRegularizer)\n                .batch_size(32)\n                .max_epochs(50)\n                .build();\n            trainer.fit(model, \u0026in_memory).expect(\"Failed to fit model\")\n        }),\n        (4, {\n            let subset = dataset.select_features(\u0026[0, 1, 2, 3]);\n            let (train, _) = subset.split(0.8);\n            let in_memory = train.to_in_memory_dataset().unwrap();\n            let model = LinearRegression::\u003cCpuBackend\u003e::new(4);\n            let optimizer = SGD::new(0.001);\n            let trainer = Trainer::builder(MSELoss, optimizer, NoRegularizer)\n                .batch_size(32)\n                .max_epochs(50)\n                .build();\n            trainer.fit(model, \u0026in_memory).expect(\"Failed to fit model\")\n        }),\n        (8, {\n            let model = LinearRegression::\u003cCpuBackend\u003e::new(8);\n            let optimizer = SGD::new(0.001);\n            let trainer = Trainer::builder(MSELoss, optimizer, NoRegularizer)\n                .batch_size(32)\n                .max_epochs(50)\n                .build();\n            trainer\n                .fit(model, \u0026train_memory)\n                .expect(\"Failed to fit model\")\n        }),\n    ];\n\n    for (n_features, model) in models {\n        c.bench_with_input(\n            BenchmarkId::new(\"predict_by_features\", n_features),\n            \u0026n_features,\n            |b, n_feat| {\n                let input: Vec\u003cf32\u003e = (0..*n_feat).map(|i| i as f32 * 1.0).collect();\n                let input_tensor = machinelearne_rs::backend::Tensor1D::new(input);\n                b.iter(|| {\n                    let pred = model.predict(black_box(\u0026input_tensor));\n                    black_box(pred);\n                });\n            },\n        );\n    }\n}\n\ncriterion_group!(\n    benches,\n    bench_predict_single,\n    bench_predict_batch,\n    bench_predict_throughput,\n    bench_predict_warmup,\n    bench_predict_latencies,\n    bench_predict_batch_warmup,\n    bench_predict_different_features\n);\ncriterion_main!(benches);\n","traces":[],"covered":0,"coverable":0},{"path":["/","home","victor","ML","machinelearne-rs","benchmarks","benches","linear_regression","train_1_feature.rs"],"content":"use benchmarks::data::CaliforniaHousingDataset;\nuse benchmarks::metrics::Metrics;\nuse criterion::{black_box, criterion_group, criterion_main, BenchmarkId, Criterion};\nuse machinelearne_rs::{\n    backend::Tensor2D,\n    dataset::InMemoryDataset,\n    loss::MSELoss,\n    model::{linear::LinearRegression, InferenceModel},\n    optimizer::SGD,\n    regularizers::NoRegularizer,\n    trainer::Trainer,\n    CpuBackend,\n};\n\n/// Helper function to convert Vec\u003cVec\u003cf32\u003e\u003e to Tensor2D\u003cCpuBackend\u003e\nfn vec_to_tensor2d(data: \u0026[Vec\u003cf32\u003e]) -\u003e Tensor2D\u003cCpuBackend\u003e {\n    let flat: Vec\u003cf32\u003e = data.iter().flatten().copied().collect();\n    Tensor2D::new(flat, data.len(), data.first().map(|v| v.len()).unwrap_or(0))\n}\n\nfn bench_train_1_feature(c: \u0026mut Criterion) {\n    // Load dataset and select only first feature\n    let dataset = CaliforniaHousingDataset::load(\"datasets/california_housing.csv\")\n        .expect(\"Failed to load dataset\");\n    let subset = dataset.select_features(\u0026[0]); // Only MedInc\n    let (train_dataset, _test_dataset) = subset.split(0.8);\n    let in_memory = train_dataset\n        .to_in_memory_dataset()\n        .expect(\"Failed to create dataset\");\n\n    // Test different batch sizes\n    for batch_size in [16, 32, 64, 128].iter() {\n        c.bench_with_input(\n            BenchmarkId::new(\"train_1_feature\", batch_size),\n            batch_size,\n            |b, \u0026bs| {\n                let trainer = Trainer::builder(MSELoss, SGD::new(0.01), NoRegularizer)\n                    .batch_size(bs)\n                    .max_epochs(10)\n                    .build();\n\n                b.iter(|| {\n                    let model_copy = LinearRegression::\u003cCpuBackend\u003e::new(1);\n                    let fitted = trainer\n                        .fit(model_copy, \u0026in_memory)\n                        .expect(\"Failed to fit model\");\n\n                    // Verify the model works\n                    let test_x: Vec\u003cf32\u003e = vec![5.0];\n                    let test_tensor = machinelearne_rs::backend::Tensor1D::new(test_x);\n                    let _pred = fitted.predict(black_box(\u0026test_tensor));\n\n                    fitted\n                });\n            },\n        );\n    }\n}\n\nfn bench_train_1_feature_learning_rate(c: \u0026mut Criterion) {\n    // Load dataset and select only first feature\n    let dataset = CaliforniaHousingDataset::load(\"datasets/california_housing.csv\")\n        .expect(\"Failed to load dataset\");\n    let subset = dataset.select_features(\u0026[0]); // Only MedInc\n    let (train_dataset, _test_dataset) = subset.split(0.8);\n    let in_memory = train_dataset\n        .to_in_memory_dataset()\n        .expect(\"Failed to create dataset\");\n\n    // Test different learning rates\n    for lr in [0.001, 0.01, 0.1].iter() {\n        c.bench_with_input(\n            BenchmarkId::new(\"train_1_feature_lr\", lr),\n            lr,\n            |b, \u0026learning_rate| {\n                let optimizer = SGD::new(learning_rate);\n                let trainer = Trainer::builder(MSELoss, optimizer, NoRegularizer)\n                    .batch_size(32)\n                    .max_epochs(10)\n                    .build();\n\n                b.iter(|| {\n                    let model_copy = LinearRegression::\u003cCpuBackend\u003e::new(1);\n                    let _ = trainer.fit(model_copy, \u0026in_memory);\n                });\n            },\n        );\n    }\n}\n\nfn bench_train_1_feature_epochs(c: \u0026mut Criterion) {\n    // Load dataset and select only first feature\n    let dataset = CaliforniaHousingDataset::load(\"datasets/california_housing.csv\")\n        .expect(\"Failed to load dataset\");\n    let subset = dataset.select_features(\u0026[0]); // Only MedInc\n    let (train_dataset, _test_dataset) = subset.split(0.8);\n    let in_memory = train_dataset\n        .to_in_memory_dataset()\n        .expect(\"Failed to create dataset\");\n\n    // Test different epoch counts\n    for epochs in [10, 50, 100].iter() {\n        c.bench_with_input(\n            BenchmarkId::new(\"train_1_feature_epochs\", epochs),\n            epochs,\n            |b, \u0026max_epochs| {\n                let optimizer = SGD::new(0.01);\n                let trainer = Trainer::builder(MSELoss, optimizer, NoRegularizer)\n                    .batch_size(32)\n                    .max_epochs(max_epochs)\n                    .build();\n\n                b.iter(|| {\n                    let model_copy = LinearRegression::\u003cCpuBackend\u003e::new(1);\n                    let _ = trainer.fit(model_copy, \u0026in_memory);\n                });\n            },\n        );\n    }\n}\n\nfn bench_train_1_feature_with_metrics(c: \u0026mut Criterion) {\n    // Load dataset and select only first feature\n    let dataset = CaliforniaHousingDataset::load(\"datasets/california_housing.csv\")\n        .expect(\"Failed to load dataset\");\n    let subset = dataset.select_features(\u0026[0]); // Only MedInc\n    let (train_dataset, test_dataset) = subset.split(0.8);\n    let train_memory = train_dataset\n        .to_in_memory_dataset()\n        .expect(\"Failed to create dataset\");\n\n    // Test the full pipeline and measure metrics\n    c.bench_function(\"train_1_feature_with_metrics\", |b| {\n        b.iter(|| {\n            let model = LinearRegression::\u003cCpuBackend\u003e::new(1);\n            let optimizer = SGD::new(0.01);\n            let trainer = Trainer::builder(MSELoss, optimizer, NoRegularizer)\n                .batch_size(32)\n                .max_epochs(50)\n                .build();\n\n            let fitted = trainer\n                .fit(model, \u0026train_memory)\n                .expect(\"Failed to fit model\");\n\n            // Evaluate on test set\n            let test_x = test_dataset.features().to_vec();\n            let test_y = test_dataset.target().to_vec();\n            let pred_tensor = fitted.predict_batch(black_box(\u0026vec_to_tensor2d(\u0026test_x)));\n\n            let predictions: Vec\u003cf32\u003e =\n                pred_tensor.to_vec().into_iter().map(|v| v as f32).collect();\n\n            let mse = Metrics::mse(\u0026test_y, \u0026predictions);\n            let mae = Metrics::mae(\u0026test_y, \u0026predictions);\n            let r2 = Metrics::r_squared(\u0026test_y, \u0026predictions);\n\n            // Black box the metrics to ensure they're computed\n            black_box((mse, mae, r2));\n        });\n    });\n}\n\ncriterion_group!(\n    benches,\n    bench_train_1_feature,\n    bench_train_1_feature_learning_rate,\n    bench_train_1_feature_epochs,\n    bench_train_1_feature_with_metrics\n);\ncriterion_main!(benches);\n","traces":[],"covered":0,"coverable":0},{"path":["/","home","victor","ML","machinelearne-rs","benchmarks","benches","linear_regression","train_2_features.rs"],"content":"use benchmarks::data::CaliforniaHousingDataset;\nuse benchmarks::metrics::Metrics;\nuse criterion::{black_box, criterion_group, criterion_main, BenchmarkId, Criterion};\nuse machinelearne_rs::{\n    backend::Tensor2D,\n    dataset::InMemoryDataset,\n    loss::MSELoss,\n    model::{linear::LinearRegression, InferenceModel},\n    optimizer::SGD,\n    regularizers::NoRegularizer,\n    trainer::Trainer,\n    CpuBackend,\n};\n\n/// Helper function to convert Vec\u003cVec\u003cf32\u003e\u003e to Tensor2D\u003cCpuBackend\u003e\nfn vec_to_tensor2d(data: \u0026[Vec\u003cf32\u003e]) -\u003e Tensor2D\u003cCpuBackend\u003e {\n    let flat: Vec\u003cf32\u003e = data.iter().flatten().copied().collect();\n    Tensor2D::new(flat, data.len(), data.first().map(|v| v.len()).unwrap_or(0))\n}\n\nfn bench_train_2_features(c: \u0026mut Criterion) {\n    // Load dataset and select first 2 features\n    let dataset = CaliforniaHousingDataset::load(\"datasets/california_housing.csv\")\n        .expect(\"Failed to load dataset\");\n    let subset = dataset.select_features(\u0026[0, 1]); // MedInc and HouseAge\n    let (train_dataset, _test_dataset) = subset.split(0.8);\n    let in_memory = train_dataset\n        .to_in_memory_dataset()\n        .expect(\"Failed to create dataset\");\n\n    // Test different batch sizes\n    for batch_size in [16, 32, 64, 128].iter() {\n        c.bench_with_input(\n            BenchmarkId::new(\"train_2_features\", batch_size),\n            batch_size,\n            |b, \u0026bs| {\n                let optimizer = SGD::new(0.001);\n                let trainer = Trainer::builder(MSELoss, optimizer, NoRegularizer)\n                    .batch_size(bs)\n                    .max_epochs(10)\n                    .build();\n\n                b.iter(|| {\n                    let model_copy = LinearRegression::\u003cCpuBackend\u003e::new(2);\n                    let fitted = trainer\n                        .fit(model_copy, \u0026in_memory)\n                        .expect(\"Failed to fit model\");\n\n                    // Verify the model works\n                    let test_x: Vec\u003cf32\u003e = vec![5.0, 30.0];\n                    let test_tensor = machinelearne_rs::backend::Tensor1D::new(test_x);\n                    let _pred = fitted.predict(black_box(\u0026test_tensor));\n\n                    fitted\n                });\n            },\n        );\n    }\n}\n\nfn bench_train_2_features_learning_rate(c: \u0026mut Criterion) {\n    // Load dataset and select first 2 features\n    let dataset = CaliforniaHousingDataset::load(\"datasets/california_housing.csv\")\n        .expect(\"Failed to load dataset\");\n    let subset = dataset.select_features(\u0026[0, 1]); // MedInc and HouseAge\n    let (train_dataset, _test_dataset) = subset.split(0.8);\n    let in_memory = train_dataset\n        .to_in_memory_dataset()\n        .expect(\"Failed to create dataset\");\n\n    // Test different learning rates\n    for lr in [0.001, 0.01, 0.1].iter() {\n        c.bench_with_input(\n            BenchmarkId::new(\"train_2_features_lr\", lr),\n            lr,\n            |b, \u0026learning_rate| {\n                let optimizer = SGD::new(learning_rate);\n                let trainer = Trainer::builder(MSELoss, optimizer, NoRegularizer)\n                    .batch_size(32)\n                    .max_epochs(10)\n                    .build();\n\n                b.iter(|| {\n                    let model_copy = LinearRegression::\u003cCpuBackend\u003e::new(2);\n                    let _ = trainer\n                        .fit(model_copy, \u0026in_memory)\n                        .expect(\"Failed to fit model\");\n                });\n            },\n        );\n    }\n}\n\nfn bench_train_2_features_epochs(c: \u0026mut Criterion) {\n    // Load dataset and select first 2 features\n    let dataset = CaliforniaHousingDataset::load(\"datasets/california_housing.csv\")\n        .expect(\"Failed to load dataset\");\n    let subset = dataset.select_features(\u0026[0, 1]); // MedInc and HouseAge\n    let (train_dataset, _test_dataset) = subset.split(0.8);\n    let in_memory = train_dataset\n        .to_in_memory_dataset()\n        .expect(\"Failed to create dataset\");\n\n    // Test different epoch counts\n    for epochs in [10, 50, 100].iter() {\n        c.bench_with_input(\n            BenchmarkId::new(\"train_2_features_epochs\", epochs),\n            epochs,\n            |b, \u0026max_epochs| {\n                let optimizer = SGD::new(0.01);\n                let trainer = Trainer::builder(MSELoss, optimizer, NoRegularizer)\n                    .batch_size(32)\n                    .max_epochs(max_epochs)\n                    .build();\n\n                b.iter(|| {\n                    let model_copy = LinearRegression::\u003cCpuBackend\u003e::new(2);\n                    let _ = trainer\n                        .fit(model_copy, \u0026in_memory)\n                        .expect(\"Failed to fit model\");\n                });\n            },\n        );\n    }\n}\n\nfn bench_train_2_features_with_metrics(c: \u0026mut Criterion) {\n    // Load dataset and select first 2 features\n    let dataset = CaliforniaHousingDataset::load(\"datasets/california_housing.csv\")\n        .expect(\"Failed to load dataset\");\n    let subset = dataset.select_features(\u0026[0, 1]); // MedInc and HouseAge\n    let (train_dataset, test_dataset) = subset.split(0.8);\n    let train_memory = train_dataset\n        .to_in_memory_dataset()\n        .expect(\"Failed to create dataset\");\n\n    // Test the full pipeline and measure metrics\n    c.bench_function(\"train_2_features_with_metrics\", |b| {\n        b.iter(|| {\n            let model = LinearRegression::\u003cCpuBackend\u003e::new(2);\n            let optimizer = SGD::new(0.001);\n            let trainer = Trainer::builder(MSELoss, optimizer, NoRegularizer)\n                .batch_size(32)\n                .max_epochs(50)\n                .build();\n\n            let fitted = trainer\n                .fit(model, \u0026train_memory)\n                .expect(\"Failed to fit model\");\n\n            // Evaluate on test set\n            let test_x = test_dataset.features().to_vec();\n            let test_y = test_dataset.target().to_vec();\n            let pred_tensor = fitted.predict_batch(black_box(\u0026vec_to_tensor2d(\u0026test_x)));\n\n            let predictions: Vec\u003cf32\u003e =\n                pred_tensor.to_vec().into_iter().map(|v| v as f32).collect();\n\n            let mse = Metrics::mse(\u0026test_y, \u0026predictions);\n            let mae = Metrics::mae(\u0026test_y, \u0026predictions);\n            let r2 = Metrics::r_squared(\u0026test_y, \u0026predictions);\n\n            black_box((mse, mae, r2));\n        });\n    });\n}\n\ncriterion_group!(\n    benches,\n    bench_train_2_features,\n    bench_train_2_features_learning_rate,\n    bench_train_2_features_epochs,\n    bench_train_2_features_with_metrics\n);\ncriterion_main!(benches);\n","traces":[],"covered":0,"coverable":0},{"path":["/","home","victor","ML","machinelearne-rs","benchmarks","benches","linear_regression","train_4_features.rs"],"content":"use benchmarks::data::CaliforniaHousingDataset;\nuse benchmarks::metrics::Metrics;\nuse criterion::{black_box, criterion_group, criterion_main, BenchmarkId, Criterion};\nuse machinelearne_rs::{\n    backend::Tensor2D,\n    dataset::InMemoryDataset,\n    loss::MSELoss,\n    model::{linear::LinearRegression, InferenceModel},\n    optimizer::SGD,\n    regularizers::NoRegularizer,\n    trainer::Trainer,\n    CpuBackend,\n};\n\n/// Helper function to convert Vec\u003cVec\u003cf32\u003e\u003e to Tensor2D\u003cCpuBackend\u003e\nfn vec_to_tensor2d(data: \u0026[Vec\u003cf32\u003e]) -\u003e Tensor2D\u003cCpuBackend\u003e {\n    let flat: Vec\u003cf32\u003e = data.iter().flatten().copied().collect();\n    Tensor2D::new(flat, data.len(), data.first().map(|v| v.len()).unwrap_or(0))\n}\n\nfn bench_train_4_features(c: \u0026mut Criterion) {\n    // Load dataset and select first 4 features\n    let dataset = CaliforniaHousingDataset::load(\"datasets/california_housing.csv\")\n        .expect(\"Failed to load dataset\");\n    let subset = dataset.select_features(\u0026[0, 1, 2, 3]); // MedInc, HouseAge, AveRooms, AveBedrms\n    let (train_dataset, _test_dataset) = subset.split(0.8);\n    let in_memory = train_dataset\n        .to_in_memory_dataset()\n        .expect(\"Failed to create dataset\");\n\n    // Test different batch sizes\n    for batch_size in [16, 32, 64, 128].iter() {\n        c.bench_with_input(\n            BenchmarkId::new(\"train_4_features\", batch_size),\n            batch_size,\n            |b, \u0026bs| {\n                let optimizer = SGD::new(0.001);\n                let trainer = Trainer::builder(MSELoss, optimizer, NoRegularizer)\n                    .batch_size(bs)\n                    .max_epochs(10)\n                    .build();\n\n                b.iter(|| {\n                    let model_copy = LinearRegression::\u003cCpuBackend\u003e::new(4);\n                    let fitted = trainer\n                        .fit(model_copy, \u0026in_memory)\n                        .expect(\"Failed to fit model\");\n\n                    // Verify the model works\n                    let test_x: Vec\u003cf32\u003e = vec![5.0, 30.0, 6.0, 1.0];\n                    let test_tensor = machinelearne_rs::backend::Tensor1D::new(test_x);\n                    let _pred = fitted.predict(black_box(\u0026test_tensor));\n\n                    fitted\n                });\n            },\n        );\n    }\n}\n\nfn bench_train_4_features_learning_rate(c: \u0026mut Criterion) {\n    // Load dataset and select first 4 features\n    let dataset = CaliforniaHousingDataset::load(\"datasets/california_housing.csv\")\n        .expect(\"Failed to load dataset\");\n    let subset = dataset.select_features(\u0026[0, 1, 2, 3]); // MedInc, HouseAge, AveRooms, AveBedrms\n    let (train_dataset, _test_dataset) = subset.split(0.8);\n    let in_memory = train_dataset\n        .to_in_memory_dataset()\n        .expect(\"Failed to create dataset\");\n\n    // Test different learning rates\n    for lr in [0.001, 0.01, 0.1].iter() {\n        c.bench_with_input(\n            BenchmarkId::new(\"train_4_features_lr\", lr),\n            lr,\n            |b, \u0026learning_rate| {\n                let optimizer = SGD::new(learning_rate);\n                let trainer = Trainer::builder(MSELoss, optimizer, NoRegularizer)\n                    .batch_size(32)\n                    .max_epochs(10)\n                    .build();\n\n                b.iter(|| {\n                    let model_copy = LinearRegression::\u003cCpuBackend\u003e::new(4);\n                    let _ = trainer\n                        .fit(model_copy, \u0026in_memory)\n                        .expect(\"Failed to fit model\");\n                });\n            },\n        );\n    }\n}\n\nfn bench_train_4_features_epochs(c: \u0026mut Criterion) {\n    // Load dataset and select first 4 features\n    let dataset = CaliforniaHousingDataset::load(\"datasets/california_housing.csv\")\n        .expect(\"Failed to load dataset\");\n    let subset = dataset.select_features(\u0026[0, 1, 2, 3]); // MedInc, HouseAge, AveRooms, AveBedrms\n    let (train_dataset, _test_dataset) = subset.split(0.8);\n    let in_memory = train_dataset\n        .to_in_memory_dataset()\n        .expect(\"Failed to create dataset\");\n\n    // Test different epoch counts\n    for epochs in [10, 50, 100].iter() {\n        c.bench_with_input(\n            BenchmarkId::new(\"train_4_features_epochs\", epochs),\n            epochs,\n            |b, \u0026max_epochs| {\n                let optimizer = SGD::new(0.01);\n                let trainer = Trainer::builder(MSELoss, optimizer, NoRegularizer)\n                    .batch_size(32)\n                    .max_epochs(max_epochs)\n                    .build();\n\n                b.iter(|| {\n                    let model_copy = LinearRegression::\u003cCpuBackend\u003e::new(4);\n                    let _ = trainer\n                        .fit(model_copy, \u0026in_memory)\n                        .expect(\"Failed to fit model\");\n                });\n            },\n        );\n    }\n}\n\nfn bench_train_4_features_with_metrics(c: \u0026mut Criterion) {\n    // Load dataset and select first 4 features\n    let dataset = CaliforniaHousingDataset::load(\"datasets/california_housing.csv\")\n        .expect(\"Failed to load dataset\");\n    let subset = dataset.select_features(\u0026[0, 1, 2, 3]); // MedInc, HouseAge, AveRooms, AveBedrms\n    let (train_dataset, test_dataset) = subset.split(0.8);\n    let train_memory = train_dataset\n        .to_in_memory_dataset()\n        .expect(\"Failed to create dataset\");\n\n    // Test the full pipeline and measure metrics\n    c.bench_function(\"train_4_features_with_metrics\", |b| {\n        b.iter(|| {\n            let model = LinearRegression::\u003cCpuBackend\u003e::new(4);\n            let optimizer = SGD::new(0.001);\n            let trainer = Trainer::builder(MSELoss, optimizer, NoRegularizer)\n                .batch_size(32)\n                .max_epochs(50)\n                .build();\n\n            let fitted = trainer\n                .fit(model, \u0026train_memory)\n                .expect(\"Failed to fit model\");\n\n            // Evaluate on test set\n            let test_x = test_dataset.features().to_vec();\n            let test_y = test_dataset.target().to_vec();\n            let pred_tensor = fitted.predict_batch(black_box(\u0026vec_to_tensor2d(\u0026test_x)));\n\n            let predictions: Vec\u003cf32\u003e =\n                pred_tensor.to_vec().into_iter().map(|v| v as f32).collect();\n\n            let mse = Metrics::mse(\u0026test_y, \u0026predictions);\n            let mae = Metrics::mae(\u0026test_y, \u0026predictions);\n            let r2 = Metrics::r_squared(\u0026test_y, \u0026predictions);\n\n            black_box((mse, mae, r2));\n        });\n    });\n}\n\ncriterion_group!(\n    benches,\n    bench_train_4_features,\n    bench_train_4_features_learning_rate,\n    bench_train_4_features_epochs,\n    bench_train_4_features_with_metrics\n);\ncriterion_main!(benches);\n","traces":[],"covered":0,"coverable":0},{"path":["/","home","victor","ML","machinelearne-rs","benchmarks","benches","linear_regression","train_8_features.rs"],"content":"use benchmarks::data::CaliforniaHousingDataset;\nuse benchmarks::metrics::Metrics;\nuse criterion::{black_box, criterion_group, criterion_main, BenchmarkId, Criterion};\nuse machinelearne_rs::{\n    backend::Tensor2D,\n    dataset::InMemoryDataset,\n    loss::MSELoss,\n    model::{linear::LinearRegression, InferenceModel},\n    optimizer::SGD,\n    regularizers::NoRegularizer,\n    trainer::Trainer,\n    CpuBackend,\n};\n\n/// Helper function to convert Vec\u003cVec\u003cf32\u003e\u003e to Tensor2D\u003cCpuBackend\u003e\nfn vec_to_tensor2d(data: \u0026[Vec\u003cf32\u003e]) -\u003e Tensor2D\u003cCpuBackend\u003e {\n    let flat: Vec\u003cf32\u003e = data.iter().flatten().copied().collect();\n    Tensor2D::new(flat, data.len(), data.first().map(|v| v.len()).unwrap_or(0))\n}\n\nfn bench_train_8_features(c: \u0026mut Criterion) {\n    // Load dataset with all 8 features\n    let dataset = CaliforniaHousingDataset::load(\"datasets/california_housing.csv\")\n        .expect(\"Failed to load dataset\");\n    // Use all features (0-7)\n    let (train_dataset, _test_dataset) = dataset.split(0.8);\n    let in_memory = train_dataset\n        .to_in_memory_dataset()\n        .expect(\"Failed to create dataset\");\n\n    // Test different batch sizes\n    for batch_size in [16, 32, 64, 128].iter() {\n        c.bench_with_input(\n            BenchmarkId::new(\"train_8_features\", batch_size),\n            batch_size,\n            |b, \u0026bs| {\n                let optimizer = SGD::new(0.001);\n                let trainer = Trainer::builder(MSELoss, optimizer, NoRegularizer)\n                    .batch_size(bs)\n                    .max_epochs(10)\n                    .build();\n\n                b.iter(|| {\n                    let model_copy = LinearRegression::\u003cCpuBackend\u003e::new(8);\n                    let fitted = trainer\n                        .fit(model_copy, \u0026in_memory)\n                        .expect(\"Failed to fit model\");\n\n                    // Verify the model works\n                    let test_x: Vec\u003cf32\u003e = vec![5.0, 30.0, 6.0, 1.0, 500.0, 3.0, 37.8, -122.2];\n                    let test_tensor = machinelearne_rs::backend::Tensor1D::new(test_x);\n                    let _pred = fitted.predict(black_box(\u0026test_tensor));\n\n                    fitted\n                });\n            },\n        );\n    }\n}\n\nfn bench_train_8_features_learning_rate(c: \u0026mut Criterion) {\n    // Load dataset with all 8 features\n    let dataset = CaliforniaHousingDataset::load(\"datasets/california_housing.csv\")\n        .expect(\"Failed to load dataset\");\n    // Use all features (0-7)\n    let (train_dataset, _test_dataset) = dataset.split(0.8);\n    let in_memory = train_dataset\n        .to_in_memory_dataset()\n        .expect(\"Failed to create dataset\");\n\n    // Test different learning rates\n    for lr in [0.001, 0.01, 0.1].iter() {\n        c.bench_with_input(\n            BenchmarkId::new(\"train_8_features_lr\", lr),\n            lr,\n            |b, \u0026learning_rate| {\n                let optimizer = SGD::new(learning_rate);\n                let trainer = Trainer::builder(MSELoss, optimizer, NoRegularizer)\n                    .batch_size(32)\n                    .max_epochs(10)\n                    .build();\n\n                b.iter(|| {\n                    let model_copy = LinearRegression::\u003cCpuBackend\u003e::new(8);\n                    let _ = trainer\n                        .fit(model_copy, \u0026in_memory)\n                        .expect(\"Failed to fit model\");\n                });\n            },\n        );\n    }\n}\n\nfn bench_train_8_features_epochs(c: \u0026mut Criterion) {\n    // Load dataset with all 8 features\n    let dataset = CaliforniaHousingDataset::load(\"datasets/california_housing.csv\")\n        .expect(\"Failed to load dataset\");\n    // Use all features (0-7)\n    let (train_dataset, _test_dataset) = dataset.split(0.8);\n    let in_memory = train_dataset\n        .to_in_memory_dataset()\n        .expect(\"Failed to create dataset\");\n\n    // Test different epoch counts\n    for epochs in [10, 50, 100].iter() {\n        c.bench_with_input(\n            BenchmarkId::new(\"train_8_features_epochs\", epochs),\n            epochs,\n            |b, \u0026max_epochs| {\n                let optimizer = SGD::new(0.01);\n                let trainer = Trainer::builder(MSELoss, optimizer, NoRegularizer)\n                    .batch_size(32)\n                    .max_epochs(max_epochs)\n                    .build();\n\n                b.iter(|| {\n                    let model_copy = LinearRegression::\u003cCpuBackend\u003e::new(8);\n                    let _ = trainer\n                        .fit(model_copy, \u0026in_memory)\n                        .expect(\"Failed to fit model\");\n                });\n            },\n        );\n    }\n}\n\nfn bench_train_8_features_with_metrics(c: \u0026mut Criterion) {\n    // Load dataset with all 8 features\n    let dataset = CaliforniaHousingDataset::load(\"datasets/california_housing.csv\")\n        .expect(\"Failed to load dataset\");\n    // Use all features (0-7)\n    let (train_dataset, test_dataset) = dataset.split(0.8);\n    let train_memory = train_dataset\n        .to_in_memory_dataset()\n        .expect(\"Failed to create dataset\");\n\n    // Test the full pipeline and measure metrics\n    c.bench_function(\"train_8_features_with_metrics\", |b| {\n        b.iter(|| {\n            let model = LinearRegression::\u003cCpuBackend\u003e::new(8);\n            let optimizer = SGD::new(0.001);\n            let trainer = Trainer::builder(MSELoss, optimizer, NoRegularizer)\n                .batch_size(32)\n                .max_epochs(50)\n                .build();\n\n            let fitted = trainer\n                .fit(model, \u0026train_memory)\n                .expect(\"Failed to fit model\");\n\n            // Evaluate on test set\n            let test_x = test_dataset.features().to_vec();\n            let test_y = test_dataset.target().to_vec();\n            let pred_tensor = fitted.predict_batch(black_box(\u0026vec_to_tensor2d(\u0026test_x)));\n\n            let predictions: Vec\u003cf32\u003e =\n                pred_tensor.to_vec().into_iter().map(|v| v as f32).collect();\n\n            let mse = Metrics::mse(\u0026test_y, \u0026predictions);\n            let mae = Metrics::mae(\u0026test_y, \u0026predictions);\n            let r2 = Metrics::r_squared(\u0026test_y, \u0026predictions);\n\n            black_box((mse, mae, r2));\n        });\n    });\n}\n\ncriterion_group!(\n    benches,\n    bench_train_8_features,\n    bench_train_8_features_learning_rate,\n    bench_train_8_features_epochs,\n    bench_train_8_features_with_metrics\n);\ncriterion_main!(benches);\n","traces":[],"covered":0,"coverable":0},{"path":["/","home","victor","ML","machinelearne-rs","benchmarks","benches","metrics.rs"],"content":"use benchmarks::metrics::Metrics;\nuse criterion::{black_box, criterion_group, criterion_main, BenchmarkId, Criterion};\n\nfn bench_mse(c: \u0026mut Criterion) {\n    // Test different array sizes\n    for size in [100, 1000, 10000, 100000].iter() {\n        c.bench_with_input(BenchmarkId::new(\"mse\", size), size, |b, \u0026n| {\n            let y_true: Vec\u003cf32\u003e = (0..n).map(|i| i as f32 * 0.1).collect();\n            let y_pred: Vec\u003cf32\u003e = (0..n).map(|i| i as f32 * 0.1 + 0.5).collect();\n\n            b.iter(|| {\n                let mse = Metrics::mse(black_box(\u0026y_true), black_box(\u0026y_pred));\n                black_box(mse);\n            });\n        });\n    }\n}\n\nfn bench_mae(c: \u0026mut Criterion) {\n    // Test different array sizes\n    for size in [100, 1000, 10000, 100000].iter() {\n        c.bench_with_input(BenchmarkId::new(\"mae\", size), size, |b, \u0026n| {\n            let y_true: Vec\u003cf32\u003e = (0..n).map(|i| i as f32 * 0.1).collect();\n            let y_pred: Vec\u003cf32\u003e = (0..n).map(|i| i as f32 * 0.1 + 0.5).collect();\n\n            b.iter(|| {\n                let mae = Metrics::mae(black_box(\u0026y_true), black_box(\u0026y_pred));\n                black_box(mae);\n            });\n        });\n    }\n}\n\nfn bench_r_squared(c: \u0026mut Criterion) {\n    // Test different array sizes\n    for size in [100, 1000, 10000, 100000].iter() {\n        c.bench_with_input(BenchmarkId::new(\"r_squared\", size), size, |b, \u0026n| {\n            let y_true: Vec\u003cf32\u003e = (0..n).map(|i| i as f32 * 0.1).collect();\n            let y_pred: Vec\u003cf32\u003e = (0..n).map(|i| i as f32 * 0.1 + 0.5).collect();\n\n            b.iter(|| {\n                let r2 = Metrics::r_squared(black_box(\u0026y_true), black_box(\u0026y_pred));\n                black_box(r2);\n            });\n        });\n    }\n}\n\nfn bench_rmse(c: \u0026mut Criterion) {\n    // Test different array sizes\n    for size in [100, 1000, 10000, 100000].iter() {\n        c.bench_with_input(BenchmarkId::new(\"rmse\", size), size, |b, \u0026n| {\n            let y_true: Vec\u003cf32\u003e = (0..n).map(|i| i as f32 * 0.1).collect();\n            let y_pred: Vec\u003cf32\u003e = (0..n).map(|i| i as f32 * 0.1 + 0.5).collect();\n\n            b.iter(|| {\n                let rmse = Metrics::rmse(black_box(\u0026y_true), black_box(\u0026y_pred));\n                black_box(rmse);\n            });\n        });\n    }\n}\n\nfn bench_calculate_all(c: \u0026mut Criterion) {\n    // Test different array sizes\n    for size in [100, 1000, 10000, 100000].iter() {\n        c.bench_with_input(BenchmarkId::new(\"calculate_all\", size), size, |b, \u0026n| {\n            let y_true: Vec\u003cf32\u003e = (0..n).map(|i| i as f32 * 0.1).collect();\n            let y_pred: Vec\u003cf32\u003e = (0..n).map(|i| i as f32 * 0.1 + 0.5).collect();\n\n            b.iter(|| {\n                let metrics = Metrics::calculate_all(black_box(\u0026y_true), black_box(\u0026y_pred));\n                black_box(metrics);\n            });\n        });\n    }\n}\n\nfn bench_mse_perfect_prediction(c: \u0026mut Criterion) {\n    // Benchmark MSE with perfect predictions (should be fast)\n    for size in [100, 1000, 10000].iter() {\n        c.bench_with_input(BenchmarkId::new(\"mse_perfect\", size), size, |b, \u0026n| {\n            let y_true: Vec\u003cf32\u003e = (0..n).map(|i| i as f32 * 0.1).collect();\n            let y_pred = y_true.clone(); // Perfect prediction\n\n            b.iter(|| {\n                let mse = Metrics::mse(black_box(\u0026y_true), black_box(\u0026y_pred));\n                black_box(mse);\n            });\n        });\n    }\n}\n\nfn bench_r_squared_perfect_prediction(c: \u0026mut Criterion) {\n    // Benchmark RÂ² with perfect predictions (should be ~1.0)\n    for size in [100, 1000, 10000].iter() {\n        c.bench_with_input(\n            BenchmarkId::new(\"r_squared_perfect\", size),\n            size,\n            |b, \u0026n| {\n                let y_true: Vec\u003cf32\u003e = (0..n).map(|i| i as f32 * 0.1).collect();\n                let y_pred = y_true.clone(); // Perfect prediction\n\n                b.iter(|| {\n                    let r2 = Metrics::r_squared(black_box(\u0026y_true), black_box(\u0026y_pred));\n                    black_box(r2);\n                });\n            },\n        );\n    }\n}\n\nfn bench_metrics_comparison(c: \u0026mut Criterion) {\n    // Compare all metrics on the same data\n    let sizes = [1000, 10000, 100000];\n\n    for size in sizes.iter() {\n        let y_true: Vec\u003cf32\u003e = (0..*size).map(|i| i as f32 * 0.1).collect();\n        let y_pred: Vec\u003cf32\u003e = (0..*size).map(|i| i as f32 * 0.1 + 0.5).collect();\n\n        let mut group = c.benchmark_group(format!(\"metrics_comparison_{}\", size));\n\n        group.bench_function(\"mse\", |b| {\n            b.iter(|| {\n                let mse = Metrics::mse(black_box(\u0026y_true), black_box(\u0026y_pred));\n                black_box(mse);\n            });\n        });\n\n        group.bench_function(\"mae\", |b| {\n            b.iter(|| {\n                let mae = Metrics::mae(black_box(\u0026y_true), black_box(\u0026y_pred));\n                black_box(mae);\n            });\n        });\n\n        group.bench_function(\"r_squared\", |b| {\n            b.iter(|| {\n                let r2 = Metrics::r_squared(black_box(\u0026y_true), black_box(\u0026y_pred));\n                black_box(r2);\n            });\n        });\n\n        group.bench_function(\"rmse\", |b| {\n            b.iter(|| {\n                let rmse = Metrics::rmse(black_box(\u0026y_true), black_box(\u0026y_pred));\n                black_box(rmse);\n            });\n        });\n\n        group.finish();\n    }\n}\n\nfn bench_metrics_large_arrays(c: \u0026mut Criterion) {\n    // Benchmark with very large arrays\n    c.bench_function(\"mse_large_1M\", |b| {\n        let n = 1_000_000;\n        let y_true: Vec\u003cf32\u003e = (0..n).map(|i| i as f32 * 0.1).collect();\n        let y_pred: Vec\u003cf32\u003e = (0..n).map(|i| i as f32 * 0.1 + 0.5).collect();\n\n        b.iter(|| {\n            let mse = Metrics::mse(black_box(\u0026y_true), black_box(\u0026y_pred));\n            black_box(mse);\n        });\n    });\n\n    c.bench_function(\"mae_large_1M\", |b| {\n        let n = 1_000_000;\n        let y_true: Vec\u003cf32\u003e = (0..n).map(|i| i as f32 * 0.1).collect();\n        let y_pred: Vec\u003cf32\u003e = (0..n).map(|i| i as f32 * 0.1 + 0.5).collect();\n\n        b.iter(|| {\n            let mae = Metrics::mae(black_box(\u0026y_true), black_box(\u0026y_pred));\n            black_box(mae);\n        });\n    });\n\n    c.bench_function(\"r_squared_large_1M\", |b| {\n        let n = 1_000_000;\n        let y_true: Vec\u003cf32\u003e = (0..n).map(|i| i as f32 * 0.1).collect();\n        let y_pred: Vec\u003cf32\u003e = (0..n).map(|i| i as f32 * 0.1 + 0.5).collect();\n\n        b.iter(|| {\n            let r2 = Metrics::r_squared(black_box(\u0026y_true), black_box(\u0026y_pred));\n            black_box(r2);\n        });\n    });\n}\n\ncriterion_group!(\n    benches,\n    bench_mse,\n    bench_mae,\n    bench_r_squared,\n    bench_rmse,\n    bench_calculate_all,\n    bench_mse_perfect_prediction,\n    bench_r_squared_perfect_prediction,\n    bench_metrics_comparison,\n    bench_metrics_large_arrays\n);\ncriterion_main!(benches);\n","traces":[],"covered":0,"coverable":0},{"path":["/","home","victor","ML","machinelearne-rs","benchmarks","src","bin","backend_comparison.rs"],"content":"#!/usr/bin/env rust-script\n//!\n//! Compare CPU backend vs ndarray backend performance\n\nuse benchmarks::data::CaliforniaHousingDataset;\nuse benchmarks::metrics::Metrics;\nuse machinelearne_rs::{\n    backend::{CpuBackend, Tensor2D},\n    dataset::InMemoryDataset,\n    loss::MSELoss,\n    model::{linear::LinearRegression, InferenceModel},\n    optimizer::SGD,\n    regularizers::NoRegularizer,\n    trainer::Trainer,\n};\n\nuse serde_json::json;\nuse std::fs::File;\nuse std::io::Write;\nuse std::time::Instant;\n\n/// Helper function to convert Vec\u003cVec\u003cf32\u003e\u003e to Tensor2D\u003cCpuBackend\u003e\nfn vec_to_tensor2d_cpu(data: \u0026[Vec\u003cf32\u003e]) -\u003e Tensor2D\u003cCpuBackend\u003e {\n    let flat: Vec\u003cf32\u003e = data.iter().flatten().copied().collect();\n    Tensor2D::new(flat, data.len(), data.first().map(|v| v.len()).unwrap_or(0))\n}\n\n#[cfg(feature = \"ndarray\")]\nuse machinelearne_rs::backend::NdarrayBackend;\n\n/// Helper function to convert Vec\u003cVec\u003cf32\u003e\u003e to Tensor2D for ndarray backend\n#[cfg(feature = \"ndarray\")]\nfn vec_to_tensor2d_ndarray(data: \u0026[Vec\u003cf32\u003e]) -\u003e Tensor2D\u003cNdarrayBackend\u003e {\n    let flat: Vec\u003cf32\u003e = data.iter().flatten().copied().collect();\n    Tensor2D::new(flat, data.len(), data.first().map(|v| v.len()).unwrap_or(0))\n}\n\n/// Apply z-score standardization to features\nfn standardize_features(\n    train_features: \u0026[Vec\u003cf32\u003e],\n    test_features: \u0026[Vec\u003cf32\u003e],\n) -\u003e (Vec\u003cVec\u003cf32\u003e\u003e, Vec\u003cVec\u003cf32\u003e\u003e) {\n    let n_samples = train_features.len();\n    let n_features = if n_samples \u003e 0 {\n        train_features[0].len()\n    } else {\n        return (train_features.to_vec(), test_features.to_vec());\n    };\n\n    let mut means = vec![0.0_f32; n_features];\n    let mut stds = vec![0.0_f32; n_features];\n\n    for feature_idx in 0..n_features {\n        let sum: f32 = (0..n_samples).map(|i| train_features[i][feature_idx]).sum();\n        means[feature_idx] = sum / n_samples as f32;\n\n        let variance: f32 = (0..n_samples)\n            .map(|i| {\n                let diff = train_features[i][feature_idx] - means[feature_idx];\n                diff * diff\n            })\n            .sum();\n        stds[feature_idx] = f32::sqrt(variance / n_samples as f32);\n\n        if stds[feature_idx] \u003c 1e-6 {\n            stds[feature_idx] = 1.0;\n        }\n    }\n\n    let train_scaled: Vec\u003cVec\u003cf32\u003e\u003e = (0..n_samples)\n        .map(|i| {\n            (0..n_features)\n                .map(|j| (train_features[i][j] - means[j]) / stds[j])\n                .collect()\n        })\n        .collect();\n\n    let test_scaled: Vec\u003cVec\u003cf32\u003e\u003e = (0..test_features.len())\n        .map(|i| {\n            (0..n_features)\n                .map(|j| (test_features[i][j] - means[j]) / stds[j])\n                .collect()\n        })\n        .collect();\n\n    (train_scaled, test_scaled)\n}\n\n/// Benchmark CPU backend\nfn benchmark_cpu_backend(n_features: usize) -\u003e serde_json::Value {\n    let feature_indices: Vec\u003cusize\u003e = (0..n_features).collect();\n\n    let dataset = CaliforniaHousingDataset::load(\"benchmarks/datasets/california_housing.csv\")\n        .expect(\"Failed to load dataset\");\n    let subset = dataset.select_features(\u0026feature_indices);\n    let (train_dataset, _, test_dataset) = subset.split_train_val_test(0.8, 0.0);\n\n    let train_features = train_dataset.features();\n    let train_target = train_dataset.target();\n    let test_features = test_dataset.features();\n    let test_target = test_dataset.target();\n\n    let (train_features_scaled, test_features_scaled) =\n        standardize_features(train_features, test_features);\n\n    let train_memory = InMemoryDataset::new(train_features_scaled, train_target.to_vec())\n        .expect(\"Failed to create training dataset\");\n\n    let lr = 0.01;\n    let model = LinearRegression::\u003cCpuBackend\u003e::new(n_features);\n    let optimizer = SGD::\u003cCpuBackend\u003e::new(lr);\n\n    let trainer = Trainer::builder(MSELoss, optimizer, NoRegularizer)\n        .batch_size(32)\n        .max_epochs(50)\n        .build();\n\n    let start = Instant::now();\n    let fitted = trainer\n        .fit(model, \u0026train_memory)\n        .expect(\"Failed to fit model\");\n    let train_time_ms = start.elapsed().as_millis();\n\n    let test_tensor = vec_to_tensor2d_cpu(\u0026test_features_scaled);\n    let pred_tensor = fitted.predict_batch(\u0026test_tensor);\n\n    let predictions: Vec\u003cf32\u003e = pred_tensor.to_vec().into_iter().map(|v| v as f32).collect();\n\n    let mse = Metrics::mse(test_target, \u0026predictions);\n    let mae = Metrics::mae(test_target, \u0026predictions);\n    let r2 = Metrics::r_squared(test_target, \u0026predictions);\n\n    json!({\n        \"backend\": \"CpuBackend\",\n        \"n_features\": n_features,\n        \"train_time_ms\": train_time_ms,\n        \"mse\": mse,\n        \"mae\": mae,\n        \"r2\": r2,\n    })\n}\n\n/// Benchmark ndarray backend (only available with ndarray feature)\n#[cfg(feature = \"ndarray\")]\nfn benchmark_ndarray_backend(n_features: usize) -\u003e serde_json::Value {\n    let feature_indices: Vec\u003cusize\u003e = (0..n_features).collect();\n\n    let dataset = CaliforniaHousingDataset::load(\"benchmarks/datasets/california_housing.csv\")\n        .expect(\"Failed to load dataset\");\n    let subset = dataset.select_features(\u0026feature_indices);\n    let (train_dataset, _, test_dataset) = subset.split_train_val_test(0.8, 0.0);\n\n    let train_features = train_dataset.features();\n    let train_target = train_dataset.target();\n    let test_features = test_dataset.features();\n    let test_target = test_dataset.target();\n\n    let (train_features_scaled, test_features_scaled) =\n        standardize_features(train_features, test_features);\n\n    let train_memory = InMemoryDataset::new(train_features_scaled, train_target.to_vec())\n        .expect(\"Failed to create training dataset\");\n\n    let lr = 0.01;\n    let model = LinearRegression::\u003cNdarrayBackend\u003e::new(n_features);\n    let optimizer = SGD::\u003cNdarrayBackend\u003e::new(lr);\n\n    let trainer = Trainer::builder(MSELoss, optimizer, NoRegularizer)\n        .batch_size(32)\n        .max_epochs(50)\n        .build();\n\n    let start = Instant::now();\n    let fitted = trainer\n        .fit(model, \u0026train_memory)\n        .expect(\"Failed to fit model\");\n    let train_time_ms = start.elapsed().as_millis();\n\n    let test_tensor = vec_to_tensor2d_ndarray(\u0026test_features_scaled);\n    let pred_tensor = fitted.predict_batch(\u0026test_tensor);\n\n    let predictions: Vec\u003cf32\u003e = pred_tensor.to_vec().into_iter().map(|v| v as f32).collect();\n\n    let mse = Metrics::mse(test_target, \u0026predictions);\n    let mae = Metrics::mae(test_target, \u0026predictions);\n    let r2 = Metrics::r_squared(test_target, \u0026predictions);\n\n    json!({\n        \"backend\": \"NdarrayBackend\",\n        \"n_features\": n_features,\n        \"train_time_ms\": train_time_ms,\n        \"mse\": mse,\n        \"mae\": mae,\n        \"r2\": r2,\n    })\n}\n\n/// Placeholder for when ndarray feature is not enabled\n#[cfg(not(feature = \"ndarray\"))]\nfn benchmark_ndarray_backend(n_features: usize) -\u003e serde_json::Value {\n    json!({\n        \"backend\": \"NdarrayBackend\",\n        \"n_features\": n_features,\n        \"train_time_ms\": null,\n        \"mse\": null,\n        \"mae\": null,\n        \"r2\": null,\n        \"disabled_reason\": \"ndarray feature not enabled\"\n    })\n}\n\nfn main() {\n    println!(\"Running backend comparison benchmarks...\\n\");\n\n    let mut results = Vec::new();\n\n    // Test with different feature counts\n    for n_features in [1, 2, 4, 8] {\n        println!(\"Testing {} features...\", n_features);\n\n        // CPU Backend\n        println!(\"  CpuBackend...\");\n        results.push(benchmark_cpu_backend(n_features));\n\n        // Ndarray Backend (if available)\n        println!(\"  NdarrayBackend...\");\n        results.push(benchmark_ndarray_backend(n_features));\n    }\n\n    let output = json!({\n        \"results\": results\n    });\n\n    // Write to file\n    let mut file = File::create(\"benchmarks/results/backend_comparison.json\")\n        .expect(\"Failed to create output file\");\n    file.write_all(serde_json::to_string_pretty(\u0026output).unwrap().as_bytes())\n        .expect(\"Failed to write to file\");\n\n    println!(\"\\nResults saved to benchmarks/results/backend_comparison.json\");\n\n    // Print comparison table\n    println!(\"\\nBackend Comparison (50 epochs, lr=0.01, bs=32):\");\n    println!(\n        \"{:\u003c15} {:\u003c12} {:\u003e12} {:\u003e10} {:\u003e10} {:\u003e10}\",\n        \"Backend\", \"Features\", \"Time (ms)\", \"MSE\", \"MAE\", \"RÂ²\"\n    );\n    println!(\"{}\", \"-\".repeat(70));\n\n    #[cfg(feature = \"ndarray\")]\n    for n_features in [1, 2, 4, 8] {\n        let cpu_result = results\n            .iter()\n            .find(|r| r[\"backend\"] == \"CpuBackend\" \u0026\u0026 r[\"n_features\"] == n_features)\n            .unwrap();\n\n        let ndarray_result = results\n            .iter()\n            .find(|r| r[\"backend\"] == \"NdarrayBackend\" \u0026\u0026 r[\"n_features\"] == n_features)\n            .unwrap();\n\n        let cpu_time = cpu_result[\"train_time_ms\"].as_f64().unwrap();\n        let ndarray_time = ndarray_result[\"train_time_ms\"].as_f64().unwrap();\n        let speedup = if ndarray_time \u003e 0.0 {\n            cpu_time / ndarray_time\n        } else {\n            0.0\n        };\n\n        println!(\n            \"{:\u003c15} {:\u003c12} {:\u003e12.2} {:\u003e10.4} {:\u003e10.4} {:\u003e10.4}\",\n            \"CpuBackend\",\n            n_features,\n            cpu_time,\n            cpu_result[\"mse\"].as_f64().unwrap(),\n            cpu_result[\"mae\"].as_f64().unwrap(),\n            cpu_result[\"r2\"].as_f64().unwrap(),\n        );\n\n        if ndarray_time \u003e 0.0 {\n            println!(\n                \"{:\u003c15} {:\u003c12} {:\u003e12.2} {:\u003e10.4} {:\u003e10.4} {:\u003e10.4} [{:\u003e6.1}x]\",\n                \"NdarrayBackend\",\n                n_features,\n                ndarray_time,\n                ndarray_result[\"mse\"].as_f64().unwrap(),\n                ndarray_result[\"mae\"].as_f64().unwrap(),\n                ndarray_result[\"r2\"].as_f64().unwrap(),\n                speedup,\n            );\n        } else {\n            println!(\n                \"{:\u003c15} {:\u003c12} {:\u003e12.2} {:\u003e10.4} {:\u003e10.4} {:\u003e10.4} [N/A]\",\n                \"NdarrayBackend\",\n                n_features,\n                ndarray_time,\n                ndarray_result[\"mse\"].as_f64().unwrap(),\n                ndarray_result[\"mae\"].as_f64().unwrap(),\n                ndarray_result[\"r2\"].as_f64().unwrap(),\n            );\n        }\n\n        println!();\n    }\n\n    #[cfg(not(feature = \"ndarray\"))]\n    {\n        println!(\"\\nNote: Run with --features ndarray to benchmark ndarray backend\");\n    }\n\n    // Print sklearn comparison\n    println!(\"Sklearn SGDRegressor (for reference, 1000 iterations):\");\n    println!(\n        \"{:\u003c30} {:\u003e12} {:\u003e10} {:\u003e10} {:\u003e10}\",\n        \"Config\", \"Features\", \"Time (ms)\", \"MSE\", \"RÂ²\"\n    );\n    println!(\"{}\", \"-\".repeat(75));\n    println!(\n        \"{:\u003c30} {:\u003c12} {:\u003e12.2} {:\u003e10.4} {:\u003e10.4}\",\n        \"lr_constant\", 2, 5.09, 0.6697, 0.4889\n    );\n    println!(\n        \"{:\u003c30} {:\u003c12} {:\u003e12.2} {:\u003e10.4} {:\u003e10.4}\",\n        \"lr_adaptive\", 2, 25.38, 0.6630, 0.4941\n    );\n}\n","traces":[],"covered":0,"coverable":0},{"path":["/","home","victor","ML","machinelearne-rs","benchmarks","src","bin","collect_metrics.rs"],"content":"#!/usr/bin/env rust-script\n//!\n//! Collect metrics from machinelearne-rs benchmarks\n//!\n//! This runs each training configuration once and records:\n//! - Training time\n//! - MSE\n//! - MAE\n//! - RÂ²\n\nuse benchmarks::data::CaliforniaHousingDataset;\nuse benchmarks::metrics::Metrics;\nuse machinelearne_rs::{\n    backend::Tensor2D,\n    dataset::InMemoryDataset,\n    loss::MSELoss,\n    model::{linear::LinearRegression, InferenceModel},\n    optimizer::SGD,\n    regularizers::NoRegularizer,\n    trainer::Trainer,\n    CpuBackend,\n};\nuse serde_json::json;\nuse std::fs::File;\nuse std::io::Write;\nuse std::time::Instant;\n\n/// Simple black_box function to prevent optimization\n#[inline(never)]\nfn black_box\u003cT\u003e(dummy: T) -\u003e T {\n    std::hint::black_box(dummy)\n}\n\n/// Helper function to convert Vec\u003cVec\u003cf32\u003e\u003e to Tensor2D\u003cCpuBackend\u003e\nfn vec_to_tensor2d(data: \u0026[Vec\u003cf32\u003e]) -\u003e Tensor2D\u003cCpuBackend\u003e {\n    let flat: Vec\u003cf32\u003e = data.iter().flatten().copied().collect();\n    Tensor2D::new(flat, data.len(), data.first().map(|v| v.len()).unwrap_or(0))\n}\n\n/// Apply z-score standardization to features\n///\n/// This computes mean and std from training data, then applies:\n/// z-score = (x - mean) / std\n/// to both train and test data using the same statistics.\nfn standardize_features(\n    train_features: \u0026[Vec\u003cf32\u003e],\n    test_features: \u0026[Vec\u003cf32\u003e],\n) -\u003e (Vec\u003cVec\u003cf32\u003e\u003e, Vec\u003cVec\u003cf32\u003e\u003e) {\n    let n_samples = train_features.len();\n    let n_features = if n_samples \u003e 0 {\n        train_features[0].len()\n    } else {\n        return (train_features.to_vec(), test_features.to_vec());\n    };\n\n    // Compute mean and std for each feature from training data\n    let mut means = vec![0.0_f32; n_features];\n    let mut stds = vec![0.0_f32; n_features];\n\n    for feature_idx in 0..n_features {\n        // Compute mean\n        let sum: f32 = (0..n_samples).map(|i| train_features[i][feature_idx]).sum();\n        means[feature_idx] = sum / n_samples as f32;\n\n        // Compute variance\n        let variance: f32 = (0..n_samples)\n            .map(|i| {\n                let diff = train_features[i][feature_idx] - means[feature_idx];\n                diff * diff\n            })\n            .sum();\n        stds[feature_idx] = f32::sqrt(variance / n_samples as f32);\n\n        // Prevent division by zero\n        if stds[feature_idx] \u003c 1e-6 {\n            stds[feature_idx] = 1.0;\n        }\n    }\n\n    // Apply z-score to training data\n    let train_scaled: Vec\u003cVec\u003cf32\u003e\u003e = (0..n_samples)\n        .map(|i| {\n            (0..n_features)\n                .map(|j| (train_features[i][j] - means[j]) / stds[j])\n                .collect()\n        })\n        .collect();\n\n    // Apply z-score to test data (using train's mean/std)\n    let test_scaled: Vec\u003cVec\u003cf32\u003e\u003e = (0..test_features.len())\n        .map(|i| {\n            (0..n_features)\n                .map(|j| (test_features[i][j] - means[j]) / stds[j])\n                .collect()\n        })\n        .collect();\n\n    (train_scaled, test_scaled)\n}\n\n/// Collect metrics for a given feature count\nfn collect_metrics(n_features: usize) -\u003e serde_json::Value {\n    let feature_indices: Vec\u003cusize\u003e = (0..n_features).collect();\n\n    let dataset = CaliforniaHousingDataset::load(\"benchmarks/datasets/california_housing.csv\")\n        .expect(\"Failed to load dataset\");\n    let subset = dataset.select_features(\u0026feature_indices);\n\n    // Split into train and test (function returns train, val, test)\n    let (train_dataset, _, test_dataset) = subset.split_train_val_test(0.8, 0.0);\n\n    // Get raw features and targets\n    let train_features = train_dataset.features();\n    let train_target = train_dataset.target();\n    let test_features = test_dataset.features();\n    let test_target = test_dataset.target();\n\n    // Apply z-score standardization\n    let (train_features_scaled, test_features_scaled) =\n        standardize_features(train_features, test_features);\n\n    // Create in-memory datasets with scaled features\n    let train_memory = InMemoryDataset::new(train_features_scaled, train_target.to_vec())\n        .expect(\"Failed to create training dataset\");\n\n    // Use learning rate appropriate for standardized features\n    let lr = 0.01;\n    let model = LinearRegression::\u003cCpuBackend\u003e::new(n_features);\n    let optimizer = SGD::\u003cCpuBackend\u003e::new(lr);\n\n    let trainer = Trainer::builder(MSELoss, optimizer, NoRegularizer)\n        .batch_size(32)\n        .max_epochs(50)\n        .build();\n\n    // Time the training\n    let start = Instant::now();\n    let fitted = trainer\n        .fit(model, \u0026train_memory)\n        .expect(\"Failed to fit model\");\n    let train_time_ms = start.elapsed().as_millis();\n\n    // Predict on test set\n    let test_tensor = vec_to_tensor2d(\u0026test_features_scaled);\n    let pred_tensor = fitted.predict_batch(black_box(\u0026test_tensor));\n\n    let predictions: Vec\u003cf32\u003e = pred_tensor.to_vec().into_iter().map(|v| v as f32).collect();\n\n    let mse = Metrics::mse(test_target, \u0026predictions);\n    let mae = Metrics::mae(test_target, \u0026predictions);\n    let r2 = Metrics::r_squared(test_target, \u0026predictions);\n\n    json!({\n        \"n_features\": n_features,\n        \"model\": \"LinearRegression\",\n        \"train_time_ms\": train_time_ms,\n        \"mse\": mse,\n        \"mae\": mae,\n        \"r2\": r2,\n    })\n}\n\nfn main() {\n    let mut results = Vec::new();\n\n    // Collect metrics for different feature counts\n    for n_features in [1, 2, 4, 8] {\n        println!(\"Collecting metrics for {} features...\", n_features);\n        results.push(collect_metrics(n_features));\n    }\n\n    let output = json!({\n        \"results\": results\n    });\n\n    // Write to file\n    let mut file =\n        File::create(\"benchmarks/results/rust_metrics.json\").expect(\"Failed to create output file\");\n    file.write_all(serde_json::to_string_pretty(\u0026output).unwrap().as_bytes())\n        .expect(\"Failed to write to file\");\n\n    println!(\"\\nMetrics collected and saved to benchmarks/results/rust_metrics.json\");\n}\n","traces":[{"line":30,"address":[],"length":0,"stats":{"Line":0}},{"line":31,"address":[],"length":0,"stats":{"Line":0}}],"covered":0,"coverable":2},{"path":["/","home","victor","ML","machinelearne-rs","benchmarks","src","bin","fair_comparison.rs"],"content":"#!/usr/bin/env rust-script\n//!\n//! Fair comparison benchmark for machinelearne-rs vs sklearn.\n//!\n//! This benchmark runs with EXACT configurations to match sklearn:\n//! - Same number of epochs (5, 50, etc.)\n//! - Same batch sizes (full batch = all training samples)\n//! - Same learning rate (0.01)\n//! - Same data preprocessing (z-score standardization)\n//!\n//! Key principle: Compare identical workloads, not different defaults.\n\nuse machinelearne_rs::{\n    backend::{CpuBackend, Tensor1D},\n    dataset::InMemoryDataset,\n    loss::MSELoss,\n    model::{linear::LinearRegression, InferenceModel},\n    optimizer::SGD,\n    regularizers::NoRegularizer,\n    trainer::Trainer,\n};\n\nuse serde_json::json;\nuse std::fs::File;\nuse std::io::Write;\nuse std::time::{Duration, Instant};\n\ntype Features = Vec\u003cVec\u003cf32\u003e\u003e;\ntype Targets = Vec\u003cf32\u003e;\n\n// ============================================================================\n// Data Loading and Preprocessing (matches sklearn exactly)\n// ============================================================================\n\nfn load_california_housing() -\u003e (Vec\u003cVec\u003cf32\u003e\u003e, Vec\u003cf32\u003e) {\n    let data = std::fs::read_to_string(\"benchmarks/datasets/california_housing.csv\")\n        .expect(\"Failed to read California Housing dataset\");\n\n    let mut features = Vec::new();\n    let mut targets = Vec::new();\n\n    for (i, line) in data.lines().enumerate() {\n        if i == 0 {\n            continue; // Skip header\n        }\n\n        let values: Vec\u003cf32\u003e = line\n            .split(',')\n            .map(|s| s.trim().parse().expect(\"Failed to parse number\"))\n            .collect();\n\n        if values.len() == 9 {\n            features.push(values[..8].to_vec());\n            targets.push(values[8]);\n        }\n    }\n\n    (features, targets)\n}\n\nfn split_train_test(\n    features: \u0026[Vec\u003cf32\u003e],\n    targets: \u0026[f32],\n) -\u003e (Features, Features, Targets, Targets) {\n    let n = features.len();\n    let train_size = (n as f64 * 0.8) as usize;\n\n    let train_features = features[..train_size].to_vec();\n    let train_targets = targets[..train_size].to_vec();\n    let test_features = features[train_size..].to_vec();\n    let test_targets = targets[train_size..].to_vec();\n\n    (train_features, test_features, train_targets, test_targets)\n}\n\nfn standardize_features(\n    train_features: \u0026[Vec\u003cf32\u003e],\n    test_features: \u0026[Vec\u003cf32\u003e],\n) -\u003e (Vec\u003cVec\u003cf32\u003e\u003e, Vec\u003cVec\u003cf32\u003e\u003e) {\n    let n_samples = train_features.len();\n    let n_features = train_features[0].len();\n\n    // Compute mean and std from training data (same as sklearn StandardScaler)\n    let mut means = vec![0.0_f32; n_features];\n    let mut stds = vec![0.0_f32; n_features];\n\n    for feature_idx in 0..n_features {\n        let sum: f32 = (0..n_samples).map(|i| train_features[i][feature_idx]).sum();\n        means[feature_idx] = sum / n_samples as f32;\n\n        // sklearn uses biased variance (divide by n, not n-1)\n        let variance: f32 = (0..n_samples)\n            .map(|i| {\n                let diff = train_features[i][feature_idx] - means[feature_idx];\n                diff * diff\n            })\n            .sum();\n        let std = (variance / n_samples as f32).sqrt();\n        // sklearn uses with_std=True which prevents division by zero\n        stds[feature_idx] = if std \u003e 1e-8 { std } else { 1.0 };\n    }\n\n    // Apply z-score to train and test data using same statistics\n    let train_scaled: Vec\u003cVec\u003cf32\u003e\u003e = (0..n_samples)\n        .map(|i| {\n            (0..n_features)\n                .map(|j| (train_features[i][j] - means[j]) / stds[j])\n                .collect()\n        })\n        .collect();\n\n    let test_scaled: Vec\u003cVec\u003cf32\u003e\u003e = test_features\n        .iter()\n        .map(|row| {\n            (0..n_features)\n                .map(|j| (row[j] - means[j]) / stds[j])\n                .collect()\n        })\n        .collect();\n\n    (train_scaled, test_scaled)\n}\n\n// ============================================================================\n// Metrics (matches sklearn exactly)\n// ============================================================================\n\nfn compute_metrics(predictions: \u0026[f32], targets: \u0026[f32]) -\u003e (f64, f64, f64) {\n    let n = targets.len();\n\n    // MSE\n    let mse: f64 = predictions\n        .iter()\n        .zip(targets.iter())\n        .map(|(p, t)| (*p as f64 - *t as f64).powi(2))\n        .sum::\u003cf64\u003e()\n        / n as f64;\n\n    // MAE\n    let mae: f64 = predictions\n        .iter()\n        .zip(targets.iter())\n        .map(|(p, t)| (*p as f64 - *t as f64).abs())\n        .sum::\u003cf64\u003e()\n        / n as f64;\n\n    // RÂ²\n    let target_mean: f64 = targets.iter().map(|t| *t as f64).sum::\u003cf64\u003e() / n as f64;\n    let ss_res: f64 = predictions\n        .iter()\n        .zip(targets.iter())\n        .map(|(p, t)| (*p as f64 - *t as f64).powi(2))\n        .sum();\n    let ss_tot: f64 = targets\n        .iter()\n        .map(|t| (*t as f64 - target_mean).powi(2))\n        .sum();\n    let r_squared = 1.0 - ss_res / ss_tot;\n\n    (mse, mae, r_squared)\n}\n\n// ============================================================================\n// Benchmark Functions\n// ============================================================================\n\nstruct BenchmarkConfig {\n    name: String,\n    n_features: usize,\n    n_epochs: usize,\n    batch_size: usize,\n    learning_rate: f64,\n    n_runs: usize,\n}\n\nstruct BenchmarkResult {\n    config: BenchmarkConfig,\n    train_time_mean_ms: f64,\n    train_time_std_ms: f64,\n    train_time_min_ms: f64,\n    time_per_epoch_ms: f64,\n    time_per_sample_us: f64,\n    mse: f64,\n    mae: f64,\n    r2: f64,\n    weights: Vec\u003cf32\u003e,\n    bias: f32,\n    samples_processed: usize,\n}\n\nfn run_single_benchmark(\n    config: \u0026BenchmarkConfig,\n    train_features: \u0026[Vec\u003cf32\u003e],\n    train_targets: \u0026[f32],\n    test_features: \u0026[Vec\u003cf32\u003e],\n    test_targets: \u0026[f32],\n) -\u003e BenchmarkResult {\n    let n_train = train_features.len();\n    let samples_per_epoch = if config.batch_size \u003e= n_train {\n        n_train\n    } else {\n        (n_train / config.batch_size + 1) * config.batch_size\n    };\n    let samples_processed = config.n_epochs * samples_per_epoch;\n\n    let mut times: Vec\u003cDuration\u003e = Vec::with_capacity(config.n_runs);\n    let mut final_weights: Vec\u003cf32\u003e = Vec::new();\n    let mut final_bias: f32 = 0.0;\n    let mut final_mse = 0.0;\n    let mut final_mae = 0.0;\n    let mut final_r2 = 0.0;\n\n    for _ in 0..config.n_runs {\n        // Create dataset\n        let train_dataset = InMemoryDataset::new(train_features.to_vec(), train_targets.to_vec())\n            .expect(\"Failed to create dataset\");\n\n        // Create model and trainer\n        let model = LinearRegression::\u003cCpuBackend\u003e::new(config.n_features);\n        let optimizer = SGD::new(config.learning_rate);\n\n        let trainer = Trainer::builder(MSELoss, optimizer, NoRegularizer)\n            .batch_size(config.batch_size)\n            .max_epochs(config.n_epochs)\n            .verbose(false) // Quiet mode for clean benchmark output\n            .build();\n\n        // Train\n        let start = Instant::now();\n        let fitted = trainer.fit(model, \u0026train_dataset).expect(\"Training failed\");\n        let elapsed = start.elapsed();\n        times.push(elapsed);\n\n        // Get predictions\n        let predictions: Vec\u003cf32\u003e = test_features\n            .iter()\n            .map(|sample| {\n                let tensor_input = Tensor1D::\u003cCpuBackend\u003e::new(sample.clone());\n                fitted.predict(\u0026tensor_input).to_f64() as f32\n            })\n            .collect();\n\n        // Compute metrics\n        let (mse, mae, r2) = compute_metrics(\u0026predictions, test_targets);\n\n        // Store final results\n        let params = fitted.extract_params();\n        final_weights = params.weights.clone();\n        final_bias = params.bias;\n        final_mse = mse;\n        final_mae = mae;\n        final_r2 = r2;\n    }\n\n    // Calculate statistics\n    let times_ms: Vec\u003cf64\u003e = times.iter().map(|t| t.as_secs_f64() * 1000.0).collect();\n    let mean_time = times_ms.iter().sum::\u003cf64\u003e() / times_ms.len() as f64;\n    let std_time = if times_ms.len() \u003e 1 {\n        let variance = times_ms\n            .iter()\n            .map(|t| (t - mean_time).powi(2))\n            .sum::\u003cf64\u003e()\n            / (times_ms.len() - 1) as f64;\n        variance.sqrt()\n    } else {\n        0.0\n    };\n    let min_time = times_ms.iter().cloned().fold(f64::INFINITY, f64::min);\n\n    BenchmarkResult {\n        config: BenchmarkConfig {\n            name: config.name.clone(),\n            ..config.clone()\n        },\n        train_time_mean_ms: mean_time,\n        train_time_std_ms: std_time,\n        train_time_min_ms: min_time,\n        time_per_epoch_ms: mean_time / config.n_epochs as f64,\n        time_per_sample_us: mean_time * 1e6 / samples_processed as f64,\n        mse: final_mse,\n        mae: final_mae,\n        r2: final_r2,\n        weights: final_weights,\n        bias: final_bias,\n        samples_processed,\n    }\n}\n\nimpl Clone for BenchmarkConfig {\n    fn clone(\u0026self) -\u003e Self {\n        Self {\n            name: self.name.clone(),\n            n_features: self.n_features,\n            n_epochs: self.n_epochs,\n            batch_size: self.batch_size,\n            learning_rate: self.learning_rate,\n            n_runs: self.n_runs,\n        }\n    }\n}\n\nfn run_all_benchmarks() -\u003e Vec\u003cserde_json::Value\u003e {\n    println!(\"{}\", \"=\".repeat(70));\n    println!(\"MACHINELEARN-RS FAIR COMPARISON BENCHMARKS\");\n    println!(\"{}\", \"=\".repeat(70));\n\n    // Load and preprocess data\n    let (features, targets) = load_california_housing();\n    println!(\n        \"\\nLoaded {} samples with {} features\",\n        features.len(),\n        features[0].len()\n    );\n\n    let (train_features_all, test_features_all, train_targets, test_targets) =\n        split_train_test(\u0026features, \u0026targets);\n\n    let (train_scaled_all, test_scaled_all) =\n        standardize_features(\u0026train_features_all, \u0026test_features_all);\n\n    println!(\n        \"Train: {} samples, Test: {} samples\",\n        train_scaled_all.len(),\n        test_scaled_all.len()\n    );\n\n    let n_train = train_scaled_all.len();\n    let mut results = Vec::new();\n\n    // Define benchmark configurations\n    // Test different combinations of batch size and learning rate\n    let configs = vec![\n        // 2 features: Full batch with optimal LR=0.5\n        BenchmarkConfig {\n            name: \"2feat_full_lr0.5_e5\".to_string(),\n            n_features: 2,\n            n_epochs: 5,\n            batch_size: n_train,\n            learning_rate: 0.5,\n            n_runs: 10,\n        },\n        BenchmarkConfig {\n            name: \"2feat_full_lr0.5_e50\".to_string(),\n            n_features: 2,\n            n_epochs: 50,\n            batch_size: n_train,\n            learning_rate: 0.5,\n            n_runs: 5,\n        },\n        // 2 features: Full batch with sklearn-compatible LR=0.1\n        BenchmarkConfig {\n            name: \"2feat_full_lr0.1_e5\".to_string(),\n            n_features: 2,\n            n_epochs: 5,\n            batch_size: n_train,\n            learning_rate: 0.1,\n            n_runs: 10,\n        },\n        BenchmarkConfig {\n            name: \"2feat_full_lr0.1_e50\".to_string(),\n            n_features: 2,\n            n_epochs: 50,\n            batch_size: n_train,\n            learning_rate: 0.1,\n            n_runs: 5,\n        },\n        // 2 features: Mini-batch with default LR=0.01\n        BenchmarkConfig {\n            name: \"2feat_minibatch_lr0.01_e5\".to_string(),\n            n_features: 2,\n            n_epochs: 5,\n            batch_size: 32,\n            learning_rate: 0.01,\n            n_runs: 10,\n        },\n        BenchmarkConfig {\n            name: \"2feat_minibatch_lr0.01_e50\".to_string(),\n            n_features: 2,\n            n_epochs: 50,\n            batch_size: 32,\n            learning_rate: 0.01,\n            n_runs: 5,\n        },\n        // 8 features: Full batch\n        BenchmarkConfig {\n            name: \"8feat_full_lr0.1_e5\".to_string(),\n            n_features: 8,\n            n_epochs: 5,\n            batch_size: n_train,\n            learning_rate: 0.1,\n            n_runs: 10,\n        },\n        BenchmarkConfig {\n            name: \"8feat_full_lr0.1_e50\".to_string(),\n            n_features: 8,\n            n_epochs: 50,\n            batch_size: n_train,\n            learning_rate: 0.1,\n            n_runs: 5,\n        },\n    ];\n\n    for config in configs {\n        println!(\"\\n{}\", \"-\".repeat(50));\n        println!(\n            \"Test: {} ({} features, {} epochs, batch={}, lr={})\",\n            config.name,\n            config.n_features,\n            config.n_epochs,\n            config.batch_size,\n            config.learning_rate\n        );\n\n        // Select features\n        let n_feat = config.n_features;\n        let train_features: Vec\u003cVec\u003cf32\u003e\u003e = train_scaled_all\n            .iter()\n            .map(|row| row[..n_feat].to_vec())\n            .collect();\n        let test_features: Vec\u003cVec\u003cf32\u003e\u003e = test_scaled_all\n            .iter()\n            .map(|row| row[..n_feat].to_vec())\n            .collect();\n\n        let result = run_single_benchmark(\n            \u0026config,\n            \u0026train_features,\n            \u0026train_targets,\n            \u0026test_features,\n            \u0026test_targets,\n        );\n\n        println!(\n            \"  Time: {:.3} +/- {:.3} ms\",\n            result.train_time_mean_ms, result.train_time_std_ms\n        );\n        println!(\"  Time per epoch: {:.3} ms\", result.time_per_epoch_ms);\n        println!(\"  Time per sample: {:.4} us\", result.time_per_sample_us);\n        println!(\"  MSE: {:.6}, R2: {:.6}\", result.mse, result.r2);\n        println!(\"  Weights: {:?}, Bias: {:.6}\", result.weights, result.bias);\n\n        results.push(json!({\n            \"test\": result.config.name,\n            \"implementation\": \"rust\",\n            \"n_features\": result.config.n_features,\n            \"n_epochs\": result.config.n_epochs,\n            \"batch_size\": result.config.batch_size,\n            \"learning_rate\": result.config.learning_rate,\n            \"n_train_samples\": n_train,\n            \"n_test_samples\": test_features.len(),\n            \"samples_processed\": result.samples_processed,\n            \"train_time_mean_ms\": result.train_time_mean_ms,\n            \"train_time_std_ms\": result.train_time_std_ms,\n            \"train_time_min_ms\": result.train_time_min_ms,\n            \"time_per_epoch_ms\": result.time_per_epoch_ms,\n            \"time_per_sample_us\": result.time_per_sample_us,\n            \"mse\": result.mse,\n            \"mae\": result.mae,\n            \"r2\": result.r2,\n            \"weights\": result.weights,\n            \"bias\": result.bias,\n            \"n_runs\": result.config.n_runs,\n        }));\n    }\n\n    results\n}\n\nfn main() {\n    let results = run_all_benchmarks();\n\n    // Save results\n    let output = json!({\n        \"results\": results\n    });\n\n    let mut file = File::create(\"benchmarks/results/rust_fair_comparison.json\")\n        .expect(\"Failed to create output file\");\n    file.write_all(serde_json::to_string_pretty(\u0026output).unwrap().as_bytes())\n        .expect(\"Failed to write to file\");\n\n    println!(\"\\nResults saved to benchmarks/results/rust_fair_comparison.json\");\n\n    // Print summary\n    println!(\"\\n{}\", \"=\".repeat(70));\n    println!(\"BENCHMARK COMPLETE\");\n    println!(\"{}\", \"=\".repeat(70));\n\n    println!(\"\\nLegend:\");\n    println!(\"  full = full batch (all training samples)\");\n    println!(\"  minibatch = batch size 32\");\n    println!(\"  lr = learning rate\");\n    println!(\"  e = epochs\");\n\n    println!(\"\\nResults:\");\n    println!(\n        \"{:\u003c30} {:\u003e10} {:\u003e10} {:\u003e10} {:\u003e8}\",\n        \"Test\", \"Time (ms)\", \"ms/epoch\", \"MSE\", \"R2\"\n    );\n    println!(\"{}\", \"-\".repeat(70));\n\n    for r in \u0026results {\n        let test = r[\"test\"].as_str().unwrap();\n        let time_ms = r[\"train_time_mean_ms\"].as_f64().unwrap();\n        let time_per_epoch = r[\"time_per_epoch_ms\"].as_f64().unwrap();\n        let mse = r[\"mse\"].as_f64().unwrap();\n        let r2 = r[\"r2\"].as_f64().unwrap();\n\n        println!(\n            \"{:\u003c30} {:\u003e10.2} {:\u003e10.3} {:\u003e10.4} {:\u003e8.4}\",\n            test, time_ms, time_per_epoch, mse, r2\n        );\n    }\n}\n","traces":[],"covered":0,"coverable":0},{"path":["/","home","victor","ML","machinelearne-rs","benchmarks","src","bin","full_batch_comparison.rs"],"content":"use machinelearne_rs::{\n    dataset::InMemoryDataset, loss::MSELoss, model::linear::LinearRegression,\n    model::InferenceModel, optimizer::SGD, regularizers::NoRegularizer, trainer::TrainerBuilder,\n    Tensor1D,\n};\n\nuse std::time::Instant;\n\nuse machinelearne_rs::backend::CpuBackend;\n\ntype Features = Vec\u003cVec\u003cf32\u003e\u003e;\ntype Targets = Vec\u003cf32\u003e;\n\nfn load_california_housing() -\u003e (Vec\u003cVec\u003cf32\u003e\u003e, Vec\u003cf32\u003e) {\n    // Load CSV and parse\n    let data = std::fs::read_to_string(\"benchmarks/datasets/california_housing.csv\")\n        .expect(\"Failed to read California Housing dataset\");\n\n    let mut features = Vec::new();\n    let mut targets = Vec::new();\n\n    for (i, line) in data.lines().enumerate() {\n        if i == 0 {\n            continue; // Skip header\n        }\n\n        let values: Vec\u003cf32\u003e = line\n            .split(',')\n            .map(|s| s.trim().parse().expect(\"Failed to parse number\"))\n            .collect();\n\n        if values.len() == 9 {\n            features.push(values[..8].to_vec());\n            targets.push(values[8]);\n        }\n    }\n\n    (features, targets)\n}\n\nfn split_train_test(\n    features: \u0026[Vec\u003cf32\u003e],\n    targets: \u0026[f32],\n) -\u003e (Features, Features, Targets, Targets) {\n    let n = features.len();\n    let train_size = (n as f64 * 0.8) as usize;\n\n    let train_features = features[..train_size].to_vec();\n    let train_targets = targets[..train_size].to_vec();\n    let test_features = features[train_size..].to_vec();\n    let test_targets = targets[train_size..].to_vec();\n\n    (train_features, test_features, train_targets, test_targets)\n}\n\nfn standardize_features(\n    train_features: \u0026[Vec\u003cf32\u003e],\n    test_features: \u0026[Vec\u003cf32\u003e],\n) -\u003e (Vec\u003cVec\u003cf32\u003e\u003e, Vec\u003cVec\u003cf32\u003e\u003e) {\n    let n_samples = train_features.len();\n    let n_features = train_features[0].len();\n\n    // Compute mean and std from training data\n    let mut means = vec![0.0_f32; n_features];\n    let mut stds = vec![0.0_f32; n_features];\n\n    for feature_idx in 0..n_features {\n        let sum: f32 = (0..n_samples).map(|i| train_features[i][feature_idx]).sum();\n        means[feature_idx] = sum / n_samples as f32;\n\n        let variance: f32 = (0..n_samples)\n            .map(|i| {\n                let diff = train_features[i][feature_idx] - means[feature_idx];\n                diff * diff\n            })\n            .sum();\n        let std = (variance / n_samples as f32).sqrt();\n        stds[feature_idx] = if std \u003e 1e-8 { std } else { 1.0 };\n    }\n\n    // Apply z-score to train and test data using same statistics\n    let train_scaled: Vec\u003cVec\u003cf32\u003e\u003e = (0..n_samples)\n        .map(|i| {\n            (0..n_features)\n                .map(|j| (train_features[i][j] - means[j]) / stds[j])\n                .collect()\n        })\n        .collect();\n\n    let test_scaled: Vec\u003cVec\u003cf32\u003e\u003e = test_features\n        .iter()\n        .map(|row| {\n            (0..n_features)\n                .map(|j| (row[j] - means[j]) / stds[j])\n                .collect()\n        })\n        .collect();\n\n    (train_scaled, test_scaled)\n}\n\nfn compute_metrics(predictions: \u0026[f32], targets: \u0026[f32]) -\u003e (f64, f64, f64) {\n    let n = targets.len();\n    let mse: f64 = predictions\n        .iter()\n        .zip(targets.iter())\n        .map(|(p, t)| (p - t) as f64 * (p - t) as f64)\n        .sum::\u003cf64\u003e()\n        / n as f64;\n\n    let mae: f64 = predictions\n        .iter()\n        .zip(targets.iter())\n        .map(|(p, t)| (p - t).abs() as f64)\n        .sum::\u003cf64\u003e()\n        / n as f64;\n\n    let target_mean: f64 = targets.iter().map(|t| *t as f64).sum::\u003cf64\u003e() / n as f64;\n    let ss_res: f64 = predictions\n        .iter()\n        .zip(targets.iter())\n        .map(|(p, t)| (p - t) as f64 * (p - t) as f64)\n        .sum();\n    let ss_tot: f64 = targets\n        .iter()\n        .map(|t| (*t as f64 - target_mean) * (*t as f64 - target_mean))\n        .sum();\n    let r_squared = 1.0 - ss_res / ss_tot;\n\n    (mse, mae, r_squared)\n}\n\nfn main() {\n    println!(\"Full-Batch vs Mini-Batch Comparison for Rust\");\n    println!(\"==============================================\\n\");\n\n    let (features, targets) = load_california_housing();\n    println!(\n        \"Loaded {} samples with {} features\\n\",\n        features.len(),\n        features[0].len()\n    );\n\n    let (train_features, test_features, train_targets, test_targets) =\n        split_train_test(\u0026features, \u0026targets);\n    let (train_features_scaled, test_features_scaled) =\n        standardize_features(\u0026train_features, \u0026test_features);\n\n    // Test configurations\n    let test_cases = vec![\n        (2, 5, 16512, \"Full-batch, 5 epochs (like sklearn)\"),\n        (2, 50, 16512, \"Full-batch, 50 epochs\"),\n        (2, 50, 32, \"Mini-batch (bs=32), 50 epochs (current default)\"),\n        (2, 1000, 32, \"Mini-batch (bs=32), 1000 epochs\"),\n    ];\n\n    for (n_features, n_epochs, batch_size, desc) in test_cases {\n        println!(\"Test: {}\", desc);\n        println!(\n            \"Features: {}, Epochs: {}, Batch size: {}\",\n            n_features, n_epochs, batch_size\n        );\n\n        // Select features\n        let train_selected: Vec\u003cVec\u003cf32\u003e\u003e = train_features_scaled\n            .iter()\n            .map(|row| row[..n_features].to_vec())\n            .collect();\n        let test_selected: Vec\u003cVec\u003cf32\u003e\u003e = test_features_scaled\n            .iter()\n            .map(|row| row[..n_features].to_vec())\n            .collect();\n\n        let train_dataset = InMemoryDataset::new(train_selected.clone(), train_targets.clone())\n            .expect(\"Failed to create dataset\");\n\n        // Train with CPU backend\n        let start = Instant::now();\n        let model_cpu = LinearRegression::\u003cCpuBackend\u003e::new(n_features);\n\n        let loss_fn = MSELoss;\n        let optimizer = SGD::new(0.01);\n        let regularizer = NoRegularizer;\n\n        let trainer_cpu = TrainerBuilder::new(loss_fn, optimizer, regularizer)\n            .batch_size(batch_size)\n            .max_epochs(n_epochs)\n            .build();\n\n        let fitted_cpu = trainer_cpu\n            .fit(model_cpu, \u0026train_dataset)\n            .expect(\"Training failed\");\n        let training_time_cpu = start.elapsed();\n\n        // Predictions\n        let predictions_cpu: Vec\u003cf32\u003e = test_selected\n            .iter()\n            .map(|sample| {\n                let tensor_input = Tensor1D::\u003cCpuBackend\u003e::new(sample.clone());\n                let result = fitted_cpu.predict(\u0026tensor_input);\n                result.to_f64() as f32\n            })\n            .collect();\n\n        // Metrics\n        let (mse, mae, r_squared) = compute_metrics(\u0026predictions_cpu, \u0026test_targets);\n\n        println!(\"CPU Backend:\");\n        println!(\n            \"  Training time: {:.2} ms\",\n            training_time_cpu.as_secs_f64() * 1000.0\n        );\n        println!(\"  MSE: {:.4}\", mse);\n        println!(\"  MAE: {:.4}\", mae);\n        println!(\"  RÂ²: {:.4}\", r_squared);\n        println!();\n    }\n\n    // Compare with sklearn numbers\n    println!(\"Sklearn SGDRegressor (for reference):\");\n    println!(\"  lr_constant: 5.09 ms (2 features, full-batch, converges in 5 epochs)\");\n    println!(\"  lr_adaptive: 25.38 ms (2 features, full-batch, converges in 5 epochs)\");\n    println!(\"  Training details (verbose output):\");\n    println!(\"    -- Epoch 1, T: 100, Avg. loss: 0.584\");\n    println!(\"    -- Epoch 2, T: 200, Avg. loss: 0.574\");\n    println!(\"    -- Epoch 3, T: 300, Avg. loss: 0.571\");\n    println!(\"    -- Epoch 4, T: 400, Avg. loss: 0.570\");\n    println!(\"    -- Epoch 5, T: 500, Avg. loss: 0.566\");\n    println!(\"  Converges in 5 epochs (not full 1000)\");\n}\n","traces":[],"covered":0,"coverable":0},{"path":["/","home","victor","ML","machinelearne-rs","benchmarks","src","bin","learning_rate_search.rs"],"content":"use machinelearne_rs::{\n    dataset::InMemoryDataset, loss::MSELoss, model::linear::LinearRegression,\n    model::InferenceModel, optimizer::SGD, regularizers::NoRegularizer, trainer::TrainerBuilder,\n    Tensor1D,\n};\n\nuse std::time::Instant;\n\nuse machinelearne_rs::backend::CpuBackend;\n\ntype Features = Vec\u003cVec\u003cf32\u003e\u003e;\ntype Targets = Vec\u003cf32\u003e;\n\nfn load_california_housing() -\u003e (Vec\u003cVec\u003cf32\u003e\u003e, Vec\u003cf32\u003e) {\n    // Load CSV and parse\n    let data = std::fs::read_to_string(\"benchmarks/datasets/california_housing.csv\")\n        .expect(\"Failed to read California Housing dataset\");\n\n    let mut features = Vec::new();\n    let mut targets = Vec::new();\n\n    for (i, line) in data.lines().enumerate() {\n        if i == 0 {\n            continue; // Skip header\n        }\n\n        let values: Vec\u003cf32\u003e = line\n            .split(',')\n            .map(|s| s.trim().parse().expect(\"Failed to parse number\"))\n            .collect();\n\n        if values.len() == 9 {\n            features.push(values[..8].to_vec());\n            targets.push(values[8]);\n        }\n    }\n\n    (features, targets)\n}\n\nfn split_train_test(\n    features: \u0026[Vec\u003cf32\u003e],\n    targets: \u0026[f32],\n) -\u003e (Features, Features, Targets, Targets) {\n    let n = features.len();\n    let train_size = (n as f64 * 0.8) as usize;\n\n    let train_features = features[..train_size].to_vec();\n    let train_targets = targets[..train_size].to_vec();\n    let test_features = features[train_size..].to_vec();\n    let test_targets = targets[train_size..].to_vec();\n\n    (train_features, test_features, train_targets, test_targets)\n}\n\nfn standardize_features(\n    train_features: \u0026[Vec\u003cf32\u003e],\n    test_features: \u0026[Vec\u003cf32\u003e],\n) -\u003e (Vec\u003cVec\u003cf32\u003e\u003e, Vec\u003cVec\u003cf32\u003e\u003e) {\n    let n_samples = train_features.len();\n    let n_features = train_features[0].len();\n\n    // Compute mean and std from training data\n    let mut means = vec![0.0_f32; n_features];\n    let mut stds = vec![0.0_f32; n_features];\n\n    for feature_idx in 0..n_features {\n        let sum: f32 = (0..n_samples).map(|i| train_features[i][feature_idx]).sum();\n        means[feature_idx] = sum / n_samples as f32;\n\n        let variance: f32 = (0..n_samples)\n            .map(|i| {\n                let diff = train_features[i][feature_idx] - means[feature_idx];\n                diff * diff\n            })\n            .sum();\n        let std = (variance / n_samples as f32).sqrt();\n        stds[feature_idx] = if std \u003e 1e-8 { std } else { 1.0 };\n    }\n\n    // Apply z-score to train and test data using same statistics\n    let train_scaled: Vec\u003cVec\u003cf32\u003e\u003e = (0..n_samples)\n        .map(|i| {\n            (0..n_features)\n                .map(|j| (train_features[i][j] - means[j]) / stds[j])\n                .collect()\n        })\n        .collect();\n\n    let test_scaled: Vec\u003cVec\u003cf32\u003e\u003e = test_features\n        .iter()\n        .map(|row| {\n            (0..n_features)\n                .map(|j| (row[j] - means[j]) / stds[j])\n                .collect()\n        })\n        .collect();\n\n    (train_scaled, test_scaled)\n}\n\nfn compute_metrics(predictions: \u0026[f32], targets: \u0026[f32]) -\u003e (f64, f64, f64) {\n    let n = targets.len();\n    let mse: f64 = predictions\n        .iter()\n        .zip(targets.iter())\n        .map(|(p, t)| (p - t) as f64 * (p - t) as f64)\n        .sum::\u003cf64\u003e()\n        / n as f64;\n\n    let mae: f64 = predictions\n        .iter()\n        .zip(targets.iter())\n        .map(|(p, t)| (p - t).abs() as f64)\n        .sum::\u003cf64\u003e()\n        / n as f64;\n\n    let target_mean: f64 = targets.iter().map(|t| *t as f64).sum::\u003cf64\u003e() / n as f64;\n    let ss_res: f64 = predictions\n        .iter()\n        .zip(targets.iter())\n        .map(|(p, t)| (p - t) as f64 * (p - t) as f64)\n        .sum();\n    let ss_tot: f64 = targets\n        .iter()\n        .map(|t| (*t as f64 - target_mean) * (*t as f64 - target_mean))\n        .sum();\n    let r_squared = 1.0 - ss_res / ss_tot;\n\n    (mse, mae, r_squared)\n}\n\nfn main() {\n    println!(\"Learning Rate Search for Full-Batch Training\");\n    println!(\"===========================================\\n\");\n\n    let (features, targets) = load_california_housing();\n    println!(\n        \"Loaded {} samples with {} features\\n\",\n        features.len(),\n        features[0].len()\n    );\n\n    let (train_features, test_features, train_targets, test_targets) =\n        split_train_test(\u0026features, \u0026targets);\n    let (train_features_scaled, test_features_scaled) =\n        standardize_features(\u0026train_features, \u0026test_features);\n\n    let n_features = 2;\n    let n_epochs = 5;\n    let batch_size = 16512; // Full batch (all training samples)\n\n    // Learning rates to test (logarithmic scale)\n    let learning_rates = vec![\n        1e-4, 2e-4, 5e-4, 1e-3, 2e-3, 5e-3, 1e-2, 2e-2, 5e-2, 1e-1, 2e-1, 5e-1,\n    ];\n\n    println!(\n        \"Testing {} learning rates ({} epochs, batch size = {})\\n\",\n        learning_rates.len(),\n        n_epochs,\n        batch_size\n    );\n\n    let mut best_lr = 0.0;\n    let mut best_mse = f64::MAX;\n    let mut best_r2 = f64::MIN;\n\n    for lr in learning_rates {\n        // Select features\n        let train_selected: Vec\u003cVec\u003cf32\u003e\u003e = train_features_scaled\n            .iter()\n            .map(|row| row[..n_features].to_vec())\n            .collect();\n        let test_selected: Vec\u003cVec\u003cf32\u003e\u003e = test_features_scaled\n            .iter()\n            .map(|row| row[..n_features].to_vec())\n            .collect();\n\n        let train_dataset = InMemoryDataset::new(train_selected.clone(), train_targets.clone())\n            .expect(\"Failed to create dataset\");\n\n        // Train with CPU backend\n        let start = Instant::now();\n        let model_cpu = LinearRegression::\u003cCpuBackend\u003e::new(n_features);\n\n        let loss_fn = MSELoss;\n        let optimizer = SGD::new(lr);\n        let regularizer = NoRegularizer;\n\n        let trainer_cpu = TrainerBuilder::new(loss_fn, optimizer, regularizer)\n            .batch_size(batch_size)\n            .max_epochs(n_epochs)\n            .build();\n\n        let fitted_cpu = trainer_cpu\n            .fit(model_cpu, \u0026train_dataset)\n            .expect(\"Training failed\");\n        let training_time = start.elapsed();\n\n        // Predictions\n        let predictions: Vec\u003cf32\u003e = test_selected\n            .iter()\n            .map(|sample| {\n                let tensor_input = Tensor1D::\u003cCpuBackend\u003e::new(sample.clone());\n                let result = fitted_cpu.predict(\u0026tensor_input);\n                result.to_f64() as f32\n            })\n            .collect();\n\n        // Metrics\n        let (mse, mae, r_squared) = compute_metrics(\u0026predictions, \u0026test_targets);\n\n        println!(\n            \"LR = {:.6}: Time = {:.2} ms, MSE = {:.4}, MAE = {:.4}, RÂ² = {:.4}\",\n            lr,\n            training_time.as_secs_f64() * 1000.0,\n            mse,\n            mae,\n            r_squared\n        );\n\n        // Track best\n        if mse \u003c best_mse {\n            best_mse = mse;\n            best_r2 = r_squared;\n            best_lr = lr;\n        }\n    }\n\n    println!(\"\\n=== Best Learning Rate ===\");\n    println!(\"LR = {:.6}\", best_lr);\n    println!(\"MSE = {:.4}\", best_mse);\n    println!(\"RÂ² = {:.4}\", best_r2);\n\n    println!(\"\\n=== Sklearn Comparison ===\");\n    println!(\"Sklearn lr_constant: 5.09 ms, MSE = 0.6697, RÂ² = 0.4889\");\n    println!(\"Sklearn lr_adaptive: 25.38 ms, MSE = 0.6630, RÂ² = 0.4941\");\n\n    if best_lr \u003e 0.0 {\n        println!(\"Rust is competitive at LR = {:.6}\", best_lr);\n    }\n}\n","traces":[],"covered":0,"coverable":0},{"path":["/","home","victor","ML","machinelearne-rs","benchmarks","src","bin","sgd_comparison.rs"],"content":"#!/usr/bin/env rust-script\n//!\n//! Compare Rust SGD performance against sklearn SGDRegressor\n//!\n//! Matches sklearn's SGDRegressor configuration:\n//! - max_iter=1000\n//! - tol=1e-3 (early stopping)\n//! - learning_rate=0.01\n//! - StandardScaler for feature scaling\n\nuse benchmarks::data::CaliforniaHousingDataset;\nuse benchmarks::metrics::Metrics;\nuse machinelearne_rs::{\n    backend::Tensor2D,\n    dataset::InMemoryDataset,\n    loss::MSELoss,\n    model::{linear::LinearRegression, InferenceModel},\n    optimizer::SGD,\n    regularizers::NoRegularizer,\n    trainer::Trainer,\n    CpuBackend,\n};\nuse serde_json::json;\nuse std::fs::File;\nuse std::io::Write;\nuse std::time::Instant;\n\n/// Helper function to convert Vec\u003cVec\u003cf32\u003e\u003e to Tensor2D\u003cCpuBackend\u003e\nfn vec_to_tensor2d(data: \u0026[Vec\u003cf32\u003e]) -\u003e Tensor2D\u003cCpuBackend\u003e {\n    let flat: Vec\u003cf32\u003e = data.iter().flatten().copied().collect();\n    Tensor2D::new(flat, data.len(), data.first().map(|v| v.len()).unwrap_or(0))\n}\n\n/// Apply z-score standardization to features\nfn standardize_features(\n    train_features: \u0026[Vec\u003cf32\u003e],\n    test_features: \u0026[Vec\u003cf32\u003e],\n) -\u003e (Vec\u003cVec\u003cf32\u003e\u003e, Vec\u003cVec\u003cf32\u003e\u003e) {\n    let n_samples = train_features.len();\n    let n_features = if n_samples \u003e 0 {\n        train_features[0].len()\n    } else {\n        return (train_features.to_vec(), test_features.to_vec());\n    };\n\n    // Compute mean and std for each feature from training data\n    let mut means = vec![0.0_f32; n_features];\n    let mut stds = vec![0.0_f32; n_features];\n\n    for feature_idx in 0..n_features {\n        // Compute mean\n        let sum: f32 = (0..n_samples).map(|i| train_features[i][feature_idx]).sum();\n        means[feature_idx] = sum / n_samples as f32;\n\n        // Compute variance\n        let variance: f32 = (0..n_samples)\n            .map(|i| {\n                let diff = train_features[i][feature_idx] - means[feature_idx];\n                diff * diff\n            })\n            .sum();\n        stds[feature_idx] = f32::sqrt(variance / n_samples as f32);\n\n        // Prevent division by zero\n        if stds[feature_idx] \u003c 1e-6 {\n            stds[feature_idx] = 1.0;\n        }\n    }\n\n    // Apply z-score to training data\n    let train_scaled: Vec\u003cVec\u003cf32\u003e\u003e = (0..n_samples)\n        .map(|i| {\n            (0..n_features)\n                .map(|j| (train_features[i][j] - means[j]) / stds[j])\n                .collect()\n        })\n        .collect();\n\n    // Apply z-score to test data (using train's mean/std)\n    let test_scaled: Vec\u003cVec\u003cf32\u003e\u003e = (0..test_features.len())\n        .map(|i| {\n            (0..n_features)\n                .map(|j| (test_features[i][j] - means[j]) / stds[j])\n                .collect()\n        })\n        .collect();\n\n    (train_scaled, test_scaled)\n}\n\n/// Collect metrics with different configurations\nfn run_comparison() -\u003e serde_json::Value {\n    let dataset = CaliforniaHousingDataset::load(\"benchmarks/datasets/california_housing.csv\")\n        .expect(\"Failed to load dataset\");\n\n    // Test with 2 features (same as sklearn benchmark)\n    let feature_indices: Vec\u003cusize\u003e = vec![0, 1];\n    let subset = dataset.select_features(\u0026feature_indices);\n    let (train_dataset, _, test_dataset) = subset.split_train_val_test(0.8, 0.0);\n\n    let train_features = train_dataset.features();\n    let train_target = train_dataset.target();\n    let test_features = test_dataset.features();\n    let test_target = test_dataset.target();\n\n    // Apply z-score standardization\n    let (train_features_scaled, test_features_scaled) =\n        standardize_features(train_features, test_features);\n\n    let train_memory = InMemoryDataset::new(train_features_scaled, train_target.to_vec())\n        .expect(\"Failed to create training dataset\");\n\n    let mut results = Vec::new();\n\n    // Configuration 1: 50 epochs (current config)\n    let lr = 0.01;\n    let model1 = LinearRegression::\u003cCpuBackend\u003e::new(2);\n    let optimizer = SGD::\u003cCpuBackend\u003e::new(lr);\n\n    let trainer = Trainer::builder(MSELoss, optimizer, NoRegularizer)\n        .batch_size(32)\n        .max_epochs(50)\n        .build();\n\n    let start = Instant::now();\n    let fitted1 = trainer\n        .fit(model1, \u0026train_memory)\n        .expect(\"Failed to fit model\");\n    let train_time_50 = start.elapsed().as_millis();\n\n    let test_tensor = vec_to_tensor2d(\u0026test_features_scaled);\n    let pred_tensor = fitted1.predict_batch(\u0026test_tensor);\n    let predictions: Vec\u003cf32\u003e = pred_tensor.to_vec().into_iter().map(|v| v as f32).collect();\n\n    results.push(json!({\n        \"config\": \"50 epochs\",\n        \"train_time_ms\": train_time_50,\n        \"mse\": Metrics::mse(test_target, \u0026predictions),\n        \"mae\": Metrics::mae(test_target, \u0026predictions),\n        \"r2\": Metrics::r_squared(test_target, \u0026predictions),\n    }));\n\n    // Configuration 2: 1000 epochs (matching sklearn)\n    let model2 = LinearRegression::\u003cCpuBackend\u003e::new(2);\n    let optimizer2 = SGD::\u003cCpuBackend\u003e::new(lr);\n\n    let trainer2 = Trainer::builder(MSELoss, optimizer2, NoRegularizer)\n        .batch_size(32)\n        .max_epochs(1000)\n        .build();\n\n    let start = Instant::now();\n    let fitted2 = trainer2\n        .fit(model2, \u0026train_memory)\n        .expect(\"Failed to fit model\");\n    let train_time_1000 = start.elapsed().as_millis();\n\n    let pred_tensor2 = fitted2.predict_batch(\u0026test_tensor);\n    let predictions2: Vec\u003cf32\u003e = pred_tensor2\n        .to_vec()\n        .into_iter()\n        .map(|v| v as f32)\n        .collect();\n\n    results.push(json!({\n        \"config\": \"1000 epochs (sklearn match)\",\n        \"train_time_ms\": train_time_1000,\n        \"mse\": Metrics::mse(test_target, \u0026predictions2),\n        \"mae\": Metrics::mae(test_target, \u0026predictions2),\n        \"r2\": Metrics::r_squared(test_target, \u0026predictions2),\n    }));\n\n    // Configuration 3: 1000 epochs with larger batch size\n    let model3 = LinearRegression::\u003cCpuBackend\u003e::new(2);\n    let optimizer3 = SGD::\u003cCpuBackend\u003e::new(lr);\n\n    let trainer3 = Trainer::builder(MSELoss, optimizer3, NoRegularizer)\n        .batch_size(128)\n        .max_epochs(1000)\n        .build();\n\n    let start = Instant::now();\n    let fitted3 = trainer3\n        .fit(model3, \u0026train_memory)\n        .expect(\"Failed to fit model\");\n    let train_time_1000_bs128 = start.elapsed().as_millis();\n\n    let pred_tensor3 = fitted3.predict_batch(\u0026test_tensor);\n    let predictions3: Vec\u003cf32\u003e = pred_tensor3\n        .to_vec()\n        .into_iter()\n        .map(|v| v as f32)\n        .collect();\n\n    results.push(json!({\n        \"config\": \"1000 epochs, batch_size=128\",\n        \"train_time_ms\": train_time_1000_bs128,\n        \"mse\": Metrics::mse(test_target, \u0026predictions3),\n        \"mae\": Metrics::mae(test_target, \u0026predictions3),\n        \"r2\": Metrics::r_squared(test_target, \u0026predictions3),\n    }));\n\n    json!({\n        \"n_features\": 2,\n        \"model\": \"LinearRegression\",\n        \"results\": results\n    })\n}\n\nfn main() {\n    println!(\"Running SGD comparison benchmarks...\");\n    let result = run_comparison();\n\n    // Write to file\n    let mut file = File::create(\"benchmarks/results/rust_sgd_comparison.json\")\n        .expect(\"Failed to create output file\");\n    file.write_all(serde_json::to_string_pretty(\u0026result).unwrap().as_bytes())\n        .expect(\"Failed to write to file\");\n\n    println!(\"\\nResults saved to benchmarks/results/rust_sgd_comparison.json\");\n\n    // Print summary\n    let results = result[\"results\"].as_array().unwrap();\n    println!(\"\\nRust SGD Performance:\");\n    println!(\n        \"{:\u003c30} {:\u003e12} {:\u003e10} {:\u003e10} {:\u003e10}\",\n        \"Config\", \"Time (ms)\", \"MSE\", \"MAE\", \"RÂ²\"\n    );\n    for r in results {\n        let config = r[\"config\"].as_str().unwrap();\n        let time = r[\"train_time_ms\"].as_f64().unwrap();\n        let mse = r[\"mse\"].as_f64().unwrap();\n        let mae = r[\"mae\"].as_f64().unwrap();\n        let r2 = r[\"r2\"].as_f64().unwrap();\n        println!(\n            \"{:\u003c30} {:\u003e12.2} {:\u003e10.4} {:\u003e10.4} {:\u003e10.4}\",\n            config, time, mse, mae, r2\n        );\n    }\n\n    println!(\"\\nSklearn SGDRegressor (for comparison):\");\n    println!(\n        \"{:\u003c30} {:\u003e12} {:\u003e10} {:\u003e10} {:\u003e10}\",\n        \"Config\", \"Time (ms)\", \"MSE\", \"MAE\", \"RÂ²\"\n    );\n    println!(\n        \"{:\u003c30} {:\u003e12.2} {:\u003e10.4} {:\u003e10.4} {:\u003e10.4}\",\n        \"lr_constant (1000 iter)\", 5.09, 0.6697, 0.5978, 0.4889\n    );\n    println!(\n        \"{:\u003c30} {:\u003e12.2} {:\u003e10.4} {:\u003e10.4} {:\u003e10.4}\",\n        \"lr_adaptive (1000 iter)\", 25.38, 0.6630, 0.6061, 0.4941\n    );\n}\n","traces":[],"covered":0,"coverable":0},{"path":["/","home","victor","ML","machinelearne-rs","benchmarks","src","data","california_housing.rs"],"content":"use csv::ReaderBuilder;\nuse machinelearne_rs::dataset::InMemoryDataset;\nuse std::fs::File;\nuse std::io::{self, BufReader};\n\n/// California Housing dataset loader.\n///\n/// Loads the California Housing dataset from CSV and provides methods\n/// to select subsets of features and split into train/validation/test sets.\n///\n/// The dataset contains 20640 samples with 8 features:\n/// - MedInc: Median income in block group\n/// - HouseAge: Median house age in block group\n/// - AveRooms: Average number of rooms per household\n/// - AveBedrms: Average number of bedrooms per household\n/// - Population: Block group population\n/// - AveOccup: Average number of household members\n/// - Latitude: Block group latitude\n/// - Longitude: Block group longitude\n///\n/// Target variable: MedHouseVal (Median house value for California districts)\n#[derive(Debug, Clone)]\npub struct CaliforniaHousingDataset {\n    features: Vec\u003cVec\u003cf32\u003e\u003e,\n    target: Vec\u003cf32\u003e,\n    feature_names: Vec\u003c\u0026'static str\u003e,\n}\n\nimpl CaliforniaHousingDataset {\n    /// Load the California Housing dataset from CSV.\n    ///\n    /// # Arguments\n    ///\n    /// * `path` - Path to the CSV file (default: \"benchmarks/datasets/california_housing.csv\")\n    ///\n    /// # Example\n    ///\n    /// ```no_run\n    /// use benchmarks::data::CaliforniaHousingDataset;\n    ///\n    /// let dataset = CaliforniaHousingDataset::load(\n    ///     \"benchmarks/datasets/california_housing.csv\"\n    /// ).unwrap();\n    /// ```\n    pub fn load(path: \u0026str) -\u003e io::Result\u003cSelf\u003e {\n        let file = File::open(path)?;\n        let reader = BufReader::new(file);\n        let mut rdr = ReaderBuilder::new().from_reader(reader);\n\n        let mut features = Vec::new();\n        let mut target = Vec::new();\n\n        for result in rdr.records() {\n            let record = result?;\n            // Columns: MedInc, HouseAge, AveRooms, AveBedrms, Population, AveOccup, Latitude, Longitude, MedHouseVal\n            let med_inc: f32 = record[0].parse().unwrap_or(0.0);\n            let house_age: f32 = record[1].parse().unwrap_or(0.0);\n            let ave_rooms: f32 = record[2].parse().unwrap_or(0.0);\n            let ave_bedrms: f32 = record[3].parse().unwrap_or(0.0);\n            let population: f32 = record[4].parse().unwrap_or(0.0);\n            let ave_occup: f32 = record[5].parse().unwrap_or(0.0);\n            let latitude: f32 = record[6].parse().unwrap_or(0.0);\n            let longitude: f32 = record[7].parse().unwrap_or(0.0);\n            let med_house_val: f32 = record[8].parse().unwrap_or(0.0);\n\n            features.push(vec![\n                med_inc, house_age, ave_rooms, ave_bedrms, population, ave_occup, latitude,\n                longitude,\n            ]);\n            target.push(med_house_val);\n        }\n\n        Ok(Self {\n            features,\n            target,\n            feature_names: vec![\n                \"MedInc\",\n                \"HouseAge\",\n                \"AveRooms\",\n                \"AveBedrms\",\n                \"Population\",\n                \"AveOccup\",\n                \"Latitude\",\n                \"Longitude\",\n            ],\n        })\n    }\n\n    /// Get the number of samples in the dataset.\n    pub fn len(\u0026self) -\u003e usize {\n        self.features.len()\n    }\n\n    /// Check if the dataset is empty.\n    pub fn is_empty(\u0026self) -\u003e bool {\n        self.features.is_empty()\n    }\n\n    /// Get the number of features.\n    pub fn n_features(\u0026self) -\u003e usize {\n        self.features.first().map(|f| f.len()).unwrap_or(0)\n    }\n\n    /// Get feature names.\n    pub fn feature_names(\u0026self) -\u003e \u0026[\u0026'static str] {\n        \u0026self.feature_names\n    }\n\n    /// Select a subset of features.\n    ///\n    /// # Arguments\n    ///\n    /// * `feature_indices` - Indices of features to select (0-indexed)\n    ///\n    /// # Example\n    ///\n    /// ```no_run\n    /// let dataset = CaliforniaHousingDataset::load(...).unwrap();\n    /// // Select only MedInc and HouseAge (first 2 features)\n    /// let subset = dataset.select_features(\u0026[0, 1]);\n    /// ```\n    pub fn select_features(\u0026self, feature_indices: \u0026[usize]) -\u003e Self {\n        let features: Vec\u003cVec\u003cf32\u003e\u003e = self\n            .features\n            .iter()\n            .map(|row| {\n                feature_indices\n                    .iter()\n                    .map(|\u0026idx| row.get(idx).copied().unwrap_or(0.0))\n                    .collect()\n            })\n            .collect();\n        let feature_names: Vec\u003c\u0026'static str\u003e = feature_indices\n            .iter()\n            .map(|\u0026idx| self.feature_names.get(idx).copied().unwrap_or(\"Unknown\"))\n            .collect();\n\n        Self {\n            features,\n            target: self.target.clone(),\n            feature_names,\n        }\n    }\n\n    /// Split the dataset into train and test sets.\n    ///\n    /// # Arguments\n    ///\n    /// * `train_ratio` - Fraction of data to use for training (0.0 to 1.0)\n    ///\n    /// # Returns\n    ///\n    /// (train_dataset, test_dataset)\n    ///\n    /// # Example\n    ///\n    /// ```no_run\n    /// let dataset = CaliforniaHousingDataset::load(...).unwrap();\n    /// let (train, test) = dataset.split(0.8); // 80% train, 20% test\n    /// ```\n    pub fn split(\u0026self, train_ratio: f32) -\u003e (Self, Self) {\n        let n_samples = self.len();\n        let n_train = (n_samples as f32 * train_ratio) as usize;\n\n        let train_features = self.features[..n_train].to_vec();\n        let train_target = self.target[..n_train].to_vec();\n\n        let test_features = self.features[n_train..].to_vec();\n        let test_target = self.target[n_train..].to_vec();\n\n        (\n            Self {\n                features: train_features,\n                target: train_target,\n                feature_names: self.feature_names.clone(),\n            },\n            Self {\n                features: test_features,\n                target: test_target,\n                feature_names: self.feature_names.clone(),\n            },\n        )\n    }\n\n    /// Split the dataset into train, validation, and test sets.\n    ///\n    /// # Arguments\n    ///\n    /// * `train_ratio` - Fraction of data to use for training\n    /// * `val_ratio` - Fraction of data to use for validation\n    ///\n    /// # Returns\n    ///\n    /// (train_dataset, val_dataset, test_dataset)\n    pub fn split_train_val_test(\u0026self, train_ratio: f32, val_ratio: f32) -\u003e (Self, Self, Self) {\n        let n_samples = self.len();\n        let n_train = (n_samples as f32 * train_ratio) as usize;\n        let n_val = (n_samples as f32 * val_ratio) as usize;\n\n        let train_features = self.features[..n_train].to_vec();\n        let train_target = self.target[..n_train].to_vec();\n\n        let val_features = self.features[n_train..n_train + n_val].to_vec();\n        let val_target = self.target[n_train..n_train + n_val].to_vec();\n\n        let test_features = self.features[n_train + n_val..].to_vec();\n        let test_target = self.target[n_train + n_val..].to_vec();\n\n        (\n            Self {\n                features: train_features,\n                target: train_target,\n                feature_names: self.feature_names.clone(),\n            },\n            Self {\n                features: val_features,\n                target: val_target,\n                feature_names: self.feature_names.clone(),\n            },\n            Self {\n                features: test_features,\n                target: test_target,\n                feature_names: self.feature_names.clone(),\n            },\n        )\n    }\n\n    /// Convert to an InMemoryDataset for use with machinelearne-rs.\n    pub fn to_in_memory_dataset(\u0026self) -\u003e Result\u003cInMemoryDataset, String\u003e {\n        InMemoryDataset::new(self.features.clone(), self.target.clone())\n    }\n\n    /// Get a reference to the features.\n    pub fn features(\u0026self) -\u003e \u0026[Vec\u003cf32\u003e] {\n        \u0026self.features\n    }\n\n    /// Get a reference to the target values.\n    pub fn target(\u0026self) -\u003e \u0026[f32] {\n        \u0026self.target\n    }\n}\n\n#[cfg(test)]\nmod tests {\n    use super::*;\n    use machinelearne_rs::dataset::Dataset;\n\n    #[test]\n    fn test_load_dataset() {\n        let dataset = CaliforniaHousingDataset::load(\"benchmarks/datasets/california_housing.csv\")\n            .expect(\"Failed to load dataset\");\n        assert_eq!(dataset.len(), 20640);\n        assert_eq!(dataset.n_features(), 8);\n        assert_eq!(dataset.feature_names().len(), 8);\n    }\n\n    #[test]\n    fn test_select_features() {\n        let dataset = CaliforniaHousingDataset::load(\"benchmarks/datasets/california_housing.csv\")\n            .expect(\"Failed to load dataset\");\n        let subset = dataset.select_features(\u0026[0, 1]); // MedInc and HouseAge\n        assert_eq!(subset.n_features(), 2);\n        assert_eq!(subset.feature_names(), \u0026[\"MedInc\", \"HouseAge\"]);\n    }\n\n    #[test]\n    fn test_split() {\n        let dataset = CaliforniaHousingDataset::load(\"benchmarks/datasets/california_housing.csv\")\n            .expect(\"Failed to load dataset\");\n        let (train, test) = dataset.split(0.8);\n        assert_eq!(train.len(), 16512);\n        assert_eq!(test.len(), 4128);\n    }\n\n    #[test]\n    fn test_to_in_memory_dataset() {\n        let dataset = CaliforniaHousingDataset::load(\"benchmarks/datasets/california_housing.csv\")\n            .expect(\"Failed to load dataset\");\n        let im_dataset = dataset.to_in_memory_dataset().expect(\"Failed to convert\");\n        assert_eq!(im_dataset.len(), Some(20640));\n    }\n}\n","traces":[],"covered":0,"coverable":0},{"path":["/","home","victor","ML","machinelearne-rs","benchmarks","src","data","mod.rs"],"content":"pub mod california_housing;\n\npub use california_housing::CaliforniaHousingDataset;\n","traces":[],"covered":0,"coverable":0},{"path":["/","home","victor","ML","machinelearne-rs","benchmarks","src","lib.rs"],"content":"//! Benchmark utilities and common modules for machinelearne-rs benchmarks.\n//!\n//! This library provides common functionality for benchmarking the machinelearne-rs\n//! library against sklearn, including:\n//!\n//! - Data loading utilities\n//! - Metrics calculation (MSE, MAE, RÂ²)\n//! - Timing and benchmarking utilities\n\npub mod data;\npub mod metrics;\npub mod utils;\n\npub use data::CaliforniaHousingDataset;\npub use metrics::{Metrics, RegressionMetrics};\npub use utils::{benchmark_fn, benchmark_with_warmup, time_fn, BenchmarkStats, Timer};\n","traces":[],"covered":0,"coverable":0},{"path":["/","home","victor","ML","machinelearne-rs","benchmarks","src","main.rs"],"content":"// Benchmark utilities and common modules\n// This is the main entry point for running benchmarks\n\nfn main() {\n    println!(\"Machinelearne-rs Benchmark Suite\");\n    println!();\n    println!(\"Usage:\");\n    println!(\"  cargo bench --package benchmarks --all-benchmarks\");\n    println!(\"  cargo bench --package benchmarks --bench \u003cbenchmark_name\u003e\");\n    println!();\n    println!(\"Available benchmarks:\");\n    println!(\"  - train_1_feature: Training benchmarks with 1 feature\");\n    println!(\"  - train_2_features: Training benchmarks with 2 features\");\n    println!(\"  - train_4_features: Training benchmarks with 4 features\");\n    println!(\"  - train_8_features: Training benchmarks with 8 features\");\n    println!(\"  - predict: Prediction latency and throughput benchmarks\");\n    println!(\"  - metrics: Metrics computation benchmarks\");\n    println!();\n    println!(\"To run all benchmarks:\");\n    println!(\"  bash benchmarks/scripts/run_all.sh\");\n}\n","traces":[],"covered":0,"coverable":0},{"path":["/","home","victor","ML","machinelearne-rs","benchmarks","src","metrics.rs"],"content":"/// Metrics for evaluating regression models.\npub struct Metrics;\n\nimpl Metrics {\n    /// Calculate Mean Squared Error (MSE).\n    ///\n    /// MSE = mean((y_true - y_pred)^2)\n    ///\n    /// # Arguments\n    ///\n    /// * `y_true` - Ground truth values\n    /// * `y_pred` - Predicted values\n    ///\n    /// # Returns\n    ///\n    /// The MSE value (lower is better)\n    pub fn mse(y_true: \u0026[f32], y_pred: \u0026[f32]) -\u003e f32 {\n        assert_eq!(\n            y_true.len(),\n            y_pred.len(),\n            \"Arrays must have the same length\"\n        );\n\n        if y_true.is_empty() {\n            return 0.0;\n        }\n\n        let sum_sq: f32 = y_true\n            .iter()\n            .zip(y_pred.iter())\n            .map(|(\u0026t, \u0026p)| (t - p).powi(2))\n            .sum();\n\n        sum_sq / y_true.len() as f32\n    }\n\n    /// Calculate Root Mean Squared Error (RMSE).\n    ///\n    /// RMSE = sqrt(MSE)\n    ///\n    /// # Arguments\n    ///\n    /// * `y_true` - Ground truth values\n    /// * `y_pred` - Predicted values\n    ///\n    /// # Returns\n    ///\n    /// The RMSE value (lower is better, in same units as the target)\n    pub fn rmse(y_true: \u0026[f32], y_pred: \u0026[f32]) -\u003e f32 {\n        Self::mse(y_true, y_pred).sqrt()\n    }\n\n    /// Calculate Mean Absolute Error (MAE).\n    ///\n    /// MAE = mean(|y_true - y_pred|)\n    ///\n    /// # Arguments\n    ///\n    /// * `y_true` - Ground truth values\n    /// * `y_pred` - Predicted values\n    ///\n    /// # Returns\n    ///\n    /// The MAE value (lower is better)\n    pub fn mae(y_true: \u0026[f32], y_pred: \u0026[f32]) -\u003e f32 {\n        assert_eq!(\n            y_true.len(),\n            y_pred.len(),\n            \"Arrays must have the same length\"\n        );\n\n        if y_true.is_empty() {\n            return 0.0;\n        }\n\n        let sum_abs: f32 = y_true\n            .iter()\n            .zip(y_pred.iter())\n            .map(|(\u0026t, \u0026p)| (t - p).abs())\n            .sum();\n\n        sum_abs / y_true.len() as f32\n    }\n\n    /// Calculate RÂ² (coefficient of determination).\n    ///\n    /// RÂ² = 1 - (SS_res / SS_tot)\n    ///\n    /// where:\n    /// - SS_res = sum((y_true - y_pred)^2)  (residual sum of squares)\n    /// - SS_tot = sum((y_true - mean(y_true))^2)  (total sum of squares)\n    ///\n    /// RÂ² ranges from 0 to 1, where 1 indicates perfect prediction.\n    /// Values can be negative if the model is arbitrarily worse than the mean.\n    ///\n    /// # Arguments\n    ///\n    /// * `y_true` - Ground truth values\n    /// * `y_pred` - Predicted values\n    ///\n    /// # Returns\n    ///\n    /// The RÂ² value (higher is better)\n    pub fn r_squared(y_true: \u0026[f32], y_pred: \u0026[f32]) -\u003e f32 {\n        assert_eq!(\n            y_true.len(),\n            y_pred.len(),\n            \"Arrays must have the same length\"\n        );\n\n        if y_true.is_empty() {\n            return 0.0;\n        }\n\n        let mean_true: f32 = y_true.iter().copied().sum::\u003cf32\u003e() / y_true.len() as f32;\n\n        let ss_res: f32 = y_true\n            .iter()\n            .zip(y_pred.iter())\n            .map(|(\u0026t, \u0026p)| (t - p).powi(2))\n            .sum();\n\n        let ss_tot: f32 = y_true.iter().map(|\u0026t| (t - mean_true).powi(2)).sum();\n\n        if ss_tot == 0.0 {\n            // All values are the same, perfect prediction if predictions are also the same\n            return if ss_res == 0.0 { 1.0 } else { 0.0 };\n        }\n\n        1.0 - (ss_res / ss_tot)\n    }\n\n    /// Calculate all metrics at once.\n    ///\n    /// # Arguments\n    ///\n    /// * `y_true` - Ground truth values\n    /// * `y_pred` - Predicted values\n    ///\n    /// # Returns\n    ///\n    /// A struct containing MSE, RMSE, MAE, and RÂ²\n    pub fn calculate_all(y_true: \u0026[f32], y_pred: \u0026[f32]) -\u003e RegressionMetrics {\n        RegressionMetrics {\n            mse: Self::mse(y_true, y_pred),\n            rmse: Self::rmse(y_true, y_pred),\n            mae: Self::mae(y_true, y_pred),\n            r_squared: Self::r_squared(y_true, y_pred),\n        }\n    }\n}\n\n/// Struct to hold all regression metrics.\n#[derive(Debug, Clone, Copy)]\npub struct RegressionMetrics {\n    pub mse: f32,\n    pub rmse: f32,\n    pub mae: f32,\n    pub r_squared: f32,\n}\n\nimpl RegressionMetrics {\n    /// Create a new RegressionMetrics instance.\n    pub fn new(mse: f32, mae: f32, r_squared: f32) -\u003e Self {\n        Self {\n            mse,\n            rmse: mse.sqrt(),\n            mae,\n            r_squared,\n        }\n    }\n}\n\n#[cfg(test)]\nmod tests {\n    use super::*;\n\n    #[test]\n    fn test_mse_perfect() {\n        let y_true = vec![1.0, 2.0, 3.0, 4.0];\n        let y_pred = vec![1.0, 2.0, 3.0, 4.0];\n        assert!((Metrics::mse(\u0026y_true, \u0026y_pred) - 0.0).abs() \u003c 1e-6);\n    }\n\n    #[test]\n    fn test_mse_error() {\n        let y_true = vec![1.0, 2.0, 3.0, 4.0];\n        let y_pred = vec![2.0, 3.0, 4.0, 5.0];\n        // Errors: [-1, -1, -1, -1], squared: [1, 1, 1, 1], mean: 1.0\n        assert!((Metrics::mse(\u0026y_true, \u0026y_pred) - 1.0).abs() \u003c 1e-6);\n    }\n\n    #[test]\n    fn test_mae() {\n        let y_true = vec![1.0, 2.0, 3.0, 4.0];\n        let y_pred = vec![2.0, 3.0, 4.0, 5.0];\n        // Errors: [-1, -1, -1, -1], abs: [1, 1, 1, 1], mean: 1.0\n        assert!((Metrics::mae(\u0026y_true, \u0026y_pred) - 1.0).abs() \u003c 1e-6);\n    }\n\n    #[test]\n    fn test_r_squared_perfect() {\n        let y_true = vec![1.0, 2.0, 3.0, 4.0];\n        let y_pred = vec![1.0, 2.0, 3.0, 4.0];\n        assert!((Metrics::r_squared(\u0026y_true, \u0026y_pred) - 1.0).abs() \u003c 1e-6);\n    }\n\n    #[test]\n    fn test_r_squared_mean() {\n        let y_true = vec![2.0, 2.0, 2.0, 2.0];\n        let y_pred = vec![2.0, 2.0, 2.0, 2.0];\n        // All predictions equal the mean (which is 2.0), so RÂ² should be 1.0\n        assert!((Metrics::r_squared(\u0026y_true, \u0026y_pred) - 1.0).abs() \u003c 1e-6);\n    }\n\n    #[test]\n    fn test_calculate_all() {\n        let y_true = vec![1.0, 2.0, 3.0, 4.0];\n        let y_pred = vec![1.0, 2.0, 3.0, 4.0];\n        let metrics = Metrics::calculate_all(\u0026y_true, \u0026y_pred);\n        assert!((metrics.mse - 0.0).abs() \u003c 1e-6);\n        assert!((metrics.mae - 0.0).abs() \u003c 1e-6);\n        assert!((metrics.r_squared - 1.0).abs() \u003c 1e-6);\n    }\n}\n","traces":[],"covered":0,"coverable":0},{"path":["/","home","victor","ML","machinelearne-rs","benchmarks","src","utils.rs"],"content":"use std::time::{Duration, Instant};\n\n/// Timer for measuring elapsed time.\n#[derive(Debug)]\npub struct Timer {\n    start: Option\u003cInstant\u003e,\n    total: Duration,\n}\n\nimpl Timer {\n    /// Create a new timer.\n    pub fn new() -\u003e Self {\n        Self {\n            start: None,\n            total: Duration::ZERO,\n        }\n    }\n\n    /// Start the timer.\n    pub fn start(\u0026mut self) {\n        self.start = Some(Instant::now());\n    }\n\n    /// Stop the timer and add the elapsed time to the total.\n    pub fn stop(\u0026mut self) -\u003e Duration {\n        if let Some(start) = self.start.take() {\n            let elapsed = start.elapsed();\n            self.total += elapsed;\n            elapsed\n        } else {\n            Duration::ZERO\n        }\n    }\n\n    /// Get the total elapsed time.\n    pub fn total(\u0026self) -\u003e Duration {\n        self.total\n    }\n\n    /// Get the total elapsed time in milliseconds.\n    pub fn total_ms(\u0026self) -\u003e f64 {\n        self.total.as_secs_f64() * 1000.0\n    }\n\n    /// Get the total elapsed time in seconds.\n    pub fn total_secs(\u0026self) -\u003e f64 {\n        self.total.as_secs_f64()\n    }\n\n    /// Reset the timer.\n    pub fn reset(\u0026mut self) {\n        self.start = None;\n        self.total = Duration::ZERO;\n    }\n}\n\nimpl Default for Timer {\n    fn default() -\u003e Self {\n        Self::new()\n    }\n}\n\n/// Run a function and measure its execution time.\n///\n/// # Arguments\n///\n/// * `f` - Function to execute\n///\n/// # Returns\n///\n/// A tuple of (result, elapsed_time)\npub fn time_fn\u003cF, R\u003e(f: F) -\u003e (R, Duration)\nwhere\n    F: FnOnce() -\u003e R,\n{\n    let start = Instant::now();\n    let result = f();\n    let elapsed = start.elapsed();\n    (result, elapsed)\n}\n\n/// Run a function multiple times and return the mean and std dev of execution times.\n///\n/// # Arguments\n///\n/// * `iterations` - Number of iterations to run\n/// * `f` - Function to execute\n///\n/// # Returns\n///\n/// A tuple of (results, mean_time_ms, std_dev_ms)\npub fn benchmark_fn\u003cF, R\u003e(iterations: usize, mut f: F) -\u003e (Vec\u003cR\u003e, f64, f64)\nwhere\n    F: FnMut() -\u003e R,\n{\n    let mut results = Vec::with_capacity(iterations);\n    let mut times = Vec::with_capacity(iterations);\n\n    for _ in 0..iterations {\n        let (result, elapsed) = time_fn(\u0026mut f);\n        results.push(result);\n        times.push(elapsed.as_secs_f64() * 1000.0);\n    }\n\n    let mean = times.iter().sum::\u003cf64\u003e() / times.len() as f64;\n    let variance = times.iter().map(|\u0026t| (t - mean).powi(2)).sum::\u003cf64\u003e() / times.len() as f64;\n    let std_dev = variance.sqrt();\n\n    (results, mean, std_dev)\n}\n\n/// Run a function multiple times with warmup and return statistics.\n///\n/// # Arguments\n///\n/// * `warmup` - Number of warmup iterations (not counted in results)\n/// * `iterations` - Number of measurement iterations\n/// * `f` - Function to execute\n///\n/// # Returns\n///\n/// A tuple of (results, mean_time_ms, std_dev_ms, min_ms, max_ms)\npub fn benchmark_with_warmup\u003cF, R\u003e(\n    warmup: usize,\n    iterations: usize,\n    mut f: F,\n) -\u003e (Vec\u003cR\u003e, f64, f64, f64, f64)\nwhere\n    F: FnMut() -\u003e R,\n{\n    // Warmup\n    for _ in 0..warmup {\n        let _ = f();\n    }\n\n    // Actual benchmarking\n    let mut results = Vec::with_capacity(iterations);\n    let mut times = Vec::with_capacity(iterations);\n\n    for _ in 0..iterations {\n        let start = Instant::now();\n        let result = f();\n        let elapsed = start.elapsed().as_secs_f64() * 1000.0;\n        results.push(result);\n        times.push(elapsed);\n    }\n\n    let mean = times.iter().sum::\u003cf64\u003e() / times.len() as f64;\n    let variance = times.iter().map(|\u0026t| (t - mean).powi(2)).sum::\u003cf64\u003e() / times.len() as f64;\n    let std_dev = variance.sqrt();\n    let min = times.iter().copied().fold(f64::INFINITY, f64::min);\n    let max = times.iter().copied().fold(f64::NEG_INFINITY, f64::max);\n\n    (results, mean, std_dev, min, max)\n}\n\n/// Statistics for benchmarking results.\n#[derive(Debug, Clone)]\npub struct BenchmarkStats {\n    pub mean_ms: f64,\n    pub std_dev_ms: f64,\n    pub min_ms: f64,\n    pub max_ms: f64,\n    pub median_ms: f64,\n    pub p95_ms: f64,\n    pub p99_ms: f64,\n}\n\nimpl BenchmarkStats {\n    /// Calculate statistics from a list of times in milliseconds.\n    pub fn from_times(mut times: Vec\u003cf64\u003e) -\u003e Self {\n        times.sort_by(|a, b| a.partial_cmp(b).unwrap());\n\n        let mean = times.iter().sum::\u003cf64\u003e() / times.len() as f64;\n        let variance = times.iter().map(|\u0026t| (t - mean).powi(2)).sum::\u003cf64\u003e() / times.len() as f64;\n        let std_dev = variance.sqrt();\n        let min = times.first().copied().unwrap_or(0.0);\n        let max = times.last().copied().unwrap_or(0.0);\n\n        let n = times.len();\n        let median = if n.is_multiple_of(2) {\n            (times[n / 2 - 1] + times[n / 2]) / 2.0\n        } else {\n            times[n / 2]\n        };\n\n        let p95_idx = ((n as f64 * 0.95) as usize).min(n - 1);\n        let p95 = times[p95_idx];\n\n        let p99_idx = ((n as f64 * 0.99) as usize).min(n - 1);\n        let p99 = times[p99_idx];\n\n        Self {\n            mean_ms: mean,\n            std_dev_ms: std_dev,\n            min_ms: min,\n            max_ms: max,\n            median_ms: median,\n            p95_ms: p95,\n            p99_ms: p99,\n        }\n    }\n}\n\n#[cfg(test)]\nmod tests {\n    use super::*;\n\n    #[test]\n    fn test_timer() {\n        let mut timer = Timer::new();\n        timer.start();\n        std::thread::sleep(Duration::from_millis(10));\n        let elapsed = timer.stop();\n        assert!(elapsed.as_millis() \u003e= 10);\n        assert!(timer.total_ms() \u003e= 10.0);\n    }\n\n    #[test]\n    fn test_benchmark_fn() {\n        let fn_to_test = || {\n            let mut sum = 0i32;\n            for i in 0..1000 {\n                sum += i;\n            }\n            sum\n        };\n\n        let (results, mean, std_dev) = benchmark_fn(10, fn_to_test);\n        assert_eq!(results.len(), 10);\n        assert!(mean \u003e 0.0);\n        assert!(std_dev \u003e= 0.0);\n    }\n\n    #[test]\n    fn test_benchmark_with_warmup() {\n        let fn_to_test = || 42;\n\n        let (results, mean, std_dev, min, max) = benchmark_with_warmup(5, 10, fn_to_test);\n        assert_eq!(results.len(), 10);\n        assert_eq!(results[0], 42);\n        assert!(mean \u003e= 0.0);\n        assert!(std_dev \u003e= 0.0);\n        assert!(max \u003e= min);\n    }\n\n    #[test]\n    fn test_benchmark_stats() {\n        let times = vec![1.0, 2.0, 3.0, 4.0, 5.0];\n        let stats = BenchmarkStats::from_times(times);\n\n        assert!((stats.mean_ms - 3.0).abs() \u003c 1e-6);\n        assert!((stats.median_ms - 3.0).abs() \u003c 1e-6);\n        assert!((stats.min_ms - 1.0).abs() \u003c 1e-6);\n        assert!((stats.max_ms - 5.0).abs() \u003c 1e-6);\n    }\n}\n","traces":[{"line":76,"address":[],"length":0,"stats":{"Line":0}},{"line":77,"address":[],"length":0,"stats":{"Line":0}},{"line":78,"address":[],"length":0,"stats":{"Line":0}},{"line":79,"address":[],"length":0,"stats":{"Line":0}},{"line":96,"address":[],"length":0,"stats":{"Line":0}},{"line":97,"address":[],"length":0,"stats":{"Line":0}},{"line":99,"address":[],"length":0,"stats":{"Line":0}},{"line":100,"address":[],"length":0,"stats":{"Line":0}},{"line":101,"address":[],"length":0,"stats":{"Line":0}},{"line":102,"address":[],"length":0,"stats":{"Line":0}},{"line":105,"address":[],"length":0,"stats":{"Line":0}},{"line":106,"address":[],"length":0,"stats":{"Line":0}},{"line":107,"address":[],"length":0,"stats":{"Line":0}},{"line":109,"address":[],"length":0,"stats":{"Line":0}},{"line":132,"address":[],"length":0,"stats":{"Line":0}},{"line":133,"address":[],"length":0,"stats":{"Line":0}},{"line":137,"address":[],"length":0,"stats":{"Line":0}},{"line":138,"address":[],"length":0,"stats":{"Line":0}},{"line":140,"address":[],"length":0,"stats":{"Line":0}},{"line":141,"address":[],"length":0,"stats":{"Line":0}},{"line":142,"address":[],"length":0,"stats":{"Line":0}},{"line":143,"address":[],"length":0,"stats":{"Line":0}},{"line":144,"address":[],"length":0,"stats":{"Line":0}},{"line":145,"address":[],"length":0,"stats":{"Line":0}},{"line":148,"address":[],"length":0,"stats":{"Line":0}},{"line":149,"address":[],"length":0,"stats":{"Line":0}},{"line":150,"address":[],"length":0,"stats":{"Line":0}},{"line":151,"address":[],"length":0,"stats":{"Line":0}},{"line":152,"address":[],"length":0,"stats":{"Line":0}},{"line":154,"address":[],"length":0,"stats":{"Line":0}}],"covered":0,"coverable":30},{"path":["/","home","victor","ML","machinelearne-rs","lib","examples","house_price_pipeline.rs"],"content":"//! Comprehensive ML Pipeline Example: House Price Prediction\n//!\n//! This example demonstrates a complete end-to-end ML workflow including:\n//! - Mixed-type dataset (numerical + categorical features)\n//! - Different preprocessing per column type using ColumnTransformer\n//! - Missing value imputation\n//! - Feature engineering with polynomial features\n//! - Model training\n//! - Pipeline serialization and loading\n//! - Inference on new data\n//!\n//! # Features:\n//! - Column 0: sqft (numerical, needs scaling)\n//! - Column 1: bedrooms (numerical, may have missing values)\n//! - Column 2: bathrooms (numerical, needs scaling)\n//! - Column 3: neighborhood (categorical, one-hot encoded)\n//! - Column 4: condition (ordinal, 0-3 scale, passed through)\n//!\n//! Run with: cargo run --example house_price_pipeline\n\nuse machinelearne_rs::{\n    backend::CpuBackend,\n    dataset::memory::InMemoryDataset,\n    loss::MSELoss,\n    model::linear::{LinearModel, LinearParams, SerializableLinearParams},\n    model::state::Fitted,\n    optimizer::SGD,\n    preprocessing::{\n        ColumnSpec, ColumnTransformer, FittedTransformer, ImputeStrategy, OneHotEncoder, Pipeline,\n        PolynomialFeatures, PredictivePipeline, SimpleImputer, StandardScaler, Transformer,\n    },\n    regularizers::NoRegularizer,\n    trainer::Trainer,\n    Tensor2D,\n};\nuse std::error::Error;\n\n// Feature indices\nconst SQFT: usize = 0;\nconst BEDROOMS: usize = 1;\nconst BATHROOMS: usize = 2;\nconst NEIGHBORHOOD: usize = 3;\nconst CONDITION: usize = 4;\n\nfn main() -\u003e Result\u003c(), Box\u003cdyn Error\u003e\u003e {\n    println!(\"=== House Price Prediction Pipeline ===\\n\");\n\n    // 1. Create synthetic training data\n    // Features: [sqft, bedrooms, bathrooms, neighborhood, condition]\n    // neighborhood: 0=downtown, 1=suburban, 2=rural\n    // condition: 0=poor, 1=fair, 2=good, 3=excellent\n    let x_train_raw: Vec\u003cVec\u003cf32\u003e\u003e = vec![\n        vec![1500.0, 3.0, 2.0, 0.0, 2.0], // downtown, good\n        vec![2000.0, 4.0, 3.0, 1.0, 3.0], // suburban, excellent\n        vec![1200.0, 2.0, 1.0, 2.0, 1.0], // rural, fair\n        vec![1800.0, 3.0, 2.0, 0.0, 3.0], // downtown, excellent\n        vec![2200.0, 4.0, 3.0, 1.0, 2.0], // suburban, good\n        vec![1100.0, 2.0, 1.0, 2.0, 0.0], // rural, poor\n        vec![2500.0, 5.0, 4.0, 0.0, 3.0], // downtown, excellent\n        vec![1400.0, 3.0, 2.0, 1.0, 1.0], // suburban, fair\n        // Some with missing values (NaN for bedrooms)\n        vec![1600.0, f32::NAN, 2.0, 0.0, 2.0], // missing bedrooms\n        vec![1900.0, f32::NAN, 3.0, 1.0, 3.0], // missing bedrooms\n    ];\n\n    // Target: house prices (in thousands)\n    let y_train: Vec\u003cf32\u003e = vec![\n        350.0, 450.0, 180.0, 420.0, 480.0, 150.0, 550.0, 280.0, 360.0, 410.0,\n    ];\n\n    println!(\n        \"Training data: {} samples with {} features each\",\n        x_train_raw.len(),\n        x_train_raw[0].len()\n    );\n\n    // 2. Convert to tensor\n    let flat_x: Vec\u003cf32\u003e = x_train_raw.iter().flatten().copied().collect();\n    let x_train = Tensor2D::\u003cCpuBackend\u003e::new(flat_x, x_train_raw.len(), 5);\n\n    // 3. Build ColumnTransformer for heterogeneous preprocessing\n    println!(\"\\nBuilding ColumnTransformer...\");\n\n    // Numerical columns (sqft, bedrooms, bathrooms): impute + scale\n    let numerical_pipeline = Pipeline::\u003cCpuBackend\u003e::new()\n        .add_simple_imputer(SimpleImputer::new(ImputeStrategy::Mean))\n        .add_standard_scaler(StandardScaler::new());\n\n    // Categorical column (neighborhood): one-hot encoding\n    let categorical_encoder = OneHotEncoder::\u003cCpuBackend\u003e::new();\n\n    // Ordinal column (condition): scale (for ordinal features, scaling is acceptable)\n    let ordinal_scaler = StandardScaler::\u003cCpuBackend\u003e::new();\n\n    // Build column transformer\n    let column_transformer = ColumnTransformer::\u003cCpuBackend\u003e::new()\n        // Numerical: sqft, bedrooms, bathrooms (columns 0, 1, 2)\n        .add_pipeline(\n            numerical_pipeline,\n            ColumnSpec::Indices(vec![SQFT, BEDROOMS, BATHROOMS]),\n        )\n        // Categorical: neighborhood (column 3) - one-hot encoding\n        .add_one_hot_encoder(categorical_encoder, ColumnSpec::Indices(vec![NEIGHBORHOOD]))\n        // Ordinal: condition (column 4) - scale\n        .add_standard_scaler(ordinal_scaler, ColumnSpec::Indices(vec![CONDITION]));\n\n    // 4. Fit the column transformer\n    println!(\"Fitting column transformer...\");\n    let fitted_ct = column_transformer.fit(\u0026x_train)?;\n    println!(\"  Input features: {}\", fitted_ct.n_features_in());\n    println!(\"  Output features: {}\", fitted_ct.n_features_out());\n\n    // 5. Transform training data\n    let x_preprocessed = fitted_ct.transform(\u0026x_train)?;\n\n    // 6. Add polynomial features (degree 2, interaction only to avoid too many features)\n    println!(\"\\nGenerating polynomial features...\");\n    let poly = PolynomialFeatures::\u003cCpuBackend\u003e::new()\n        .with_degree(2)\n        .with_include_bias(false)\n        .with_interaction_only(true);\n\n    let fitted_poly = poly.fit(\u0026x_preprocessed)?;\n    println!(\n        \"  Polynomial features: {} -\u003e {}\",\n        fitted_poly.n_features_in(),\n        fitted_poly.n_features_out()\n    );\n\n    let x_final = fitted_poly.transform(\u0026x_preprocessed)?;\n\n    // 7. Train the model\n    println!(\"\\nTraining linear regression model...\");\n    let n_features = fitted_poly.n_features_out();\n\n    // Create dataset from preprocessed data\n    let x_final_vec: Vec\u003cVec\u003cf32\u003e\u003e = {\n        let (rows, cols) = x_final.shape();\n        let flat = x_final.ravel().to_vec();\n        (0..rows)\n            .map(|r| (0..cols).map(|c| flat[r * cols + c] as f32).collect())\n            .collect()\n    };\n    let dataset = InMemoryDataset::new(x_final_vec, y_train.clone())?;\n\n    // Build and train\n    let model = machinelearne_rs::model::linear::LinearRegression::\u003cCpuBackend\u003e::new(n_features);\n    let loss = MSELoss;\n    let optimizer = SGD::new(0.001);\n    let regularizer = NoRegularizer;\n\n    let trainer = Trainer::builder(loss, optimizer, regularizer)\n        .batch_size(5)\n        .max_epochs(2000)\n        .build();\n\n    let fitted_model = trainer.fit(model, \u0026dataset)?;\n    println!(\"  Model trained successfully!\");\n\n    // 8. Create and save the complete pipeline\n    println!(\"\\nSaving complete pipeline...\");\n    let pipeline = PredictivePipeline::new(fitted_ct, Some(fitted_poly), fitted_model);\n\n    let temp_file = std::env::temp_dir().join(\"house_price_pipeline.bin\");\n    pipeline.save_to_file(\u0026temp_file)?;\n    println!(\"  Pipeline saved to: {:?}\", temp_file);\n\n    // 9. Load the pipeline (simulating deployment)\n    println!(\"\\nLoading pipeline for inference...\");\n    let loaded_pipeline =\n        PredictivePipeline::\u003cCpuBackend, LinearModel\u003cCpuBackend, Fitted\u003e\u003e::load_from_file(\n            \u0026temp_file,\n            |bytes| {\n                let serial_params: SerializableLinearParams =\n                    bincode::deserialize(bytes).map_err(|e| {\n                        machinelearne_rs::preprocessing::PreprocessingError::SerializationError(\n                            e.to_string(),\n                        )\n                    })?;\n                let params = LinearParams::try_from(serial_params).map_err(|e| {\n                    machinelearne_rs::preprocessing::PreprocessingError::SerializationError(\n                        e.to_string(),\n                    )\n                })?;\n                Ok(\u003cLinearModel\u003cCpuBackend, Fitted\u003e\u003e::new(params))\n            },\n        )?;\n    println!(\"  Pipeline loaded successfully!\");\n    println!(\n        \"  Loaded pipeline expects {} input features\",\n        loaded_pipeline.n_features_in()\n    );\n\n    // 10. Make predictions on new data\n    println!(\"\\n=== Predictions on New Houses ===\\n\");\n\n    // New house 1: 1700 sqft, 3 bed, 2 bath, suburban, excellent condition\n    let house1 = Tensor2D::\u003cCpuBackend\u003e::new(vec![1700.0, 3.0, 2.0, 1.0, 3.0], 1, 5);\n    let price1 = loaded_pipeline.predict(\u0026house1)?;\n    println!(\n        \"House 1 (suburban, 3bed/2bath, 1700sqft, excellent): ${:.0}k\",\n        price1.to_vec()[0]\n    );\n\n    // New house 2: 1300 sqft, 2 bed, 1 bath, downtown, good condition\n    let house2 = Tensor2D::\u003cCpuBackend\u003e::new(vec![1300.0, 2.0, 1.0, 0.0, 2.0], 1, 5);\n    let price2 = loaded_pipeline.predict(\u0026house2)?;\n    println!(\n        \"House 2 (downtown, 2bed/1bath, 1300sqft, good): ${:.0}k\",\n        price2.to_vec()[0]\n    );\n\n    // New house 3: 2400 sqft, with missing bedrooms, rural, fair condition\n    let house3 = Tensor2D::\u003cCpuBackend\u003e::new(vec![2400.0, f32::NAN, 3.0, 2.0, 1.0], 1, 5);\n    let price3 = loaded_pipeline.predict(\u0026house3)?;\n    println!(\n        \"House 3 (rural, ?bed/3bath, 2400sqft, fair, missing data): ${:.0}k\",\n        price3.to_vec()[0]\n    );\n\n    // Batch prediction: multiple houses at once\n    let houses_batch = Tensor2D::\u003cCpuBackend\u003e::new(\n        vec![\n            1500.0, 3.0, 2.0, 0.0, 2.0, // house A\n            2000.0, 4.0, 3.0, 1.0, 3.0, // house B\n            1200.0, 2.0, 1.0, 2.0, 1.0, // house C\n        ],\n        3,\n        5,\n    );\n    let prices_batch = loaded_pipeline.predict(\u0026houses_batch)?;\n    println!(\"\\nBatch predictions:\");\n    let prices = prices_batch.to_vec();\n    for (i, price) in prices.iter().enumerate() {\n        println!(\"  House {}: ${:.0}k\", i + 1, price);\n    }\n\n    // Cleanup\n    std::fs::remove_file(\u0026temp_file).ok();\n\n    println!(\"\\n=== Pipeline Complete ===\");\n    Ok(())\n}\n","traces":[],"covered":0,"coverable":0},{"path":["/","home","victor","ML","machinelearne-rs","lib","examples","titanic_pipeline.rs"],"content":"//! Titanic Survival Prediction Pipeline\n//!\n//! This example demonstrates a complete ML workflow on the classic Titanic dataset,\n//! showcasing all preprocessing capabilities:\n//! - Mixed feature types (numerical, categorical, ordinal)\n//! - Missing value imputation\n//! - Feature scaling and encoding\n//! - Binary classification with accuracy metrics\n//! - Pipeline serialization\n//!\n//! Run with: cargo run --example titanic_pipeline\n\nuse machinelearne_rs::{\n    backend::CpuBackend,\n    dataset::memory::InMemoryDataset,\n    loss::BCEWithLogitsLoss,\n    model::linear::{LinearModel, LinearParams, SerializableLinearParams},\n    model::state::Fitted,\n    model::InferenceModel,\n    optimizer::SGD,\n    preprocessing::{\n        ColumnSpec, ColumnTransformer, FittedTransformer, ImputeStrategy, OneHotEncoder, Pipeline,\n        PredictivePipeline, SimpleImputer, StandardScaler, Transformer,\n    },\n    regularizers::NoRegularizer,\n    trainer::Trainer,\n    Tensor2D,\n};\nuse std::error::Error;\n\n// Feature indices for the raw data\n// [Pclass, Sex, Age, SibSp, Parch, Fare, Embarked]\nconst PCLASS: usize = 0;\nconst SEX: usize = 1;\nconst AGE: usize = 2;\nconst SIBSP: usize = 3;\nconst PARCH: usize = 4;\nconst FARE: usize = 5;\nconst EMBARKED: usize = 6;\n\n/// Titanic passenger data (subset of the classic dataset).\n/// Features: [Pclass, Sex, Age, SibSp, Parch, Fare, Embarked]\n/// - Pclass: 1=1st, 2=2nd, 3=3rd (ordinal)\n/// - Sex: 0=male, 1=female (will be one-hot encoded)\n/// - Age: in years (has missing values as f32::NAN)\n/// - SibSp: number of siblings/spouses aboard\n/// - Parch: number of parents/children aboard\n/// - Fare: ticket fare\n/// - Embarked: 0=C, 1=Q, 2=S (Cherbourg, Queenstown, Southampton - will be one-hot encoded)\n/// Target: Survived (0=no, 1=yes)\nfn get_titanic_data() -\u003e (Vec\u003cVec\u003cf32\u003e\u003e, Vec\u003cf32\u003e) {\n    let features: Vec\u003cVec\u003cf32\u003e\u003e = vec![\n        // First class passengers\n        vec![1.0, 1.0, 29.0, 0.0, 0.0, 211.3375, 0.0], // Female, survived\n        vec![1.0, 0.0, 0.9167, 1.0, 2.0, 151.55, 0.0], // Male infant, survived\n        vec![1.0, 1.0, 2.0, 1.0, 2.0, 151.55, 0.0],    // Female child, survived\n        vec![1.0, 0.0, 30.0, 1.0, 0.0, 164.8667, 0.0], // Male, survived\n        vec![1.0, 1.0, 25.0, 1.0, 0.0, 151.55, 0.0],   // Female, survived\n        vec![1.0, 0.0, 48.0, 0.0, 0.0, 26.55, 1.0],    // Male, didn't survive\n        vec![1.0, 0.0, 36.0, 1.0, 0.0, 135.6333, 1.0], // Male, survived\n        vec![1.0, 1.0, 27.0, 1.0, 0.0, 153.4625, 1.0], // Female, survived\n        vec![1.0, 0.0, 22.0, 0.0, 0.0, 135.6333, 1.0], // Male, didn't survive\n        vec![1.0, 1.0, 38.0, 0.0, 0.0, 80.0, 2.0],     // Female, survived\n        // Second class passengers\n        vec![2.0, 1.0, 29.0, 0.0, 2.0, 23.0, 2.0], // Female, survived\n        vec![2.0, 0.0, 32.0, 0.0, 0.0, 10.5, 2.0], // Male, didn't survive\n        vec![2.0, 1.0, 24.0, 0.0, 0.0, 13.0, 2.0], // Female, survived\n        vec![2.0, 0.0, 36.0, 0.0, 0.0, 13.0, 2.0], // Male, didn't survive\n        vec![2.0, 1.0, 28.0, 0.0, 0.0, 12.65, 2.0], // Female, survived\n        vec![2.0, 0.0, 25.0, 0.0, 0.0, 13.0, 2.0], // Male, didn't survive\n        vec![2.0, 1.0, 18.0, 0.0, 1.0, 33.0, 2.0], // Female, survived\n        vec![2.0, 0.0, 19.0, 1.0, 0.0, 26.0, 2.0], // Male, didn't survive\n        vec![2.0, 1.0, 23.0, 0.0, 0.0, 10.5, 2.0], // Female, survived\n        vec![2.0, 0.0, 34.0, 0.0, 0.0, 13.0, 2.0], // Male, didn't survive\n        // Third class passengers\n        vec![3.0, 0.0, 22.0, 0.0, 0.0, 7.25, 2.0], // Male, didn't survive\n        vec![3.0, 1.0, 26.0, 0.0, 0.0, 7.925, 2.0], // Female, survived\n        vec![3.0, 0.0, 24.0, 0.0, 0.0, 8.4583, 0.0], // Male, didn't survive\n        vec![3.0, 1.0, 21.0, 0.0, 0.0, 7.75, 1.0], // Female, survived\n        vec![3.0, 0.0, 22.0, 0.0, 0.0, 7.8958, 2.0], // Male, didn't survive\n        vec![3.0, 1.0, 27.0, 0.0, 2.0, 21.075, 2.0], // Female, survived\n        vec![3.0, 0.0, 30.0, 0.0, 0.0, 7.8958, 2.0], // Male, didn't survive\n        vec![3.0, 1.0, 18.0, 0.0, 0.0, 7.775, 2.0], // Female, survived\n        vec![3.0, 0.0, 19.0, 0.0, 0.0, 7.8958, 2.0], // Male, didn't survive\n        vec![3.0, 1.0, 15.0, 0.0, 0.0, 8.0292, 1.0], // Female, survived\n        // More diverse examples with missing ages\n        vec![1.0, 1.0, f32::NAN, 1.0, 0.0, 78.85, 2.0], // Female, missing age, survived\n        vec![3.0, 0.0, f32::NAN, 0.0, 0.0, 7.75, 1.0],  // Male, missing age, didn't survive\n        vec![2.0, 1.0, f32::NAN, 0.0, 0.0, 10.5, 2.0],  // Female, missing age, survived\n        vec![3.0, 0.0, f32::NAN, 1.0, 0.0, 15.5, 2.0],  // Male, missing age, didn't survive\n        vec![1.0, 0.0, 45.0, 0.0, 0.0, 28.7125, 2.0],   // Male, didn't survive\n        vec![3.0, 1.0, 31.0, 1.0, 0.0, 18.0, 2.0],      // Female, survived\n        vec![1.0, 0.0, 54.0, 0.0, 0.0, 51.8625, 2.0],   // Male, didn't survive\n        vec![3.0, 1.0, 4.0, 3.0, 1.0, 31.275, 2.0],     // Female child, survived\n        vec![2.0, 0.0, 29.0, 0.0, 0.0, 13.0, 2.0],      // Male, didn't survive\n        vec![3.0, 0.0, 25.0, 1.0, 0.0, 7.775, 2.0],     // Male, didn't survive\n        // Additional samples for better training\n        vec![1.0, 1.0, 35.0, 0.0, 0.0, 128.0, 2.0], // Female, survived\n        vec![3.0, 0.0, 28.0, 0.0, 0.0, 7.05, 2.0],  // Male, didn't survive\n        vec![2.0, 1.0, 30.0, 0.0, 0.0, 26.0, 2.0],  // Female, survived\n        vec![3.0, 0.0, 20.0, 0.0, 0.0, 8.05, 2.0],  // Male, didn't survive\n        vec![1.0, 1.0, 49.0, 0.0, 0.0, 110.8833, 0.0], // Female, survived\n        vec![3.0, 0.0, 21.0, 2.0, 0.0, 11.5, 2.0],  // Male, didn't survive\n        vec![2.0, 0.0, 39.0, 0.0, 0.0, 26.0, 2.0],  // Male, didn't survive\n        vec![3.0, 1.0, 16.0, 1.0, 1.0, 20.2125, 2.0], // Female, survived\n        vec![1.0, 0.0, 80.0, 0.0, 0.0, 30.0, 2.0],  // Male, survived (oldest passenger)\n        vec![3.0, 0.0, 33.0, 0.0, 0.0, 7.8958, 2.0], // Male, didn't survive\n    ];\n\n    let targets: Vec\u003cf32\u003e = vec![\n        // First class\n        1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, // Second class\n        1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, // Third class\n        0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, // Missing ages\n        1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, // Additional\n        1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0,\n    ];\n\n    (features, targets)\n}\n\n/// Calculate sigmoid for binary classification probability.\nfn sigmoid(x: f64) -\u003e f64 {\n    1.0 / (1.0 + (-x).exp())\n}\n\n/// Calculate classification metrics.\nfn calculate_metrics(predictions: \u0026[bool], targets: \u0026[bool]) -\u003e (f64, f64, f64, f64) {\n    let mut tp = 0usize; // True positives\n    let mut tn = 0usize; // True negatives\n    let mut fp = 0usize; // False positives\n    let mut fn_ = 0usize; // False negatives\n\n    for (pred, target) in predictions.iter().zip(targets.iter()) {\n        match (pred, target) {\n            (true, true) =\u003e tp += 1,\n            (false, false) =\u003e tn += 1,\n            (true, false) =\u003e fp += 1,\n            (false, true) =\u003e fn_ += 1,\n        }\n    }\n\n    let accuracy = (tp + tn) as f64 / predictions.len() as f64;\n    let precision = if tp + fp == 0 {\n        0.0\n    } else {\n        tp as f64 / (tp + fp) as f64\n    };\n    let recall = if tp + fn_ == 0 {\n        0.0\n    } else {\n        tp as f64 / (tp + fn_) as f64\n    };\n    let f1 = if precision + recall == 0.0 {\n        0.0\n    } else {\n        2.0 * precision * recall / (precision + recall)\n    };\n\n    (accuracy, precision, recall, f1)\n}\n\nfn main() -\u003e Result\u003c(), Box\u003cdyn Error\u003e\u003e {\n    println!(\"=== Titanic Survival Prediction Pipeline ===\\n\");\n\n    // 1. Load data\n    println!(\"Loading Titanic dataset...\");\n    let (features, targets) = get_titanic_data();\n    let n_samples = features.len();\n    println!(\"  {} passengers loaded\", n_samples);\n    println!(\n        \"  {} survivors ({}%)\",\n        targets.iter().filter(|\u0026\u0026t| t == 1.0).count(),\n        100.0 * targets.iter().filter(|\u0026\u0026t| t == 1.0).count() as f64 / n_samples as f64\n    );\n\n    // 2. Train/test split (80/20)\n    let split_idx = (n_samples as f64 * 0.8) as usize;\n    let (train_features, test_features) = features.split_at(split_idx);\n    let (train_targets, test_targets) = targets.split_at(split_idx);\n\n    println!(\n        \"\\nTrain/test split: {}/{}\",\n        train_features.len(),\n        test_features.len()\n    );\n\n    // 3. Convert to tensors\n    let flat_train: Vec\u003cf32\u003e = train_features.iter().flatten().copied().collect();\n    let x_train = Tensor2D::\u003cCpuBackend\u003e::new(flat_train, train_features.len(), 7);\n    let y_train = train_targets.to_vec();\n\n    let flat_test: Vec\u003cf32\u003e = test_features.iter().flatten().copied().collect();\n    let x_test = Tensor2D::\u003cCpuBackend\u003e::new(flat_test, test_features.len(), 7);\n\n    // 4. Build preprocessing pipeline\n    println!(\"\\nBuilding preprocessing pipeline...\");\n\n    // Numerical features (Age, SibSp, Parch, Fare): impute + scale\n    let numerical_pipeline = Pipeline::\u003cCpuBackend\u003e::new()\n        .add_simple_imputer(SimpleImputer::new(ImputeStrategy::Mean))\n        .add_standard_scaler(StandardScaler::new());\n\n    // Categorical features (Sex, Embarked): one-hot encoding\n    // Ordinal feature (Pclass): scale (treat as continuous for simplicity)\n\n    let column_transformer = ColumnTransformer::\u003cCpuBackend\u003e::new()\n        // Pclass (ordinal, column 0) - scale\n        .add_standard_scaler(StandardScaler::new(), ColumnSpec::Indices(vec![PCLASS]))\n        // Sex (categorical, column 1) - one-hot encode\n        .add_one_hot_encoder(OneHotEncoder::new(), ColumnSpec::Indices(vec![SEX]))\n        // Age, SibSp, Parch, Fare (numerical, columns 2,3,4,5) - impute + scale\n        .add_pipeline(\n            numerical_pipeline,\n            ColumnSpec::Indices(vec![AGE, SIBSP, PARCH, FARE]),\n        )\n        // Embarked (categorical, column 6) - one-hot encode\n        .add_one_hot_encoder(OneHotEncoder::new(), ColumnSpec::Indices(vec![EMBARKED]));\n\n    // 5. Fit preprocessor\n    println!(\"Fitting preprocessor...\");\n    let fitted_ct = column_transformer.fit(\u0026x_train)?;\n    println!(\"  Input features: {}\", fitted_ct.n_features_in());\n    println!(\"  Output features: {}\", fitted_ct.n_features_out());\n\n    // 6. Transform data\n    let x_train_processed = fitted_ct.transform(\u0026x_train)?;\n    let x_test_processed = fitted_ct.transform(\u0026x_test)?;\n\n    // 7. Optionally add polynomial features (comment out for simpler model)\n    // Uncomment the following block to use polynomial features:\n    /*\n    println!(\"\\nAdding polynomial features...\");\n    let poly = PolynomialFeatures::\u003cCpuBackend\u003e::new()\n        .with_degree(2)\n        .with_include_bias(false)\n        .with_interaction_only(true);\n    let fitted_poly = poly.fit(\u0026x_train_processed)?;\n    println!(\"  Features after polynomial: {}\", fitted_poly.n_features_out());\n    let x_train_final = fitted_poly.transform(\u0026x_train_processed)?;\n    let x_test_final = fitted_poly.transform(\u0026x_test_processed)?;\n    */\n    // For now, use linear features\n    let fitted_poly = None;\n    let x_train_final = x_train_processed;\n    let x_test_final = x_test_processed;\n\n    // 8. Create dataset for training\n    let (n_rows, n_features) = x_train_final.shape();\n    let x_train_vec: Vec\u003cVec\u003cf32\u003e\u003e = {\n        let flat = x_train_final.ravel().to_vec();\n        (0..n_rows)\n            .map(|r| {\n                (0..n_features)\n                    .map(|c| flat[r * n_features + c] as f32)\n                    .collect()\n            })\n            .collect()\n    };\n\n    let dataset = InMemoryDataset::new(x_train_vec, y_train.clone())?;\n\n    // 9. Train model (Logistic Regression using BCEWithLogitsLoss)\n    println!(\"\\nTraining logistic regression model...\");\n    let model = machinelearne_rs::model::linear::LinearRegression::\u003cCpuBackend\u003e::new(n_features);\n    let loss = BCEWithLogitsLoss;\n    let optimizer = SGD::new(0.5);\n    let regularizer = NoRegularizer;\n\n    let trainer = Trainer::builder(loss, optimizer, regularizer)\n        .batch_size(10)\n        .max_epochs(1000)\n        .build();\n\n    let fitted_model = trainer.fit(model, \u0026dataset)?;\n\n    // 10. Evaluate on test set\n    println!(\"\\n=== Evaluation on Test Set ===\\n\");\n\n    // Get predictions\n    let test_logits = fitted_model.predict_batch(\u0026x_test_final);\n    let logits_vec = test_logits.to_vec();\n\n    // Convert to binary predictions (threshold at 0.5 probability)\n    let predictions: Vec\u003cbool\u003e = logits_vec\n        .iter()\n        .map(|\u0026logit| sigmoid(logit as f64) \u003e= 0.5)\n        .collect();\n    let actual: Vec\u003cbool\u003e = test_targets.iter().map(|\u0026t| t == 1.0).collect();\n\n    // Calculate metrics\n    let (accuracy, precision, recall, f1) = calculate_metrics(\u0026predictions, \u0026actual);\n\n    println!(\"Predictions vs Actual:\");\n    println!(\n        \"{:\u003c5} {:\u003c10} {:\u003c10} {:\u003c10}\",\n        \"Idx\", \"Prob\", \"Pred\", \"Actual\"\n    );\n    println!(\"{}\", \"-\".repeat(40));\n    for (i, (logit, \u0026actual_val)) in logits_vec.iter().zip(test_targets.iter()).enumerate() {\n        let prob = sigmoid(*logit as f64);\n        let pred = if prob \u003e= 0.5 { \"Survived\" } else { \"Died\" };\n        let act = if actual_val == 1.0 {\n            \"Survived\"\n        } else {\n            \"Died\"\n        };\n        println!(\"{:\u003c5} {:\u003c10.3} {:\u003c10} {:\u003c10}\", i, prob, pred, act);\n    }\n\n    println!(\"\\n=== Classification Metrics ===\");\n    println!(\"Accuracy:  {:.1}%\", accuracy * 100.0);\n    println!(\"Precision: {:.1}%\", precision * 100.0);\n    println!(\"Recall:    {:.1}%\", recall * 100.0);\n    println!(\"F1 Score:  {:.1}%\", f1 * 100.0);\n\n    // 11. Save complete pipeline\n    println!(\"\\n=== Saving Pipeline ===\");\n    let pipeline = PredictivePipeline::new(fitted_ct, fitted_poly, fitted_model);\n\n    let temp_file = std::env::temp_dir().join(\"titanic_pipeline.bin\");\n    pipeline.save_to_file(\u0026temp_file)?;\n    println!(\"Pipeline saved to: {:?}\", temp_file);\n\n    // 12. Load and verify\n    println!(\"\\nLoading pipeline...\");\n    let loaded_pipeline =\n        PredictivePipeline::\u003cCpuBackend, LinearModel\u003cCpuBackend, Fitted\u003e\u003e::load_from_file(\n            \u0026temp_file,\n            |bytes| {\n                let serial_params: SerializableLinearParams =\n                    bincode::deserialize(bytes).map_err(|e| {\n                        machinelearne_rs::preprocessing::PreprocessingError::SerializationError(\n                            e.to_string(),\n                        )\n                    })?;\n                let params = LinearParams::try_from(serial_params).map_err(|e| {\n                    machinelearne_rs::preprocessing::PreprocessingError::SerializationError(\n                        e.to_string(),\n                    )\n                })?;\n                Ok(\u003cLinearModel\u003cCpuBackend, Fitted\u003e\u003e::new(params))\n            },\n        )?;\n    println!(\"Pipeline loaded successfully!\");\n    println!(\n        \"Expected input features: {}\",\n        loaded_pipeline.n_features_in()\n    );\n\n    // 13. Demo prediction for a new passenger\n    println!(\"\\n=== Demo: Predict for New Passenger ===\");\n    // New passenger: 1st class, female, 25 years old, no family, $100 fare, embarked at Southampton\n    let new_passenger =\n        Tensor2D::\u003cCpuBackend\u003e::new(vec![1.0, 1.0, 25.0, 0.0, 0.0, 100.0, 2.0], 1, 7);\n    let logit = loaded_pipeline.predict(\u0026new_passenger)?;\n    let prob = sigmoid(logit.to_vec()[0] as f64);\n    println!(\"New passenger: 1st class, female, 25yo, no family, $100 fare, Southampton\");\n    println!(\"Survival probability: {:.1}%\", prob * 100.0);\n\n    // Another passenger: 3rd class, male, 30 years old, no family, $10 fare, Southampton\n    let another_passenger =\n        Tensor2D::\u003cCpuBackend\u003e::new(vec![3.0, 0.0, 30.0, 0.0, 0.0, 10.0, 2.0], 1, 7);\n    let logit2 = loaded_pipeline.predict(\u0026another_passenger)?;\n    let prob2 = sigmoid(logit2.to_vec()[0] as f64);\n    println!(\"\\nAnother passenger: 3rd class, male, 30yo, no family, $10 fare, Southampton\");\n    println!(\"Survival probability: {:.1}%\", prob2 * 100.0);\n\n    // Cleanup\n    std::fs::remove_file(\u0026temp_file).ok();\n\n    println!(\"\\n=== Pipeline Complete ===\");\n    Ok(())\n}\n","traces":[],"covered":0,"coverable":0},{"path":["/","home","victor","ML","machinelearne-rs","lib","examples","train_linear.rs"],"content":"// examples/train_linear.rs Ð¸Ð»Ð¸ Ð² ÑÐµÑÑÐ°Ñ\nuse machinelearne_rs::{\n    dataset::memory::InMemoryDataset,\n    loss::MSELoss,\n    model::{linear::LinearRegressor, InferenceModel},\n    optimizer::SGD,\n    regularizers::NoRegularizer,\n    trainer::Trainer,\n    CpuBackend, Tensor1D,\n};\n\nfn main() {\n    let model = LinearRegressor::new(2); // 2 ÑÐ¸ÑÐ¸\n    let loss = MSELoss;\n    let opt = SGD::new(0.1);\n    let reg = NoRegularizer;\n    let trainer = Trainer::builder(loss, opt, reg)\n        .batch_size(20)\n        .max_epochs(500)\n        .build();\n    let x = vec![\n        vec![1.0, 1.0], // y â 1*1 + 2*1 = 3\n        vec![2.0, 1.0], // y â 2 + 2 = 4\n        vec![1.0, 2.0], // y â 1 + 4 = 5\n        vec![2.0, 2.0], // y â 2 + 4 = 6\n        vec![3.0, 3.0], // â Ð²ÑÐ±ÑÐ¾Ñ Ð¿Ð¾ y!\n    ];\n    let y = vec![3.0, 4.0, 5.0, 6.0, 30.0];\n    let dataset = InMemoryDataset::new(x, y).unwrap();\n\n    let fitted_model = trainer.fit(model, \u0026dataset).unwrap();\n    let inp = Tensor1D::\u003cCpuBackend\u003e::new((\u0026[4.0, 5.0]).to_vec());\n    let pred = fitted_model.predict(\u0026inp);\n    println!(\"Prediction: {:?}\", pred);\n}\n","traces":[],"covered":0,"coverable":0},{"path":["/","home","victor","ML","machinelearne-rs","lib","examples","train_linear_l2.rs"],"content":"// examples/train_linear.rs Ð¸Ð»Ð¸ Ð² ÑÐµÑÑÐ°Ñ\nuse machinelearne_rs::{\n    dataset::memory::InMemoryDataset, loss::MSELoss, model::linear::LinearRegressor,\n    model::InferenceModel, optimizer::SGD, regularizers::L2, trainer::Trainer, CpuBackend,\n    Tensor1D,\n};\n\nfn main() {\n    let model = LinearRegressor::new(2); // 2 ÑÐ¸ÑÐ¸\n    let loss = MSELoss;\n    let opt = SGD::new(0.01);\n    let reg = L2::new(0.1);\n    let trainer = Trainer::builder(loss, opt, reg)\n        .batch_size(20)\n        .max_epochs(500)\n        .build();\n    let x = vec![vec![1.0, 2.0], vec![2.0, 3.0], vec![3.0, 4.0]];\n    let y = vec![3.0, 5.0, 7.0];\n\n    let dataset = InMemoryDataset::new(x, y).unwrap();\n\n    let fitted_model = trainer.fit(model, \u0026dataset).unwrap();\n    println!(\n        \"Prediction: {:?}\",\n        fitted_model.predict(\u0026Tensor1D::\u003cCpuBackend\u003e::new((\u0026[4.0, 5.0]).to_vec()))\n    );\n}\n","traces":[],"covered":0,"coverable":0},{"path":["/","home","victor","ML","machinelearne-rs","lib","examples","train_linear_mae.rs"],"content":"// examples/train_linear.rs Ð¸Ð»Ð¸ Ð² ÑÐµÑÑÐ°Ñ\nuse machinelearne_rs::{\n    dataset::memory::InMemoryDataset, loss::MAELoss, model::linear::LinearRegressor,\n    model::InferenceModel, optimizer::SGD, regularizers::NoRegularizer, trainer::Trainer,\n    CpuBackend, Tensor1D,\n};\n\nfn main() {\n    let model = LinearRegressor::new(2); // 2 ÑÐ¸ÑÐ¸\n    let loss = MAELoss;\n    let opt = SGD::new(0.01);\n    let reg = NoRegularizer;\n    let trainer = Trainer::builder(loss, opt, reg)\n        .batch_size(20)\n        .max_epochs(500)\n        .build();\n    let x = vec![\n        vec![1.0, 1.0], // y â 1*1 + 2*1 = 3\n        vec![2.0, 1.0], // y â 2 + 2 = 4\n        vec![1.0, 2.0], // y â 1 + 4 = 5\n        vec![2.0, 2.0], // y â 2 + 4 = 6\n        vec![3.0, 3.0], // â Ð²ÑÐ±ÑÐ¾Ñ Ð¿Ð¾ y!\n    ];\n    let y = vec![3.0, 4.0, 5.0, 6.0, 30.0];\n\n    let dataset = InMemoryDataset::new(x, y).unwrap();\n\n    let fitted_model = trainer.fit(model, \u0026dataset).unwrap();\n    println!(\n        \"Prediction: {:?}\",\n        fitted_model.predict(\u0026Tensor1D::\u003cCpuBackend\u003e::new((\u0026[4.0, 5.0]).to_vec()))\n    );\n}\n","traces":[],"covered":0,"coverable":0},{"path":["/","home","victor","ML","machinelearne-rs","lib","examples","train_linear_ndarray.rs"],"content":"// examples/train_linear.rs Ð¸Ð»Ð¸ Ð² ÑÐµÑÑÐ°Ñ\n\nuse machinelearne_rs::{\n    dataset::memory::InMemoryDataset,\n    loss::MSELoss,\n    model::{linear::LinearRegression, InferenceModel},\n    optimizer::SGD,\n    regularizers::NoRegularizer,\n    trainer::Trainer,\n    Tensor1D,\n};\n#[cfg(feature = \"ndarray\")]\nfn main() {\n    use machinelearne_rs::backend::NdarrayBackend;\n    let model = LinearRegression::\u003cNdarrayBackend\u003e::new(2); // 2 ÑÐ¸ÑÐ¸\n    let loss = MSELoss;\n    let opt = SGD::new(0.1);\n    let reg = NoRegularizer;\n    let trainer = Trainer::builder(loss, opt, reg)\n        .batch_size(20)\n        .max_epochs(500)\n        .build();\n    let x = vec![\n        vec![1.0, 1.0], // y â 1*1 + 2*1 = 3\n        vec![2.0, 1.0], // y â 2 + 2 = 4\n        vec![1.0, 2.0], // y â 1 + 4 = 5\n        vec![2.0, 2.0], // y â 2 + 4 = 6\n        vec![3.0, 3.0], // â Ð²ÑÐ±ÑÐ¾Ñ Ð¿Ð¾ y!\n    ];\n    let y = vec![3.0, 4.0, 5.0, 6.0, 30.0];\n    let dataset = InMemoryDataset::new(x, y).unwrap();\n\n    let fitted_model = trainer.fit(model, \u0026dataset).unwrap();\n    let inp = Tensor1D::\u003cNdarrayBackend\u003e::new((\u0026[4.0, 5.0]).to_vec());\n    let pred = fitted_model.predict(\u0026inp);\n    println!(\"Prediction: {:?}\", pred);\n}\n\n#[cfg(not(feature = \"ndarray\"))]\nfn main() {\n    println!(\"This example requires the `ndarray` feature. Run with:\");\n    println!(\"cargo run --example train_linear_ndarray --features ndarray\");\n    std::process::exit(1);\n}\n","traces":[],"covered":0,"coverable":0},{"path":["/","home","victor","ML","machinelearne-rs","lib","examples","train_logistic.rs"],"content":"// examples/train_logistic.rs\nuse machinelearne_rs::{\n    backend::scalar::Scalar, dataset::memory::InMemoryDataset, loss::BCEWithLogitsLoss,\n    model::linear::LinearRegressor, model::InferenceModel, optimizer::SGD,\n    regularizers::NoRegularizer, trainer::Trainer, CpuBackend, Tensor1D,\n};\n\nfn main() {\n    // ÐÐ¸Ð½Ð°ÑÐ½ÑÐµ Ð´Ð°Ð½Ð½ÑÐµ: y = 1 ÐµÑÐ»Ð¸ x1 + x2 \u003e 3, Ð¸Ð½Ð°ÑÐµ 0\n    let x = vec![\n        vec![1.0, 1.0], // sum=2 â y=0\n        vec![1.0, 2.0], // sum=3 â y=0 (Ð³ÑÐ°Ð½Ð¸ÑÐ½ÑÐ¹ ÑÐ»ÑÑÐ°Ð¹)\n        vec![2.0, 2.0], // sum=4 â y=1\n        vec![3.0, 1.0], // sum=4 â y=1\n        vec![0.5, 0.5], // sum=1 â y=0\n        // ÐÐ¾Ð±Ð°Ð²Ð¸Ð¼ Ð²ÑÐ±ÑÐ¾Ñ: ÑÐ¾ÑÐºÐ° Ñ y=1, ÑÐ¾ÑÑ ÑÑÐ¼Ð¼Ð° Ð¼Ð°Ð»Ð°\n        vec![1.0, 1.0], // sum=2 â Ð½Ð¾ Ð¿Ð¾Ð¼ÐµÑÐ¸Ð¼ ÐºÐ°Ðº y=1 (ÑÑÐ¼/Ð¾ÑÐ¸Ð±ÐºÐ°)\n    ];\n    let y = vec![0.0, 0.0, 1.0, 1.0, 0.0, 1.0];\n\n    let model = LinearRegressor::new(2); // 2 Ð¿ÑÐ¸Ð·Ð½Ð°ÐºÐ° â 1 Ð»Ð¾Ð³Ð¸Ñ\n    let loss = BCEWithLogitsLoss;\n    let opt = SGD::new(0.5); // Ð¼Ð¾Ð¶ÐµÑ Ð¿Ð¾ÑÑÐµÐ±Ð¾Ð²Ð°ÑÑÑÑ Ð±Ð¾Ð»ÑÑÐ¸Ð¹ LR Ð´Ð»Ñ BCE\n    let reg = NoRegularizer;\n\n    let trainer = Trainer::builder(loss, opt, reg)\n        .batch_size(10)\n        .max_epochs(1000)\n        .build();\n\n    let dataset = InMemoryDataset::new(x, y).unwrap();\n\n    let fitted_model = trainer.fit(model, \u0026dataset).unwrap();\n    let logit = fitted_model.predict(\u0026Tensor1D::\u003cCpuBackend\u003e::new((\u0026[2.5, 2.0]).to_vec()));\n    let one = Scalar::\u003cCpuBackend\u003e::new(1.);\n    let minus_one = Scalar::\u003cCpuBackend\u003e::new(-1.);\n    let prob = one / (one + (logit * minus_one));\n    let prob = prob.exp();\n    println!(\"Logit: {:?}, Probability: {:?}\", logit, prob);\n}\n","traces":[],"covered":0,"coverable":0},{"path":["/","home","victor","ML","machinelearne-rs","lib","src","backend","cpu.rs"],"content":"//! # CPU Backend\n//!\n//! Pure-Rust CPU backend implementation with zero external dependencies.\n//! Uses `f64` precision and row-major memory layout for all tensors.\n//!\n//! ## Design Characteristics\n//!\n//! - **Minimal dependencies**: No external crates required (enabled via `cpu` feature)\n//! - **Row-major layout**: 2D tensors stored as flat `Vec\u003cf64\u003e` in row-major order\n//! - **f64 precision**: All computations use double precision for numerical stability\n//! - **Naive implementations**: Straightforward algorithms prioritizing correctness;\n//!   optimizations (SIMD, cache-aware layouts) are future work\n//!\n//! ## Tensor Representations\n//!\n//! | Type          | Rust Type     | Layout                     |\n//! |---------------|---------------|----------------------------|\n//! | 1D Tensor     | `Vec\u003cf64\u003e`    | Contiguous elements        |\n//! | 2D Tensor     | `CpuTensor2D` | `(Vec\u003cf64\u003e, rows, cols)`   |\n//!\n//! ## Performance Notes\n//!\n//! - Matrix-vector multiplication (`matvec`) uses naive O(nÂ²) implementation\n//! - Transpose creates a new allocated tensor (no view/slice optimization yet)\n//! - Element-wise operations are not SIMD-accelerated (future optimization target)\n//!\n//! ## Example\n//!\n//! ```rust\n//! use machinelearne_rs::backend::{Backend, CpuBackend, cpu::CpuTensor2D};\n//!\n//! // Create a 2x2 matrix: [[1.0, 2.0], [3.0, 4.0]]\n//! let w = CpuTensor2D::new(vec![1.0, 2.0, 3.0, 4.0], 2, 2);\n//!\n//! // Create input vector [1.0, 0.0]\n//! let x = vec![1.0, 0.0];\n//!\n//! // Compute matrix-vector product\n//! let y = CpuBackend::matvec(\u0026w, \u0026x); // Result: [1.0, 3.0]\n//! ```\n\nuse super::Backend;\n\n/// Pure-Rust CPU computation backend.\n///\n/// Provides baseline tensor operations without external dependencies.\n/// All computations use `f64` precision for numerical stability during training.\n///\n/// # Usage\n///\n/// ```rust\n/// use machinelearne_rs::backend::{Backend, CpuBackend};\n///\n/// let zeros = CpuBackend::zeros_1d(5);\n/// assert_eq!(zeros, vec![0.0; 5]);\n/// ```\n#[derive(Clone, Debug, Copy)]\npub struct CpuBackend;\n\n/// Two-dimensional tensor representation for CPU backend.\n///\n/// Stores data in **row-major order** as a flat `Vec\u003cf64\u003e` with explicit shape metadata.\n///\n/// # Memory Layout\n///\n/// For a `(rows=2, cols=3)` matrix:\n/// ```text\n/// [[a, b, c],\n///  [d, e, f]]\n/// ```\n/// Stored as: `[a, b, c, d, e, f]`\n///\n/// # Invariants\n///\n/// - `data.len() == rows * cols` (enforced in `new()`)\n/// - Row `i`, column `j` element at index `i * cols + j`\n#[derive(Debug, Clone)]\npub struct CpuTensor2D(pub Vec\u003cf64\u003e, pub usize, pub usize);\n\nimpl CpuTensor2D {\n    /// Creates a new 2D tensor with explicit shape validation.\n    ///\n    /// # Arguments\n    /// * `data` - Elements in row-major order\n    /// * `rows` - Number of rows\n    /// * `cols` - Number of columns\n    ///\n    /// # Panics\n    /// Panics if `data.len() != rows * cols`.\n    ///\n    /// # Example\n    /// ```rust\n    /// use machinelearne_rs::backend::cpu::CpuTensor2D;\n    ///\n    /// let t = CpuTensor2D::new(vec![1.0, 2.0, 3.0, 4.0], 2, 2);\n    /// assert_eq!(t.0, vec![1.0, 2.0, 3.0, 4.0]);\n    /// assert_eq!((t.1, t.2), (2, 2));\n    /// ```\n    pub fn new(data: Vec\u003cf64\u003e, rows: usize, cols: usize) -\u003e Self {\n        assert_eq!(data.len(), rows * cols, \"Inconsistent shape\");\n        Self(data, rows, cols)\n    }\n}\n\nimpl From\u003c\u0026[Vec\u003cf64\u003e]\u003e for CpuTensor2D {\n    /// Converts a nested vector representation into a row-major 2D tensor.\n    ///\n    /// # Arguments\n    /// * `x` - Slice of rows, where each row is a `Vec\u003cf64\u003e`\n    ///\n    /// # Panics\n    /// * If rows have inconsistent lengths\n    /// * If input is non-empty but contains empty rows\n    ///\n    /// # Example\n    /// ```rust\n    /// use machinelearne_rs::backend::cpu::CpuTensor2D;\n    ///\n    /// let nested = vec![vec![1.0, 2.0], vec![3.0, 4.0]];\n    /// let t = CpuTensor2D::from(\u0026nested[..]);\n    /// assert_eq!(t.0, vec![1.0, 2.0, 3.0, 4.0]);\n    /// assert_eq!((t.1, t.2), (2, 2));\n    /// ```\n    fn from(x: \u0026[Vec\u003cf64\u003e]) -\u003e Self {\n        if x.is_empty() {\n            return CpuTensor2D::new(Vec::new(), 0, 0);\n        }\n        let rows = x.len();\n        let cols = x[0].len();\n        assert!(\n            x.iter().all(|row| row.len() == cols),\n            \"All rows must have same length\"\n        );\n        let data: Vec\u003cf64\u003e = x.iter().flat_map(|row| row.iter()).copied().collect();\n        CpuTensor2D::new(data, rows, cols)\n    }\n}\n\nimpl Backend for CpuBackend {\n    type Scalar = f64;\n    type Tensor1D = Vec\u003cf64\u003e;\n    type Tensor2D = CpuTensor2D;\n    type Device = ();\n\n    /// Returns the default device identifier for CPU backend.\n    ///\n    /// Always returns unit type `()` since CPU operations don't require device selection.\n    fn default_device() -\u003e Self::Device {}\n\n    // --- Constructors ---\n\n    /// Creates a 1D tensor filled with zeros.\n    ///\n    /// # Arguments\n    /// * `len` - Number of elements\n    ///\n    /// # Example\n    /// ```rust\n    /// use machinelearne_rs::backend::{Backend, CpuBackend};\n    /// let zeros = CpuBackend::zeros_1d(3);\n    /// assert_eq!(zeros, vec![0.0, 0.0, 0.0]);\n    /// ```\n    fn zeros_1d(len: usize) -\u003e Self::Tensor1D {\n        vec![0.; len]\n    }\n\n    /// Creates a 2D tensor filled with zeros.\n    ///\n    /// # Arguments\n    /// * `rows` - Number of rows\n    /// * `cols` - Number of columns\n    ///\n    /// # Example\n    /// ```rust\n    /// use machinelearne_rs::backend::{Backend, CpuBackend};\n    /// let zeros = CpuBackend::zeros_2d(2, 3);\n    /// assert_eq!(zeros.0, vec![0.0; 6]);\n    /// assert_eq!((zeros.1, zeros.2), (2, 3));\n    /// ```\n    fn zeros_2d(rows: usize, cols: usize) -\u003e Self::Tensor2D {\n        CpuTensor2D::new(vec![0.; rows * cols], rows, cols)\n    }\n\n    /// Constructs a 1D tensor from `f32` data (converts to `f64`).\n    ///\n    /// # Arguments\n    /// * `data` - Source values as `f32`\n    ///\n    /// # Example\n    /// ```rust\n    /// use machinelearne_rs::backend::{Backend, CpuBackend};\n    /// let t = CpuBackend::from_vec_1d(vec![1.0f32, 2.5]);\n    /// assert_eq!(t, vec![1.0, 2.5]);\n    /// ```\n    fn from_vec_1d(data: Vec\u003cf32\u003e) -\u003e Self::Tensor1D {\n        data.into_iter().map(|x| x as f64).collect()\n    }\n\n    /// Constructs a 2D tensor from `f32` data (converts to `f64`).\n    ///\n    /// # Arguments\n    /// * `data` - Source values in row-major order as `f32`\n    /// * `rows` - Number of rows\n    /// * `cols` - Number of columns\n    ///\n    /// # Panics\n    /// Panics if `data.len() != rows * cols`.\n    fn from_vec_2d(data: Vec\u003cf32\u003e, rows: usize, cols: usize) -\u003e Self::Tensor2D {\n        CpuTensor2D::new(data.into_iter().map(|x| x as f64).collect(), rows, cols)\n    }\n\n    // --- Element-wise operations (1D) ---\n\n    /// Element-wise addition of two 1D tensors.\n    ///\n    /// # Panics\n    /// Panics if tensors have different lengths.\n    fn add_1d(a: \u0026Self::Tensor1D, b: \u0026Self::Tensor1D) -\u003e Self::Tensor1D {\n        a.iter().zip(b.iter()).map(|(a, b)| a + b).collect()\n    }\n\n    /// Element-wise subtraction of two 1D tensors.\n    ///\n    /// # Panics\n    /// Panics if tensors have different lengths.\n    fn sub_1d(a: \u0026Self::Tensor1D, b: \u0026Self::Tensor1D) -\u003e Self::Tensor1D {\n        a.iter().zip(b.iter()).map(|(a, b)| a - b).collect()\n    }\n\n    /// Element-wise multiplication of two 1D tensors.\n    ///\n    /// # Panics\n    /// Panics if tensors have different lengths.\n    fn mul_1d(a: \u0026Self::Tensor1D, b: \u0026Self::Tensor1D) -\u003e Self::Tensor1D {\n        a.iter().zip(b.iter()).map(|(a, b)| a * b).collect()\n    }\n\n    /// Element-wise division of two 1D tensors.\n    ///\n    /// # Panics\n    /// Panics if tensors have different lengths or divisor contains zeros.\n    fn div_1d(a: \u0026Self::Tensor1D, b: \u0026Self::Tensor1D) -\u003e Self::Tensor1D {\n        a.iter().zip(b.iter()).map(|(a, b)| a / b).collect()\n    }\n\n    /// Multiplies each element of a 1D tensor by a scalar.\n    fn mul_scalar_1d(t: \u0026Self::Tensor1D, s: \u0026Self::Scalar) -\u003e Self::Tensor1D {\n        t.iter().map(|x| *x * s).collect()\n    }\n\n    /// Adds a scalar to each element of a 1D tensor.\n    fn add_scalar_1d(t: \u0026Self::Tensor1D, s: \u0026Self::Scalar) -\u003e Self::Tensor1D {\n        t.iter().map(|x| x + s).collect()\n    }\n\n    // --- Element-wise operations (2D) ---\n\n    /// Multiplies each element of a 2D tensor by a scalar.\n    fn mul_scalar_2d(t: \u0026Self::Tensor2D, s: \u0026Self::Scalar) -\u003e Self::Tensor2D {\n        CpuTensor2D::new(t.0.iter().map(|x| *x * s).collect(), t.1, t.2)\n    }\n\n    /// Adds a scalar to each element of a 2D tensor.\n    fn add_scalar_2d(t: \u0026Self::Tensor2D, s: \u0026Self::Scalar) -\u003e Self::Tensor2D {\n        CpuTensor2D::new(t.0.iter().map(|x| *x + s).collect(), t.1, t.2)\n    }\n\n    /// Element-wise addition of two 2D tensors.\n    ///\n    /// # Panics\n    /// Panics if tensors have different shapes.\n    fn add_2d(a: \u0026Self::Tensor2D, b: \u0026Self::Tensor2D) -\u003e Self::Tensor2D {\n        CpuTensor2D::new(\n            a.0.iter().zip(b.0.iter()).map(|(a, b)| a + b).collect(),\n            a.1,\n            a.2,\n        )\n    }\n\n    /// Element-wise subtraction of two 2D tensors.\n    ///\n    /// # Panics\n    /// Panics if tensors have different shapes (rows or columns mismatch).\n    ///\n    /// # Why panic instead of silent truncation?\n    /// Silent truncation via `.zip()` would produce mathematically invalid results\n    /// (e.g., subtracting a 3Ã2 matrix from 2Ã3) without any indication of error.\n    /// This violates the principle of least surprise and makes debugging extremely hard.\n    fn sub_2d(a: \u0026Self::Tensor2D, b: \u0026Self::Tensor2D) -\u003e Self::Tensor2D {\n        if a.1 != b.1 || a.2 != b.2 {\n            panic!(\n                \"sub_2d: shape mismatch â cannot subtract ({}, {}) from ({}, {}). \\\n             Shapes must be identical for element-wise operations.\",\n                a.1, a.2, b.1, b.2\n            );\n        }\n        CpuTensor2D::new(\n            a.0.iter().zip(b.0.iter()).map(|(x, y)| x - y).collect(),\n            a.1,\n            a.2,\n        )\n    }\n\n    /// Element-wise multiplication of two 2D tensors.\n    ///\n    /// # Panics\n    /// Panics if tensors have different shapes.\n    fn mul_2d(a: \u0026Self::Tensor2D, b: \u0026Self::Tensor2D) -\u003e Self::Tensor2D {\n        CpuTensor2D::new(\n            a.0.iter().zip(b.0.iter()).map(|(a, b)| a * b).collect(),\n            a.1,\n            a.2,\n        )\n    }\n\n    /// Element-wise division of two 2D tensors.\n    ///\n    /// # Panics\n    /// Panics if tensors have different shapes or divisor contains zeros.\n    fn div_2d(a: \u0026Self::Tensor2D, b: \u0026Self::Tensor2D) -\u003e Self::Tensor2D {\n        CpuTensor2D::new(\n            a.0.iter().zip(b.0.iter()).map(|(a, b)| a / b).collect(),\n            a.1,\n            a.2,\n        )\n    }\n\n    // --- Reduction operations ---\n\n    /// Computes the arithmetic mean of all elements in a 1D tensor.\n    ///\n    /// # Returns\n    /// * Mean value as `f64`\n    /// * Returns `0.0` for empty tensors (by convention)\n    fn mean_all_1d(t: \u0026Self::Tensor1D) -\u003e Self::Scalar {\n        if t.is_empty() {\n            0.0\n        } else {\n            t.iter().sum::\u003cf64\u003e() / t.len() as f64\n        }\n    }\n\n    /// Computes the arithmetic mean of all elements in a 2D tensor.\n    ///\n    /// # Panics\n    /// Panics if the tensor is empty (0 elements). This follows the \"fail fast\" principle\n    /// common in numerical libraries â an empty tensor almost always indicates a bug in\n    /// data preprocessing or batch construction rather than a legitimate edge case.\n    ///\n    /// # Why panic instead of returning 0.0 or NaN?\n    /// - `0.0` is mathematically incorrect and masks critical bugs (e.g., empty batches)\n    /// - `NaN` propagates silently through computations, making root cause analysis harder\n    /// - Immediate panic forces developers to fix data pipeline issues early\n    ///\n    /// # Example of legitimate use\n    /// ```should_panic\n    /// # use machinelearne_rs::backend::{Tensor2D, CpuBackend};\n    /// let empty = Tensor2D::\u003cCpuBackend\u003e::zeros(0, 5);\n    /// let m = empty.mean(); // PANIC: empty tensor detected\n    /// ```\n    fn mean_all_2d(t: \u0026Self::Tensor2D) -\u003e Self::Scalar {\n        if t.0.is_empty() {\n            panic!(\n                \"mean_all_2d: cannot compute mean of empty tensor (shape: {:?}). \\\n             This likely indicates a bug in data loading or batch construction. \\\n             Check: batch size = 0, empty dataset partition, or incorrect slicing.\",\n                t.0.len()\n            );\n        }\n        t.0.iter().sum::\u003cf64\u003e() / t.0.len() as f64\n    }\n\n    /// Computes the sum of all elements in a 2D tensor.\n    fn sum_all_2d(t: \u0026Self::Tensor2D) -\u003e Self::Scalar {\n        t.0.iter().sum::\u003cf64\u003e()\n    }\n\n    /// Computes the sum of all elements in a 1D tensor.\n    fn sum_all_1d(t: \u0026Self::Tensor1D) -\u003e Self::Scalar {\n        t.iter().sum::\u003cf64\u003e()\n    }\n\n    // --- Scalar operations ---\n\n    /// Creates a backend-specific scalar from an `f64` value.\n    ///\n    /// For CPU backend, this is a trivial identity conversion.\n    fn scalar_f64(value: f64) -\u003e Self::Scalar {\n        value\n    }\n\n    // --- Data access ---\n\n    /// Converts a 1D tensor to a `Vec\u003cf64\u003e` for inspection or metric computation.\n    ///\n    /// # Note\n    /// This clones the underlying data. Avoid in performance-critical paths.\n    fn to_vec_1d(t: \u0026Self::Tensor1D) -\u003e Vec\u003cf64\u003e {\n        t.clone()\n    }\n\n    /// Returns the number of elements in a 1D tensor.\n    fn len_1d(t: \u0026Self::Tensor1D) -\u003e usize {\n        t.len()\n    }\n\n    /// Returns the number of rows.\n    fn len_2d(t: \u0026Self::Tensor2D) -\u003e usize {\n        t.1\n    }\n\n    // --- Mathematical functions (1D) ---\n\n    /// Element-wise absolute value operation.\n    fn abs_1d(t: \u0026Self::Tensor1D) -\u003e Self::Tensor1D {\n        t.iter().map(|x| x.abs()).collect()\n    }\n\n    /// Element-wise sign function.\n    ///\n    /// Returns:\n    /// * `1.0` for positive values\n    /// * `-1.0` for negative values\n    /// * `0.0` for zero (subgradient convention)\n    fn sign_1d(x: \u0026Self::Tensor1D) -\u003e Self::Tensor1D {\n        x.iter()\n            .map(|\u0026x| {\n                if x \u003e 0.0 {\n                    1.0\n                } else if x \u003c 0.0 {\n                    -1.0\n                } else {\n                    0.0\n                }\n            })\n            .collect()\n    }\n\n    /// Element-wise maximum between two tensors.\n    ///\n    /// # Panics\n    /// Panics if tensors have different lengths.\n    fn maximum_1d(x: \u0026Self::Tensor1D, other: \u0026Self::Tensor1D) -\u003e Self::Tensor1D {\n        x.iter()\n            .zip(other.iter())\n            .map(|(\u0026a, \u0026b)| a.max(b))\n            .collect()\n    }\n\n    /// Element-wise exponential function (`e^x`).\n    fn exp_1d(x: \u0026Self::Tensor1D) -\u003e Self::Tensor1D {\n        x.iter().map(|\u0026v| v.exp()).collect()\n    }\n\n    /// Computes element-wise natural logarithm: `ln(x)`.\n    ///\n    /// # Behavior for edge cases (IEEE 754 compliant)\n    /// - `x \u003e 0.0` â finite value `ln(x)`\n    /// - `x == 0.0` â `-â` (`f64::NEG_INFINITY`)\n    /// - `x \u003c 0.0` â `NaN` (`f64::NAN`)\n    ///\n    /// Does **not** panic â follows standard floating-point semantics used by\n    /// NumPy, PyTorch, and TensorFlow.\n    ///\n    /// # Numerical stability in ML\n    /// For loss functions involving logarithms (e.g., cross-entropy):\n    /// - Avoid raw `log(x)` on unnormalized probabilities â use `log_softmax` instead\n    /// - Clip inputs when necessary: `log(max(x, Îµ))` with `Îµ = 1e-12` to avoid `-inf`\n    ///\n    /// # Example\n    /// ```\n    /// # use machinelearne_rs::backend::{Tensor1D, CpuBackend};\n    /// let x = Tensor1D::\u003cCpuBackend\u003e::new(vec![1.0f32, 0.0, -1.0]);\n    /// let y = x.log();  // Returns [0.0, -inf, NaN]\n    /// assert!((y.to_vec()[0] - 0.0).abs() \u003c 1e-12); // ln(1) = 0\n    /// assert!(y.to_vec()[1].is_infinite() \u0026\u0026 y.to_vec()[1] \u003c 0.0); // ln(0) = -inf\n    /// assert!(y.to_vec()[2].is_nan()); // ln(-1) = NaN\n    /// ```\n    fn log_1d(x: \u0026Self::Tensor1D) -\u003e Self::Tensor1D {\n        x.iter().map(|\u0026v| v.ln()).collect()\n    }\n    /// Element-wise sigmoid function with numerical stability.\n    ///\n    /// Computed as:\n    /// * `1 / (1 + exp(-x))` for `x \u003e= 0`\n    /// * `exp(x) / (1 + exp(x))` for `x \u003c 0`\n    ///\n    /// This formulation avoids overflow for large positive/negative values.\n    fn sigmoid_1d(x: \u0026Self::Tensor1D) -\u003e Self::Tensor1D {\n        x.iter()\n            .map(|\u0026z| {\n                if z \u003e= 0.0 {\n                    1.0 / (1.0 + (-z).exp())\n                } else {\n                    let ez = z.exp();\n                    ez / (1.0 + ez)\n                }\n            })\n            .collect()\n    }\n\n    // --- Mathematical functions (2D) ---\n\n    /// Element-wise absolute value for 2D tensors.\n    fn abs_2d(t: \u0026Self::Tensor2D) -\u003e Self::Tensor2D {\n        CpuTensor2D::new(t.0.iter().map(|x| x.abs()).collect(), t.1, t.2)\n    }\n\n    /// Element-wise sign function for 2D tensors.\n    fn sign_2d(x: \u0026Self::Tensor2D) -\u003e Self::Tensor2D {\n        CpuTensor2D::new(\n            x.0.iter()\n                .map(|\u0026x| {\n                    if x \u003e 0.0 {\n                        1.0\n                    } else if x \u003c 0.0 {\n                        -1.0\n                    } else {\n                        0.0\n                    }\n                })\n                .collect(),\n            x.1,\n            x.2,\n        )\n    }\n\n    /// Element-wise maximum between two 2D tensors.\n    ///\n    /// # Panics\n    /// Panics if tensors have different shapes.\n    fn maximum_2d(x: \u0026Self::Tensor2D, other: \u0026Self::Tensor2D) -\u003e Self::Tensor2D {\n        CpuTensor2D::new(Self::maximum_1d(\u0026x.0, \u0026other.0), x.1, x.2)\n    }\n\n    /// Element-wise exponential function for 2D tensors.\n    fn exp_2d(x: \u0026Self::Tensor2D) -\u003e Self::Tensor2D {\n        CpuTensor2D::new(Self::exp_1d(\u0026x.0), x.1, x.2)\n    }\n\n    /// Element-wise natural logarithm for 2D tensors.\n    ///\n    /// # Panics\n    /// Panics if any element is â¤ 0.0.\n    fn log_2d(x: \u0026Self::Tensor2D) -\u003e Self::Tensor2D {\n        CpuTensor2D::new(Self::log_1d(\u0026x.0), x.1, x.2)\n    }\n\n    /// Element-wise sigmoid function for 2D tensors.\n    fn sigmoid_2d(x: \u0026Self::Tensor2D) -\u003e Self::Tensor2D {\n        CpuTensor2D::new(Self::sigmoid_1d(\u0026x.0), x.1, x.2)\n    }\n\n    // --- Linear algebra ---\n\n    /// Matrix-vector multiplication with explicit shape checking.\n    ///\n    /// Computes `y = A * x` where:\n    /// * `A` has shape `(m, n)`\n    /// * `x` has shape `(n,)`\n    /// * Returns vector of shape `(m,)`\n    ///\n    /// # Panics\n    /// Panics if `A.cols() != x.len()`.\n    ///\n    /// # Implementation\n    /// Currently delegates to `_matvec_unchecked()` after shape validation.\n    /// TODO: Implement efficient shape checking before computation.\n    fn matvec(a: \u0026Self::Tensor2D, x: \u0026Self::Tensor1D) -\u003e Self::Tensor1D {\n        assert_eq!(\n            a.2,\n            x.len(),\n            \"Shape mismatch: A.cols={} != x.len={}\",\n            a.2,\n            x.len()\n        );\n        Self::_matvec_unchecked(a, x)\n    }\n\n    /// Matrix-vector multiplication without shape checking.\n    ///\n    /// # Safety\n    /// Caller must ensure `A.cols() == x.len()`. Undefined behavior may occur\n    /// if shapes are incompatible.\n    ///\n    /// # Implementation\n    /// Naive O(mÃn) implementation. Future optimizations:\n    /// * SIMD vectorization\n    /// * Cache-aware blocking\n    /// * Strided access patterns\n    fn _matvec_unchecked(a: \u0026CpuTensor2D, x: \u0026Vec\u003cf64\u003e) -\u003e Vec\u003cf64\u003e {\n        let CpuTensor2D(data, rows, cols) = a;\n        let mut result = Vec::with_capacity(*rows);\n        for i in 0..*rows {\n            let mut sum = 0.0;\n            for j in 0..*cols {\n                sum += data[i * *cols + j] * x[j];\n            }\n            result.push(sum);\n        }\n        result\n    }\n\n    /// Transposed matrix-vector multiplication with shape checking.\n    ///\n    /// Computes `y = Aáµ * x` where:\n    /// * `A` has shape `(m, n)`\n    /// * `x` has shape `(m,)`\n    /// * Returns vector of shape `(n,)`\n    ///\n    /// # Panics\n    /// Panics if `A.rows() != x.len()`.\n    fn matvec_transposed(a: \u0026Self::Tensor2D, x: \u0026Self::Tensor1D) -\u003e Self::Tensor1D {\n        assert_eq!(\n            a.1,\n            x.len(),\n            \"Shape mismatch: A.rows={} != x.len={}\",\n            a.1,\n            x.len()\n        );\n        Self::_matvec_transposed_unchecked(a, x)\n    }\n\n    /// Transposed matrix-vector multiplication without shape checking.\n    ///\n    /// # Safety\n    /// Caller must ensure `A.rows() == x.len()`.\n    ///\n    /// # Implementation\n    /// Currently computes full transpose then multiplies. Future optimization:\n    /// direct strided access without allocation.\n    fn _matvec_transposed_unchecked(a: \u0026Self::Tensor2D, x: \u0026Self::Tensor1D) -\u003e Self::Tensor1D {\n        Self::_matvec_unchecked(\u0026Self::transpose(a), x)\n    }\n\n    /// Returns the transpose of a 2D tensor.\n    ///\n    /// Converts matrix of shape `(m, n)` to shape `(n, m)` with elements:\n    /// `output[j][i] = input[i][j]`\n    ///\n    /// # Implementation\n    /// Allocates a new tensor. Future optimization: return view/slice without allocation.\n    fn transpose(t: \u0026CpuTensor2D) -\u003e Self::Tensor2D {\n        let CpuTensor2D(inp, rows, cols) = t;\n        let mut out = Vec::with_capacity(cols * rows);\n        for col in 0..*cols {\n            for row in 0..*rows {\n                out.push(inp[row * cols + col]);\n            }\n        }\n        CpuTensor2D::new(out, *cols, *rows)\n    }\n\n    /// Returns the shape of a 2D tensor as `(rows, cols)`.\n    fn shape(t: \u0026Self::Tensor2D) -\u003e (usize, usize) {\n        (t.1, t.2)\n    }\n\n    //Returns copy of the inner 1d vector\n    fn ravel_2d(x: \u0026Self::Tensor2D) -\u003e Self::Tensor1D {\n        x.0.clone()\n    }\n\n    // --- Column-wise operations ---\n\n    fn col_mean_2d(t: \u0026Self::Tensor2D) -\u003e Self::Tensor1D {\n        let (_, cols) = Self::shape(t);\n        if cols == 0 || t.0.is_empty() {\n            return Vec::new();\n        }\n        let rows = t.1;\n\n        let mut means = vec![0.0; cols];\n        for (col, mean) in means.iter_mut().enumerate() {\n            let mut sum = 0.0;\n            for row in 0..rows {\n                sum += t.0[row * cols + col];\n            }\n            *mean = sum / rows as f64;\n        }\n        means\n    }\n\n    fn col_std_2d(t: \u0026Self::Tensor2D, ddof: usize) -\u003e Self::Tensor1D {\n        let (_, cols) = Self::shape(t);\n        if cols == 0 || t.0.is_empty() {\n            return Vec::new();\n        }\n        let rows = t.1;\n\n        let means = Self::col_mean_2d(t);\n        let mut stds = vec![0.0; cols];\n\n        for col in 0..cols {\n            let mut var_sum = 0.0;\n            for row in 0..rows {\n                let diff = t.0[row * cols + col] - means[col];\n                var_sum += diff * diff;\n            }\n            let divisor = (rows - ddof) as f64;\n            stds[col] = (var_sum / divisor).sqrt();\n        }\n        stds\n    }\n\n    fn col_min_2d(t: \u0026Self::Tensor2D) -\u003e Self::Tensor1D {\n        let (_, cols) = Self::shape(t);\n        if cols == 0 || t.0.is_empty() {\n            return Vec::new();\n        }\n        let rows = t.1;\n\n        let mut mins = vec![f64::INFINITY; cols];\n        for (col, min) in mins.iter_mut().enumerate() {\n            for row in 0..rows {\n                let val = t.0[row * cols + col];\n                if val \u003c *min {\n                    *min = val;\n                }\n            }\n        }\n        mins\n    }\n\n    fn col_max_2d(t: \u0026Self::Tensor2D) -\u003e Self::Tensor1D {\n        let (_, cols) = Self::shape(t);\n        if cols == 0 || t.0.is_empty() {\n            return Vec::new();\n        }\n        let rows = t.1;\n\n        let mut maxs = vec![f64::NEG_INFINITY; cols];\n        for (col, max) in maxs.iter_mut().enumerate() {\n            for row in 0..rows {\n                let val = t.0[row * cols + col];\n                if val \u003e *max {\n                    *max = val;\n                }\n            }\n        }\n        maxs\n    }\n\n    fn col_sum_2d(t: \u0026Self::Tensor2D) -\u003e Self::Tensor1D {\n        let (_, cols) = Self::shape(t);\n        if cols == 0 || t.0.is_empty() {\n            return Vec::new();\n        }\n        let rows = t.1;\n\n        let mut sums = vec![0.0; cols];\n        for (col, sum) in sums.iter_mut().enumerate() {\n            for row in 0..rows {\n                *sum += t.0[row * cols + col];\n            }\n        }\n        sums\n    }\n\n    // --- Row-wise operations ---\n\n    fn row_sum_2d(t: \u0026Self::Tensor2D) -\u003e Self::Tensor1D {\n        let (rows, cols) = Self::shape(t);\n        if rows == 0 || cols == 0 {\n            return Vec::new();\n        }\n\n        let mut sums = vec![0.0; rows];\n        for (row, sum) in sums.iter_mut().enumerate() {\n            for col in 0..cols {\n                *sum += t.0[row * cols + col];\n            }\n        }\n        sums\n    }\n\n    // --- Broadcasting operations ---\n\n    fn broadcast_sub_1d_to_2d_rows(t: \u0026Self::Tensor2D, v: \u0026Self::Tensor1D) -\u003e Self::Tensor2D {\n        let (rows, cols) = Self::shape(t);\n        assert_eq!(v.len(), cols, \"Vector length must match number of columns\");\n\n        let mut result = Vec::with_capacity(rows * cols);\n        for row in 0..rows {\n            for (col, \u0026val) in v.iter().enumerate() {\n                result.push(t.0[row * cols + col] - val);\n            }\n        }\n        CpuTensor2D::new(result, rows, cols)\n    }\n\n    fn broadcast_div_1d_to_2d_rows(t: \u0026Self::Tensor2D, v: \u0026Self::Tensor1D) -\u003e Self::Tensor2D {\n        let (rows, cols) = Self::shape(t);\n        assert_eq!(v.len(), cols, \"Vector length must match number of columns\");\n\n        let mut result = Vec::with_capacity(rows * cols);\n        for row in 0..rows {\n            for (col, \u0026val) in v.iter().enumerate() {\n                result.push(t.0[row * cols + col] / val);\n            }\n        }\n        CpuTensor2D::new(result, rows, cols)\n    }\n\n    fn broadcast_mul_1d_to_2d_rows(t: \u0026Self::Tensor2D, v: \u0026Self::Tensor1D) -\u003e Self::Tensor2D {\n        let (rows, cols) = Self::shape(t);\n        assert_eq!(v.len(), cols, \"Vector length must match number of columns\");\n\n        let mut result = Vec::with_capacity(rows * cols);\n        for row in 0..rows {\n            for (col, \u0026val) in v.iter().enumerate() {\n                result.push(t.0[row * cols + col] * val);\n            }\n        }\n        CpuTensor2D::new(result, rows, cols)\n    }\n\n    fn broadcast_add_1d_to_2d_rows(t: \u0026Self::Tensor2D, v: \u0026Self::Tensor1D) -\u003e Self::Tensor2D {\n        let (rows, cols) = Self::shape(t);\n        assert_eq!(v.len(), cols, \"Vector length must match number of columns\");\n\n        let mut result = Vec::with_capacity(rows * cols);\n        for row in 0..rows {\n            for (col, \u0026val) in v.iter().enumerate() {\n                result.push(t.0[row * cols + col] + val);\n            }\n        }\n        CpuTensor2D::new(result, rows, cols)\n    }\n\n    fn sqrt_1d(t: \u0026Self::Tensor1D) -\u003e Self::Tensor1D {\n        t.iter().map(|x| x.sqrt()).collect()\n    }\n\n    fn sqrt_2d(t: \u0026Self::Tensor2D) -\u003e Self::Tensor2D {\n        CpuTensor2D::new(t.0.iter().map(|x| x.sqrt()).collect(), t.1, t.2)\n    }\n\n    // --- Column manipulation operations ---\n\n    fn hcat_2d(\n        tensors: \u0026[Self::Tensor2D],\n    ) -\u003e Result\u003cSelf::Tensor2D, crate::preprocessing::PreprocessingError\u003e {\n        if tensors.is_empty() {\n            return Err(crate::preprocessing::PreprocessingError::InvalidParameter(\n                \"Cannot horizontally concatenate empty slice of tensors\".to_string(),\n            ));\n        }\n\n        let rows = tensors[0].1;\n        if rows == 0 {\n            return Ok(CpuTensor2D::new(vec![], 0, 0));\n        }\n\n        // Verify all tensors have the same number of rows\n        for t in tensors.iter() {\n            if t.1 != rows {\n                return Err(crate::preprocessing::PreprocessingError::InvalidShape {\n                    expected: format!(\"({}, ?)\", rows),\n                    got: format!(\"({}, ?)\", t.1),\n                });\n            }\n        }\n\n        // Calculate total columns\n        let total_cols: usize = tensors.iter().map(|t| t.2).sum();\n\n        // Concatenate row by row\n        let mut result = Vec::with_capacity(rows * total_cols);\n        for row in 0..rows {\n            for t in tensors {\n                let start = row * t.2;\n                let end = start + t.2;\n                result.extend_from_slice(\u0026t.0[start..end]);\n            }\n        }\n\n        Ok(CpuTensor2D::new(result, rows, total_cols))\n    }\n\n    fn select_columns_2d(t: \u0026Self::Tensor2D, columns: \u0026[usize]) -\u003e Self::Tensor2D {\n        let (rows, cols) = Self::shape(t);\n        if columns.is_empty() {\n            return CpuTensor2D::new(vec![], rows, 0);\n        }\n\n        // Validate column indices\n        for \u0026col in columns {\n            assert!(\n                col \u003c cols,\n                \"Column index {} out of bounds (max {})\",\n                col,\n                cols - 1\n            );\n        }\n\n        let mut result = Vec::with_capacity(rows * columns.len());\n        for row in 0..rows {\n            for \u0026col in columns {\n                result.push(t.0[row * cols + col]);\n            }\n        }\n\n        CpuTensor2D::new(result, rows, columns.len())\n    }\n\n    fn one_hot_from_indices(indices: \u0026Self::Tensor1D, num_classes: usize) -\u003e Self::Tensor2D {\n        let n_samples = indices.len();\n        if n_samples == 0 || num_classes == 0 {\n            return CpuTensor2D::new(vec![], n_samples, num_classes);\n        }\n\n        // Validate indices\n        for (i, \u0026idx) in indices.iter().enumerate() {\n            assert!(\n                idx \u003e= 0.0 \u0026\u0026 idx \u003c num_classes as f64 \u0026\u0026 idx.fract() == 0.0,\n                \"Index {} at position {} is not a valid integer in range [0, {})\",\n                idx,\n                i,\n                num_classes\n            );\n        }\n\n        let mut result = vec![0.0; n_samples * num_classes];\n        for (i, \u0026idx) in indices.iter().enumerate() {\n            let col = idx as usize;\n            result[i * num_classes + col] = 1.0;\n        }\n\n        CpuTensor2D::new(result, n_samples, num_classes)\n    }\n}\n\n#[cfg(test)]\nmod matvec_tests {\n    use super::*;\n\n    #[test]\n    fn test_matvec_transpose() {\n        // ÐÑÐ¸Ð¼ÐµÑ 1: X â (3, 2), v â (3,)\n        // X = [[1.0, 2.0],\n        //      [3.0, 4.0],\n        //      [5.0, 6.0]]\n        // v = [1.0, 0.0, 2.0]\n        // Xáµ @ v = [1*1 + 3*0 + 5*2, 2*1 + 4*0 + 6*2] = [11.0, 14.0]\n\n        let x_data = vec![1.0, 2.0, 3.0, 4.0, 5.0, 6.0]; // row-major\n        let x = CpuTensor2D::new(x_data, 3, 2); // (rows=3, cols=2)\n        let v = vec![1.0, 0.0, 2.0];\n\n        let result = CpuBackend::matvec_transposed(\u0026x, \u0026v);\n        let expected = vec![11.0, 14.0];\n\n        assert_eq!(result.len(), expected.len());\n        for (r, e) in result.iter().zip(expected.iter()) {\n            assert!((r - e).abs() \u003c 1e-12, \"Expected {}, got {}\", e, r);\n        }\n\n        // ÐÑÐ¸Ð¼ÐµÑ 2: (4, 1) â Ð²ÐµÐºÑÐ¾Ñ-ÑÑÐ¾Ð»Ð±ÐµÑ (ÐºÐ°Ðº Ð² ÑÐ²Ð¾Ð¸Ñ Ð»Ð¸Ð½ÐµÐ¹Ð½ÑÑ ÑÐµÐ³ÑÐµÑÑÐ¸ÑÑ)\n        // X = [[2.0],\n        //      [3.0],\n        //      [4.0],\n        //      [5.0]]\n        // v = [1.0, 1.0, 1.0, 1.0]\n        // Xáµ @ v = [2+3+4+5] = [14.0]\n\n        let x2 = CpuTensor2D::new(vec![2.0, 3.0, 4.0, 5.0], 4, 1);\n        let v2 = vec![1.0, 1.0, 1.0, 1.0];\n        let result2 = CpuBackend::matvec_transposed(\u0026x2, \u0026v2);\n        let expected2 = vec![14.0];\n\n        assert_eq!(result2, expected2);\n\n        // ÐÑÐ¸Ð¼ÐµÑ 3: (2, 3) â output len = 3\n        // X = [[1, 0, 0],\n        //      [0, 1, 0]]\n        // v = [5.0, 7.0]\n        // Xáµ @ v = [5, 7, 0]\n\n        let x3 = CpuTensor2D::new(vec![1.0, 0.0, 0.0, 0.0, 1.0, 0.0], 2, 3);\n        let v3 = vec![5.0, 7.0];\n        let result3 = CpuBackend::matvec_transposed(\u0026x3, \u0026v3);\n        let expected3 = vec![5.0, 7.0, 0.0];\n\n        assert_eq!(result3.len(), 3);\n        for (r, e) in result3.iter().zip(expected3.iter()) {\n            assert!((r - e).abs() \u003c 1e-12, \"Expected {}, got {}\", e, r);\n        }\n    }\n\n    #[test]\n    fn test_matvec_transpose_consistency_with_transpose_and_matvec() {\n        let x = CpuTensor2D::new(vec![1.0, 2.0, 3.0, 4.0, 5.0, 6.0], 3, 2); // (3,2)\n        let v = vec![1.0, 0.0, 2.0];\n\n        let result1 = CpuBackend::matvec_transposed(\u0026x, \u0026v);\n        let x_t = CpuBackend::transpose(\u0026x);\n        let result2 = CpuBackend::matvec(\u0026x_t, \u0026v);\n\n        assert_eq!(result1.len(), result2.len());\n        for (a, b) in result1.iter().zip(result2.iter()) {\n            assert!((a - b).abs() \u003c 1e-12);\n        }\n    }\n}\n\n#[cfg(test)]\nmod tests {\n    use super::*;\n\n    #[test]\n    fn test_constructors() {\n        // zeros_1d\n        let z1 = CpuBackend::zeros_1d(3);\n        assert_eq!(z1, vec![0.0, 0.0, 0.0]);\n\n        // zeros_2d\n        let z2 = CpuBackend::zeros_2d(2, 3);\n        assert_eq!(z2.0, vec![0.0; 6]);\n        assert_eq!((z2.1, z2.2), (2, 3));\n\n        // from_vec_1d\n        let v1 = CpuBackend::from_vec_1d(vec![1.0f32, 2.0, 3.0]);\n        assert_eq!(v1, vec![1.0, 2.0, 3.0]);\n\n        // from_vec_2d\n        let v2 = CpuBackend::from_vec_2d(vec![1.0f32, 2.0, 3.0, 4.0], 2, 2);\n        assert_eq!(v2.0, vec![1.0, 2.0, 3.0, 4.0]);\n        assert_eq!((v2.1, v2.2), (2, 2));\n\n        // From\u003c\u0026[Vec\u003cf64\u003e]\u003e for CpuTensor2D\n        let nested = vec![vec![1.0, 2.0], vec![3.0, 4.0]];\n        let t = CpuTensor2D::from(\u0026nested[..]);\n        assert_eq!(t.0, vec![1.0, 2.0, 3.0, 4.0]);\n        assert_eq!((t.1, t.2), (2, 2));\n    }\n\n    #[test]\n    fn test_elementwise_ops_1d() {\n        let a = vec![1.0, 2.0];\n        let b = vec![3.0, 4.0];\n\n        assert_eq!(CpuBackend::add_1d(\u0026a, \u0026b), vec![4.0, 6.0]);\n        assert_eq!(CpuBackend::sub_1d(\u0026a, \u0026b), vec![-2.0, -2.0]);\n        assert_eq!(CpuBackend::mul_1d(\u0026a, \u0026b), vec![3.0, 8.0]);\n        assert_eq!(CpuBackend::div_1d(\u0026a, \u0026b), vec![1.0 / 3.0, 0.5]);\n\n        assert_eq!(CpuBackend::add_scalar_1d(\u0026a, \u00265.0), vec![6.0, 7.0]);\n        assert_eq!(CpuBackend::mul_scalar_1d(\u0026a, \u00262.0), vec![2.0, 4.0]);\n    }\n\n    #[test]\n    fn test_elementwise_ops_2d() {\n        let a = CpuTensor2D::new(vec![1.0, 2.0], 1, 2);\n        let b = CpuTensor2D::new(vec![3.0, 4.0], 1, 2);\n\n        let add = CpuBackend::add_2d(\u0026a, \u0026b);\n        assert_eq!(add.0, vec![4.0, 6.0]);\n\n        let mul_s = CpuBackend::mul_scalar_2d(\u0026a, \u00262.0);\n        assert_eq!(mul_s.0, vec![2.0, 4.0]);\n    }\n\n    #[test]\n    fn test_reductions() {\n        let v = vec![1.0, 2.0, 3.0];\n        assert_eq!(CpuBackend::sum_all_1d(\u0026v), 6.0);\n        assert!((CpuBackend::mean_all_1d(\u0026v) - 2.0).abs() \u003c 1e-12);\n\n        let m = CpuTensor2D::new(vec![1.0, 2.0, 3.0, 4.0], 2, 2);\n        assert_eq!(CpuBackend::sum_all_2d(\u0026m), 10.0);\n        assert!((CpuBackend::mean_all_2d(\u0026m) - 2.5).abs() \u003c 1e-12);\n    }\n\n    #[test]\n    fn test_abs_and_sign() {\n        let v = vec![-2.0, 0.0, 3.0];\n        assert_eq!(CpuBackend::abs_1d(\u0026v), vec![2.0, 0.0, 3.0]);\n        assert_eq!(CpuBackend::sign_1d(\u0026v), vec![-1.0, 0.0, 1.0]);\n\n        let m = CpuTensor2D::new(vec![-1.0, 0.0, 2.0], 1, 3);\n        let sign_m = CpuBackend::sign_2d(\u0026m);\n        assert_eq!(sign_m.0, vec![-1.0, 0.0, 1.0]);\n    }\n\n    #[test]\n    fn test_math_functions() {\n        let v = vec![0.0, 1.0];\n        assert_eq!(CpuBackend::exp_1d(\u0026v), vec![1.0, std::f64::consts::E]);\n        assert_eq!(\n            CpuBackend::log_1d(\u0026vec![1.0, std::f64::consts::E]),\n            vec![0.0, 1.0]\n        );\n\n        // sigmoid(0) = 0.5\n        let sig = CpuBackend::sigmoid_1d(\u0026vec![0.0]);\n        assert!((sig[0] - 0.5).abs() \u003c 1e-12);\n\n        // maximum\n        let a = vec![1.0, 3.0];\n        let b = vec![2.0, 2.0];\n        assert_eq!(CpuBackend::maximum_1d(\u0026a, \u0026b), vec![2.0, 3.0]);\n    }\n\n    #[test]\n    fn test_transpose() {\n        let m = CpuTensor2D::new(vec![1.0, 2.0, 3.0, 4.0, 5.0, 6.0], 2, 3); // 2x3\n        let t = CpuBackend::transpose(\u0026m); // 3x2\n        assert_eq!(t.0, vec![1.0, 4.0, 2.0, 5.0, 3.0, 6.0]);\n        assert_eq!((t.1, t.2), (3, 2));\n\n        // Double transpose = original\n        let tt = CpuBackend::transpose(\u0026t);\n        assert_eq!(tt.0, m.0);\n        assert_eq!((tt.1, tt.2), (2, 3));\n    }\n\n    #[test]\n    fn test_matvec() {\n        let m = CpuTensor2D::new(vec![1.0, 2.0, 3.0, 4.0], 2, 2);\n        let v = vec![1.0, 0.0];\n        let res = CpuBackend::matvec(\u0026m, \u0026v);\n        assert_eq!(res, vec![1.0, 3.0]); // [1*1 + 2*0, 3*1 + 4*0]\n    }\n\n    #[test]\n    fn test_edge_cases() {\n        // ÐÑÑÑÐ¾Ð¹ ÑÐµÐ½Ð·Ð¾Ñ 1D\n        let empty1d = CpuBackend::zeros_1d(0);\n        assert_eq!(empty1d.len(), 0);\n        assert_eq!(CpuBackend::sum_all_1d(\u0026empty1d), 0.0);\n\n        // ÐÑÑÑÐ¾Ð¹ ÑÐµÐ½Ð·Ð¾Ñ 2D\n        let empty2d = CpuBackend::zeros_2d(0, 0);\n        assert_eq!(empty2d.0.len(), 0);\n        assert_eq!(CpuBackend::sum_all_2d(\u0026empty2d), 0.0);\n\n        // From empty nested vec\n        let t = CpuTensor2D::from(\u0026[][..]);\n        assert_eq!(t.0.len(), 0);\n        assert_eq!((t.1, t.2), (0, 0));\n\n        // ÐÐµÐ»ÐµÐ½Ð¸Ðµ Ð½Ð° Ð½Ð¾Ð»Ñ â Ð¾Ð¶Ð¸Ð´Ð°ÐµÐ¼ panic Ð¸Ð»Ð¸ NaN?\n        // Ð ÑÐµÐºÑÑÐµÐ¹ ÑÐµÐ°Ð»Ð¸Ð·Ð°ÑÐ¸Ð¸ Ð±ÑÐ´ÐµÑ panic Ð¿ÑÐ¸ Ð´ÐµÐ»ÐµÐ½Ð¸Ð¸ Ð½Ð° 0.0.\n        // ÐÑÐ»Ð¸ ÑÑÐ¾ Ð½Ðµ Ð¶ÐµÐ»Ð°ÐµÐ¼Ð¾ â ÑÑÐ¾Ð¸Ñ Ð¾Ð±ÑÑÐ´Ð¸ÑÑ, Ð½Ð¾ Ð¿Ð¾ÐºÐ° Ð¿Ð¾ÐºÑÐ¾ÐµÐ¼ ÐºÐ°Ðº ÐµÑÑÑ.\n        let a = vec![1.0];\n        let b = vec![0.0];\n        let res = CpuBackend::div_1d(\u0026a, \u0026b);\n        assert!(res[0].is_infinite()); // Ð¸Ð»Ð¸ assert_panics, ÐµÑÐ»Ð¸ ÑÐ¾ÑÐµÑÑ ÑÐ²Ð½ÑÐ¹ ÐºÐ¾Ð½ÑÑÐ¾Ð»Ñ\n    }\n\n    #[test]\n    fn test_ravel_2d_row_major_order() {\n        // Create a 2x3 matrix: [[1.0, 2.0, 3.0],\n        //                       [4.0, 5.0, 6.0]]\n        // Larger matrix to verify no data corruption\n        let data: Vec\u003cf64\u003e = (0..12).map(|i| i as f64).collect();\n        let matrix = CpuTensor2D::new(data.clone(), 3, 4);\n        let flattened = CpuBackend::ravel_2d(\u0026matrix);\n\n        assert_eq!(flattened.to_vec(), data);\n    }\n\n    // === Column manipulation tests ===\n\n    #[test]\n    fn test_hcat_2d_basic() {\n        // [[1, 2]] + [[3, 4]] -\u003e [[1, 2, 3, 4]]\n        let a = CpuTensor2D::new(vec![1.0, 2.0], 1, 2);\n        let b = CpuTensor2D::new(vec![3.0, 4.0], 1, 2);\n        let result = CpuBackend::hcat_2d(\u0026[a, b]).unwrap();\n\n        assert_eq!(result.0, vec![1.0, 2.0, 3.0, 4.0]);\n        assert_eq!((result.1, result.2), (1, 4));\n    }\n\n    #[test]\n    fn test_hcat_2d_multiple_rows() {\n        // [[1, 2],    [[5],    [[1, 2, 5],\n        //  [3, 4]]  +  [6]] -\u003e  [3, 4, 6]]\n        let a = CpuTensor2D::new(vec![1.0, 2.0, 3.0, 4.0], 2, 2);\n        let b = CpuTensor2D::new(vec![5.0, 6.0], 2, 1);\n        let result = CpuBackend::hcat_2d(\u0026[a, b]).unwrap();\n\n        assert_eq!(result.0, vec![1.0, 2.0, 5.0, 3.0, 4.0, 6.0]);\n        assert_eq!((result.1, result.2), (2, 3));\n    }\n\n    #[test]\n    fn test_hcat_2d_three_tensors() {\n        let a = CpuTensor2D::new(vec![1.0, 4.0], 2, 1);\n        let b = CpuTensor2D::new(vec![2.0, 5.0], 2, 1);\n        let c = CpuTensor2D::new(vec![3.0, 6.0], 2, 1);\n        let result = CpuBackend::hcat_2d(\u0026[a, b, c]).unwrap();\n\n        assert_eq!(result.0, vec![1.0, 2.0, 3.0, 4.0, 5.0, 6.0]);\n        assert_eq!((result.1, result.2), (2, 3));\n    }\n\n    #[test]\n    fn test_hcat_2d_empty_error() {\n        let result = CpuBackend::hcat_2d(\u0026[]);\n        assert!(result.is_err());\n    }\n\n    #[test]\n    fn test_select_columns_2d_basic() {\n        // [[1, 2, 3],    select [0, 2]   [[1, 3],\n        //  [4, 5, 6]]                  -\u003e [4, 6]]\n        let t = CpuTensor2D::new(vec![1.0, 2.0, 3.0, 4.0, 5.0, 6.0], 2, 3);\n        let result = CpuBackend::select_columns_2d(\u0026t, \u0026[0, 2]);\n\n        assert_eq!(result.0, vec![1.0, 3.0, 4.0, 6.0]);\n        assert_eq!((result.1, result.2), (2, 2));\n    }\n\n    #[test]\n    fn test_select_columns_2d_single() {\n        let t = CpuTensor2D::new(vec![1.0, 2.0, 3.0, 4.0, 5.0, 6.0], 2, 3);\n        let result = CpuBackend::select_columns_2d(\u0026t, \u0026[1]);\n\n        assert_eq!(result.0, vec![2.0, 5.0]);\n        assert_eq!((result.1, result.2), (2, 1));\n    }\n\n    #[test]\n    fn test_select_columns_2d_reorder() {\n        // Reordering columns: [2, 0, 1]\n        let t = CpuTensor2D::new(vec![1.0, 2.0, 3.0, 4.0, 5.0, 6.0], 2, 3);\n        let result = CpuBackend::select_columns_2d(\u0026t, \u0026[2, 0, 1]);\n\n        assert_eq!(result.0, vec![3.0, 1.0, 2.0, 6.0, 4.0, 5.0]);\n    }\n\n    #[test]\n    fn test_select_columns_2d_empty() {\n        let t = CpuTensor2D::new(vec![1.0, 2.0, 3.0, 4.0, 5.0, 6.0], 2, 3);\n        let result = CpuBackend::select_columns_2d(\u0026t, \u0026[]);\n\n        assert_eq!(result.0, vec![]);\n        assert_eq!((result.1, result.2), (2, 0));\n    }\n\n    #[test]\n    fn test_one_hot_from_indices_basic() {\n        // [0, 1, 2] with 3 classes -\u003e [[1,0,0], [0,1,0], [0,0,1]]\n        let indices = vec![0.0, 1.0, 2.0];\n        let result = CpuBackend::one_hot_from_indices(\u0026indices, 3);\n\n        assert_eq!(result.0, vec![1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0]);\n        assert_eq!((result.1, result.2), (3, 3));\n    }\n\n    #[test]\n    fn test_one_hot_from_indices_repeated() {\n        // [0, 1, 0] with 2 classes -\u003e [[1,0], [0,1], [1,0]]\n        let indices = vec![0.0, 1.0, 0.0];\n        let result = CpuBackend::one_hot_from_indices(\u0026indices, 2);\n\n        assert_eq!(result.0, vec![1.0, 0.0, 0.0, 1.0, 1.0, 0.0]);\n        assert_eq!((result.1, result.2), (3, 2));\n    }\n\n    #[test]\n    fn test_one_hot_from_indices_empty() {\n        let indices: Vec\u003cf64\u003e = vec![];\n        let result = CpuBackend::one_hot_from_indices(\u0026indices, 3);\n\n        assert_eq!(result.0, vec![]);\n        assert_eq!((result.1, result.2), (0, 3));\n    }\n\n    #[test]\n    fn test_one_hot_zero_classes() {\n        let indices = vec![0.0, 1.0];\n        let result = CpuBackend::one_hot_from_indices(\u0026indices, 0);\n\n        assert_eq!(result.0, vec![]);\n        assert_eq!((result.1, result.2), (2, 0));\n    }\n}\n","traces":[{"line":99,"address":[3044064,3044494],"length":1,"stats":{"Line":6}},{"line":100,"address":[3044120,3044184,3044417],"length":1,"stats":{"Line":13}},{"line":101,"address":[3044331],"length":1,"stats":{"Line":6}},{"line":124,"address":[3043696],"length":1,"stats":{"Line":1}},{"line":125,"address":[3043755],"length":1,"stats":{"Line":1}},{"line":126,"address":[3043786],"length":1,"stats":{"Line":1}},{"line":128,"address":[3043769],"length":1,"stats":{"Line":1}},{"line":129,"address":[3043777,3043828,3043889],"length":1,"stats":{"Line":2}},{"line":130,"address":[2945440,2945472],"length":1,"stats":{"Line":3}},{"line":134,"address":[2945504,2945529],"length":1,"stats":{"Line":3}},{"line":135,"address":[3044025],"length":1,"stats":{"Line":1}},{"line":148,"address":[3052032],"length":1,"stats":{"Line":0}},{"line":163,"address":[3065888],"length":1,"stats":{"Line":2}},{"line":164,"address":[3065905],"length":1,"stats":{"Line":2}},{"line":180,"address":[3065936],"length":1,"stats":{"Line":1}},{"line":181,"address":[3065973,3066046],"length":1,"stats":{"Line":1}},{"line":195,"address":[3051008],"length":1,"stats":{"Line":1}},{"line":196,"address":[2946075,2946064],"length":1,"stats":{"Line":3}},{"line":208,"address":[3051072],"length":1,"stats":{"Line":1}},{"line":209,"address":[2946091,2946080],"length":1,"stats":{"Line":3}},{"line":218,"address":[3060944],"length":1,"stats":{"Line":1}},{"line":219,"address":[2946352,2946395],"length":1,"stats":{"Line":3}},{"line":226,"address":[3062880],"length":1,"stats":{"Line":1}},{"line":227,"address":[2946928,2946971],"length":1,"stats":{"Line":3}},{"line":234,"address":[3062496],"length":1,"stats":{"Line":1}},{"line":235,"address":[3062547],"length":1,"stats":{"Line":3}},{"line":242,"address":[3061328],"length":1,"stats":{"Line":1}},{"line":243,"address":[2946555,2946512],"length":1,"stats":{"Line":4}},{"line":247,"address":[3051808],"length":1,"stats":{"Line":1}},{"line":248,"address":[2946192,2946206],"length":1,"stats":{"Line":3}},{"line":252,"address":[3051584],"length":1,"stats":{"Line":1}},{"line":253,"address":[2946096,2946121],"length":1,"stats":{"Line":3}},{"line":259,"address":[3051904],"length":1,"stats":{"Line":1}},{"line":260,"address":[2946240,2946254],"length":1,"stats":{"Line":3}},{"line":264,"address":[3051680],"length":1,"stats":{"Line":1}},{"line":265,"address":[2946144,2946158],"length":1,"stats":{"Line":3}},{"line":272,"address":[3061120],"length":1,"stats":{"Line":1}},{"line":274,"address":[3061170],"length":1,"stats":{"Line":3}},{"line":275,"address":[3061282],"length":1,"stats":{"Line":1}},{"line":276,"address":[3061286],"length":1,"stats":{"Line":1}},{"line":289,"address":[3063056],"length":1,"stats":{"Line":1}},{"line":290,"address":[3063099],"length":1,"stats":{"Line":1}},{"line":291,"address":[3063143],"length":1,"stats":{"Line":1}},{"line":298,"address":[2947008,2947051],"length":1,"stats":{"Line":3}},{"line":299,"address":[3063564],"length":1,"stats":{"Line":1}},{"line":300,"address":[3063568],"length":1,"stats":{"Line":1}},{"line":308,"address":[3062672],"length":1,"stats":{"Line":0}},{"line":310,"address":[3062722],"length":1,"stats":{"Line":0}},{"line":311,"address":[3062834],"length":1,"stats":{"Line":0}},{"line":312,"address":[3062838],"length":1,"stats":{"Line":0}},{"line":320,"address":[3061504],"length":1,"stats":{"Line":0}},{"line":322,"address":[2946592,2946635],"length":1,"stats":{"Line":0}},{"line":323,"address":[3061666],"length":1,"stats":{"Line":0}},{"line":324,"address":[3061670],"length":1,"stats":{"Line":0}},{"line":335,"address":[3051200],"length":1,"stats":{"Line":1}},{"line":336,"address":[3051214,3051326],"length":1,"stats":{"Line":2}},{"line":337,"address":[3051328],"length":1,"stats":{"Line":0}},{"line":339,"address":[3051228],"length":1,"stats":{"Line":1}},{"line":361,"address":[3051360],"length":1,"stats":{"Line":1}},{"line":362,"address":[3051374],"length":1,"stats":{"Line":1}},{"line":363,"address":[3051490],"length":1,"stats":{"Line":1}},{"line":370,"address":[3051388],"length":1,"stats":{"Line":1}},{"line":374,"address":[3050000],"length":1,"stats":{"Line":1}},{"line":375,"address":[3050005],"length":1,"stats":{"Line":1}},{"line":379,"address":[3049952],"length":1,"stats":{"Line":1}},{"line":380,"address":[3049957],"length":1,"stats":{"Line":1}},{"line":388,"address":[3049760],"length":1,"stats":{"Line":2}},{"line":398,"address":[3066064],"length":1,"stats":{"Line":3}},{"line":399,"address":[3066081],"length":1,"stats":{"Line":3}},{"line":403,"address":[3061888],"length":1,"stats":{"Line":1}},{"line":404,"address":[3061893],"length":1,"stats":{"Line":1}},{"line":408,"address":[3061904],"length":1,"stats":{"Line":1}},{"line":409,"address":[3061909],"length":1,"stats":{"Line":1}},{"line":415,"address":[3060736],"length":1,"stats":{"Line":1}},{"line":416,"address":[3060768],"length":1,"stats":{"Line":3}},{"line":425,"address":[3065424],"length":1,"stats":{"Line":1}},{"line":426,"address":[3065456],"length":1,"stats":{"Line":1}},{"line":427,"address":[3065478],"length":1,"stats":{"Line":3}},{"line":428,"address":[2947170,2947130],"length":1,"stats":{"Line":2}},{"line":429,"address":[2947156],"length":1,"stats":{"Line":1}},{"line":430,"address":[2947145,2947181],"length":1,"stats":{"Line":2}},{"line":431,"address":[2947183],"length":1,"stats":{"Line":1}},{"line":433,"address":[2947172],"length":1,"stats":{"Line":1}},{"line":443,"address":[3048624],"length":1,"stats":{"Line":1}},{"line":444,"address":[3048675],"length":1,"stats":{"Line":1}},{"line":445,"address":[3048706],"length":1,"stats":{"Line":1}},{"line":446,"address":[3048748],"length":1,"stats":{"Line":3}},{"line":451,"address":[3061712],"length":1,"stats":{"Line":1}},{"line":452,"address":[2946672,2946685],"length":1,"stats":{"Line":3}},{"line":479,"address":[3061920],"length":1,"stats":{"Line":1}},{"line":480,"address":[3061952],"length":1,"stats":{"Line":3}},{"line":489,"address":[3049776],"length":1,"stats":{"Line":1}},{"line":490,"address":[3049808],"length":1,"stats":{"Line":1}},{"line":491,"address":[2945904,2945918],"length":1,"stats":{"Line":3}},{"line":492,"address":[2945982,2945934],"length":1,"stats":{"Line":3}},{"line":493,"address":[2945990],"length":1,"stats":{"Line":1}},{"line":495,"address":[2945949],"length":1,"stats":{"Line":1}},{"line":496,"address":[2945960],"length":1,"stats":{"Line":2}},{"line":505,"address":[3060832],"length":1,"stats":{"Line":1}},{"line":506,"address":[2946334,2946320],"length":1,"stats":{"Line":3}},{"line":510,"address":[3065520],"length":1,"stats":{"Line":1}},{"line":512,"address":[3065550],"length":1,"stats":{"Line":1}},{"line":513,"address":[3065572],"length":1,"stats":{"Line":3}},{"line":514,"address":[2947242,2947282],"length":1,"stats":{"Line":2}},{"line":515,"address":[2947268],"length":1,"stats":{"Line":1}},{"line":516,"address":[2947293,2947257],"length":1,"stats":{"Line":2}},{"line":517,"address":[2947295],"length":1,"stats":{"Line":1}},{"line":519,"address":[2947284],"length":1,"stats":{"Line":1}},{"line":522,"address":[3065580],"length":1,"stats":{"Line":1}},{"line":523,"address":[3065599],"length":1,"stats":{"Line":1}},{"line":524,"address":[3065603],"length":1,"stats":{"Line":1}},{"line":532,"address":[3048800],"length":1,"stats":{"Line":1}},{"line":533,"address":[3048829],"length":1,"stats":{"Line":1}},{"line":537,"address":[3061808],"length":1,"stats":{"Line":1}},{"line":538,"address":[3061831],"length":1,"stats":{"Line":1}},{"line":545,"address":[3062016],"length":1,"stats":{"Line":3}},{"line":546,"address":[3062039],"length":1,"stats":{"Line":1}},{"line":550,"address":[3049872],"length":1,"stats":{"Line":1}},{"line":551,"address":[3049895],"length":1,"stats":{"Line":1}},{"line":569,"address":[3062096],"length":1,"stats":{"Line":1}},{"line":570,"address":[3062241],"length":1,"stats":{"Line":1}},{"line":577,"address":[3062473],"length":1,"stats":{"Line":1}},{"line":591,"address":[3052828,3052834,3052048],"length":1,"stats":{"Line":1}},{"line":592,"address":[3052094],"length":1,"stats":{"Line":1}},{"line":593,"address":[3052140],"length":1,"stats":{"Line":1}},{"line":594,"address":[3052172,3052263],"length":1,"stats":{"Line":2}},{"line":595,"address":[3052377],"length":1,"stats":{"Line":1}},{"line":596,"address":[3052480,3052823,3052389],"length":1,"stats":{"Line":3}},{"line":597,"address":[3052599,3052660],"length":1,"stats":{"Line":2}},{"line":599,"address":[3052614],"length":1,"stats":{"Line":1}},{"line":601,"address":[3052427],"length":1,"stats":{"Line":1}},{"line":613,"address":[3052848],"length":1,"stats":{"Line":2}},{"line":614,"address":[3052931,3053010,3052899],"length":1,"stats":{"Line":5}},{"line":621,"address":[3053225],"length":1,"stats":{"Line":2}},{"line":632,"address":[3060704,3060592,3060710],"length":1,"stats":{"Line":2}},{"line":633,"address":[3060630],"length":1,"stats":{"Line":2}},{"line":643,"address":[3066112,3066821,3066849],"length":1,"stats":{"Line":2}},{"line":644,"address":[3066150],"length":1,"stats":{"Line":2}},{"line":645,"address":[3066203],"length":1,"stats":{"Line":2}},{"line":646,"address":[3066251,3066337],"length":1,"stats":{"Line":4}},{"line":647,"address":[3066583,3066448],"length":1,"stats":{"Line":4}},{"line":648,"address":[3066693],"length":1,"stats":{"Line":2}},{"line":651,"address":[3066491],"length":1,"stats":{"Line":2}},{"line":655,"address":[3060720],"length":1,"stats":{"Line":2}},{"line":656,"address":[3060725],"length":1,"stats":{"Line":2}},{"line":660,"address":[3065840],"length":1,"stats":{"Line":1}},{"line":661,"address":[3065857],"length":1,"stats":{"Line":1}},{"line":666,"address":[3050978,3050984,3050048],"length":1,"stats":{"Line":4}},{"line":667,"address":[3050098],"length":1,"stats":{"Line":4}},{"line":668,"address":[3050119,3050151],"length":1,"stats":{"Line":8}},{"line":669,"address":[3050133],"length":1,"stats":{"Line":0}},{"line":671,"address":[3050176],"length":1,"stats":{"Line":4}},{"line":673,"address":[3050193],"length":1,"stats":{"Line":4}},{"line":674,"address":[3050226,3050843,3050305],"length":1,"stats":{"Line":13}},{"line":675,"address":[3050547],"length":1,"stats":{"Line":5}},{"line":676,"address":[3050653,3050559,3050973],"length":1,"stats":{"Line":15}},{"line":677,"address":[3050858,3050762],"length":1,"stats":{"Line":10}},{"line":679,"address":[3050787],"length":1,"stats":{"Line":5}},{"line":681,"address":[3050592],"length":1,"stats":{"Line":5}},{"line":684,"address":[3047740,3047734,3046656],"length":1,"stats":{"Line":5}},{"line":685,"address":[3046722],"length":1,"stats":{"Line":5}},{"line":686,"address":[3046775,3046743],"length":1,"stats":{"Line":10}},{"line":687,"address":[3046757],"length":1,"stats":{"Line":0}},{"line":689,"address":[3046792],"length":1,"stats":{"Line":5}},{"line":691,"address":[3046820],"length":1,"stats":{"Line":5}},{"line":692,"address":[3046833],"length":1,"stats":{"Line":5}},{"line":694,"address":[3046906,3046990,3047532],"length":1,"stats":{"Line":15}},{"line":695,"address":[3047104],"length":1,"stats":{"Line":5}},{"line":696,"address":[3047116,3047223,3047729],"length":1,"stats":{"Line":15}},{"line":697,"address":[3047332,3047547],"length":1,"stats":{"Line":10}},{"line":698,"address":[3047707],"length":1,"stats":{"Line":5}},{"line":700,"address":[3047465,3047370],"length":1,"stats":{"Line":5}},{"line":701,"address":[3047490,3047439],"length":1,"stats":{"Line":10}},{"line":703,"address":[3047149],"length":1,"stats":{"Line":5}},{"line":706,"address":[3046633,3046627,3045744],"length":1,"stats":{"Line":2}},{"line":707,"address":[3045794],"length":1,"stats":{"Line":2}},{"line":708,"address":[3045847,3045815],"length":1,"stats":{"Line":4}},{"line":709,"address":[3045829],"length":1,"stats":{"Line":0}},{"line":711,"address":[3045872],"length":1,"stats":{"Line":2}},{"line":713,"address":[3045889],"length":1,"stats":{"Line":2}},{"line":714,"address":[3045927,3046006],"length":1,"stats":{"Line":3}},{"line":715,"address":[3046342,3046248],"length":1,"stats":{"Line":4}},{"line":716,"address":[3046455],"length":1,"stats":{"Line":2}},{"line":717,"address":[3046594,3046622],"length":1,"stats":{"Line":4}},{"line":718,"address":[3046618],"length":1,"stats":{"Line":2}},{"line":722,"address":[3046281],"length":1,"stats":{"Line":2}},{"line":725,"address":[3045717,3044832,3045711],"length":1,"stats":{"Line":3}},{"line":726,"address":[3044882],"length":1,"stats":{"Line":2}},{"line":727,"address":[3044935,3044903],"length":1,"stats":{"Line":5}},{"line":728,"address":[3044917],"length":1,"stats":{"Line":0}},{"line":730,"address":[3044960],"length":1,"stats":{"Line":2}},{"line":732,"address":[3044977],"length":1,"stats":{"Line":1}},{"line":733,"address":[3045015,3045094],"length":1,"stats":{"Line":5}},{"line":734,"address":[3045430,3045336],"length":1,"stats":{"Line":5}},{"line":735,"address":[3045543],"length":1,"stats":{"Line":2}},{"line":736,"address":[3045706,3045682],"length":1,"stats":{"Line":5}},{"line":737,"address":[3045702],"length":1,"stats":{"Line":2}},{"line":741,"address":[3045369],"length":1,"stats":{"Line":3}},{"line":744,"address":[3048602,3047760,3048596],"length":1,"stats":{"Line":0}},{"line":745,"address":[3047810],"length":1,"stats":{"Line":0}},{"line":746,"address":[3047831,3047863],"length":1,"stats":{"Line":0}},{"line":747,"address":[3047845],"length":1,"stats":{"Line":0}},{"line":749,"address":[3047888],"length":1,"stats":{"Line":0}},{"line":751,"address":[3047905],"length":1,"stats":{"Line":0}},{"line":752,"address":[3048011,3047938],"length":1,"stats":{"Line":0}},{"line":753,"address":[3048253,3048347,3048591],"length":1,"stats":{"Line":0}},{"line":754,"address":[3048460],"length":1,"stats":{"Line":0}},{"line":757,"address":[3048286],"length":1,"stats":{"Line":0}},{"line":762,"address":[3049733,3049727,3048880],"length":1,"stats":{"Line":0}},{"line":763,"address":[3048930],"length":1,"stats":{"Line":0}},{"line":764,"address":[3048967,3048996],"length":1,"stats":{"Line":0}},{"line":765,"address":[3048981],"length":1,"stats":{"Line":0}},{"line":768,"address":[3049010],"length":1,"stats":{"Line":0}},{"line":769,"address":[3049043,3049138],"length":1,"stats":{"Line":0}},{"line":770,"address":[3049466,3049722,3049383],"length":1,"stats":{"Line":0}},{"line":771,"address":[3049589],"length":1,"stats":{"Line":0}},{"line":774,"address":[3049416],"length":1,"stats":{"Line":0}},{"line":779,"address":[3059472,3060558,3060586],"length":1,"stats":{"Line":1}},{"line":780,"address":[3059538],"length":1,"stats":{"Line":2}},{"line":781,"address":[3059583],"length":1,"stats":{"Line":1}},{"line":783,"address":[3059748,3059838],"length":1,"stats":{"Line":2}},{"line":784,"address":[3059813,3059902],"length":1,"stats":{"Line":3}},{"line":785,"address":[3060016,3060146],"length":1,"stats":{"Line":3}},{"line":786,"address":[3060398],"length":1,"stats":{"Line":1}},{"line":789,"address":[3060049],"length":1,"stats":{"Line":2}},{"line":792,"address":[3058318,3057232,3058346],"length":1,"stats":{"Line":1}},{"line":793,"address":[3057298],"length":1,"stats":{"Line":2}},{"line":794,"address":[3057343],"length":1,"stats":{"Line":1}},{"line":796,"address":[3057508,3057598],"length":1,"stats":{"Line":2}},{"line":797,"address":[3057662,3057573],"length":1,"stats":{"Line":3}},{"line":798,"address":[3057906,3057776],"length":1,"stats":{"Line":3}},{"line":799,"address":[3058158],"length":1,"stats":{"Line":1}},{"line":802,"address":[3057809],"length":1,"stats":{"Line":2}},{"line":805,"address":[3058352,3059438,3059466],"length":1,"stats":{"Line":2}},{"line":806,"address":[3058418],"length":1,"stats":{"Line":2}},{"line":807,"address":[3058463],"length":1,"stats":{"Line":2}},{"line":809,"address":[3058718,3058628],"length":1,"stats":{"Line":2}},{"line":810,"address":[3058693,3058782],"length":1,"stats":{"Line":8}},{"line":811,"address":[3058896,3059026],"length":1,"stats":{"Line":8}},{"line":812,"address":[3059278],"length":1,"stats":{"Line":2}},{"line":815,"address":[3058929],"length":1,"stats":{"Line":6}},{"line":818,"address":[3057198,3057226,3056112],"length":1,"stats":{"Line":1}},{"line":819,"address":[3056178],"length":1,"stats":{"Line":1}},{"line":820,"address":[3056223],"length":1,"stats":{"Line":1}},{"line":822,"address":[3056478,3056388],"length":1,"stats":{"Line":1}},{"line":823,"address":[3056453,3056542],"length":1,"stats":{"Line":2}},{"line":824,"address":[3056656,3056786],"length":1,"stats":{"Line":2}},{"line":825,"address":[3057038],"length":1,"stats":{"Line":1}},{"line":828,"address":[3056689],"length":1,"stats":{"Line":1}},{"line":831,"address":[3065632],"length":1,"stats":{"Line":0}},{"line":832,"address":[3065664],"length":1,"stats":{"Line":0}},{"line":835,"address":[3065728],"length":1,"stats":{"Line":0}},{"line":836,"address":[2947360,2947374],"length":1,"stats":{"Line":0}},{"line":841,"address":[3063600,3064980,3064952],"length":1,"stats":{"Line":1}},{"line":844,"address":[3063677],"length":1,"stats":{"Line":1}},{"line":845,"address":[3063752],"length":1,"stats":{"Line":1}},{"line":846,"address":[3063718],"length":1,"stats":{"Line":1}},{"line":850,"address":[3063849,3063882,3063702],"length":1,"stats":{"Line":5}},{"line":851,"address":[3063861],"length":1,"stats":{"Line":3}},{"line":852,"address":[3063899],"length":1,"stats":{"Line":0}},{"line":856,"address":[3064009,3064059],"length":1,"stats":{"Line":6}},{"line":857,"address":[3064135],"length":1,"stats":{"Line":3}},{"line":858,"address":[3065293],"length":1,"stats":{"Line":0}},{"line":859,"address":[3064986],"length":1,"stats":{"Line":0}},{"line":860,"address":[3065125,3065185],"length":1,"stats":{"Line":0}},{"line":866,"address":[2947098,2947088],"length":1,"stats":{"Line":9}},{"line":869,"address":[3064310,3064217],"length":1,"stats":{"Line":3}},{"line":870,"address":[3064277,3064374],"length":1,"stats":{"Line":6}},{"line":871,"address":[3064669,3064496],"length":1,"stats":{"Line":6}},{"line":872,"address":[3064779,3064830],"length":1,"stats":{"Line":3}},{"line":873,"address":[3064865,3064897,3064811],"length":1,"stats":{"Line":6}},{"line":874,"address":[3064925,3064873],"length":1,"stats":{"Line":6}},{"line":878,"address":[3064524],"length":1,"stats":{"Line":3}},{"line":881,"address":[3053248,3054360,3054075],"length":1,"stats":{"Line":1}},{"line":882,"address":[3053324],"length":1,"stats":{"Line":1}},{"line":883,"address":[3053393],"length":1,"stats":{"Line":2}},{"line":884,"address":[3053442],"length":1,"stats":{"Line":1}},{"line":888,"address":[3053418,3053493],"length":1,"stats":{"Line":2}},{"line":889,"address":[3054621,3054396],"length":1,"stats":{"Line":0}},{"line":897,"address":[3053683,3053604],"length":1,"stats":{"Line":1}},{"line":898,"address":[3053747,3053658],"length":1,"stats":{"Line":2}},{"line":899,"address":[3053869,3054091],"length":1,"stats":{"Line":6}},{"line":900,"address":[3054216],"length":1,"stats":{"Line":3}},{"line":904,"address":[3054065,3053988,3053917],"length":1,"stats":{"Line":3}},{"line":907,"address":[3054640,3055717,3055745],"length":1,"stats":{"Line":1}},{"line":908,"address":[3054686],"length":1,"stats":{"Line":2}},{"line":909,"address":[3054764,3054712],"length":1,"stats":{"Line":3}},{"line":910,"address":[3054718],"length":1,"stats":{"Line":1}},{"line":914,"address":[3054896,3054780],"length":1,"stats":{"Line":4}},{"line":915,"address":[3055751],"length":1,"stats":{"Line":0}},{"line":924,"address":[3055101,3055025],"length":1,"stats":{"Line":2}},{"line":925,"address":[3055084,3055165,3055712],"length":1,"stats":{"Line":6}},{"line":926,"address":[3055409],"length":1,"stats":{"Line":2}},{"line":927,"address":[3055607,3055491],"length":1,"stats":{"Line":4}},{"line":930,"address":[3055521],"length":1,"stats":{"Line":2}}],"covered":252,"coverable":295},{"path":["/","home","victor","ML","machinelearne-rs","lib","src","backend","mod.rs"],"content":"//! # Backend Abstraction\n//!\n//! This module provides a trait-based abstraction over computation backends,\n//! enabling models to run on different hardware (CPU, GPU) and tensor libraries\n//! without code changes.\n//!\n//! ## Design Philosophy\n//!\n//! - **Minimal trait surface**: Only essential operations are exposed, keeping backend\n//!   implementations simple and testable.\n//! - **Zero-cost generics**: Backend selection happens at compile time via type parameters,\n//!   avoiding runtime dispatch overhead.\n//! - **Type-safe tensor handling**: Each backend defines its own tensor types (`Tensor1D`,\n//!   `Tensor2D`) that encapsulate storage details while exposing a uniform API.\n//! - **Feature-gated implementations**: Backends are enabled via Cargo features (`cpu`,\n//!   `ndarray`, future `cuda`, etc.), allowing users to minimize dependencies.\n//!\n//! ## Available Backends\n//!\n//! | Backend      | Feature    | Use Case                          |\n//! |--------------|------------|-----------------------------------|\n//! | `CpuBackend` | `cpu`      | Default, pure-Rust implementation |\n//! | `NdarrayBackend` | `ndarray` | Interop with `ndarray` ecosystem |\n//!\n//! ## Example\n//!\n//! ```rust\n//! use machinelearne_rs::backend::{Backend, CpuBackend, Tensor1D, Tensor2D};\n//!\n//! // Backend selection via type parameter\n//! let x: Tensor1D\u003cCpuBackend\u003e = Tensor1D::new(vec![1.0, 2.0]);\n//! let w: Tensor2D\u003cCpuBackend\u003e = Tensor2D::new(vec![0.5, 0.5, 0.5, 0.5], 2, 2);\n//!\n//! // Operations work identically across backends\n//! let y = w.dot(\u0026x);\n//! ```\n//!\n//! ## Implementing a New Backend\n//!\n//! To add a backend (e.g., CUDA):\n//! 1. Create a module with feature gate (`#[cfg(feature = \"cuda\")]`)\n//! 2. Implement concrete tensor types (`CudaTensor1D`, `CudaTensor2D`)\n//! 3. Implement the `Backend` trait with GPU-accelerated operations\n//! 4. Export types via `pub use` for user access\n//!\n//! See `cpu.rs` for a reference implementation.\n\nuse crate::preprocessing::PreprocessingError;\n\n#[cfg(feature = \"cpu\")]\npub mod cpu;\n#[cfg(feature = \"cpu\")]\n/// Pure-Rust CPU backend implementation with zero external dependencies.\npub use cpu::{CpuBackend, CpuTensor2D};\n\n#[cfg(feature = \"ndarray\")]\nmod ndarray_backend;\n#[cfg(feature = \"ndarray\")]\n/// Backend backed by the `ndarray` crate for ecosystem interoperability.\npub use ndarray_backend::{NdarrayBackend, NdarrayTensor2D};\n\n/// Scalar value representation and arithmetic operations.\npub mod scalar;\n/// One-dimensional tensor abstraction.\npub mod tensor1d;\n/// Two-dimensional tensor abstraction.\npub mod tensor2d;\n/// Shared tensor-like operations trait.\npub mod tensorlike;\n\npub use scalar::{Scalar, ScalarOps};\npub use tensor1d::Tensor1D;\npub use tensor2d::Tensor2D;\n\n/// Abstraction over computation devices and tensor operations.\n///\n/// The `Backend` trait defines a minimal set of operations required for\n/// training and inference in machine learning models. Implementations\n/// provide concrete tensor types and device-specific optimizations while\n/// maintaining a uniform API surface.\n///\n/// # Type Parameters\n///\n/// - `Scalar`: Primitive numeric type with arithmetic capabilities\n/// - `Tensor1D`: One-dimensional array representation\n/// - `Tensor2D`: Two-dimensional matrix representation\n/// - `Device`: Hardware device identifier (CPU core, GPU ID, etc.)\n///\n/// # Safety Guarantees\n///\n/// - All checked operations (`matvec`, `matvec_transposed`) validate shapes\n///   and panic on mismatch\n/// - Unchecked variants (`_matvec_unchecked`) skip validation for performance;\n///   caller must ensure correctness\n/// - Tensor types are `Clone + Send + Sync` for safe concurrent usage\n///\n/// # Example Implementation Sketch\n///\n/// ```ignore\n/// use machinelearne_rs::backend::{Backend, ScalarOps};\n/// #[derive(Clone, Debug, Copy)]\n/// struct MyBackend;\n///\n///  #[derive(Clone, Debug, Copy)]\n/// struct MyTensor1D;\n///\n///  #[derive(Clone, Debug, Copy)]\n/// struct MyTensor2D;\n///\n/// impl Backend for MyBackend {\n///     type Scalar = f64;\n///     type Tensor1D = MyTensor1D;\n///     type Tensor2D = MyTensor2D;\n///     type Device = ();\n///\n///     fn default_device() -\u003e Self::Device { () }\n///     // ... implement all required methods\n/// }\n/// ```\npub trait Backend: Clone + Copy + 'static {\n    /// Scalar type supporting arithmetic operations.\n    type Scalar: ScalarOps + Clone;\n\n    /// One-dimensional tensor type.\n    type Tensor1D: Clone + Send + Sync;\n\n    /// Two-dimensional tensor type.\n    type Tensor2D: Clone + Send + Sync;\n\n    /// Device identifier type (CPU core index, GPU handle, etc.).\n    type Device: Clone + Send + Sync;\n\n    /// Returns the default device for this backend.\n    ///\n    /// For CPU backends, typically returns a unit type `()`.\n    /// For GPU backends, may return a device handle or index.\n    fn default_device() -\u003e Self::Device;\n\n    // --- Constructors ---\n\n    /// Creates a 1D tensor filled with zeros of given length.\n    fn zeros_1d(len: usize) -\u003e Self::Tensor1D;\n\n    /// Creates a 2D tensor filled with zeros of given dimensions.\n    fn zeros_2d(rows: usize, cols: usize) -\u003e Self::Tensor2D;\n\n    /// Constructs a 1D tensor from owned data.\n    ///\n    /// # Panics\n    /// Implementation-defined (typically none for valid inputs).\n    fn from_vec_1d(data: Vec\u003cf32\u003e) -\u003e Self::Tensor1D;\n\n    /// Constructs a 2D tensor from row-major ordered data.\n    ///\n    /// # Arguments\n    /// * `data` - Flattened tensor values in row-major order\n    /// * `rows` - Number of rows\n    /// * `cols` - Number of columns\n    ///\n    /// # Panics\n    /// If `data.len() != rows * cols`.\n    fn from_vec_2d(data: Vec\u003cf32\u003e, rows: usize, cols: usize) -\u003e Self::Tensor2D;\n\n    // --- Element-wise operations (1D) ---\n\n    /// Element-wise addition of two 1D tensors.\n    ///\n    /// # Panics\n    /// If tensors have different lengths.\n    fn add_1d(a: \u0026Self::Tensor1D, b: \u0026Self::Tensor1D) -\u003e Self::Tensor1D;\n\n    /// Element-wise subtraction of two 1D tensors.\n    ///\n    /// # Panics\n    /// If tensors have different lengths.\n    fn sub_1d(a: \u0026Self::Tensor1D, b: \u0026Self::Tensor1D) -\u003e Self::Tensor1D;\n\n    /// Element-wise multiplication of two 1D tensors.\n    ///\n    /// # Panics\n    /// If tensors have different lengths.\n    fn mul_1d(a: \u0026Self::Tensor1D, b: \u0026Self::Tensor1D) -\u003e Self::Tensor1D;\n\n    /// Element-wise division of two 1D tensors.\n    ///\n    /// # Panics\n    /// If tensors have different lengths or divisor contains zeros.\n    fn div_1d(a: \u0026Self::Tensor1D, b: \u0026Self::Tensor1D) -\u003e Self::Tensor1D;\n\n    /// Multiplies each element of tensor by a scalar.\n    fn mul_scalar_1d(t: \u0026Self::Tensor1D, s: \u0026Self::Scalar) -\u003e Self::Tensor1D;\n\n    /// Adds a scalar to each element of tensor.\n    fn add_scalar_1d(t: \u0026Self::Tensor1D, s: \u0026Self::Scalar) -\u003e Self::Tensor1D;\n\n    // --- Element-wise operations (2D) ---\n\n    /// Multiplies each element of 2D tensor by a scalar.\n    fn mul_scalar_2d(t: \u0026Self::Tensor2D, s: \u0026Self::Scalar) -\u003e Self::Tensor2D;\n\n    /// Adds a scalar to each element of 2D tensor.\n    fn add_scalar_2d(t: \u0026Self::Tensor2D, s: \u0026Self::Scalar) -\u003e Self::Tensor2D;\n\n    /// Element-wise addition of two 2D tensors.\n    ///\n    /// # Panics\n    /// If tensors have different shapes.\n    fn add_2d(a: \u0026Self::Tensor2D, b: \u0026Self::Tensor2D) -\u003e Self::Tensor2D;\n\n    /// Element-wise subtraction of two 2D tensors.\n    ///\n    /// # Panics\n    /// If tensors have different shapes.\n    fn sub_2d(a: \u0026Self::Tensor2D, b: \u0026Self::Tensor2D) -\u003e Self::Tensor2D;\n\n    /// Element-wise multiplication of two 2D tensors.\n    ///\n    /// # Panics\n    /// If tensors have different shapes.\n    fn mul_2d(a: \u0026Self::Tensor2D, b: \u0026Self::Tensor2D) -\u003e Self::Tensor2D;\n\n    /// Element-wise division of two 2D tensors.\n    ///\n    /// # Panics\n    /// If tensors have different shapes or divisor contains zeros.\n    fn div_2d(a: \u0026Self::Tensor2D, b: \u0026Self::Tensor2D) -\u003e Self::Tensor2D;\n\n    // --- Reduction operations ---\n\n    /// Computes the arithmetic mean of all elements in a 1D tensor.\n    fn mean_all_1d(t: \u0026Self::Tensor1D) -\u003e Self::Scalar;\n\n    /// Computes the arithmetic mean of all elements in a 2D tensor.\n    fn mean_all_2d(t: \u0026Self::Tensor2D) -\u003e Self::Scalar;\n\n    /// Computes the sum of all elements in a 2D tensor.\n    fn sum_all_2d(t: \u0026Self::Tensor2D) -\u003e Self::Scalar;\n\n    /// Computes the sum of all elements in a 1D tensor.\n    fn sum_all_1d(t: \u0026Self::Tensor1D) -\u003e Self::Scalar;\n\n    // --- Scalar operations ---\n\n    /// Creates a backend-specific scalar from an f64 value.\n    ///\n    /// Used for loss gradients, learning rate updates, and other scalar computations.\n    fn scalar_f64(value: f64) -\u003e Self::Scalar;\n\n    // --- Data access ---\n\n    /// Converts a 1D tensor to a Vec of f64 values.\n    ///\n    /// Primarily used for metrics computation and debugging.\n    /// Not intended for hot paths due to allocation overhead.\n    fn to_vec_1d(t: \u0026Self::Tensor1D) -\u003e Vec\u003cf64\u003e;\n\n    /// Returns the number of elements in a 1D tensor.\n    fn len_1d(t: \u0026Self::Tensor1D) -\u003e usize;\n\n    /// Returns the total number of elements in a 2D tensor (rows Ã cols).\n    fn len_2d(t: \u0026Self::Tensor2D) -\u003e usize;\n\n    // --- Mathematical functions (1D) ---\n\n    /// Element-wise absolute value.\n    fn abs_1d(t: \u0026Self::Tensor1D) -\u003e Self::Tensor1D;\n\n    /// Element-wise sign function: returns -1.0, 0.0, or 1.0.\n    fn sign_1d(x: \u0026Self::Tensor1D) -\u003e Self::Tensor1D;\n\n    /// Element-wise maximum between two tensors.\n    ///\n    /// # Panics\n    /// If tensors have different lengths.\n    fn maximum_1d(x: \u0026Self::Tensor1D, other: \u0026Self::Tensor1D) -\u003e Self::Tensor1D;\n\n    /// Element-wise exponential function (e^x).\n    fn exp_1d(x: \u0026Self::Tensor1D) -\u003e Self::Tensor1D;\n\n    /// Element-wise natural logarithm (ln(x)).\n    ///\n    /// # Panics\n    /// If any element is â¤ 0.0.\n    fn log_1d(x: \u0026Self::Tensor1D) -\u003e Self::Tensor1D;\n\n    /// Element-wise sigmoid function: 1 / (1 + e^(-x)).\n    fn sigmoid_1d(x: \u0026Self::Tensor1D) -\u003e Self::Tensor1D;\n\n    // --- Mathematical functions (2D) ---\n\n    /// Element-wise absolute value for 2D tensors.\n    fn abs_2d(t: \u0026Self::Tensor2D) -\u003e Self::Tensor2D;\n\n    /// Element-wise sign function for 2D tensors.\n    fn sign_2d(x: \u0026Self::Tensor2D) -\u003e Self::Tensor2D;\n\n    /// Element-wise maximum between two 2D tensors.\n    ///\n    /// # Panics\n    /// If tensors have different shapes.\n    fn maximum_2d(x: \u0026Self::Tensor2D, other: \u0026Self::Tensor2D) -\u003e Self::Tensor2D;\n\n    /// Element-wise exponential function for 2D tensors.\n    fn exp_2d(x: \u0026Self::Tensor2D) -\u003e Self::Tensor2D;\n\n    /// Element-wise natural logarithm for 2D tensors.\n    ///\n    /// # Panics\n    /// If any element is â¤ 0.0.\n    fn log_2d(x: \u0026Self::Tensor2D) -\u003e Self::Tensor2D;\n\n    /// Element-wise sigmoid function for 2D tensors.\n    fn sigmoid_2d(x: \u0026Self::Tensor2D) -\u003e Self::Tensor2D;\n\n    // --- Linear algebra ---\n\n    /// Matrix-vector multiplication with shape checking.\n    ///\n    /// Computes `y = A * x` where `A` is (m Ã n) and `x` is (n,).\n    /// Returns a (m,) vector.\n    ///\n    /// # Panics\n    /// If `A.cols() != x.len()`.\n    fn matvec(a: \u0026Self::Tensor2D, x: \u0026Self::Tensor1D) -\u003e Self::Tensor1D;\n\n    /// Matrix-vector multiplication without shape checking.\n    ///\n    /// # Safety\n    /// Caller must ensure `A.cols() == x.len()`. Undefined behavior may occur\n    /// if shapes are incompatible.\n    fn _matvec_unchecked(a: \u0026Self::Tensor2D, x: \u0026Self::Tensor1D) -\u003e Self::Tensor1D;\n\n    /// Transposed matrix-vector multiplication with shape checking.\n    ///\n    /// Computes `y = A^T * x` where `A` is (m Ã n) and `x` is (m,).\n    /// Returns a (n,) vector.\n    ///\n    /// # Panics\n    /// If `A.rows() != x.len()`.\n    fn matvec_transposed(a: \u0026Self::Tensor2D, x: \u0026Self::Tensor1D) -\u003e Self::Tensor1D;\n\n    /// Transposed matrix-vector multiplication without shape checking.\n    ///\n    /// # Safety\n    /// Caller must ensure `A.rows() == x.len()`. Undefined behavior may occur\n    /// if shapes are incompatible.\n    fn _matvec_transposed_unchecked(a: \u0026Self::Tensor2D, x: \u0026Self::Tensor1D) -\u003e Self::Tensor1D;\n\n    /// Returns the transpose of a 2D tensor.\n    ///\n    /// Converts an (m Ã n) matrix to (n Ã m) with elements at (i,j) â (j,i).\n    fn transpose(t: \u0026Self::Tensor2D) -\u003e Self::Tensor2D;\n\n    /// Returns the shape of a 2D tensor as (rows, cols).\n    fn shape(t: \u0026Self::Tensor2D) -\u003e (usize, usize);\n\n    //Flattens 2d tensor into 1d tensor\n    fn ravel_2d(x: \u0026Self::Tensor2D) -\u003e Self::Tensor1D;\n\n    // --- Column-wise operations (for preprocessing) ---\n\n    /// Computes the mean of each column in a 2D tensor.\n    ///\n    /// Returns a 1D tensor of length `cols` where each element is the mean\n    /// of the corresponding column.\n    ///\n    /// For a tensor with shape (rows, cols), computes mean along axis 0.\n    fn col_mean_2d(t: \u0026Self::Tensor2D) -\u003e Self::Tensor1D;\n\n    /// Computes the standard deviation of each column in a 2D tensor.\n    ///\n    /// # Arguments\n    /// * `t` - Input 2D tensor\n    /// * `ddof` - Delta degrees of freedom (1 for sample std, 0 for population std)\n    ///\n    /// Returns a 1D tensor of length `cols`.\n    fn col_std_2d(t: \u0026Self::Tensor2D, ddof: usize) -\u003e Self::Tensor1D;\n\n    /// Computes the minimum value of each column in a 2D tensor.\n    ///\n    /// Returns a 1D tensor of length `cols`.\n    fn col_min_2d(t: \u0026Self::Tensor2D) -\u003e Self::Tensor1D;\n\n    /// Computes the maximum value of each column in a 2D tensor.\n    ///\n    /// Returns a 1D tensor of length `cols`.\n    fn col_max_2d(t: \u0026Self::Tensor2D) -\u003e Self::Tensor1D;\n\n    /// Computes the sum of each column in a 2D tensor.\n    ///\n    /// Returns a 1D tensor of length `cols`.\n    fn col_sum_2d(t: \u0026Self::Tensor2D) -\u003e Self::Tensor1D;\n\n    // --- Row-wise operations ---\n\n    /// Computes the sum of each row in a 2D tensor.\n    ///\n    /// Returns a 1D tensor of length `rows`.\n    fn row_sum_2d(t: \u0026Self::Tensor2D) -\u003e Self::Tensor1D;\n\n    // --- Broadcasting operations ---\n\n    /// Broadcasts a 1D tensor and subtracts from each row of a 2D tensor.\n    ///\n    /// For a 2D tensor with shape (rows, cols) and a 1D tensor with shape (cols,),\n    /// subtracts the 1D tensor from each row of the 2D tensor.\n    ///\n    /// Result[i, j] = t[i, j] - v[j]\n    fn broadcast_sub_1d_to_2d_rows(t: \u0026Self::Tensor2D, v: \u0026Self::Tensor1D) -\u003e Self::Tensor2D;\n\n    /// Broadcasts a 1D tensor and divides each row of a 2D tensor.\n    ///\n    /// For a 2D tensor with shape (rows, cols) and a 1D tensor with shape (cols,),\n    /// divides each row of the 2D tensor by the 1D tensor element-wise.\n    ///\n    /// Result[i, j] = t[i, j] / v[j]\n    fn broadcast_div_1d_to_2d_rows(t: \u0026Self::Tensor2D, v: \u0026Self::Tensor1D) -\u003e Self::Tensor2D;\n\n    /// Broadcasts a 1D tensor and multiplies each row of a 2D tensor.\n    ///\n    /// For a 2D tensor with shape (rows, cols) and a 1D tensor with shape (cols,),\n    /// multiplies each row of the 2D tensor by the 1D tensor element-wise.\n    ///\n    /// Result[i, j] = t[i, j] * v[j]\n    fn broadcast_mul_1d_to_2d_rows(t: \u0026Self::Tensor2D, v: \u0026Self::Tensor1D) -\u003e Self::Tensor2D;\n\n    /// Broadcasts a 1D tensor and adds to each row of a 2D tensor.\n    ///\n    /// For a 2D tensor with shape (rows, cols) and a 1D tensor with shape (cols,),\n    /// adds the 1D tensor to each row of the 2D tensor.\n    ///\n    /// Result[i, j] = t[i, j] + v[j]\n    fn broadcast_add_1d_to_2d_rows(t: \u0026Self::Tensor2D, v: \u0026Self::Tensor1D) -\u003e Self::Tensor2D;\n\n    /// Sqrt of all elements in a 1D tensor.\n    fn sqrt_1d(t: \u0026Self::Tensor1D) -\u003e Self::Tensor1D;\n\n    /// Sqrt of all elements in a 2D tensor.\n    fn sqrt_2d(t: \u0026Self::Tensor2D) -\u003e Self::Tensor2D;\n\n    // --- Column manipulation operations ---\n\n    /// Horizontally concatenate 2D tensors (stack columns side by side).\n    ///\n    /// All input tensors must have the same number of rows.\n    /// Returns a new tensor with shape (rows, sum of all cols).\n    ///\n    /// # Arguments\n    /// * `tensors` - Slice of 2D tensors to concatenate\n    ///\n    /// # Panics\n    /// Panics if tensors have different row counts or if the slice is empty.\n    ///\n    /// # Example\n    /// ```ignore\n    /// // [[1, 2]] + [[3]] -\u003e [[1, 2, 3]]\n    /// let a = Tensor2D::new(vec![1.0, 2.0], 1, 2);\n    /// let b = Tensor2D::new(vec![3.0], 1, 1);\n    /// let c = B::hcat_2d(\u0026[a, b]); // shape (1, 3)\n    /// ```\n    fn hcat_2d(tensors: \u0026[Self::Tensor2D]) -\u003e Result\u003cSelf::Tensor2D, PreprocessingError\u003e;\n\n    /// Extract specific columns from a 2D tensor.\n    ///\n    /// Returns a new tensor with only the specified columns, preserving row order.\n    ///\n    /// # Arguments\n    /// * `t` - Input 2D tensor\n    /// * `columns` - Indices of columns to extract (in order)\n    ///\n    /// # Panics\n    /// Panics if any column index is out of bounds.\n    fn select_columns_2d(t: \u0026Self::Tensor2D, columns: \u0026[usize]) -\u003e Self::Tensor2D;\n\n    /// Create a one-hot encoded matrix from integer indices.\n    ///\n    /// Each index becomes a row with a 1 at the index position and 0 elsewhere.\n    ///\n    /// # Arguments\n    /// * `indices` - 1D tensor of integer class indices (0 to num_classes-1)\n    /// * `num_classes` - Total number of classes (determines output column count)\n    ///\n    /// # Returns\n    /// A 2D tensor of shape (indices.len(), num_classes)\n    ///\n    /// # Panics\n    /// Panics if any index \u003e= num_classes.\n    fn one_hot_from_indices(indices: \u0026Self::Tensor1D, num_classes: usize) -\u003e Self::Tensor2D;\n}\n","traces":[],"covered":0,"coverable":0},{"path":["/","home","victor","ML","machinelearne-rs","lib","src","backend","ndarray_backend.rs"],"content":"use super::Backend;\nuse ndarray::{Array1, Array2, Ix1};\n\n/// CPU-based tensor backend implementation using the `ndarray` crate.\n///\n/// This backend provides efficient CPU tensor operations for machine learning workloads\n/// using ndarray's optimized linear algebra routines. It supports both 1D and 2D tensors\n/// with element-wise operations, reductions, and matrix-vector products.\n///\n/// # Type mappings\n/// - `Scalar`: `f64` (double-precision floating point)\n/// - `Tensor1D`: `ndarray::Array1\u003cf64\u003e` (1-dimensional array)\n/// - `Tensor2D`: `NdarrayTensor2D` wrapper around `ndarray::Array2\u003cf64\u003e`\n/// - `Device`: `()` (unit type, CPU-only execution)\n///\n/// # Numerical stability\n/// Non-linear operations like `sigmoid` use numerically stable implementations\n/// to avoid overflow/underflow for extreme input values (e.g., Â±100).\n#[derive(Clone, Debug, Copy)]\npub struct NdarrayBackend;\n\n/// Wrapper type for 2D tensors using ndarray's `Array2\u003cf64\u003e`.\n///\n/// This wrapper enables trait implementation for external types while providing\n/// convenient conversion from nested Vec representations commonly used in tests\n/// and data loading.\n///\n/// # Example\n/// ```\n/// use machinelearne_rs::backend::{NdarrayBackend, NdarrayTensor2D, Backend};\n/// use ndarray::{Array1, Array2, Ix1, Ix2};\n/// let tensor = NdarrayTensor2D::from(\u0026[\n///     vec![1.0f64, 2.0f64, 3.0f64],\n///     vec![4.0f64, 5.0f64, 6.0f64],\n/// ][..]);\n/// assert_eq!(tensor.0.shape(), \u0026[2, 3]);\n/// ```\n#[derive(Debug, Clone)]\npub struct NdarrayTensor2D(pub Array2\u003cf64\u003e);\n\nimpl From\u003c\u0026[Vec\u003cf64\u003e]\u003e for NdarrayTensor2D {\n    /// Converts a slice of row vectors into a 2D tensor.\n    ///\n    /// # Panics\n    /// Panics if rows have inconsistent lengths or if shape reconstruction fails.\n    ///\n    /// # Example\n    /// ```\n    /// use machinelearne_rs::backend::{NdarrayBackend, NdarrayTensor2D, Backend};\n    /// use ndarray::{Array1, Array2, Ix1, Ix2};\n    /// let data = \u0026[vec![1.0f64, 2.0f64], vec![3.0f64, 4.0f64]][..];\n    /// let tensor = NdarrayTensor2D::from(data);\n    /// assert_eq!(tensor.0[[0, 0]], 1.0);\n    /// assert_eq!(tensor.0[[1, 1]], 4.0);\n    /// ```\n    fn from(x: \u0026[Vec\u003cf64\u003e]) -\u003e Self {\n        let rows = x.len();\n        if rows == 0 {\n            return NdarrayTensor2D(Array2::from_shape_vec((0, 0), vec![]).unwrap());\n        }\n        let cols = x[0].len();\n        assert!(x.iter().all(|r| r.len() == cols));\n        let data: Vec\u003cf64\u003e = x.iter().flat_map(|r| r.iter()).copied().collect();\n        NdarrayTensor2D(Array2::from_shape_vec((rows, cols), data).unwrap())\n    }\n}\n\nimpl super::Backend for NdarrayBackend {\n    type Scalar = f64;\n    type Tensor1D = Array1\u003cf64\u003e;\n    type Tensor2D = NdarrayTensor2D;\n    type Device = (); // CPU-only for now\n\n    /// Returns the default device for this backend (unit type for CPU execution).\n    fn default_device() -\u003e Self::Device {\n        ()\n    }\n\n    /// Creates a 1D tensor filled with zeros.\n    ///\n    /// # Arguments\n    /// * `len` - Length of the resulting 1D tensor\n    ///\n    /// # Example\n    /// ```\n    /// use machinelearne_rs::backend::{NdarrayBackend, NdarrayTensor2D, Backend};\n    /// let zeros = NdarrayBackend::zeros_1d(3);\n    /// assert_eq!(zeros.to_vec(), vec![0.0, 0.0, 0.0]);\n    /// ```\n    fn zeros_1d(len: usize) -\u003e Self::Tensor1D {\n        Array1::zeros(len)\n    }\n\n    /// Creates a 2D tensor filled with zeros.\n    ///\n    /// # Arguments\n    /// * `rows` - Number of rows\n    /// * `cols` - Number of columns\n    ///\n    /// # Example\n    /// ```\n    /// use machinelearne_rs::backend::{NdarrayBackend, NdarrayTensor2D, Backend};\n    /// let zeros = NdarrayBackend::zeros_2d(2, 3);\n    /// assert_eq!(NdarrayBackend::shape(\u0026zeros), (2, 3));\n    /// ```\n    fn zeros_2d(rows: usize, cols: usize) -\u003e Self::Tensor2D {\n        NdarrayTensor2D(Array2::zeros((rows, cols)))\n    }\n\n    /// Converts a vector of f32 values to a 1D tensor (f64 precision).\n    ///\n    /// # Arguments\n    /// * `data` - Input vector with f32 values\n    ///\n    /// # Example\n    /// ```\n    /// use machinelearne_rs::backend::{NdarrayBackend, NdarrayTensor2D, Backend};\n    /// let tensor = NdarrayBackend::from_vec_1d(vec![1.0f32, 2.0, 3.0]);\n    /// assert_eq!(tensor.to_vec(), vec![1.0, 2.0, 3.0]);\n    /// ```\n    fn from_vec_1d(data: Vec\u003cf32\u003e) -\u003e Self::Tensor1D {\n        Array1::from_iter(data.into_iter().map(|x| x as f64))\n    }\n\n    /// Converts a flat vector of f32 values to a 2D tensor with specified shape.\n    ///\n    /// # Arguments\n    /// * `data` - Flat vector containing row-major ordered elements\n    /// * `rows` - Number of rows in output tensor\n    /// * `cols` - Number of columns in output tensor\n    ///\n    /// # Panics\n    /// Panics if `data.len() != rows * cols`.\n    ///\n    /// # Example\n    /// ```\n    /// use machinelearne_rs::backend::{NdarrayBackend, NdarrayTensor2D, Backend};\n    /// let tensor = NdarrayBackend::from_vec_2d(vec![1.0, 2.0, 3.0, 4.0], 2, 2);\n    /// assert_eq!(tensor.0[[0, 0]], 1.0);\n    /// assert_eq!(tensor.0[[1, 1]], 4.0);\n    /// ```\n    fn from_vec_2d(data: Vec\u003cf32\u003e, rows: usize, cols: usize) -\u003e Self::Tensor2D {\n        let data_f64: Vec\u003cf64\u003e = data.into_iter().map(|x| x as f64).collect();\n        NdarrayTensor2D(Array2::from_shape_vec((rows, cols), data_f64).unwrap())\n    }\n\n    /// Element-wise addition of two 1D tensors.\n    ///\n    /// # Panics\n    /// Panics if tensors have different lengths.\n    ///\n    /// # Example\n    /// ```\n    /// use machinelearne_rs::backend::{NdarrayBackend, NdarrayTensor2D, Backend};\n    /// use ndarray::{Array1, Array2, Ix1, Ix2};\n    /// let a = Array1::from_vec(vec![1.0, 2.0]);\n    /// let b = Array1::from_vec(vec![3.0, 4.0]);\n    /// let c = NdarrayBackend::add_1d(\u0026a, \u0026b);\n    /// assert_eq!(c.to_vec(), vec![4.0, 6.0]);\n    /// ```\n    fn add_1d(a: \u0026Self::Tensor1D, b: \u0026Self::Tensor1D) -\u003e Self::Tensor1D {\n        a + b\n    }\n\n    /// Multiplies a 2D tensor by a scalar value.\n    ///\n    /// # Arguments\n    /// * `t` - Input 2D tensor\n    /// * `s` - Scalar multiplier\n    ///\n    /// # Example\n    /// ```\n    /// use machinelearne_rs::backend::{NdarrayBackend, NdarrayTensor2D, Backend};\n    /// use ndarray::{Array1, Array2, Ix1, Ix2};\n    /// let t = NdarrayTensor2D(Array2::from_shape_vec((2, 2), vec![1.0, 2.0, 3.0, 4.0]).unwrap());\n    /// let scaled = NdarrayBackend::mul_scalar_2d(\u0026t, \u00262.0);\n    /// assert_eq!(scaled.0[[0, 1]], 4.0);\n    /// ```\n    fn mul_scalar_2d(t: \u0026Self::Tensor2D, s: \u0026Self::Scalar) -\u003e Self::Tensor2D {\n        NdarrayTensor2D(\u0026t.0 * *s)\n    }\n\n    /// Matrix-vector multiplication: `A @ x`.\n    ///\n    /// Computes the product of a 2D matrix and a 1D vector.\n    ///\n    /// # Arguments\n    /// * `a` - Matrix of shape (m, n)\n    /// * `x` - Vector of shape (n,)\n    ///\n    /// # Returns\n    /// Vector of shape (m,)\n    ///\n    /// # Panics\n    /// Panics if matrix columns != vector length.\n    ///\n    /// # Example\n    /// ```\n    /// // [[1, 2],    [1]   [5]\n    /// //  [3, 4]] @ [2] = [11]\n    /// use machinelearne_rs::backend::{NdarrayBackend, NdarrayTensor2D, Backend};\n    /// use ndarray::{Array1, Array2, Ix1, Ix2};\n    /// let a = NdarrayTensor2D::from(\u0026[vec![1.0, 2.0], vec![3.0, 4.0]][..]);\n    /// let x = Array1::from_vec(vec![1.0, 2.0]);\n    /// let y = NdarrayBackend::matvec(\u0026a, \u0026x);\n    /// assert_eq!(y.to_vec(), vec![5.0, 11.0]);\n    /// ```\n    fn matvec(a: \u0026Self::Tensor2D, x: \u0026Self::Tensor1D) -\u003e Self::Tensor1D {\n        a.0.dot(x) // ndarray has efficient matvec\n    }\n\n    /// Transposed matrix-vector multiplication: `A^T @ x`.\n    ///\n    /// Computes the product of a transposed matrix and a vector.\n    ///\n    /// # Arguments\n    /// * `a` - Matrix of shape (m, n)\n    /// * `x` - Vector of shape (m,)\n    ///\n    /// # Returns\n    /// Vector of shape (n,)\n    ///\n    /// # Example\n    /// ```\n    /// // [[1, 2],^T   [1]   [1]\n    /// //  [3, 4]]  @ [0] = [2]\n    /// use machinelearne_rs::backend::{NdarrayBackend, NdarrayTensor2D, Backend};\n    /// use ndarray::{Array1, Array2, Ix1, Ix2};\n    /// let a = NdarrayTensor2D::from(\u0026[vec![1.0, 2.0], vec![3.0, 4.0]][..]);\n    /// let x = Array1::from_vec(vec![1.0, 0.0]);\n    /// let y = NdarrayBackend::matvec_transposed(\u0026a, \u0026x);\n    /// assert_eq!(y.to_vec(), vec![1.0, 2.0]);\n    /// ```\n    fn matvec_transposed(a: \u0026Self::Tensor2D, x: \u0026Self::Tensor1D) -\u003e Self::Tensor1D {\n        a.0.t().dot(x)\n    }\n\n    /// Returns the transpose of a 2D tensor.\n    ///\n    /// # Arguments\n    /// * `a` - Input tensor of shape (m, n)\n    ///\n    /// # Returns\n    /// Tensor of shape (n, m)\n    ///\n    /// # Example\n    /// ```\n    /// use machinelearne_rs::backend::{NdarrayBackend, NdarrayTensor2D, Backend};\n    /// let a = NdarrayTensor2D::from(\u0026[vec![1.0, 2.0], vec![3.0, 4.0]][..]);\n    /// let at = NdarrayBackend::transpose(\u0026a);\n    /// assert_eq!(at.0[[0, 1]], 3.0); // Original [1,0] becomes [0,1]\n    /// ```\n    fn transpose(a: \u0026Self::Tensor2D) -\u003e Self::Tensor2D {\n        NdarrayTensor2D(a.0.t().to_owned())\n    }\n\n    /// Returns the shape of a 2D tensor as (rows, columns).\n    ///\n    /// # Arguments\n    /// * `t` - Input 2D tensor\n    ///\n    /// # Returns\n    /// Tuple `(rows, cols)`\n    ///\n    /// # Example\n    /// ```\n    /// use machinelearne_rs::backend::{NdarrayBackend, NdarrayTensor2D, Backend};\n    /// use ndarray::{Array1, Array2, Ix1, Ix2};\n    /// let t = NdarrayTensor2D::from(\u0026[vec![1.0, 2.0, 3.0]][..]);\n    /// assert_eq!(NdarrayBackend::shape(\u0026t), (1, 3));\n    /// ```\n    fn shape(t: \u0026Self::Tensor2D) -\u003e (usize, usize) {\n        let shape = t.0.shape();\n        (shape[0], shape[1])\n    }\n\n    // --- Element-wise non-linear ops (1D) ---\n\n    /// Computes element-wise exponential: `e^x`.\n    ///\n    /// # Example\n    /// ```\n    /// use machinelearne_rs::backend::{NdarrayBackend, NdarrayTensor2D, Backend};\n    /// use ndarray::{Array1, Array2, Ix1, Ix2};\n    /// let x = Array1::from_vec(vec![0.0, 1.0]);\n    /// let y = NdarrayBackend::exp_1d(\u0026x);\n    /// assert!((y[0] - 1.0).abs() \u003c 1e-6);\n    /// assert!((y[1] - std::f64::consts::E).abs() \u003c 1e-6);\n    /// ```\n    fn exp_1d(x: \u0026Self::Tensor1D) -\u003e Self::Tensor1D {\n        x.mapv(f64::exp)\n    }\n\n    /// Computes element-wise natural logarithm: `ln(x)`.\n    ///\n    /// # Panics\n    /// Panics for non-positive values (ndarray behavior).\n    ///\n    /// # Example\n    /// ```\n    /// use machinelearne_rs::backend::{NdarrayBackend, NdarrayTensor2D, Backend};\n    /// use ndarray::{Array1, Array2, Ix1, Ix2};\n    /// let x = Array1::from_vec(vec![1.0, std::f64::consts::E]);\n    /// let y = NdarrayBackend::log_1d(\u0026x);\n    /// assert!((y[0] - 0.0).abs() \u003c 1e-6);\n    /// assert!((y[1] - 1.0).abs() \u003c 1e-6);\n    /// ```\n    fn log_1d(x: \u0026Self::Tensor1D) -\u003e Self::Tensor1D {\n        x.mapv(f64::ln)\n    }\n\n    /// Computes numerically stable sigmoid activation: `1 / (1 + e^{-x})`.\n    ///\n    /// Uses a numerically stable implementation that avoids overflow for\n    /// extreme values (Â±100+).\n    ///\n    /// # Example\n    /// ```\n    /// use machinelearne_rs::backend::{NdarrayBackend, NdarrayTensor2D, Backend};\n    /// use ndarray::{Array1, Array2, Ix1, Ix2};\n    /// let x = Array1::from_vec(vec![-100.0, 0.0, 100.0]);\n    /// let y = NdarrayBackend::sigmoid_1d(\u0026x);\n    /// assert!((y[0] - 0.0).abs() \u003c 1e-6);   // â0\n    /// assert!((y[1] - 0.5).abs() \u003c 1e-6);   // =0.5\n    /// assert!((y[2] - 1.0).abs() \u003c 1e-6);   // â1\n    /// ```\n    fn sigmoid_1d(x: \u0026Self::Tensor1D) -\u003e Self::Tensor1D {\n        x.mapv(|z| {\n            if z \u003e= 0.0 {\n                1.0 / (1.0 + (-z).exp())\n            } else {\n                let ez = z.exp();\n                ez / (1.0 + ez)\n            }\n        })\n    }\n\n    /// Computes element-wise absolute value.\n    ///\n    /// # Example\n    /// ```\n    /// use machinelearne_rs::backend::{NdarrayBackend, NdarrayTensor2D, Backend};\n    /// use ndarray::{Array1, Array2, Ix1, Ix2};\n    /// let x = Array1::from_vec(vec![-2.0, 3.0]);\n    /// let y = NdarrayBackend::abs_1d(\u0026x);\n    /// assert_eq!(y.to_vec(), vec![2.0, 3.0]);\n    /// ```\n    fn abs_1d(x: \u0026Self::Tensor1D) -\u003e Self::Tensor1D {\n        x.mapv(f64::abs)\n    }\n\n    /// Computes element-wise sign function.\n    ///\n    /// Returns -1.0 for negative values, 0.0 for zero, 1.0 for positive values.\n    ///\n    /// # Example\n    /// ```\n    /// use machinelearne_rs::backend::{NdarrayBackend, NdarrayTensor2D, Backend};\n    /// use ndarray::{Array1, Array2, Ix1, Ix2};\n    /// let x = Array1::from_vec(vec![-2.0, 0.0, 3.0]);\n    /// let y = NdarrayBackend::sign_1d(\u0026x);\n    /// assert_eq!(y.to_vec(), vec![-1.0, 0.0, 1.0]);\n    /// ```\n    fn sign_1d(x: \u0026Self::Tensor1D) -\u003e Self::Tensor1D {\n        x.mapv(|x| {\n            if x \u003e 0.0 {\n                1.0\n            } else if x \u003c 0.0 {\n                -1.0\n            } else {\n                0.0\n            }\n        })\n    }\n\n    /// Element-wise maximum of two 1D tensors.\n    ///\n    /// # Panics\n    /// Panics if tensors have different lengths.\n    ///\n    /// # Example\n    /// ```\n    /// use machinelearne_rs::backend::{NdarrayBackend, NdarrayTensor2D, Backend};\n    /// use ndarray::{Array1, Array2, Ix1, Ix2};\n    /// let a = Array1::from_vec(vec![1.0, 5.0, 3.0]);\n    /// let b = Array1::from_vec(vec![2.0, 4.0, 6.0]);\n    /// let m = NdarrayBackend::maximum_1d(\u0026a, \u0026b);\n    /// assert_eq!(m.to_vec(), vec![2.0, 5.0, 6.0]);\n    /// ```\n    fn maximum_1d(a: \u0026Self::Tensor1D, b: \u0026Self::Tensor1D) -\u003e Self::Tensor1D {\n        assert_eq!(a.len(), b.len(), \"Shapes must match\");\n        a.iter().zip(b.iter()).map(|(\u0026x, \u0026y)| x.max(y)).collect()\n    }\n\n    // --- 2D versions (delegating to 1D via flat view or direct map) ---\n\n    /// Computes element-wise exponential for 2D tensors.\n    fn exp_2d(x: \u0026Self::Tensor2D) -\u003e Self::Tensor2D {\n        NdarrayTensor2D(x.0.mapv(f64::exp))\n    }\n\n    /// Computes element-wise natural logarithm for 2D tensors.\n    ///\n    /// # Panics\n    /// Panics for non-positive values.\n    fn log_2d(x: \u0026Self::Tensor2D) -\u003e Self::Tensor2D {\n        NdarrayTensor2D(x.0.mapv(f64::ln))\n    }\n\n    /// Computes numerically stable sigmoid activation for 2D tensors.\n    ///\n    /// Uses the same stable implementation as `sigmoid_1d` applied element-wise.\n    fn sigmoid_2d(x: \u0026Self::Tensor2D) -\u003e Self::Tensor2D {\n        NdarrayTensor2D(x.0.mapv(|z| {\n            if z \u003e= 0.0 {\n                1.0 / (1.0 + (-z).exp())\n            } else {\n                let ez = z.exp();\n                ez / (1.0 + ez)\n            }\n        }))\n    }\n\n    /// Computes element-wise absolute value for 2D tensors.\n    fn abs_2d(x: \u0026Self::Tensor2D) -\u003e Self::Tensor2D {\n        NdarrayTensor2D(x.0.mapv(f64::abs))\n    }\n\n    /// Computes element-wise sign function for 2D tensors.\n    fn sign_2d(x: \u0026Self::Tensor2D) -\u003e Self::Tensor2D {\n        NdarrayTensor2D(x.0.mapv(|x| {\n            if x \u003e 0.0 {\n                1.0\n            } else if x \u003c 0.0 {\n                -1.0\n            } else {\n                0.0\n            }\n        }))\n    }\n\n    /// Element-wise maximum of two 2D tensors.\n    ///\n    /// # Panics\n    /// Panics if tensors have different shapes.\n    fn maximum_2d(a: \u0026Self::Tensor2D, b: \u0026Self::Tensor2D) -\u003e Self::Tensor2D {\n        let (rows, cols) = a.0.dim();\n        assert_eq!(a.0.dim(), b.0.dim(), \"Shapes must match\");\n        let data: Vec\u003cf64\u003e =\n            a.0.iter()\n                .zip(b.0.iter())\n                .map(|(\u0026x, \u0026y)| x.max(y))\n                .collect();\n        NdarrayTensor2D(Array2::from_shape_vec((rows, cols), data).unwrap())\n    }\n\n    // --- Reductions (already partially covered, but for completeness) ---\n\n    /// Sums all elements in a 1D tensor.\n    fn sum_all_1d(t: \u0026Self::Tensor1D) -\u003e Self::Scalar {\n        t.sum()\n    }\n\n    /// Computes mean of all elements in a 1D tensor.\n    ///\n    /// # Panics\n    /// Panics if tensor is empty (ndarray behavior).\n    fn mean_all_1d(t: \u0026Self::Tensor1D) -\u003e Self::Scalar {\n        t.mean().unwrap()\n    }\n\n    /// Sums all elements in a 2D tensor.\n    fn sum_all_2d(t: \u0026Self::Tensor2D) -\u003e Self::Scalar {\n        t.0.sum()\n    }\n\n    /// Computes mean of all elements in a 2D tensor.\n    ///\n    /// # Panics\n    /// Panics if tensor is empty.\n    fn mean_all_2d(t: \u0026Self::Tensor2D) -\u003e Self::Scalar {\n        t.0.mean().unwrap()\n    }\n\n    // --- Scalar and access helpers ---\n\n    /// Creates a scalar value from f64.\n    ///\n    /// This is a trivial conversion since `Scalar = f64`.\n    fn scalar_f64(value: f64) -\u003e Self::Scalar {\n        value\n    }\n\n    /// Returns the length (number of elements) of a 1D tensor.\n    fn len_1d(t: \u0026Self::Tensor1D) -\u003e usize {\n        t.len()\n    }\n\n    /// Returns the number of rows in a 2D tensor.\n    ///\n    /// Note: This returns `nrows()`, not total element count.\n    fn len_2d(t: \u0026Self::Tensor2D) -\u003e usize {\n        t.0.nrows()\n    }\n\n    /// Converts a 1D tensor to a standard Vec\u003cf64\u003e.\n    fn to_vec_1d(t: \u0026Self::Tensor1D) -\u003e Vec\u003cf64\u003e {\n        t.to_vec()\n    }\n\n    // --- Element-wise binary ops (1D) ---\n\n    /// Element-wise subtraction of 1D tensors.\n    fn sub_1d(a: \u0026Self::Tensor1D, b: \u0026Self::Tensor1D) -\u003e Self::Tensor1D {\n        a - b\n    }\n\n    /// Element-wise multiplication of 1D tensors.\n    fn mul_1d(a: \u0026Self::Tensor1D, b: \u0026Self::Tensor1D) -\u003e Self::Tensor1D {\n        a * b\n    }\n\n    /// Element-wise division of 1D tensors.\n    fn div_1d(a: \u0026Self::Tensor1D, b: \u0026Self::Tensor1D) -\u003e Self::Tensor1D {\n        a / b\n    }\n\n    /// Multiplies each element of a 1D tensor by a scalar.\n    fn mul_scalar_1d(t: \u0026Self::Tensor1D, s: \u0026Self::Scalar) -\u003e Self::Tensor1D {\n        t.mapv(|x| x * *s)\n    }\n\n    /// Adds a scalar to each element of a 1D tensor.\n    fn add_scalar_1d(t: \u0026Self::Tensor1D, s: \u0026Self::Scalar) -\u003e Self::Tensor1D {\n        t.mapv(|x| x + *s)\n    }\n\n    // --- 2D scalar and binary ops ---\n\n    /// Adds a scalar to each element of a 2D tensor.\n    fn add_scalar_2d(t: \u0026Self::Tensor2D, s: \u0026Self::Scalar) -\u003e Self::Tensor2D {\n        NdarrayTensor2D(t.0.mapv(|x| x + *s))\n    }\n\n    /// Element-wise addition of 2D tensors.\n    fn add_2d(a: \u0026Self::Tensor2D, b: \u0026Self::Tensor2D) -\u003e Self::Tensor2D {\n        NdarrayTensor2D(\u0026a.0 + \u0026b.0)\n    }\n\n    /// Element-wise subtraction of 2D tensors.\n    fn sub_2d(a: \u0026Self::Tensor2D, b: \u0026Self::Tensor2D) -\u003e Self::Tensor2D {\n        NdarrayTensor2D(\u0026a.0 - \u0026b.0)\n    }\n\n    /// Element-wise multiplication of 2D tensors (Hadamard product).\n    fn mul_2d(a: \u0026Self::Tensor2D, b: \u0026Self::Tensor2D) -\u003e Self::Tensor2D {\n        NdarrayTensor2D(\u0026a.0 * \u0026b.0)\n    }\n\n    /// Element-wise division of 2D tensors.\n    fn div_2d(a: \u0026Self::Tensor2D, b: \u0026Self::Tensor2D) -\u003e Self::Tensor2D {\n        NdarrayTensor2D(\u0026a.0 / \u0026b.0)\n    }\n\n    // --- \"Unchecked\" matvec helpers (Ð´Ð»Ñ ÑÐ¾Ð²Ð¼ÐµÑÑÐ¸Ð¼Ð¾ÑÑÐ¸ Ñ CpuBackend) ---\n    // Ð ndarray Ð¾Ð½Ð¸ Ð½Ðµ Ð½ÑÐ¶Ð½Ñ, Ð½Ð¾ ÑÑÐµÐ¹Ñ ÑÑÐµÐ±ÑÐµÑ â Ð´ÐµÐ»Ð°ÐµÐ¼ Ð¿ÑÐ¾ÑÑÐ¾ Ð¾Ð±ÑÑÑÐºÐ¸\n\n    /// Unchecked matrix-vector multiplication (same as `matvec`).\n    ///\n    /// Provided for trait compatibility; delegates to `matvec`.\n    fn _matvec_unchecked(a: \u0026Self::Tensor2D, x: \u0026Self::Tensor1D) -\u003e Self::Tensor1D {\n        Self::matvec(a, x)\n    }\n\n    /// Unchecked transposed matrix-vector multiplication (same as `matvec_transposed`).\n    ///\n    /// Provided for trait compatibility; delegates to `matvec_transposed`.\n    fn _matvec_transposed_unchecked(a: \u0026Self::Tensor2D, x: \u0026Self::Tensor1D) -\u003e Self::Tensor1D {\n        Self::matvec_transposed(a, x)\n    }\n\n    //Returns copy of the inner 1d vector\n    fn ravel_2d(x: \u0026Self::Tensor2D) -\u003e Self::Tensor1D {\n        x.0.clone()\n            .into_dimensionality::\u003cIx1\u003e()\n            .expect(\"Failed to ravel 2D tensor: shape conversion error\")\n    }\n\n    // --- Column-wise operations ---\n\n    fn col_mean_2d(t: \u0026Self::Tensor2D) -\u003e Self::Tensor1D {\n        t.0.mean_axis(ndarray::Axis(0))\n            .unwrap_or_else(|| Array1::zeros(0))\n    }\n\n    fn col_std_2d(t: \u0026Self::Tensor2D, ddof: usize) -\u003e Self::Tensor1D {\n        let ncols = t.0.ncols();\n        if ncols == 0 {\n            return Array1::zeros(0);\n        }\n\n        let means = Self::col_mean_2d(t);\n        let nrows = t.0.nrows();\n\n        let mut stds = Array1::zeros(ncols);\n        for col in 0..ncols {\n            let mut var_sum = 0.0;\n            for row in 0..nrows {\n                let diff = t.0[[row, col]] - means[col];\n                var_sum += diff * diff;\n            }\n            let divisor = (nrows - ddof) as f64;\n            stds[col] = (var_sum / divisor).sqrt();\n        }\n        stds\n    }\n\n    fn col_min_2d(t: \u0026Self::Tensor2D) -\u003e Self::Tensor1D {\n        let ncols = t.0.ncols();\n        if ncols == 0 {\n            return Array1::zeros(0);\n        }\n\n        let mut mins = Array1::from_elem(ncols, f64::INFINITY);\n        for col in 0..ncols {\n            for row in 0..t.0.nrows() {\n                let val = t.0[[row, col]];\n                if val \u003c mins[col] {\n                    mins[col] = val;\n                }\n            }\n        }\n        mins\n    }\n\n    fn col_max_2d(t: \u0026Self::Tensor2D) -\u003e Self::Tensor1D {\n        let ncols = t.0.ncols();\n        if ncols == 0 {\n            return Array1::zeros(0);\n        }\n\n        let mut maxs = Array1::from_elem(ncols, f64::NEG_INFINITY);\n        for col in 0..ncols {\n            for row in 0..t.0.nrows() {\n                let val = t.0[[row, col]];\n                if val \u003e maxs[col] {\n                    maxs[col] = val;\n                }\n            }\n        }\n        maxs\n    }\n\n    fn col_sum_2d(t: \u0026Self::Tensor2D) -\u003e Self::Tensor1D {\n        t.0.sum_axis(ndarray::Axis(0))\n    }\n\n    // --- Row-wise operations ---\n\n    fn row_sum_2d(t: \u0026Self::Tensor2D) -\u003e Self::Tensor1D {\n        t.0.sum_axis(ndarray::Axis(1))\n    }\n\n    // --- Broadcasting operations ---\n\n    fn broadcast_sub_1d_to_2d_rows(t: \u0026Self::Tensor2D, v: \u0026Self::Tensor1D) -\u003e Self::Tensor2D {\n        NdarrayTensor2D(\u0026t.0 - \u0026v.view().insert_axis(ndarray::Axis(0)))\n    }\n\n    fn broadcast_div_1d_to_2d_rows(t: \u0026Self::Tensor2D, v: \u0026Self::Tensor1D) -\u003e Self::Tensor2D {\n        NdarrayTensor2D(\u0026t.0 / \u0026v.view().insert_axis(ndarray::Axis(0)))\n    }\n\n    fn broadcast_mul_1d_to_2d_rows(t: \u0026Self::Tensor2D, v: \u0026Self::Tensor1D) -\u003e Self::Tensor2D {\n        NdarrayTensor2D(\u0026t.0 * \u0026v.view().insert_axis(ndarray::Axis(0)))\n    }\n\n    fn broadcast_add_1d_to_2d_rows(t: \u0026Self::Tensor2D, v: \u0026Self::Tensor1D) -\u003e Self::Tensor2D {\n        NdarrayTensor2D(\u0026t.0 + \u0026v.view().insert_axis(ndarray::Axis(0)))\n    }\n\n    fn sqrt_1d(t: \u0026Self::Tensor1D) -\u003e Self::Tensor1D {\n        t.mapv(f64::sqrt)\n    }\n\n    fn sqrt_2d(t: \u0026Self::Tensor2D) -\u003e Self::Tensor2D {\n        NdarrayTensor2D(t.0.mapv(f64::sqrt))\n    }\n\n    // --- Column manipulation operations ---\n\n    fn hcat_2d(\n        tensors: \u0026[Self::Tensor2D],\n    ) -\u003e Result\u003cSelf::Tensor2D, crate::preprocessing::PreprocessingError\u003e {\n        if tensors.is_empty() {\n            return Err(crate::preprocessing::PreprocessingError::InvalidParameter(\n                \"Cannot horizontally concatenate empty slice of tensors\".to_string(),\n            ));\n        }\n\n        let rows = tensors[0].0.nrows();\n        if rows == 0 {\n            return Ok(NdarrayTensor2D(\n                Array2::from_shape_vec((0, 0), vec![]).unwrap(),\n            ));\n        }\n\n        // Verify all tensors have the same number of rows\n        for t in tensors.iter() {\n            if t.0.nrows() != rows {\n                return Err(crate::preprocessing::PreprocessingError::InvalidShape {\n                    expected: format!(\"({}, ?)\", rows),\n                    got: format!(\"({}, ?)\", t.0.nrows()),\n                });\n            }\n        }\n\n        // Calculate total columns\n        let total_cols: usize = tensors.iter().map(|t| t.0.ncols()).sum();\n\n        // Manually concatenate by copying data\n        let mut result = Array2::zeros((rows, total_cols));\n        let mut col_offset = 0;\n        for t in tensors {\n            let ncols = t.0.ncols();\n            for r in 0..rows {\n                for c in 0..ncols {\n                    result[[r, col_offset + c]] = t.0[[r, c]];\n                }\n            }\n            col_offset += ncols;\n        }\n\n        Ok(NdarrayTensor2D(result))\n    }\n\n    fn select_columns_2d(t: \u0026Self::Tensor2D, columns: \u0026[usize]) -\u003e Self::Tensor2D {\n        let (rows, ncols) = t.0.dim();\n        if columns.is_empty() {\n            return NdarrayTensor2D(Array2::from_shape_vec((rows, 0), vec![]).unwrap());\n        }\n\n        // Validate column indices\n        for \u0026col in columns {\n            assert!(\n                col \u003c ncols,\n                \"Column index {} out of bounds (max {})\",\n                col,\n                ncols - 1\n            );\n        }\n\n        // Use ndarray's select method\n        let selected = t.0.select(ndarray::Axis(1), columns);\n        NdarrayTensor2D(selected)\n    }\n\n    fn one_hot_from_indices(indices: \u0026Self::Tensor1D, num_classes: usize) -\u003e Self::Tensor2D {\n        let n_samples = indices.len();\n        if n_samples == 0 || num_classes == 0 {\n            return NdarrayTensor2D(\n                Array2::from_shape_vec((n_samples, num_classes), vec![]).unwrap(),\n            );\n        }\n\n        // Validate indices\n        for (i, \u0026idx) in indices.iter().enumerate() {\n            assert!(\n                idx \u003e= 0.0 \u0026\u0026 idx \u003c num_classes as f64 \u0026\u0026 idx.fract() == 0.0,\n                \"Index {} at position {} is not a valid integer in range [0, {})\",\n                idx,\n                i,\n                num_classes\n            );\n        }\n\n        let mut result = Array2::zeros((n_samples, num_classes));\n        for (i, \u0026idx) in indices.iter().enumerate() {\n            let col = idx as usize;\n            result[[i, col]] = 1.0;\n        }\n\n        NdarrayTensor2D(result)\n    }\n}\n\n#[cfg(test)]\n#[cfg(feature = \"ndarray\")]\nmod tests {\n    use super::*;\n    use ndarray::{Array1, Array2};\n\n    // Helper to create 2D tensor from nested vec\n    fn tensor2d_from(data: \u0026[Vec\u003cf64\u003e]) -\u003e NdarrayTensor2D {\n        NdarrayTensor2D::from(data)\n    }\n\n    #[test]\n    fn test_zeros_1d() {\n        let t = NdarrayBackend::zeros_1d(3);\n        assert_eq!(t.to_vec(), vec![0.0, 0.0, 0.0]);\n    }\n\n    #[test]\n    fn test_zeros_2d() {\n        let t = NdarrayBackend::zeros_2d(2, 3);\n        assert_eq!(NdarrayBackend::shape(\u0026t), (2, 3));\n        assert_eq!(t.0.iter().sum::\u003cf64\u003e(), 0.0);\n    }\n\n    #[test]\n    fn test_from_vec_1d() {\n        let t = NdarrayBackend::from_vec_1d(vec![1.0, 2.0, 3.0]);\n        assert_eq!(t.to_vec(), vec![1.0, 2.0, 3.0]);\n    }\n\n    #[test]\n    fn test_from_vec_2d() {\n        let t = NdarrayBackend::from_vec_2d(vec![1.0, 2.0, 3.0, 4.0], 2, 2);\n        assert_eq!(NdarrayBackend::shape(\u0026t), (2, 2));\n        assert_eq!(\n            t.0,\n            Array2::from_shape_vec((2, 2), vec![1.0, 2.0, 3.0, 4.0]).unwrap()\n        );\n    }\n\n    #[test]\n    fn test_from_nested_vec() {\n        let data = \u0026[vec![1.0, 2.0], vec![3.0, 4.0]];\n        let t = tensor2d_from(data);\n        assert_eq!(\n            t.0,\n            Array2::from_shape_vec((2, 2), vec![1.0, 2.0, 3.0, 4.0]).unwrap()\n        );\n    }\n\n    #[test]\n    fn test_add_1d() {\n        let a = Array1::from_vec(vec![1.0, 2.0]);\n        let b = Array1::from_vec(vec![3.0, 4.0]);\n        let c = NdarrayBackend::add_1d(\u0026a, \u0026b);\n        assert_eq!(c.to_vec(), vec![4.0, 6.0]);\n    }\n\n    #[test]\n    fn test_add_2d() {\n        let a = tensor2d_from(\u0026[vec![1.0, 2.0]]);\n        let b = tensor2d_from(\u0026[vec![3.0, 4.0]]);\n        let c = NdarrayBackend::add_2d(\u0026a, \u0026b);\n        assert_eq!(c.0, Array2::from_shape_vec((1, 2), vec![4.0, 6.0]).unwrap());\n    }\n\n    #[test]\n    fn test_sub_1d() {\n        let a = Array1::from_vec(vec![5.0, 6.0]);\n        let b = Array1::from_vec(vec![2.0, 1.0]);\n        let c = NdarrayBackend::sub_1d(\u0026a, \u0026b);\n        assert_eq!(c.to_vec(), vec![3.0, 5.0]);\n    }\n\n    #[test]\n    fn test_sub_2d() {\n        let a = tensor2d_from(\u0026[vec![5.0, 6.0]]);\n        let b = tensor2d_from(\u0026[vec![2.0, 1.0]]);\n        let c = NdarrayBackend::sub_2d(\u0026a, \u0026b);\n        assert_eq!(c.0, Array2::from_shape_vec((1, 2), vec![3.0, 5.0]).unwrap());\n    }\n\n    #[test]\n    fn test_mul_1d() {\n        let a = Array1::from_vec(vec![2.0, 3.0]);\n        let b = Array1::from_vec(vec![4.0, 5.0]);\n        let c = NdarrayBackend::mul_1d(\u0026a, \u0026b);\n        assert_eq!(c.to_vec(), vec![8.0, 15.0]);\n    }\n\n    #[test]\n    fn test_mul_2d() {\n        let a = tensor2d_from(\u0026[vec![2.0, 3.0]]);\n        let b = tensor2d_from(\u0026[vec![4.0, 5.0]]);\n        let c = NdarrayBackend::mul_2d(\u0026a, \u0026b);\n        assert_eq!(\n            c.0,\n            Array2::from_shape_vec((1, 2), vec![8.0, 15.0]).unwrap()\n        );\n    }\n\n    #[test]\n    fn test_div_1d() {\n        let a = Array1::from_vec(vec![8.0, 15.0]);\n        let b = Array1::from_vec(vec![2.0, 3.0]);\n        let c = NdarrayBackend::div_1d(\u0026a, \u0026b);\n        assert_eq!(c.to_vec(), vec![4.0, 5.0]);\n    }\n\n    #[test]\n    fn test_div_2d() {\n        let a = tensor2d_from(\u0026[vec![8.0, 15.0]]);\n        let b = tensor2d_from(\u0026[vec![2.0, 3.0]]);\n        let c = NdarrayBackend::div_2d(\u0026a, \u0026b);\n        assert_eq!(c.0, Array2::from_shape_vec((1, 2), vec![4.0, 5.0]).unwrap());\n    }\n\n    #[test]\n    fn test_add_scalar_1d() {\n        let t = Array1::from_vec(vec![1.0, 2.0]);\n        let s = 10.0;\n        let out = NdarrayBackend::add_scalar_1d(\u0026t, \u0026s);\n        assert_eq!(out.to_vec(), vec![11.0, 12.0]);\n    }\n\n    #[test]\n    fn test_add_scalar_2d() {\n        let t = tensor2d_from(\u0026[vec![1.0, 2.0]]);\n        let s = 10.0;\n        let out = NdarrayBackend::add_scalar_2d(\u0026t, \u0026s);\n        assert_eq!(\n            out.0,\n            Array2::from_shape_vec((1, 2), vec![11.0, 12.0]).unwrap()\n        );\n    }\n\n    #[test]\n    fn test_mul_scalar_1d() {\n        let t = Array1::from_vec(vec![2.0, 3.0]);\n        let s = 5.0;\n        let out = NdarrayBackend::mul_scalar_1d(\u0026t, \u0026s);\n        assert_eq!(out.to_vec(), vec![10.0, 15.0]);\n    }\n\n    #[test]\n    fn test_mul_scalar_2d() {\n        let t = tensor2d_from(\u0026[vec![2.0, 3.0]]);\n        let s = 5.0;\n        let out = NdarrayBackend::mul_scalar_2d(\u0026t, \u0026s);\n        assert_eq!(\n            out.0,\n            Array2::from_shape_vec((1, 2), vec![10.0, 15.0]).unwrap()\n        );\n    }\n\n    #[test]\n    fn test_matvec() {\n        let a = tensor2d_from(\u0026[vec![1.0, 2.0], vec![3.0, 4.0]]);\n        let x = Array1::from_vec(vec![1.0, 2.0]);\n        let y = NdarrayBackend::matvec(\u0026a, \u0026x);\n        assert_eq!(y.to_vec(), vec![5.0, 11.0]); // 1*1+2*2=5, 3*1+4*2=11\n    }\n\n    #[test]\n    fn test_matvec_transposed() {\n        let a = tensor2d_from(\u0026[vec![1.0, 2.0], vec![3.0, 4.0]]); // 2x2\n        let x = Array1::from_vec(vec![1.0, 0.0]); // shape (2,)\n        let y = NdarrayBackend::matvec_transposed(\u0026a, \u0026x); // A^T @ x â (2,)\n                                                           // A^T = [[1,3],[2,4]], so [1*1 + 3*0, 2*1 + 4*0] = [1, 2]\n        assert_eq!(y.to_vec(), vec![1.0, 2.0]);\n    }\n\n    #[test]\n    fn test_transpose() {\n        let a = tensor2d_from(\u0026[vec![1.0, 2.0], vec![3.0, 4.0]]);\n        let at = NdarrayBackend::transpose(\u0026a);\n        assert_eq!(NdarrayBackend::shape(\u0026at), (2, 2));\n        assert_eq!(\n            at.0,\n            Array2::from_shape_vec((2, 2), vec![1.0, 3.0, 2.0, 4.0]).unwrap()\n        );\n    }\n\n    #[test]\n    fn test_shape() {\n        let a = tensor2d_from(\u0026[vec![1.0, 2.0, 3.0]]);\n        assert_eq!(NdarrayBackend::shape(\u0026a), (1, 3));\n    }\n\n    #[test]\n    fn test_len_1d() {\n        let t = Array1::from_vec(vec![1.0, 2.0, 3.0]);\n        assert_eq!(NdarrayBackend::len_1d(\u0026t), 3);\n    }\n\n    #[test]\n    fn test_len_2d() {\n        let t = tensor2d_from(\u0026[vec![1.0, 2.0], vec![3.0, 4.0]]);\n        assert_eq!(NdarrayBackend::len_2d(\u0026t), 2); // nrows\n    }\n\n    #[test]\n    fn test_to_vec_1d() {\n        let t = Array1::from_vec(vec![1.0, 2.0, 3.0]);\n        assert_eq!(NdarrayBackend::to_vec_1d(\u0026t), vec![1.0, 2.0, 3.0]);\n    }\n\n    #[test]\n    fn test_exp_1d() {\n        let t = Array1::from_vec(vec![0.0, 1.0]);\n        let out = NdarrayBackend::exp_1d(\u0026t);\n        assert!((out[0] - 1.0).abs() \u003c 1e-6);\n        assert!((out[1] - std::f64::consts::E).abs() \u003c 1e-6);\n    }\n\n    #[test]\n    fn test_log_1d() {\n        let t = Array1::from_vec(vec![1.0, std::f64::consts::E]);\n        let out = NdarrayBackend::log_1d(\u0026t);\n        assert!((out[0] - 0.0).abs() \u003c 1e-6);\n        assert!((out[1] - 1.0).abs() \u003c 1e-6);\n    }\n\n    #[test]\n    fn test_sigmoid_stability() {\n        let input = Array1::from_vec(vec![-100.0, 0.0, 100.0]);\n        let out = NdarrayBackend::sigmoid_1d(\u0026input);\n        let expected = vec![0.0, 0.5, 1.0];\n        for (o, e) in out.iter().zip(expected) {\n            assert!((o - e).abs() \u003c 1e-6);\n        }\n    }\n\n    #[test]\n    fn test_abs_1d() {\n        let t = Array1::from_vec(vec![-2.0, 3.0]);\n        let out = NdarrayBackend::abs_1d(\u0026t);\n        assert_eq!(out.to_vec(), vec![2.0, 3.0]);\n    }\n\n    #[test]\n    fn test_sign_1d() {\n        let t = Array1::from_vec(vec![-2.0, 0.0, 3.0]);\n        let out = NdarrayBackend::sign_1d(\u0026t);\n        assert_eq!(out.to_vec(), vec![-1.0, 0.0, 1.0]);\n    }\n\n    #[test]\n    fn test_maximum_1d() {\n        let a = Array1::from_vec(vec![1.0, 5.0, 3.0]);\n        let b = Array1::from_vec(vec![2.0, 4.0, 6.0]);\n        let out = NdarrayBackend::maximum_1d(\u0026a, \u0026b);\n        assert_eq!(out.to_vec(), vec![2.0, 5.0, 6.0]);\n    }\n\n    #[test]\n    fn test_exp_2d() {\n        let t = tensor2d_from(\u0026[vec![0.0, 1.0]]);\n        let out = NdarrayBackend::exp_2d(\u0026t);\n        assert!((out.0[[0, 0]] - 1.0).abs() \u003c 1e-6);\n        assert!((out.0[[0, 1]] - std::f64::consts::E).abs() \u003c 1e-6);\n    }\n\n    #[test]\n    fn test_log_2d() {\n        let t = tensor2d_from(\u0026[vec![1.0, std::f64::consts::E]]);\n        let out = NdarrayBackend::log_2d(\u0026t);\n        assert!((out.0[[0, 0]] - 0.0).abs() \u003c 1e-6);\n        assert!((out.0[[0, 1]] - 1.0).abs() \u003c 1e-6);\n    }\n\n    #[test]\n    fn test_sigmoid_2d() {\n        let t = tensor2d_from(\u0026[vec![-100.0, 0.0, 100.0]]);\n        let out = NdarrayBackend::sigmoid_2d(\u0026t);\n        let expected = vec![0.0, 0.5, 1.0];\n        for (i, \u0026e) in expected.iter().enumerate() {\n            assert!((out.0[[0, i]] - e).abs() \u003c 1e-6);\n        }\n    }\n\n    #[test]\n    fn test_abs_2d() {\n        let t = tensor2d_from(\u0026[vec![-2.0, 3.0]]);\n        let out = NdarrayBackend::abs_2d(\u0026t);\n        assert_eq!(\n            out.0,\n            Array2::from_shape_vec((1, 2), vec![2.0, 3.0]).unwrap()\n        );\n    }\n\n    #[test]\n    fn test_sign_2d() {\n        let t = tensor2d_from(\u0026[vec![-2.0, 0.0, 3.0]]);\n        let out = NdarrayBackend::sign_2d(\u0026t);\n        assert_eq!(\n            out.0,\n            Array2::from_shape_vec((1, 3), vec![-1.0, 0.0, 1.0]).unwrap()\n        );\n    }\n\n    #[test]\n    fn test_maximum_2d() {\n        let a = tensor2d_from(\u0026[vec![1.0, 5.0, 3.0]]);\n        let b = tensor2d_from(\u0026[vec![2.0, 4.0, 6.0]]);\n        let out = NdarrayBackend::maximum_2d(\u0026a, \u0026b);\n        assert_eq!(\n            out.0,\n            Array2::from_shape_vec((1, 3), vec![2.0, 5.0, 6.0]).unwrap()\n        );\n    }\n\n    #[test]\n    fn test_sum_all_1d() {\n        let t = Array1::from_vec(vec![1.0, 2.0, 3.0]);\n        assert_eq!(NdarrayBackend::sum_all_1d(\u0026t), 6.0);\n    }\n\n    #[test]\n    fn test_mean_all_1d() {\n        let t = Array1::from_vec(vec![1.0, 2.0, 3.0]);\n        assert!((NdarrayBackend::mean_all_1d(\u0026t) - 2.0).abs() \u003c 1e-6);\n    }\n\n    #[test]\n    fn test_sum_all_2d() {\n        let t = tensor2d_from(\u0026[vec![1.0, 2.0], vec![3.0, 4.0]]);\n        assert_eq!(NdarrayBackend::sum_all_2d(\u0026t), 10.0);\n    }\n\n    #[test]\n    fn test_mean_all_2d() {\n        let t = tensor2d_from(\u0026[vec![1.0, 2.0], vec![3.0, 4.0]]);\n        assert!((NdarrayBackend::mean_all_2d(\u0026t) - 2.5).abs() \u003c 1e-6);\n    }\n\n    #[test]\n    fn test_scalar_f64() {\n        assert_eq!(NdarrayBackend::scalar_f64(42.0), 42.0);\n    }\n\n    #[test]\n    fn test_empty_tensor_2d() {\n        let empty: \u0026[Vec\u003cf64\u003e] = \u0026[];\n        let t = tensor2d_from(empty);\n        assert_eq!(NdarrayBackend::shape(\u0026t), (0, 0));\n    }\n\n    #[test]\n    #[should_panic(expected = \"Shapes must match\")]\n    fn test_maximum_1d_mismatch() {\n        let a = Array1::from_vec(vec![1.0]);\n        let b = Array1::from_vec(vec![1.0, 2.0]);\n        NdarrayBackend::maximum_1d(\u0026a, \u0026b);\n    }\n\n    #[test]\n    #[should_panic(expected = \"Shapes must match\")]\n    fn test_maximum_2d_mismatch() {\n        let a = tensor2d_from(\u0026[vec![1.0]]);\n        let b = tensor2d_from(\u0026[vec![1.0, 2.0]]);\n        NdarrayBackend::maximum_2d(\u0026a, \u0026b);\n    }\n}\n","traces":[{"line":56,"address":[3077328],"length":1,"stats":{"Line":3}},{"line":57,"address":[3077371],"length":1,"stats":{"Line":3}},{"line":58,"address":[3077379],"length":1,"stats":{"Line":3}},{"line":59,"address":[3077385],"length":1,"stats":{"Line":1}},{"line":61,"address":[3077472,3077575,3077499],"length":1,"stats":{"Line":4}},{"line":62,"address":[3077592,3077522],"length":1,"stats":{"Line":12}},{"line":63,"address":[3077627],"length":1,"stats":{"Line":14}},{"line":64,"address":[3077695],"length":1,"stats":{"Line":2}},{"line":90,"address":[3077104],"length":1,"stats":{"Line":1}},{"line":91,"address":[3077121],"length":1,"stats":{"Line":1}},{"line":106,"address":[3077136],"length":1,"stats":{"Line":1}},{"line":107,"address":[3077160],"length":1,"stats":{"Line":1}},{"line":121,"address":[3070400],"length":1,"stats":{"Line":1}},{"line":122,"address":[3070414],"length":1,"stats":{"Line":3}},{"line":142,"address":[3070464],"length":1,"stats":{"Line":1}},{"line":143,"address":[3079744,3079755],"length":1,"stats":{"Line":3}},{"line":144,"address":[3070555],"length":1,"stats":{"Line":1}},{"line":161,"address":[3073936],"length":1,"stats":{"Line":1}},{"line":162,"address":[3073957],"length":1,"stats":{"Line":1}},{"line":179,"address":[3070864],"length":1,"stats":{"Line":1}},{"line":180,"address":[3070888],"length":1,"stats":{"Line":1}},{"line":208,"address":[3074416],"length":1,"stats":{"Line":1}},{"line":209,"address":[3074437],"length":1,"stats":{"Line":1}},{"line":234,"address":[3071008],"length":1,"stats":{"Line":1}},{"line":235,"address":[3071037],"length":1,"stats":{"Line":1}},{"line":253,"address":[3077232],"length":1,"stats":{"Line":1}},{"line":254,"address":[3077257],"length":1,"stats":{"Line":1}},{"line":272,"address":[3073696],"length":1,"stats":{"Line":1}},{"line":273,"address":[3073705],"length":1,"stats":{"Line":1}},{"line":274,"address":[3073730,3073813],"length":1,"stats":{"Line":1}},{"line":290,"address":[3074192],"length":1,"stats":{"Line":1}},{"line":291,"address":[3074209],"length":1,"stats":{"Line":1}},{"line":308,"address":[3074320],"length":1,"stats":{"Line":1}},{"line":309,"address":[3074337],"length":1,"stats":{"Line":1}},{"line":327,"address":[3070192],"length":1,"stats":{"Line":1}},{"line":328,"address":[3070209],"length":1,"stats":{"Line":2}},{"line":329,"address":[3079428,3079475],"length":1,"stats":{"Line":2}},{"line":330,"address":[3079482],"length":1,"stats":{"Line":1}},{"line":332,"address":[3079442],"length":1,"stats":{"Line":1}},{"line":333,"address":[3079453],"length":1,"stats":{"Line":1}},{"line":348,"address":[3073840],"length":1,"stats":{"Line":1}},{"line":349,"address":[3073857],"length":1,"stats":{"Line":1}},{"line":364,"address":[3076816],"length":1,"stats":{"Line":1}},{"line":365,"address":[3076833],"length":1,"stats":{"Line":2}},{"line":366,"address":[3079961,3079921],"length":1,"stats":{"Line":2}},{"line":367,"address":[3079947],"length":1,"stats":{"Line":1}},{"line":368,"address":[3079936,3079972],"length":1,"stats":{"Line":2}},{"line":369,"address":[3079974],"length":1,"stats":{"Line":1}},{"line":371,"address":[3079963],"length":1,"stats":{"Line":1}},{"line":390,"address":[3069360],"length":1,"stats":{"Line":1}},{"line":391,"address":[3069409],"length":1,"stats":{"Line":2}},{"line":392,"address":[3069552],"length":1,"stats":{"Line":3}},{"line":398,"address":[3074224],"length":1,"stats":{"Line":1}},{"line":399,"address":[3074242],"length":1,"stats":{"Line":1}},{"line":406,"address":[3074352],"length":1,"stats":{"Line":1}},{"line":407,"address":[3074370],"length":1,"stats":{"Line":1}},{"line":413,"address":[3070224],"length":1,"stats":{"Line":1}},{"line":414,"address":[3070242],"length":1,"stats":{"Line":2}},{"line":415,"address":[3079619,3079572],"length":1,"stats":{"Line":2}},{"line":416,"address":[3079626],"length":1,"stats":{"Line":1}},{"line":418,"address":[3079586],"length":1,"stats":{"Line":1}},{"line":419,"address":[3079597],"length":1,"stats":{"Line":1}},{"line":425,"address":[3073872],"length":1,"stats":{"Line":1}},{"line":426,"address":[3073890],"length":1,"stats":{"Line":1}},{"line":430,"address":[3076848],"length":1,"stats":{"Line":1}},{"line":431,"address":[3076866],"length":1,"stats":{"Line":2}},{"line":432,"address":[3080057,3080017],"length":1,"stats":{"Line":2}},{"line":433,"address":[3080043],"length":1,"stats":{"Line":1}},{"line":434,"address":[3080068,3080032],"length":1,"stats":{"Line":2}},{"line":435,"address":[3080070],"length":1,"stats":{"Line":1}},{"line":437,"address":[3080059],"length":1,"stats":{"Line":1}},{"line":446,"address":[3069664],"length":1,"stats":{"Line":1}},{"line":447,"address":[3069715],"length":1,"stats":{"Line":1}},{"line":448,"address":[3069751],"length":1,"stats":{"Line":1}},{"line":449,"address":[3069917],"length":1,"stats":{"Line":1}},{"line":451,"address":[3069935],"length":1,"stats":{"Line":1}},{"line":452,"address":[3069977],"length":1,"stats":{"Line":3}},{"line":454,"address":[3070029],"length":1,"stats":{"Line":1}},{"line":460,"address":[3070288],"length":1,"stats":{"Line":1}},{"line":461,"address":[3070293],"length":1,"stats":{"Line":1}},{"line":468,"address":[3070640],"length":1,"stats":{"Line":1}},{"line":469,"address":[3070645],"length":1,"stats":{"Line":1}},{"line":473,"address":[3070304],"length":1,"stats":{"Line":1}},{"line":474,"address":[3070309],"length":1,"stats":{"Line":1}},{"line":481,"address":[3070672],"length":1,"stats":{"Line":1}},{"line":482,"address":[3070677],"length":1,"stats":{"Line":1}},{"line":490,"address":[3070176],"length":1,"stats":{"Line":1}},{"line":495,"address":[3074288],"length":1,"stats":{"Line":1}},{"line":496,"address":[3074293],"length":1,"stats":{"Line":1}},{"line":502,"address":[3074304],"length":1,"stats":{"Line":1}},{"line":503,"address":[3074309],"length":1,"stats":{"Line":1}},{"line":507,"address":[3077200],"length":1,"stats":{"Line":1}},{"line":508,"address":[3077217],"length":1,"stats":{"Line":1}},{"line":514,"address":[3074592],"length":1,"stats":{"Line":1}},{"line":515,"address":[3074613],"length":1,"stats":{"Line":1}},{"line":519,"address":[3074464],"length":1,"stats":{"Line":1}},{"line":520,"address":[3074485],"length":1,"stats":{"Line":1}},{"line":524,"address":[3074064],"length":1,"stats":{"Line":1}},{"line":525,"address":[3074085],"length":1,"stats":{"Line":1}},{"line":529,"address":[3070816],"length":1,"stats":{"Line":1}},{"line":530,"address":[3070837],"length":1,"stats":{"Line":3}},{"line":534,"address":[3070704],"length":1,"stats":{"Line":1}},{"line":535,"address":[3070725],"length":1,"stats":{"Line":3}},{"line":541,"address":[3070752],"length":1,"stats":{"Line":1}},{"line":542,"address":[3070776],"length":1,"stats":{"Line":3}},{"line":546,"address":[3073984],"length":1,"stats":{"Line":1}},{"line":547,"address":[3074008],"length":1,"stats":{"Line":1}},{"line":551,"address":[3074640],"length":1,"stats":{"Line":1}},{"line":552,"address":[3074664],"length":1,"stats":{"Line":1}},{"line":556,"address":[3074512],"length":1,"stats":{"Line":1}},{"line":557,"address":[3074536],"length":1,"stats":{"Line":1}},{"line":561,"address":[3074112],"length":1,"stats":{"Line":1}},{"line":562,"address":[3074136],"length":1,"stats":{"Line":1}},{"line":571,"address":[3070960],"length":1,"stats":{"Line":0}},{"line":572,"address":[3070981],"length":1,"stats":{"Line":0}},{"line":578,"address":[3073648],"length":1,"stats":{"Line":0}},{"line":579,"address":[3073669],"length":1,"stats":{"Line":0}},{"line":583,"address":[3077008],"length":1,"stats":{"Line":0}},{"line":584,"address":[3077032],"length":1,"stats":{"Line":0}},{"line":591,"address":[3070320],"length":1,"stats":{"Line":0}},{"line":592,"address":[3070338],"length":1,"stats":{"Line":0}},{"line":593,"address":[3070363],"length":1,"stats":{"Line":0}},{"line":596,"address":[3069287,3069281,3068304],"length":1,"stats":{"Line":0}},{"line":597,"address":[3068370],"length":1,"stats":{"Line":0}},{"line":598,"address":[3068391],"length":1,"stats":{"Line":0}},{"line":599,"address":[3068405],"length":1,"stats":{"Line":0}},{"line":602,"address":[3068432],"length":1,"stats":{"Line":0}},{"line":603,"address":[3068445,3068532],"length":1,"stats":{"Line":0}},{"line":605,"address":[3068548],"length":1,"stats":{"Line":0}},{"line":606,"address":[3068635,3069178,3068563],"length":1,"stats":{"Line":0}},{"line":607,"address":[3068749],"length":1,"stats":{"Line":0}},{"line":608,"address":[3069276,3068761,3068840],"length":1,"stats":{"Line":0}},{"line":609,"address":[3069193,3068954],"length":1,"stats":{"Line":0}},{"line":610,"address":[3069254],"length":1,"stats":{"Line":0}},{"line":612,"address":[3069016,3069111],"length":1,"stats":{"Line":0}},{"line":613,"address":[3069136,3069085],"length":1,"stats":{"Line":0}},{"line":615,"address":[3068794],"length":1,"stats":{"Line":0}},{"line":618,"address":[3068282,3067584,3068276],"length":1,"stats":{"Line":0}},{"line":619,"address":[3067625],"length":1,"stats":{"Line":0}},{"line":620,"address":[3067646],"length":1,"stats":{"Line":0}},{"line":621,"address":[3067657],"length":1,"stats":{"Line":0}},{"line":624,"address":[3067692],"length":1,"stats":{"Line":0}},{"line":625,"address":[3067796,3067705],"length":1,"stats":{"Line":0}},{"line":626,"address":[3067952,3067907],"length":1,"stats":{"Line":0}},{"line":627,"address":[3068102],"length":1,"stats":{"Line":0}},{"line":628,"address":[3068174,3068271],"length":1,"stats":{"Line":0}},{"line":629,"address":[3068231],"length":1,"stats":{"Line":0}},{"line":633,"address":[3067924],"length":1,"stats":{"Line":0}},{"line":636,"address":[3067558,3067552,3066864],"length":1,"stats":{"Line":0}},{"line":637,"address":[3066905],"length":1,"stats":{"Line":0}},{"line":638,"address":[3066926],"length":1,"stats":{"Line":0}},{"line":639,"address":[3066937],"length":1,"stats":{"Line":0}},{"line":642,"address":[3066972],"length":1,"stats":{"Line":0}},{"line":643,"address":[3066985,3067076],"length":1,"stats":{"Line":0}},{"line":644,"address":[3067187,3067232],"length":1,"stats":{"Line":0}},{"line":645,"address":[3067382],"length":1,"stats":{"Line":0}},{"line":646,"address":[3067454,3067547],"length":1,"stats":{"Line":0}},{"line":647,"address":[3067507],"length":1,"stats":{"Line":0}},{"line":651,"address":[3067204],"length":1,"stats":{"Line":0}},{"line":654,"address":[3069312],"length":1,"stats":{"Line":0}},{"line":655,"address":[3069329],"length":1,"stats":{"Line":0}},{"line":660,"address":[3070128],"length":1,"stats":{"Line":0}},{"line":661,"address":[3070145],"length":1,"stats":{"Line":0}},{"line":666,"address":[3073488],"length":1,"stats":{"Line":0}},{"line":667,"address":[3073539],"length":1,"stats":{"Line":0}},{"line":670,"address":[3073168],"length":1,"stats":{"Line":0}},{"line":671,"address":[3073219],"length":1,"stats":{"Line":0}},{"line":674,"address":[3073328],"length":1,"stats":{"Line":0}},{"line":675,"address":[3073379],"length":1,"stats":{"Line":0}},{"line":678,"address":[3073008],"length":1,"stats":{"Line":0}},{"line":679,"address":[3073059],"length":1,"stats":{"Line":0}},{"line":682,"address":[3076912],"length":1,"stats":{"Line":0}},{"line":683,"address":[3076929],"length":1,"stats":{"Line":0}},{"line":686,"address":[3076944],"length":1,"stats":{"Line":0}},{"line":687,"address":[3076962],"length":1,"stats":{"Line":0}},{"line":692,"address":[3076318,3074720,3076312],"length":1,"stats":{"Line":0}},{"line":695,"address":[3074797],"length":1,"stats":{"Line":0}},{"line":696,"address":[3074864],"length":1,"stats":{"Line":0}},{"line":697,"address":[3074830],"length":1,"stats":{"Line":0}},{"line":701,"address":[3074814,3074980,3075017],"length":1,"stats":{"Line":0}},{"line":702,"address":[3074993],"length":1,"stats":{"Line":0}},{"line":703,"address":[3075103],"length":1,"stats":{"Line":0}},{"line":704,"address":[3075034],"length":1,"stats":{"Line":0}},{"line":709,"address":[3075223,3075173],"length":1,"stats":{"Line":0}},{"line":710,"address":[3075299],"length":1,"stats":{"Line":0}},{"line":711,"address":[3076664],"length":1,"stats":{"Line":0}},{"line":712,"address":[3076331],"length":1,"stats":{"Line":0}},{"line":713,"address":[3076460,3076521],"length":1,"stats":{"Line":0}},{"line":719,"address":[3075336],"length":1,"stats":{"Line":0}},{"line":722,"address":[3075374],"length":1,"stats":{"Line":0}},{"line":723,"address":[3075411],"length":1,"stats":{"Line":0}},{"line":724,"address":[3075965,3075423,3075502],"length":1,"stats":{"Line":0}},{"line":725,"address":[3075718,3075612],"length":1,"stats":{"Line":0}},{"line":726,"address":[3075726],"length":1,"stats":{"Line":0}},{"line":727,"address":[3075895,3076000,3076307],"length":1,"stats":{"Line":0}},{"line":728,"address":[3076123],"length":1,"stats":{"Line":0}},{"line":731,"address":[3075928,3075970],"length":1,"stats":{"Line":0}},{"line":734,"address":[3075627],"length":1,"stats":{"Line":0}},{"line":737,"address":[3071088],"length":1,"stats":{"Line":0}},{"line":738,"address":[3071152],"length":1,"stats":{"Line":0}},{"line":739,"address":[3071193],"length":1,"stats":{"Line":0}},{"line":740,"address":[3071236],"length":1,"stats":{"Line":0}},{"line":744,"address":[3071212,3071329],"length":1,"stats":{"Line":0}},{"line":745,"address":[3071532],"length":1,"stats":{"Line":0}},{"line":754,"address":[3071433],"length":1,"stats":{"Line":0}},{"line":755,"address":[3071463],"length":1,"stats":{"Line":0}},{"line":758,"address":[3071744,3072641,3072635],"length":1,"stats":{"Line":0}},{"line":759,"address":[3071787],"length":1,"stats":{"Line":0}},{"line":760,"address":[3071912,3071805],"length":1,"stats":{"Line":0}},{"line":761,"address":[3071895],"length":1,"stats":{"Line":0}},{"line":762,"address":[3071811],"length":1,"stats":{"Line":0}},{"line":767,"address":[3071925,3072021],"length":1,"stats":{"Line":0}},{"line":768,"address":[3072910,3072118],"length":1,"stats":{"Line":0}},{"line":777,"address":[3072150],"length":1,"stats":{"Line":0}},{"line":778,"address":[3072181,3072244,3072630],"length":1,"stats":{"Line":0}},{"line":779,"address":[3072431],"length":1,"stats":{"Line":0}},{"line":780,"address":[3072618,3072508],"length":1,"stats":{"Line":0}},{"line":783,"address":[3072559],"length":1,"stats":{"Line":0}}],"covered":113,"coverable":218},{"path":["/","home","victor","ML","machinelearne-rs","lib","src","backend","scalar.rs"],"content":"use crate::backend::Backend;\nuse std::marker::PhantomData;\n\n/// Trait for scalar operations required by numerical backends.\n///\n/// Defines a minimal set of arithmetic and mathematical operations needed for\n/// machine learning computations. Implemented for primitive floating-point types\n/// used by backends (e.g., `f64`).\n///\n/// # Design rationale\n/// This trait abstracts scalar operations to enable backend-agnostic generic code\n/// while maintaining performance through `Copy` semantics and avoiding dynamic dispatch.\n///\n/// # Required operations\n/// - Basic arithmetic via standard library traits (`Add`, `Sub`, `Mul`, `Div`)\n/// - Mathematical functions: `sqrt`, `abs`, `exp`\n/// - Type conversion: `from_f64`/`to_f64` for interoperability\n/// - Constants: `zero()`, `one()` for initialization\n///\n/// # Safety guarantees\n/// Implementations must satisfy:\n/// - `Copy` + `Clone` for zero-cost abstractions\n/// - `Send` + `Sync` for thread-safe parallel computations\n/// - Numerical stability for edge cases (e.g., `sqrt` of negative numbers should panic)\n///\n/// # Example\n/// ```\n/// use machinelearne_rs::backend::ScalarOps;\n///\n/// let x = 4.0f64;\n/// assert_eq!(x.sqrt(), 2.0);\n/// assert_eq!(f64::zero(), 0.0);\n/// assert_eq!(f64::one(), 1.0);\n/// ```\npub trait ScalarOps:\n    Clone\n    + Copy\n    + Send\n    + Sync\n    + std::ops::Add\u003cOutput = Self\u003e\n    + std::ops::Mul\u003cOutput = Self\u003e\n    + std::ops::Sub\u003cOutput = Self\u003e\n    + std::ops::Div\u003cOutput = Self\u003e\n{\n    /// Computes the square root of the scalar.\n    ///\n    /// # Panics\n    /// Panics if called on a negative value (for real-number implementations).\n    fn sqrt(self) -\u003e Self;\n\n    /// Returns the absolute value of the scalar.\n    fn abs(self) -\u003e Self;\n\n    /// Returns the additive identity (zero) for this scalar type.\n    fn zero() -\u003e Self;\n\n    /// Returns the multiplicative identity (one) for this scalar type.\n    fn one() -\u003e Self;\n\n    /// Converts an `f64` value to this scalar type.\n    ///\n    /// Used for backend-agnostic initialization from host values.\n    fn from_f64(v: f64) -\u003e Self;\n\n    /// Converts this scalar to an `f64` value.\n    ///\n    /// Used for interoperability with host code and debugging.\n    fn to_f64(self) -\u003e f64;\n\n    /// Computes the exponential function `e^x`.\n    fn exp(self) -\u003e Self;\n}\n\n// === Implementations for primitive types ===\n\n/// `f64` implementation of `ScalarOps`.\n///\n/// Provides IEEE 754 double-precision floating point operations with\n/// hardware-accelerated math functions.\n///\n/// # Numerical behavior\n/// - Follows standard IEEE 754 semantics (NaN propagation, infinities)\n/// - `sqrt` panics on negative inputs via Rust's built-in `sqrt()` method\n/// - `exp` handles overflow by returning `INFINITY` per IEEE 754\nimpl ScalarOps for f64 {\n    fn sqrt(self) -\u003e Self {\n        self.sqrt()\n    }\n\n    fn abs(self) -\u003e Self {\n        self.abs()\n    }\n\n    fn zero() -\u003e Self {\n        0.0\n    }\n\n    fn one() -\u003e Self {\n        1.0\n    }\n\n    fn from_f64(v: f64) -\u003e Self {\n        v\n    }\n\n    fn to_f64(self) -\u003e f64 {\n        self\n    }\n\n    fn exp(self) -\u003e Self {\n        self.exp()\n    }\n}\n\n/// Backend-typed scalar wrapper providing compile-time type safety.\n///\n/// Wraps a backend's native scalar type (`B::Scalar`) while carrying phantom\n/// type information about its originating backend. This prevents accidental\n/// mixing of scalars from different backends at compile time.\n///\n/// # Type safety guarantees\n/// ```compile_fail\n/// use machinelearne_rs::backend::{CpuBackend, NdarrayBackend};\n/// use machinelearne_rs::backend::Scalar;\n///\n/// let cpu_scalar: Scalar\u003cCpuBackend\u003e = Scalar::new(1.0);\n/// let ndarray_scalar: Scalar\u003cNdarrayBackend\u003e = Scalar::new(2.0);\n/// let _ = cpu_scalar + ndarray_scalar; // COMPILE ERROR: mismatched backends\n/// ```\n///\n/// # Zero-cost abstraction\n/// - `PhantomData\u003cB\u003e` adds no runtime overhead (zero-sized type)\n/// - All operations delegate directly to backend's scalar type\n/// - Implements `Copy` for efficient pass-by-value semantics\n///\n/// # Example\n/// ```\n/// use machinelearne_rs::backend::CpuBackend;\n/// use machinelearne_rs::backend::{Scalar, ScalarOps};\n///\n/// let s: Scalar\u003cCpuBackend\u003e = Scalar::new(2.0);\n/// let squared = s * s; // Backend-safe multiplication\n/// assert_eq!(squared.to_f64(), 4.0);\n/// ```\n#[derive(Clone, Debug, Copy)]\npub struct Scalar\u003cB: Backend\u003e {\n    pub(crate) data: B::Scalar,\n    pub(crate) backend: PhantomData\u003cB\u003e,\n}\n\nimpl\u003cB: Backend\u003e Scalar\u003cB\u003e {\n    /// Creates a new scalar from an `f64` host value.\n    ///\n    /// Converts the host `f64` value to the backend's native scalar representation\n    /// using the backend's `scalar_f64` conversion function.\n    ///\n    /// # Example\n    /// ```\n    /// use machinelearne_rs::backend::CpuBackend;\n    /// use machinelearne_rs::backend::Scalar;\n    ///\n    /// let s: Scalar\u003cCpuBackend\u003e = Scalar::new(3.14);\n    /// assert_eq!(s.to_f64(), 3.14);\n    /// ```\n    pub fn new(f: f64) -\u003e Self {\n        Self {\n            data: B::scalar_f64(f),\n            backend: PhantomData,\n        }\n    }\n    /// Converts this backend scalar to a host `f64` value.\n    ///\n    /// Extracts the scalar value from the backend's native representation and\n    /// converts it to a standard Rust `f64` for interoperability with host code,\n    /// debugging, logging, or serialization.\n    ///\n    /// # Precision considerations\n    /// For backends using `f64` internally (e.g., `CpuBackend`, `NdarrayBackend`),\n    /// this is a zero-cost identity conversion. For hypothetical future backends\n    /// assert!((host_value - std::f64::consts::E).abs() \u003c 1e-5);\n    pub fn to_f64(\u0026self) -\u003e f64 {\n        self.data.to_f64()\n    }\n\n    /// Computes the exponential function `e^x` for this scalar.\n    ///\n    /// Delegates to the backend's scalar implementation of `exp`.\n    ///\n    /// # Example\n    /// ```\n    /// use machinelearne_rs::backend::CpuBackend;\n    /// use machinelearne_rs::backend::Scalar;\n    ///\n    /// let s: Scalar\u003cCpuBackend\u003e = Scalar::new(1.0);\n    /// let e = s.exp();\n    /// assert!((e.to_f64() - std::f64::consts::E).abs() \u003c 1e-12);\n    /// ```\n    pub fn exp(\u0026self) -\u003e Self {\n        Self {\n            data: self.data.exp(),\n            backend: PhantomData,\n        }\n    }\n}\n\n// === Standard arithmetic trait implementations ===\n\n/// Implements addition for backend-typed scalars.\n///\n/// Enables ergonomic `a + b` syntax while preserving backend type safety.\n/// Only scalars from the *same* backend can be added together.\n///\n/// # Example\n/// ```\n/// use machinelearne_rs::backend::CpuBackend;\n/// use machinelearne_rs::backend::Scalar;\n///\n/// let a: Scalar\u003cCpuBackend\u003e = Scalar::new(2.0);\n/// let b: Scalar\u003cCpuBackend\u003e = Scalar::new(3.0);\n/// let sum = a + b;\n/// assert_eq!(sum.to_f64(), 5.0);\n/// ```\nimpl\u003cB: Backend + Copy\u003e std::ops::Add for Scalar\u003cB\u003e {\n    type Output = Self;\n\n    fn add(self, rhs: Self) -\u003e Self {\n        Self {\n            data: self.data + rhs.data,\n            backend: PhantomData,\n        }\n    }\n}\n\n/// Implements subtraction for backend-typed scalars.\nimpl\u003cB: Backend\u003e std::ops::Sub for Scalar\u003cB\u003e {\n    type Output = Self;\n\n    fn sub(self, rhs: Self) -\u003e Self {\n        Self {\n            data: self.data - rhs.data,\n            backend: PhantomData,\n        }\n    }\n}\n\n/// Implements multiplication for backend-typed scalars.\nimpl\u003cB: Backend\u003e std::ops::Mul for Scalar\u003cB\u003e {\n    type Output = Self;\n\n    fn mul(self, rhs: Self) -\u003e Self {\n        Self {\n            data: self.data * rhs.data,\n            backend: PhantomData,\n        }\n    }\n}\n\n/// Implements division for backend-typed scalars.\n///\n/// # Panics\n/// Panics on division by zero according to backend's scalar implementation.\nimpl\u003cB: Backend\u003e std::ops::Div for Scalar\u003cB\u003e {\n    type Output = Self;\n\n    fn div(self, rhs: Self) -\u003e Self {\n        Self {\n            data: self.data / rhs.data,\n            backend: PhantomData,\n        }\n    }\n}\n\n#[cfg(test)]\nmod tests {\n    use super::*;\n    use crate::backend::CpuBackend;\n\n    #[test]\n    fn test_scalar_ops_f64() {\n        let a = 4.0f64;\n        assert_eq!(a.sqrt(), 2.0);\n        assert_eq!(a.abs(), 4.0);\n        assert_eq!(f64::zero(), 0.0);\n        assert_eq!(f64::one(), 1.0);\n        assert_eq!(f64::from_f64(3.14), 3.14);\n        assert_eq!(3.14f64.to_f64(), 3.14);\n        assert_eq!(1.0f64.exp(), std::f64::consts::E);\n    }\n\n    #[test]\n    fn test_scalar_new_and_exp() {\n        let s: Scalar\u003cCpuBackend\u003e = Scalar::new(1.0);\n        assert_eq!(s.data, 1.0);\n\n        let e = s.exp();\n        assert!((e.data - std::f64::consts::E).abs() \u003c 1e-12);\n    }\n\n    #[test]\n    fn test_scalar_arithmetic() {\n        let a: Scalar\u003cCpuBackend\u003e = Scalar::new(5.0);\n        let b: Scalar\u003cCpuBackend\u003e = Scalar::new(2.0);\n\n        let sum = a + b;\n        assert_eq!(sum.data, 7.0);\n\n        let diff = sum - Scalar::new(3.0);\n        assert_eq!(diff.data, 4.0);\n\n        let prod = diff * Scalar::new(0.5);\n        assert_eq!(prod.data, 2.0);\n\n        let quot = prod / Scalar::new(4.0);\n        assert_eq!(quot.data, 0.5);\n    }\n\n    #[test]\n    fn test_scalar_type_safety() {\n        // Verify that scalars from the same backend compose correctly\n        let x: Scalar\u003cCpuBackend\u003e = Scalar::new(10.0);\n        let y: Scalar\u003cCpuBackend\u003e = Scalar::new(3.0);\n        let _ = x / y; // Must compile successfully\n\n        // Note: Cross-backend operations are prevented at compile time:\n        // let z: Scalar\u003cNdarrayBackend\u003e = Scalar::new(2.0);\n        // let _ = x + z; // \u003c-- Would fail to compile with type mismatch error\n    }\n}\n","traces":[{"line":86,"address":[2825440],"length":1,"stats":{"Line":0}},{"line":87,"address":[2825446],"length":1,"stats":{"Line":0}},{"line":90,"address":[2825392],"length":1,"stats":{"Line":0}},{"line":91,"address":[2825398],"length":1,"stats":{"Line":0}},{"line":102,"address":[2825488],"length":1,"stats":{"Line":1}},{"line":106,"address":[2825472],"length":1,"stats":{"Line":1}},{"line":110,"address":[2825408],"length":1,"stats":{"Line":1}},{"line":111,"address":[2825414],"length":1,"stats":{"Line":1}},{"line":165,"address":[1755872],"length":1,"stats":{"Line":1}},{"line":167,"address":[1755878],"length":1,"stats":{"Line":2}},{"line":181,"address":[1755888],"length":1,"stats":{"Line":5}},{"line":182,"address":[1755893],"length":1,"stats":{"Line":5}},{"line":198,"address":[1755856],"length":1,"stats":{"Line":1}},{"line":200,"address":[1755861],"length":1,"stats":{"Line":1}},{"line":226,"address":[1755952],"length":1,"stats":{"Line":1}},{"line":228,"address":[1755968],"length":1,"stats":{"Line":1}},{"line":238,"address":[1756080],"length":1,"stats":{"Line":1}},{"line":240,"address":[1756096],"length":1,"stats":{"Line":1}},{"line":250,"address":[1756032],"length":1,"stats":{"Line":1}},{"line":252,"address":[1756048],"length":1,"stats":{"Line":1}},{"line":265,"address":[1756000],"length":1,"stats":{"Line":1}},{"line":267,"address":[1756016],"length":1,"stats":{"Line":1}}],"covered":18,"coverable":22},{"path":["/","home","victor","ML","machinelearne-rs","lib","src","backend","tensor1d.rs"],"content":"use super::scalar::Scalar;\nuse crate::backend::Backend;\nuse std::marker::PhantomData;\n\n/// Backend-typed 1D tensor providing compile-time type safety and zero-cost abstractions.\n///\n/// Wraps a backend's native 1D tensor representation (`B::Tensor1D`) while carrying phantom\n/// type information about its originating backend. This prevents accidental mixing of tensors\n/// from different backends at compile time while maintaining performance through zero-sized\n/// `PhantomData` overhead.\n///\n/// # Type safety guarantees\n/// ```compile_fail\n/// use machinelearne_rs::backend::{CpuBackend, NdarrayBackend};\n/// use machinelearne_rs::backend::Tensor1D;\n///\n/// let cpu_tensor: Tensor1D\u003cCpuBackend\u003e = Tensor1D::zeros(3);\n/// let ndarray_tensor: Tensor1D\u003cNdarrayBackend\u003e = Tensor1D::zeros(3);\n/// let _ = cpu_tensor.sub(\u0026ndarray_tensor); // COMPILE ERROR: mismatched backends\n/// ```\n///\n/// # Precision semantics\n/// - Constructors accept `Vec\u003cf32\u003e` for ergonomic data loading from common sources\n/// - Values are immediately converted to backend's native precision (typically `f64`)\n/// - All operations occur in native backend precision\n/// - `to_vec()` returns `Vec\u003cf64\u003e` for host interoperability\n///\n/// # Zero-cost design\n/// - `PhantomData\u003cB\u003e` adds no runtime memory overhead\n/// - All operations delegate directly to backend implementations\n/// - Implements `Clone` (but not `Copy`) due to potential heap allocation in underlying tensors\n///\n/// # Example\n/// ```\n/// use machinelearne_rs::backend::CpuBackend;\n/// use machinelearne_rs::backend::{Tensor1D, Scalar};\n///\n/// // Create tensor from f32 data (converted to f64 internally)\n/// let x: Tensor1D\u003cCpuBackend\u003e = Tensor1D::new(vec![1.0f32, 2.0, 3.0]);\n/// assert_eq!(x.len(), 3);\n///\n/// // Element-wise operations\n/// let y = x.scale(\u0026Scalar::\u003cCpuBackend\u003e::new(2.0));\n/// assert_eq!(y.to_vec(), vec![2.0, 4.0, 6.0]);\n/// ```\n#[derive(Clone)]\npub struct Tensor1D\u003cB: Backend\u003e {\n    pub(crate) data: B::Tensor1D,\n    pub(crate) backend: PhantomData\u003cB\u003e,\n}\n\nimpl\u003cB: Backend\u003e Tensor1D\u003cB\u003e {\n    /// Creates a new 1D tensor from a vector of `f32` values.\n    ///\n    /// Converts the input `Vec\u003cf32\u003e` to the backend's native scalar representation\n    /// (typically `f64`) using the backend's `from_vec_1d` conversion function.\n    ///\n    /// # Precision note\n    /// Input values are converted from `f32` to backend precision (usually `f64`).\n    /// This conversion is lossless for values representable in both types, but extremely\n    /// large `f32` values near `f32::MAX` may lose precision when converted to `f64`.\n    ///\n    /// # Example\n    /// ```\n    /// use machinelearne_rs::backend::CpuBackend;\n    /// use machinelearne_rs::backend::Tensor1D;\n    ///\n    /// let t: Tensor1D\u003cCpuBackend\u003e = Tensor1D::new(vec![1.0f32, 2.5, 3.75]);\n    /// assert_eq!(t.to_vec(), vec![1.0, 2.5, 3.75]);\n    /// ```\n    pub fn new(data: Vec\u003cf32\u003e) -\u003e Self {\n        Self {\n            data: B::from_vec_1d(data),\n            backend: PhantomData,\n        }\n    }\n\n    /// Creates a 1D tensor filled with zeros of specified length.\n    ///\n    /// # Arguments\n    /// * `len` - Number of elements in the resulting tensor\n    ///\n    /// # Example\n    /// ```\n    /// use machinelearne_rs::backend::CpuBackend;\n    /// use machinelearne_rs::backend::Tensor1D;\n    ///\n    /// let zeros: Tensor1D\u003cCpuBackend\u003e = Tensor1D::zeros(4);\n    /// assert_eq!(zeros.to_vec(), vec![0.0, 0.0, 0.0, 0.0]);\n    /// assert_eq!(zeros.len(), 4);\n    /// ```\n    pub fn zeros(len: usize) -\u003e Self {\n        Self {\n            data: B::zeros_1d(len),\n            backend: PhantomData,\n        }\n    }\n\n    /// Computes element-wise subtraction: `self - other`.\n    ///\n    /// # Panics\n    /// Panics if tensors have different lengths (backend-dependent behavior).\n    ///\n    /// # Example\n    /// ```\n    /// use machinelearne_rs::backend::CpuBackend;\n    /// use machinelearne_rs::backend::Tensor1D;\n    ///\n    /// let a = Tensor1D::\u003cCpuBackend\u003e::new(vec![5.0f32, 7.0, 9.0]);\n    /// let b = Tensor1D::\u003cCpuBackend\u003e::new(vec![2.0f32, 3.0, 4.0]);\n    /// let diff = a.sub(\u0026b);\n    /// assert_eq!(diff.to_vec(), vec![3.0, 4.0, 5.0]);\n    /// ```\n    pub fn sub(\u0026self, other: \u0026Self) -\u003e Self {\n        Self {\n            data: B::sub_1d(\u0026self.data, \u0026other.data),\n            backend: PhantomData,\n        }\n    }\n\n    /// Computes the arithmetic mean of all elements in the tensor.\n    ///\n    /// # Returns\n    /// A `Scalar\u003cB\u003e` containing the mean value.\n    ///\n    /// # Panics\n    /// Panics if the tensor is empty (backend-dependent behavior).\n    ///\n    /// # Example\n    /// ```\n    /// use machinelearne_rs::backend::CpuBackend;\n    /// use machinelearne_rs::backend::Tensor1D;\n    ///\n    /// let t = Tensor1D::\u003cCpuBackend\u003e::new(vec![1.0f32, 2.0, 3.0, 4.0]);\n    /// let mean = t.mean();\n    /// assert!((mean.to_f64() - 2.5).abs() \u003c 1e-12);\n    /// ```\n    pub fn mean(\u0026self) -\u003e Scalar\u003cB\u003e {\n        Scalar {\n            data: B::mean_all_1d(\u0026self.data),\n            backend: PhantomData,\n        }\n    }\n\n    /// Converts the tensor to a standard Rust `Vec\u003cf64\u003e` for host interoperability.\n    ///\n    /// # Use cases\n    /// - Debugging and logging\n    /// - Serialization to external formats\n    /// - Interfacing with non-backend-aware code\n    /// - Test assertions\n    ///\n    /// # Example\n    /// ```\n    /// use machinelearne_rs::backend::CpuBackend;\n    /// use machinelearne_rs::backend::Tensor1D;\n    ///\n    /// let t = Tensor1D::\u003cCpuBackend\u003e::new(vec![1.5f32, -2.5, 3.5]);\n    /// let host_vec = t.to_vec();\n    /// assert_eq!(host_vec, vec![1.5, -2.5, 3.5]);\n    /// ```\n    pub fn to_vec(\u0026self) -\u003e Vec\u003cf64\u003e {\n        B::to_vec_1d(\u0026self.data)\n    }\n\n    /// Computes the dot product (inner product) between two tensors.\n    ///\n    /// Equivalent to `sum(self * other)` where `*` denotes element-wise multiplication.\n    ///\n    /// # Formula\n    /// `dot(a, b) = Î£áµ¢ aáµ¢ * báµ¢`\n    ///\n    /// # Panics\n    /// Panics if tensors have different lengths.\n    ///\n    /// # Example\n    /// ```\n    /// use machinelearne_rs::backend::CpuBackend;\n    /// use machinelearne_rs::backend::Tensor1D;\n    ///\n    /// let a = Tensor1D::\u003cCpuBackend\u003e::new(vec![1.0f32, 2.0, 3.0]);\n    /// let b = Tensor1D::\u003cCpuBackend\u003e::new(vec![4.0f32, 5.0, 6.0]);\n    /// let dot = a.dot(\u0026b);\n    /// // 1*4 + 2*5 + 3*6 = 4 + 10 + 18 = 32\n    /// assert_eq!(dot.to_f64(), 32.0);\n    /// ```\n    pub fn dot(\u0026self, other: \u0026Self) -\u003e Scalar\u003cB\u003e {\n        let prod = B::mul_1d(\u0026self.data, \u0026other.data);\n        let sum = B::sum_all_1d(\u0026prod);\n        Scalar {\n            data: sum,\n            backend: PhantomData,\n        }\n    }\n\n    /// Computes element-wise absolute value.\n    ///\n    /// # Example\n    /// ```\n    /// use machinelearne_rs::backend::CpuBackend;\n    /// use machinelearne_rs::backend::Tensor1D;\n    ///\n    /// let t = Tensor1D::\u003cCpuBackend\u003e::new(vec![-1.0f32, 2.0, -3.0]);\n    /// let abs_t = t.abs();\n    /// assert_eq!(abs_t.to_vec(), vec![1.0, 2.0, 3.0]);\n    /// ```\n    pub fn abs(\u0026self) -\u003e Self {\n        Self {\n            data: B::abs_1d(\u0026self.data),\n            backend: PhantomData,\n        }\n    }\n\n    /// Computes element-wise sign function.\n    ///\n    /// Returns:\n    /// - `1.0` for positive values\n    /// - `-1.0` for negative values\n    /// - `0.0` for zero values\n    ///\n    /// # Example\n    /// ```\n    /// use machinelearne_rs::backend::CpuBackend;\n    /// use machinelearne_rs::backend::Tensor1D;\n    ///\n    /// let t = Tensor1D::\u003cCpuBackend\u003e::new(vec![-2.0f32, 0.0, 3.0]);\n    /// let sign_t = t.sign();\n    /// assert_eq!(sign_t.to_vec(), vec![-1.0, 0.0, 1.0]);\n    /// ```\n    pub fn sign(\u0026self) -\u003e Self {\n        Self {\n            data: B::sign_1d(\u0026self.data),\n            backend: PhantomData,\n        }\n    }\n\n    /// Returns the number of elements in the tensor.\n    ///\n    /// # Example\n    /// ```\n    /// use machinelearne_rs::backend::CpuBackend;\n    /// use machinelearne_rs::backend::Tensor1D;\n    ///\n    /// let t = Tensor1D::\u003cCpuBackend\u003e::new(vec![1.0f32, 2.0, 3.0]);\n    /// assert_eq!(t.len(), 3);\n    /// ```\n    pub fn len(\u0026self) -\u003e usize {\n        B::len_1d(\u0026self.data)\n    }\n\n    /// Returns `true` if the tensor contains no elements.\n    ///\n    /// # Example\n    /// ```\n    /// use machinelearne_rs::backend::CpuBackend;\n    /// use machinelearne_rs::backend::Tensor1D;\n    ///\n    /// let empty = Tensor1D::\u003cCpuBackend\u003e::zeros(0);\n    /// assert!(empty.is_empty());\n    ///\n    /// let non_empty = Tensor1D::\u003cCpuBackend\u003e::zeros(1);\n    /// assert!(!non_empty.is_empty());\n    /// ```\n    pub fn is_empty(\u0026self) -\u003e bool {\n        B::len_1d(\u0026self.data) == 0\n    }\n\n    /// Scales the tensor by multiplying each element by a scalar value.\n    ///\n    /// Equivalent to element-wise multiplication: `self * scalar`.\n    ///\n    /// # Example\n    /// ```\n    /// use machinelearne_rs::backend::CpuBackend;\n    /// use machinelearne_rs::backend::Tensor1D;\n    /// use machinelearne_rs::backend::Scalar;\n    ///\n    /// let t = Tensor1D::\u003cCpuBackend\u003e::new(vec![1.0f32, 2.0, 3.0]);\n    /// let s = Scalar::\u003cCpuBackend\u003e::new(2.5);\n    /// let scaled = t.scale(\u0026s);\n    /// assert_eq!(scaled.to_vec(), vec![2.5, 5.0, 7.5]);\n    /// ```\n    pub fn scale(\u0026self, a: \u0026Scalar\u003cB\u003e) -\u003e Self {\n        Self {\n            data: B::mul_scalar_1d(\u0026self.data, \u0026a.data),\n            backend: PhantomData,\n        }\n    }\n\n    /// Adds a scalar value to each element of the tensor.\n    ///\n    /// Equivalent to element-wise addition: `self + scalar`.\n    ///\n    /// # Example\n    /// ```\n    /// use machinelearne_rs::backend::CpuBackend;\n    /// use machinelearne_rs::backend::Tensor1D;\n    /// use machinelearne_rs::backend::Scalar;\n    ///\n    /// let t = Tensor1D::\u003cCpuBackend\u003e::new(vec![1.0f32, 2.0, 3.0]);\n    /// let s = Scalar::\u003cCpuBackend\u003e::new(10.0);\n    /// let shifted = t.add_scalar(\u0026s);\n    /// assert_eq!(shifted.to_vec(), vec![11.0, 12.0, 13.0]);\n    /// ```\n    pub fn add_scalar(\u0026self, a: \u0026Scalar\u003cB\u003e) -\u003e Self {\n        Self {\n            data: B::add_scalar_1d(\u0026self.data, \u0026a.data),\n            backend: PhantomData,\n        }\n    }\n\n    /// Computes element-wise maximum between two tensors.\n    ///\n    /// For each index `i`, returns `max(self[i], other[i])`.\n    ///\n    /// # Panics\n    /// Panics if tensors have different lengths.\n    ///\n    /// # Example\n    /// ```\n    /// use machinelearne_rs::backend::CpuBackend;\n    /// use machinelearne_rs::backend::Tensor1D;\n    ///\n    /// let a = Tensor1D::\u003cCpuBackend\u003e::new(vec![1.0f32, 5.0, 3.0]);\n    /// let b = Tensor1D::\u003cCpuBackend\u003e::new(vec![2.0f32, 4.0, 6.0]);\n    /// let max_ab = a.maximum(b);\n    /// assert_eq!(max_ab.to_vec(), vec![2.0, 5.0, 6.0]);\n    /// ```\n    pub fn maximum(\u0026self, other: Self) -\u003e Self {\n        Self {\n            data: B::maximum_1d(\u0026self.data, \u0026other.data),\n            backend: PhantomData,\n        }\n    }\n\n    /// Computes element-wise exponential function: `e^x`.\n    ///\n    /// # Numerical behavior\n    /// Follows IEEE 754 semantics:\n    /// - `exp(0.0)` = `1.0`\n    /// - `exp(+â)` = `+â`\n    /// - `exp(-â)` = `0.0`\n    /// - Large positive inputs may return `INFINITY` on overflow\n    ///\n    /// # Example\n    /// ```\n    /// use machinelearne_rs::backend::CpuBackend;\n    /// use machinelearne_rs::backend::Tensor1D;\n    ///\n    /// let t = Tensor1D::\u003cCpuBackend\u003e::new(vec![0.0f32, 1.0]);\n    /// let exp_t = t.exp();\n    /// assert!((exp_t.to_vec()[0] - 1.0).abs() \u003c 1e-12);\n    /// assert!((exp_t.to_vec()[1] - std::f64::consts::E).abs() \u003c 1e-12);\n    /// ```\n    pub fn exp(\u0026self) -\u003e Self {\n        Self {\n            data: B::exp_1d(\u0026self.data),\n            backend: PhantomData,\n        }\n    }\n\n    /// Computes element-wise natural logarithm: `ln(x)`.\n    ///\n    /// # Panics\n    /// Panics for non-positive values (backend-dependent behavior).\n    ///\n    /// # Example\n    /// ```\n    /// use machinelearne_rs::backend::CpuBackend;\n    /// use machinelearne_rs::backend::Tensor1D;\n    ///\n    /// let t = Tensor1D::\u003cCpuBackend\u003e::new(vec![1.0f32, std::f64::consts::E as f32]);\n    /// let log_t = t.log();\n    /// assert!((log_t.to_vec()[0] - 0.0).abs() \u003c 1e-6);\n    /// assert!((log_t.to_vec()[1] - 1.0).abs() \u003c 1e-6);\n    /// ```\n    pub fn log(\u0026self) -\u003e Self {\n        Self {\n            data: B::log_1d(\u0026self.data),\n            backend: PhantomData,\n        }\n    }\n\n    /// Computes numerically stable sigmoid activation element-wise.\n    ///\n    /// Implements the logistic function: `Ï(x) = 1 / (1 + e^(-x))`\n    ///\n    /// # Numerical stability\n    /// Uses a numerically stable implementation that avoids overflow/underflow\n    /// for extreme input values (e.g., Â±100):\n    /// - For `x \u003e= 0`: `1 / (1 + e^(-x))`\n    /// - For `x \u003c 0`: `e^x / (1 + e^x)`\n    ///\n    /// # Output range\n    /// Always returns values in the open interval `(0, 1)`.\n    ///\n    /// # Example\n    /// ```\n    /// use machinelearne_rs::backend::CpuBackend;\n    /// use machinelearne_rs::backend::Tensor1D;\n    ///\n    /// let t = Tensor1D::\u003cCpuBackend\u003e::new(vec![-100.0f32, 0.0, 100.0]);\n    /// let sig = t.sigmoid();\n    /// let output = sig.to_vec();\n    ///\n    /// // Extreme negative â â0.0\n    /// assert!(output[0] \u003c 1e-10);\n    /// // Zero â 0.5 exactly\n    /// assert!((output[1] - 0.5).abs() \u003c 1e-12);\n    /// // Extreme positive â â1.0\n    /// assert!(output[2] \u003e 1.0 - 1e-10);\n    /// ```\n    pub fn sigmoid(\u0026self) -\u003e Self {\n        Self {\n            data: B::sigmoid_1d(\u0026self.data),\n            backend: PhantomData,\n        }\n    }\n}\n\n#[cfg(test)]\nmod tests {\n    use super::*;\n    use crate::backend::CpuBackend;\n\n    // === Constructor Tests ===\n\n    #[test]\n    fn test_tensor1d_new() {\n        let t = Tensor1D::\u003cCpuBackend\u003e::new(vec![1.0, 2.0, 3.0]);\n        assert_eq!(t.to_vec(), vec![1.0, 2.0, 3.0]);\n        assert_eq!(t.len(), 3);\n    }\n\n    #[test]\n    fn test_tensor1d_new_empty() {\n        let t = Tensor1D::\u003cCpuBackend\u003e::new(vec![]);\n        assert_eq!(t.to_vec(), vec![]);\n        assert_eq!(t.len(), 0);\n        assert!(t.is_empty());\n    }\n\n    #[test]\n    fn test_tensor1d_new_single() {\n        let t = Tensor1D::\u003cCpuBackend\u003e::new(vec![42.0]);\n        assert_eq!(t.to_vec(), vec![42.0]);\n        assert_eq!(t.len(), 1);\n        assert!(!t.is_empty());\n    }\n\n    #[test]\n    fn test_tensor1d_new_negative() {\n        let t = Tensor1D::\u003cCpuBackend\u003e::new(vec![-1.0, -2.5, -0.5]);\n        assert_eq!(t.to_vec(), vec![-1.0, -2.5, -0.5]);\n    }\n\n    #[test]\n    fn test_tensor1d_zeros() {\n        let t = Tensor1D::\u003cCpuBackend\u003e::zeros(5);\n        assert_eq!(t.to_vec(), vec![0.0, 0.0, 0.0, 0.0, 0.0]);\n        assert_eq!(t.len(), 5);\n        assert!(!t.is_empty());\n    }\n\n    #[test]\n    fn test_tensor1d_zeros_empty() {\n        let t = Tensor1D::\u003cCpuBackend\u003e::zeros(0);\n        assert_eq!(t.to_vec(), vec![]);\n        assert_eq!(t.len(), 0);\n        assert!(t.is_empty());\n    }\n\n    #[test]\n    fn test_tensor1d_zeros_single() {\n        let t = Tensor1D::\u003cCpuBackend\u003e::zeros(1);\n        assert_eq!(t.to_vec(), vec![0.0]);\n        assert_eq!(t.len(), 1);\n    }\n\n    #[test]\n    fn test_tensor1d_zeros_large() {\n        let n = 10000;\n        let t = Tensor1D::\u003cCpuBackend\u003e::zeros(n);\n        assert_eq!(t.len(), n);\n        assert!(t.to_vec().iter().all(|\u0026x| x == 0.0));\n    }\n\n    // === len and is_empty Tests ===\n\n    #[test]\n    fn test_tensor1d_len() {\n        let t = Tensor1D::\u003cCpuBackend\u003e::new(vec![1.0, 2.0, 3.0, 4.0, 5.0]);\n        assert_eq!(t.len(), 5);\n    }\n\n    #[test]\n    fn test_tensor1d_is_empty_true() {\n        let t = Tensor1D::\u003cCpuBackend\u003e::new(vec![]);\n        assert!(t.is_empty());\n    }\n\n    #[test]\n    fn test_tensor1d_is_empty_false() {\n        let t = Tensor1D::\u003cCpuBackend\u003e::new(vec![1.0]);\n        assert!(!t.is_empty());\n    }\n\n    // === to_vec Tests ===\n\n    #[test]\n    fn test_tensor1d_to_vec_simple() {\n        let t = Tensor1D::\u003cCpuBackend\u003e::new(vec![1.5, 2.5, 3.5]);\n        assert_eq!(t.to_vec(), vec![1.5, 2.5, 3.5]);\n    }\n\n    #[test]\n    fn test_tensor1d_to_vec_negative() {\n        let t = Tensor1D::\u003cCpuBackend\u003e::new(vec![-1.0, -2.0, -3.0]);\n        assert_eq!(t.to_vec(), vec![-1.0, -2.0, -3.0]);\n    }\n\n    #[test]\n    fn test_tensor1d_to_vec_fractional() {\n        let t = Tensor1D::\u003cCpuBackend\u003e::new(vec![0.1, 0.25, 0.75]);\n        assert!((t.to_vec()[0] - 0.1).abs() \u003c 1e-6);\n        assert!((t.to_vec()[1] - 0.25).abs() \u003c 1e-6);\n        assert!((t.to_vec()[2] - 0.75).abs() \u003c 1e-6);\n    }\n\n    // === sub Tests ===\n\n    #[test]\n    fn test_tensor1d_sub() {\n        let a = Tensor1D::\u003cCpuBackend\u003e::new(vec![5.0, 7.0]);\n        let b = Tensor1D::\u003cCpuBackend\u003e::new(vec![2.0, 3.0]);\n        let diff = a.sub(\u0026b);\n        assert_eq!(diff.to_vec(), vec![3.0, 4.0]);\n    }\n\n    #[test]\n    fn test_tensor1d_sub_negative_result() {\n        let a = Tensor1D::\u003cCpuBackend\u003e::new(vec![1.0]);\n        let b = Tensor1D::\u003cCpuBackend\u003e::new(vec![5.0]);\n        let diff = a.sub(\u0026b);\n        assert_eq!(diff.to_vec(), vec![-4.0]);\n    }\n\n    #[test]\n    fn test_tensor1d_sub_with_negatives() {\n        let a = Tensor1D::\u003cCpuBackend\u003e::new(vec![-1.0, -5.0]);\n        let b = Tensor1D::\u003cCpuBackend\u003e::new(vec![-3.0, -2.0]);\n        let diff = a.sub(\u0026b);\n        assert_eq!(diff.to_vec(), vec![2.0, -3.0]);\n    }\n\n    // === mean Tests ===\n\n    #[test]\n    fn test_tensor1d_mean() {\n        let t = Tensor1D::\u003cCpuBackend\u003e::new(vec![1.0, 2.0, 3.0, 4.0]);\n        let mean = t.mean();\n        assert_eq!(mean.to_f64(), 2.5);\n    }\n\n    #[test]\n    fn test_tensor1d_mean_negatives() {\n        let t = Tensor1D::\u003cCpuBackend\u003e::new(vec![-1.0, 1.0, -1.0, 1.0]);\n        let mean = t.mean();\n        assert_eq!(mean.to_f64(), 0.0);\n    }\n\n    #[test]\n    fn test_tensor1d_mean_single() {\n        let t = Tensor1D::\u003cCpuBackend\u003e::new(vec![5.0]);\n        let mean = t.mean();\n        assert_eq!(mean.to_f64(), 5.0);\n    }\n\n    // === dot Tests ===\n\n    #[test]\n    fn test_tensor1d_dot_simple() {\n        let a = Tensor1D::\u003cCpuBackend\u003e::new(vec![1.0, 2.0, 3.0]);\n        let b = Tensor1D::\u003cCpuBackend\u003e::new(vec![4.0, 5.0, 6.0]);\n        let dot = a.dot(\u0026b);\n        // 1*4 + 2*5 + 3*6 = 4 + 10 + 18 = 32\n        assert_eq!(dot.to_f64(), 32.0);\n    }\n\n    #[test]\n    fn test_tensor1d_dot_single() {\n        let a = Tensor1D::\u003cCpuBackend\u003e::new(vec![5.0]);\n        let b = Tensor1D::\u003cCpuBackend\u003e::new(vec![3.0]);\n        let dot = a.dot(\u0026b);\n        assert_eq!(dot.to_f64(), 15.0);\n    }\n\n    #[test]\n    fn test_tensor1d_dot_with_negatives() {\n        let a = Tensor1D::\u003cCpuBackend\u003e::new(vec![1.0, -1.0]);\n        let b = Tensor1D::\u003cCpuBackend\u003e::new(vec![1.0, 1.0]);\n        let dot = a.dot(\u0026b);\n        // 1*1 + (-1)*1 = 0\n        assert_eq!(dot.to_f64(), 0.0);\n    }\n\n    #[test]\n    fn test_tensor1d_dot_all_zeros() {\n        let a = Tensor1D::\u003cCpuBackend\u003e::new(vec![0.0, 0.0, 0.0]);\n        let b = Tensor1D::\u003cCpuBackend\u003e::new(vec![1.0, 2.0, 3.0]);\n        let dot = a.dot(\u0026b);\n        assert_eq!(dot.to_f64(), 0.0);\n    }\n\n    #[test]\n    fn test_tensor1d_dot_fractional() {\n        let a = Tensor1D::\u003cCpuBackend\u003e::new(vec![0.5, 0.5]);\n        let b = Tensor1D::\u003cCpuBackend\u003e::new(vec![1.0, 1.0]);\n        let dot = a.dot(\u0026b);\n        // 0.5*1 + 0.5*1 = 1.0\n        assert!((dot.to_f64() - 1.0).abs() \u003c 1e-10);\n    }\n\n    // === abs Tests ===\n\n    #[test]\n    fn test_tensor1d_abs() {\n        let t = Tensor1D::\u003cCpuBackend\u003e::new(vec![-1.0, 2.0, -3.0]);\n        let abs_t = t.abs();\n        assert_eq!(abs_t.to_vec(), vec![1.0, 2.0, 3.0]);\n    }\n\n    #[test]\n    fn test_tensor1d_abs_all_negative() {\n        let t = Tensor1D::\u003cCpuBackend\u003e::new(vec![-5.0, -10.0, -0.5]);\n        let abs_t = t.abs();\n        assert_eq!(abs_t.to_vec(), vec![5.0, 10.0, 0.5]);\n    }\n\n    #[test]\n    fn test_tensor1d_abs_all_positive() {\n        let t = Tensor1D::\u003cCpuBackend\u003e::new(vec![1.0, 2.0, 3.0]);\n        let abs_t = t.abs();\n        assert_eq!(abs_t.to_vec(), vec![1.0, 2.0, 3.0]);\n    }\n\n    // === sign Tests ===\n\n    #[test]\n    fn test_tensor1d_sign() {\n        let t = Tensor1D::\u003cCpuBackend\u003e::new(vec![-2.0, 0.0, 3.0]);\n        let sign_t = t.sign();\n        assert_eq!(sign_t.to_vec(), vec![-1.0, 0.0, 1.0]);\n    }\n\n    #[test]\n    fn test_tensor1d_sign_fractional() {\n        let t = Tensor1D::\u003cCpuBackend\u003e::new(vec![-0.5, 0.5, 0.0]);\n        let sign_t = t.sign();\n        assert_eq!(sign_t.to_vec(), vec![-1.0, 1.0, 0.0]);\n    }\n\n    // === scale Tests ===\n\n    #[test]\n    fn test_tensor1d_scale() {\n        let t = Tensor1D::\u003cCpuBackend\u003e::new(vec![1.0, 2.0, 3.0]);\n        let s = Scalar::\u003cCpuBackend\u003e::new(2.0);\n        let scaled = t.scale(\u0026s);\n        assert_eq!(scaled.to_vec(), vec![2.0, 4.0, 6.0]);\n    }\n\n    #[test]\n    fn test_tensor1d_scale_fractional() {\n        let t = Tensor1D::\u003cCpuBackend\u003e::new(vec![1.0, 2.0]);\n        let s = Scalar::\u003cCpuBackend\u003e::new(0.5);\n        let scaled = t.scale(\u0026s);\n        assert_eq!(scaled.to_vec(), vec![0.5, 1.0]);\n    }\n\n    #[test]\n    fn test_tensor1d_scale_negative() {\n        let t = Tensor1D::\u003cCpuBackend\u003e::new(vec![1.0, 2.0]);\n        let s = Scalar::\u003cCpuBackend\u003e::new(-1.0);\n        let scaled = t.scale(\u0026s);\n        assert_eq!(scaled.to_vec(), vec![-1.0, -2.0]);\n    }\n\n    #[test]\n    fn test_tensor1d_scale_zero() {\n        let t = Tensor1D::\u003cCpuBackend\u003e::new(vec![1.0, 2.0, 3.0]);\n        let s = Scalar::\u003cCpuBackend\u003e::new(0.0);\n        let scaled = t.scale(\u0026s);\n        assert_eq!(scaled.to_vec(), vec![0.0, 0.0, 0.0]);\n    }\n\n    // === add_scalar Tests ===\n\n    #[test]\n    fn test_tensor1d_add_scalar() {\n        let t = Tensor1D::\u003cCpuBackend\u003e::new(vec![1.0, 2.0, 3.0]);\n        let s = Scalar::\u003cCpuBackend\u003e::new(10.0);\n        let result = t.add_scalar(\u0026s);\n        assert_eq!(result.to_vec(), vec![11.0, 12.0, 13.0]);\n    }\n\n    #[test]\n    fn test_tensor1d_add_scalar_negative() {\n        let t = Tensor1D::\u003cCpuBackend\u003e::new(vec![5.0, 10.0]);\n        let s = Scalar::\u003cCpuBackend\u003e::new(-3.0);\n        let result = t.add_scalar(\u0026s);\n        assert_eq!(result.to_vec(), vec![2.0, 7.0]);\n    }\n\n    // === maximum Tests ===\n\n    #[test]\n    fn test_tensor1d_maximum() {\n        let a = Tensor1D::\u003cCpuBackend\u003e::new(vec![1.0, 5.0, 3.0]);\n        let b = Tensor1D::\u003cCpuBackend\u003e::new(vec![2.0, 4.0, 6.0]);\n        let max_ab = a.maximum(b);\n        assert_eq!(max_ab.to_vec(), vec![2.0, 5.0, 6.0]);\n    }\n\n    #[test]\n    fn test_tensor1d_maximum_equal() {\n        let a = Tensor1D::\u003cCpuBackend\u003e::new(vec![1.0, 2.0, 3.0]);\n        let b = Tensor1D::\u003cCpuBackend\u003e::new(vec![1.0, 2.0, 3.0]);\n        let max_ab = a.maximum(b);\n        assert_eq!(max_ab.to_vec(), vec![1.0, 2.0, 3.0]);\n    }\n\n    #[test]\n    fn test_tensor1d_maximum_with_negatives() {\n        let a = Tensor1D::\u003cCpuBackend\u003e::new(vec![-1.0, 5.0]);\n        let b = Tensor1D::\u003cCpuBackend\u003e::new(vec![1.0, -5.0]);\n        let max_ab = a.maximum(b);\n        assert_eq!(max_ab.to_vec(), vec![1.0, 5.0]);\n    }\n\n    // === exp Tests ===\n\n    #[test]\n    fn test_tensor1d_exp_zero() {\n        let t = Tensor1D::\u003cCpuBackend\u003e::new(vec![0.0]);\n        let exp_t = t.exp();\n        assert!((exp_t.to_vec()[0] - 1.0).abs() \u003c 1e-12);\n    }\n\n    #[test]\n    fn test_tensor1d_exp_one() {\n        let t = Tensor1D::\u003cCpuBackend\u003e::new(vec![1.0]);\n        let exp_t = t.exp();\n        assert!((exp_t.to_vec()[0] - std::f64::consts::E).abs() \u003c 1e-12);\n    }\n\n    #[test]\n    fn test_tensor1d_exp_negative() {\n        let t = Tensor1D::\u003cCpuBackend\u003e::new(vec![-1.0]);\n        let exp_t = t.exp();\n        assert!((exp_t.to_vec()[0] - (1.0 / std::f64::consts::E)).abs() \u003c 1e-12);\n    }\n\n    // === log Tests ===\n\n    #[test]\n    fn test_tensor1d_log() {\n        let t = Tensor1D::\u003cCpuBackend\u003e::new(vec![1.0, std::f64::consts::E as f32]);\n        let log_t = t.log();\n        assert!((log_t.to_vec()[0] - 0.0).abs() \u003c 1e-6);\n        assert!((log_t.to_vec()[1] - 1.0).abs() \u003c 1e-6);\n    }\n\n    // === sigmoid Tests ===\n\n    #[test]\n    fn test_tensor1d_sigmoid_zero() {\n        let t = Tensor1D::\u003cCpuBackend\u003e::new(vec![0.0]);\n        let sig = t.sigmoid();\n        assert!((sig.to_vec()[0] - 0.5).abs() \u003c 1e-12);\n    }\n\n    #[test]\n    fn test_tensor1d_sigmoid_large_positive() {\n        let t = Tensor1D::\u003cCpuBackend\u003e::new(vec![100.0]);\n        let sig = t.sigmoid();\n        // sigmoid(100) is approximately 1.0, which satisfies \u003e 1.0 - 1e-10\n        // and equals 1.0 exactly due to floating point saturation\n        assert!(sig.to_vec()[0] \u003e= 1.0 - 1e-10);\n        assert!(sig.to_vec()[0] \u003c= 1.0);\n    }\n\n    #[test]\n    fn test_tensor1d_sigmoid_large_negative() {\n        let t = Tensor1D::\u003cCpuBackend\u003e::new(vec![-100.0]);\n        let sig = t.sigmoid();\n        assert!(sig.to_vec()[0] \u003c 1e-10);\n        assert!(sig.to_vec()[0] \u003e 0.0);\n    }\n\n    // === Method Chaining Tests ===\n\n    #[test]\n    fn test_tensor1d_chaining_scale_add() {\n        let t = Tensor1D::\u003cCpuBackend\u003e::new(vec![1.0, 2.0]);\n        let result = t\n            .scale(\u0026Scalar::\u003cCpuBackend\u003e::new(2.0))\n            .add_scalar(\u0026Scalar::\u003cCpuBackend\u003e::new(1.0));\n        assert_eq!(result.to_vec(), vec![3.0, 5.0]);\n    }\n\n    #[test]\n    fn test_tensor1d_chaining_abs_scale() {\n        let t = Tensor1D::\u003cCpuBackend\u003e::new(vec![-1.0, -2.0]);\n        let result = t.abs().scale(\u0026Scalar::\u003cCpuBackend\u003e::new(2.0));\n        assert_eq!(result.to_vec(), vec![2.0, 4.0]);\n    }\n\n    #[test]\n    fn test_tensor1d_chaining_sub_scale() {\n        let a = Tensor1D::\u003cCpuBackend\u003e::new(vec![5.0, 10.0]);\n        let b = Tensor1D::\u003cCpuBackend\u003e::new(vec![1.0, 2.0]);\n        let result = a.sub(\u0026b).scale(\u0026Scalar::\u003cCpuBackend\u003e::new(0.5));\n        assert_eq!(result.to_vec(), vec![2.0, 4.0]);\n    }\n\n    // === Clone Tests ===\n\n    #[test]\n    fn test_tensor1d_clone() {\n        let t = Tensor1D::\u003cCpuBackend\u003e::new(vec![1.0, 2.0, 3.0]);\n        let t_clone = t.clone();\n\n        assert_eq!(t.to_vec(), t_clone.to_vec());\n        assert_eq!(t.len(), t_clone.len());\n    }\n\n    #[test]\n    fn test_tensor1d_clone_independence() {\n        let t = Tensor1D::\u003cCpuBackend\u003e::new(vec![1.0, 2.0]);\n        let t_clone = t.clone();\n        let modified = t_clone.scale(\u0026Scalar::\u003cCpuBackend\u003e::new(2.0));\n\n        // Original should be unchanged\n        assert_eq!(t.to_vec(), vec![1.0, 2.0]);\n        assert_eq!(modified.to_vec(), vec![2.0, 4.0]);\n    }\n}\n","traces":[{"line":71,"address":[2547024],"length":1,"stats":{"Line":1}},{"line":73,"address":[2547037],"length":1,"stats":{"Line":1}},{"line":92,"address":[2547344],"length":1,"stats":{"Line":4}},{"line":94,"address":[2547363],"length":1,"stats":{"Line":4}},{"line":114,"address":[2547088],"length":1,"stats":{"Line":1}},{"line":116,"address":[2547111],"length":1,"stats":{"Line":1}},{"line":138,"address":[2547168],"length":1,"stats":{"Line":3}},{"line":140,"address":[2547173],"length":1,"stats":{"Line":3}},{"line":162,"address":[2547424],"length":1,"stats":{"Line":3}},{"line":163,"address":[2547441],"length":1,"stats":{"Line":3}},{"line":187,"address":[2546704,2546824,2546818],"length":1,"stats":{"Line":5}},{"line":188,"address":[2546734],"length":1,"stats":{"Line":5}},{"line":189,"address":[2546791,2546744],"length":1,"stats":{"Line":10}},{"line":207,"address":[2546624],"length":1,"stats":{"Line":1}},{"line":209,"address":[2546643],"length":1,"stats":{"Line":2}},{"line":230,"address":[2547184],"length":1,"stats":{"Line":2}},{"line":232,"address":[2547203],"length":1,"stats":{"Line":2}},{"line":247,"address":[2546928],"length":1,"stats":{"Line":1}},{"line":248,"address":[2546933],"length":1,"stats":{"Line":1}},{"line":264,"address":[2547664],"length":1,"stats":{"Line":2}},{"line":265,"address":[2547669],"length":1,"stats":{"Line":1}},{"line":283,"address":[2547264],"length":1,"stats":{"Line":3}},{"line":285,"address":[2547287],"length":1,"stats":{"Line":3}},{"line":305,"address":[2546544],"length":1,"stats":{"Line":1}},{"line":307,"address":[2546567],"length":1,"stats":{"Line":2}},{"line":329,"address":[2547568,2547456],"length":1,"stats":{"Line":3}},{"line":331,"address":[2547484],"length":1,"stats":{"Line":3}},{"line":355,"address":[2546848],"length":1,"stats":{"Line":2}},{"line":357,"address":[2546867],"length":1,"stats":{"Line":2}},{"line":377,"address":[2546944],"length":1,"stats":{"Line":1}},{"line":379,"address":[2546963],"length":1,"stats":{"Line":1}},{"line":413,"address":[2547584],"length":1,"stats":{"Line":1}},{"line":415,"address":[2547603],"length":1,"stats":{"Line":2}}],"covered":33,"coverable":33},{"path":["/","home","victor","ML","machinelearne-rs","lib","src","backend","tensor2d.rs"],"content":"use super::scalar::Scalar;\nuse super::tensor1d::Tensor1D;\nuse crate::backend::Backend;\nuse std::marker::PhantomData;\n\n/// Backend-typed 2D tensor (matrix) providing compile-time type safety and zero-cost abstractions.\n///\n/// Wraps a backend's native 2D tensor representation (`B::Tensor2D`) while carrying phantom\n/// type information about its originating backend. This prevents accidental mixing of tensors\n/// from different backends at compile time while maintaining performance through zero-sized\n/// `PhantomData` overhead.\n///\n/// # Type safety guarantees\n/// ```compile_fail\n/// use machinelearne_rs::backend::{CpuBackend, NdarrayBackend};\n/// use machinelearne_rs::backend::Tensor2D;\n///\n/// let cpu_mat: Tensor2D\u003cCpuBackend\u003e = Tensor2D::zeros(2, 2);\n/// let ndarray_mat: Tensor2D\u003cNdarrayBackend\u003e = Tensor2D::zeros(2, 2);\n/// let _ = cpu_mat.sub(\u0026ndarray_mat); // COMPILE ERROR: mismatched backends\n/// ```\n///\n/// # Precision semantics\n/// - Constructors accept `Vec\u003cf32\u003e` for ergonomic data loading (row-major order)\n/// - Values are immediately converted to backend's native precision (typically `f64`)\n/// - All operations occur in native backend precision\n/// - Row-major layout: `[aââ, aââ, ..., aââ, aââ, ..., aââ]` for an (mÃn) matrix\n///\n/// # Matrix-vector operations\n/// This tensor provides two fundamental linear algebra operations:\n/// - `dot(x)`: Computes `A @ x` where `A` is (mÃn) and `x` is (n,) â result (m,)\n/// - `tdot(x)`: Computes `Aáµ @ x` where `A` is (mÃn) and `x` is (m,) â result (n,)\n///\n/// # Zero-cost design\n/// - `PhantomData\u003cB\u003e` adds no runtime memory overhead\n/// - All operations delegate directly to backend implementations\n/// - Implements `Clone` (but not `Copy`) due to potential heap allocation in underlying tensors\n///\n/// # Example\n/// ```\n/// use machinelearne_rs::backend::CpuBackend;\n/// use machinelearne_rs::backend::{Tensor2D, Tensor1D};\n///\n/// // Create 2Ã2 matrix: [[1.0, 2.0], [3.0, 4.0]]\n/// let a: Tensor2D\u003cCpuBackend\u003e = Tensor2D::new(vec![1.0f32, 2.0, 3.0, 4.0], 2, 2);\n/// assert_eq!(a.shape(), (2, 2));\n///\n/// // Matrix-vector multiplication: A @ [1, 0]áµ = [1, 3]áµ\n/// let x = Tensor1D::\u003cCpuBackend\u003e::new(vec![1.0f32, 0.0]);\n/// let y = a.dot(\u0026x);\n/// assert_eq!(y.to_vec(), vec![1.0, 3.0]);\n/// ```\n#[derive(Clone)]\npub struct Tensor2D\u003cB: Backend\u003e {\n    pub(crate) data: B::Tensor2D,\n    pub(crate) backend: PhantomData\u003cB\u003e,\n}\n\nimpl\u003cB: Backend\u003e Tensor2D\u003cB\u003e {\n    /// Creates a new 2D tensor from a flat vector of `f32` values in row-major order.\n    ///\n    /// The input vector must contain exactly `rows * cols` elements arranged as:\n    /// `[rowâ_colâ, rowâ_colâ, ..., rowâ_colâââ, rowâ_colâ, ..., rowâââ_colâââ]`\n    ///\n    /// # Arguments\n    /// * `data` - Flat vector containing matrix elements in row-major order\n    /// * `rows` - Number of rows in the resulting matrix\n    /// * `cols` - Number of columns in the resulting matrix\n    ///\n    /// # Panics\n    /// Panics if `data.len() != rows * cols` (backend-dependent behavior).\n    ///\n    /// # Precision note\n    /// Input values are converted from `f32` to backend precision (usually `f64`).\n    /// Extremely large `f32` values near `f32::MAX` may lose precision during conversion.\n    ///\n    /// # Example\n    /// ```\n    /// use machinelearne_rs::backend::CpuBackend;\n    /// use machinelearne_rs::backend::Tensor2D;\n    ///\n    /// // Matrix: [[1.0, 2.0, 3.0],\n    /// //          [4.0, 5.0, 6.0]]\n    /// let t: Tensor2D\u003cCpuBackend\u003e = Tensor2D::new(vec![1.0f32, 2.0, 3.0, 4.0, 5.0, 6.0], 2, 3);\n    /// assert_eq!(t.shape(), (2, 3));\n    /// ```\n    pub fn new(data: Vec\u003cf32\u003e, rows: usize, cols: usize) -\u003e Self {\n        Self {\n            data: B::from_vec_2d(data, rows, cols),\n            backend: PhantomData,\n        }\n    }\n\n    /// Creates a 2D tensor filled with zeros of specified dimensions.\n    ///\n    /// # Arguments\n    /// * `rows` - Number of rows\n    /// * `cols` - Number of columns\n    ///\n    /// # Example\n    /// ```\n    /// use machinelearne_rs::backend::CpuBackend;\n    /// use machinelearne_rs::backend::Tensor2D;\n    ///\n    /// let zeros: Tensor2D\u003cCpuBackend\u003e = Tensor2D::zeros(3, 4);\n    /// assert_eq!(zeros.shape(), (3, 4));\n    /// assert_eq!(zeros.mean().to_f64(), 0.0);\n    /// ```\n    pub fn zeros(rows: usize, cols: usize) -\u003e Self {\n        Self {\n            data: B::zeros_2d(rows, cols),\n            backend: PhantomData,\n        }\n    }\n\n    /// Computes element-wise subtraction: `self - other`.\n    ///\n    /// # Panics\n    /// Panics if tensors have different shapes (backend-dependent behavior).\n    ///\n    /// # Example\n    /// ```\n    /// use machinelearne_rs::backend::CpuBackend;\n    /// use machinelearne_rs::backend::Tensor2D;\n    ///\n    /// let a = Tensor2D::\u003cCpuBackend\u003e::new(vec![5.0f32, 7.0, 9.0, 11.0], 2, 2);\n    /// let b = Tensor2D::\u003cCpuBackend\u003e::new(vec![2.0f32, 3.0, 4.0, 5.0], 2, 2);\n    /// let diff = a.sub(\u0026b);\n    /// // Result: [[3.0, 4.0], [5.0, 6.0]]\n    /// assert_eq!(diff.mean().to_f64(), 4.5);\n    /// ```\n    pub fn sub(\u0026self, other: \u0026Self) -\u003e Self {\n        Self {\n            data: B::sub_2d(\u0026self.data, \u0026other.data),\n            backend: PhantomData,\n        }\n    }\n\n    /// Computes the arithmetic mean of all elements in the tensor.\n    ///\n    /// # Returns\n    /// A `Scalar\u003cB\u003e` containing the mean value: `sum(elements) / (rows * cols)`\n    ///\n    /// # Panics\n    /// Panics if the tensor is empty (0 rows or 0 columns).\n    ///\n    /// # Example\n    /// ```\n    /// use machinelearne_rs::backend::CpuBackend;\n    /// use machinelearne_rs::backend::Tensor2D;\n    ///\n    /// // Matrix: [[1.0, 2.0],\n    /// //          [3.0, 4.0]]\n    /// let t = Tensor2D::\u003cCpuBackend\u003e::new(vec![1.0f32, 2.0, 3.0, 4.0], 2, 2);\n    /// let mean = t.mean();\n    /// assert!((mean.to_f64() - 2.5).abs() \u003c 1e-12);\n    /// ```\n    pub fn mean(\u0026self) -\u003e Scalar\u003cB\u003e {\n        Scalar {\n            data: B::mean_all_2d(\u0026self.data),\n            backend: PhantomData,\n        }\n    }\n\n    /// Matrix-vector multiplication: computes `A @ x` (no transpose).\n    ///\n    /// Multiplies this (mÃn) matrix by a vector of length n to produce a vector of length m.\n    ///\n    /// # Formula\n    /// `yáµ¢ = Î£â±¼ Aáµ¢â±¼ * xâ±¼` for i â [0, m)\n    ///\n    /// # Arguments\n    /// * `other` - Vector of length n (must match matrix columns)\n    ///\n    /// # Returns\n    /// `Tensor1D\u003cB\u003e` of length m\n    ///\n    /// # Panics\n    /// Panics if `self.shape().1 != other.len()` (columns â  vector length).\n    ///\n    /// # Example\n    /// ```\n    /// use machinelearne_rs::backend::CpuBackend;\n    /// use machinelearne_rs::backend::{Tensor2D, Tensor1D};\n    ///\n    /// // A = [[1.0, 2.0],\n    /// //      [3.0, 4.0]]\n    /// let a = Tensor2D::\u003cCpuBackend\u003e::new(vec![1.0f32, 2.0, 3.0, 4.0], 2, 2);\n    ///\n    /// // x = [1.0, 0.0]áµ\n    /// let x = Tensor1D::\u003cCpuBackend\u003e::new(vec![1.0f32, 0.0]);\n    ///\n    /// // A @ x = [1.0, 3.0]áµ\n    /// let y = a.dot(\u0026x);\n    /// assert_eq!(y.to_vec(), vec![1.0, 3.0]);\n    /// ```\n    pub fn dot(\u0026self, other: \u0026Tensor1D\u003cB\u003e) -\u003e Tensor1D\u003cB\u003e {\n        Tensor1D {\n            data: B::matvec(\u0026self.data, \u0026other.data),\n            backend: PhantomData,\n        }\n    }\n\n    /// Transposed matrix-vector multiplication: computes `Aáµ @ x`.\n    ///\n    /// Multiplies the transpose of this (mÃn) matrix by a vector of length m\n    /// to produce a vector of length n.\n    ///\n    /// # Formula\n    /// `yâ±¼ = Î£áµ¢ Aáµ¢â±¼ * xáµ¢` for j â [0, n)\n    ///\n    /// # Arguments\n    /// * `other` - Vector of length m (must match matrix rows)\n    ///\n    /// # Returns\n    /// `Tensor1D\u003cB\u003e` of length n\n    ///\n    /// # Panics\n    /// Panics if `self.shape().0 != other.len()` (rows â  vector length).\n    ///\n    /// # Example\n    /// ```\n    /// use machinelearne_rs::backend::CpuBackend;\n    /// use machinelearne_rs::backend::{Tensor2D, Tensor1D};\n    ///\n    /// // A = [[1.0, 2.0],\n    /// //      [3.0, 4.0]]\n    /// let a = Tensor2D::\u003cCpuBackend\u003e::new(vec![1.0f32, 2.0, 3.0, 4.0], 2, 2);\n    ///\n    /// // x = [1.0, 0.0]áµ\n    /// let x = Tensor1D::\u003cCpuBackend\u003e::new(vec![1.0f32, 0.0]);\n    ///\n    /// // Aáµ @ x = [1.0*1 + 3.0*0, 2.0*1 + 4.0*0]áµ = [1.0, 2.0]áµ\n    /// let y = a.tdot(\u0026x);\n    /// assert_eq!(y.to_vec(), vec![1.0, 2.0]);\n    /// ```\n    pub fn tdot(\u0026self, other: \u0026Tensor1D\u003cB\u003e) -\u003e Tensor1D\u003cB\u003e {\n        Tensor1D {\n            data: B::matvec_transposed(\u0026self.data, \u0026other.data),\n            backend: PhantomData,\n        }\n    }\n\n    /// Computes element-wise absolute value.\n    ///\n    /// # Example\n    /// ```\n    /// use machinelearne_rs::backend::CpuBackend;\n    /// use machinelearne_rs::backend::Tensor2D;\n    ///\n    /// let t = Tensor2D::\u003cCpuBackend\u003e::new(vec![-1.0f32, 2.0, -3.0, 4.0], 2, 2);\n    /// let abs_t = t.abs();\n    /// // Result: [[1.0, 2.0], [3.0, 4.0]]\n    /// assert_eq!(abs_t.mean().to_f64(), 2.5);\n    /// ```\n    pub fn abs(\u0026self) -\u003e Self {\n        Self {\n            data: B::abs_2d(\u0026self.data),\n            backend: PhantomData,\n        }\n    }\n\n    /// Computes element-wise sign function.\n    ///\n    /// Returns:\n    /// - `1.0` for positive values\n    /// - `-1.0` for negative values\n    /// - `0.0` for zero values\n    ///\n    /// # Example\n    /// ```\n    /// use machinelearne_rs::backend::CpuBackend;\n    /// use machinelearne_rs::backend::Tensor2D;\n    ///\n    /// let t = Tensor2D::\u003cCpuBackend\u003e::new(vec![-2.0f32, 0.0, 3.0, -4.0], 2, 2);\n    /// let sign_t = t.sign();\n    /// // Result: [[-1.0, 0.0], [1.0, -1.0]]\n    /// let vec = sign_t.ravel().to_vec(); // Note: to_vec() flattens the matrix\n    /// // We can verify specific elements via mean or custom checks\n    /// assert!((sign_t.mean().to_f64() + 0.25).abs() \u003c 1e-12); // (-1+0+1-1)/4 = -0.25\n    /// ```\n    pub fn sign(\u0026self) -\u003e Self {\n        Self {\n            data: B::sign_2d(\u0026self.data),\n            backend: PhantomData,\n        }\n    }\n\n    /// Returns the number of rows in the tensor as a scalar value.\n    ///\n    /// # Note\n    /// This returns a `Scalar\u003cB\u003e` rather than `usize` to maintain a uniform\n    /// tensor/scalar API for backend-agnostic generic code. For most use cases\n    /// requiring integer dimensions, prefer the `shape()` method.\n    ///\n    /// # Example\n    /// ```\n    /// use machinelearne_rs::backend::CpuBackend;\n    /// use machinelearne_rs::backend::Tensor2D;\n    ///\n    /// let t = Tensor2D::\u003cCpuBackend\u003e::zeros(5, 3);\n    /// let rows = t.len();\n    /// assert_eq!(rows.to_f64(), 5.0);\n    /// ```\n    pub fn len(\u0026self) -\u003e Scalar\u003cB\u003e {\n        Scalar {\n            data: B::scalar_f64(B::len_2d(\u0026self.data) as f64),\n            backend: PhantomData,\n        }\n    }\n\n    /// Scales the tensor by multiplying each element by a scalar value.\n    ///\n    /// Equivalent to element-wise multiplication: `self * scalar`.\n    ///\n    /// # Example\n    /// ```\n    /// use machinelearne_rs::backend::CpuBackend;\n    /// use machinelearne_rs::backend::Tensor2D;\n    /// use machinelearne_rs::backend::Scalar;\n    ///\n    /// let t = Tensor2D::\u003cCpuBackend\u003e::new(vec![1.0f32, 2.0, 3.0, 4.0], 2, 2);\n    /// let s = Scalar::\u003cCpuBackend\u003e::new(2.5);\n    /// let scaled = t.scale(s);\n    /// // Result: [[2.5, 5.0], [7.5, 10.0]]\n    /// assert!((scaled.mean().to_f64() - 6.25).abs() \u003c 1e-12);\n    /// ```\n    pub fn scale(\u0026self, a: Scalar\u003cB\u003e) -\u003e Self {\n        Self {\n            data: B::mul_scalar_2d(\u0026self.data, \u0026a.data),\n            backend: PhantomData,\n        }\n    }\n\n    /// Adds a scalar value to each element of the tensor.\n    ///\n    /// Equivalent to element-wise addition: `self + scalar`.\n    ///\n    /// # Example\n    /// ```\n    /// use machinelearne_rs::backend::CpuBackend;\n    /// use machinelearne_rs::backend::Tensor2D;\n    /// use machinelearne_rs::backend::Scalar;\n    ///\n    /// let t = Tensor2D::\u003cCpuBackend\u003e::new(vec![1.0f32, 2.0, 3.0, 4.0], 2, 2);\n    /// let s = Scalar::\u003cCpuBackend\u003e::new(10.0);\n    /// let shifted = t.add_scalar(s);\n    /// // Result: [[11.0, 12.0], [13.0, 14.0]]\n    /// assert_eq!(shifted.mean().to_f64(), 12.5);\n    /// ```\n    pub fn add_scalar(\u0026self, a: Scalar\u003cB\u003e) -\u003e Self {\n        Self {\n            data: B::add_scalar_2d(\u0026self.data, \u0026a.data),\n            backend: PhantomData,\n        }\n    }\n\n    /// Computes element-wise maximum between two tensors.\n    ///\n    /// For each index `(i, j)`, returns `max(self[i,j], other[i,j])`.\n    ///\n    /// # Panics\n    /// Panics if tensors have different shapes.\n    ///\n    /// # Example\n    /// ```\n    /// use machinelearne_rs::backend::CpuBackend;\n    /// use machinelearne_rs::backend::Tensor2D;\n    ///\n    /// let a = Tensor2D::\u003cCpuBackend\u003e::new(vec![1.0f32, 5.0, 3.0, 2.0], 2, 2);\n    /// let b = Tensor2D::\u003cCpuBackend\u003e::new(vec![2.0f32, 4.0, 6.0, 1.0], 2, 2);\n    /// let max_ab = a.maximum(b);\n    /// // Result: [[2.0, 5.0], [6.0, 2.0]]\n    /// assert_eq!(max_ab.mean().to_f64(), 3.75);\n    /// ```\n    pub fn maximum(\u0026self, other: Self) -\u003e Self {\n        Self {\n            data: B::maximum_2d(\u0026self.data, \u0026other.data),\n            backend: PhantomData,\n        }\n    }\n\n    /// Computes element-wise exponential function: `e^x`.\n    ///\n    /// # Numerical behavior\n    /// Follows IEEE 754 semantics:\n    /// - `exp(0.0)` = `1.0`\n    /// - `exp(+â)` = `+â`\n    /// - `exp(-â)` = `0.0`\n    /// - Large positive inputs may return `INFINITY` on overflow\n    ///\n    /// # Example\n    /// ```\n    /// use machinelearne_rs::backend::CpuBackend;\n    /// use machinelearne_rs::backend::Tensor2D;\n    ///\n    /// let t = Tensor2D::\u003cCpuBackend\u003e::new(vec![0.0f32, 1.0], 1, 2);\n    /// let exp_t = t.exp();\n    /// let values = exp_t.ravel().to_vec();\n    /// assert!((values[0] - 1.0).abs() \u003c 1e-12);\n    /// assert!((values[1] - std::f64::consts::E).abs() \u003c 1e-6); // f32âf64 conversion error\n    /// ```\n    pub fn exp(\u0026self) -\u003e Self {\n        Self {\n            data: B::exp_2d(\u0026self.data),\n            backend: PhantomData,\n        }\n    }\n\n    /// Computes element-wise natural logarithm: `ln(x)`.\n    ///\n    /// # Behavior for edge cases (IEEE 754 compliant)\n    /// - `x \u003e 0.0` â `ln(x)` (finite value)\n    /// - `x == 0.0` â `-â` (`f64::NEG_INFINITY`)\n    /// - `x \u003c 0.0` â `NaN` (`f64::NAN`)\n    ///\n    /// Does **not** panic â follows standard floating-point semantics used by\n    /// NumPy, PyTorch, and TensorFlow.\n    ///\n    /// # Numerical stability in ML\n    /// For loss functions involving logarithms (e.g., cross-entropy):\n    /// - Avoid raw `log(x)` on unnormalized probabilities â use `log_softmax` instead\n    /// - Clip inputs when necessary: `log(max(x, Îµ))` with `Îµ = 1e-12` to avoid `-inf`\n    /// - `-inf` propagates through computations and will cause `NaN` in gradients\n    ///\n    /// # Example\n    /// ```\n    /// use machinelearne_rs::backend::{CpuBackend, Tensor2D};\n    ///\n    /// let t = Tensor2D::\u003cCpuBackend\u003e::new(vec![1.0f32, 0.0, -1.0], 1, 3);\n    /// let log_t = t.log();\n    /// let values = log_t.ravel().to_vec();\n    ///\n    /// assert!((values[0] - 0.0).abs() \u003c 1e-12);      // ln(1) = 0\n    /// assert!(values[1].is_infinite() \u0026\u0026 values[1] \u003c 0.0); // ln(0) = -inf\n    /// assert!(values[2].is_nan());                   // ln(-1) = NaN\n    /// ```\n    pub fn log(\u0026self) -\u003e Self {\n        Self {\n            data: B::log_2d(\u0026self.data),\n            backend: PhantomData,\n        }\n    }\n\n    /// Computes numerically stable sigmoid activation element-wise.\n    ///\n    /// Implements the logistic function: `Ï(x) = 1 / (1 + e^(-x))`\n    ///\n    /// # Numerical stability\n    /// Uses a numerically stable implementation that avoids overflow/underflow\n    /// for extreme input values (e.g., Â±100):\n    /// - For `x \u003e= 0`: `1 / (1 + e^(-x))`\n    /// - For `x \u003c 0`: `e^x / (1 + e^x)`\n    ///\n    /// # Output range\n    /// Always returns values in the open interval `(0, 1)`.\n    ///\n    /// # Example\n    /// ```\n    /// use machinelearne_rs::backend::CpuBackend;\n    /// use machinelearne_rs::backend::Tensor2D;\n    ///\n    /// let t = Tensor2D::\u003cCpuBackend\u003e::new(vec![-100.0f32, 0.0, 100.0], 1, 3);\n    /// let sig = t.sigmoid();\n    /// let values = sig.ravel().to_vec();\n    ///\n    /// // Extreme negative â â0.0\n    /// assert!(values[0] \u003c 1e-10);\n    /// // Zero â 0.5 exactly\n    /// assert!((values[1] - 0.5).abs() \u003c 1e-12);\n    /// // Extreme positive â â1.0\n    /// assert!(values[2] \u003e 1.0 - 1e-10);\n    /// ```\n    pub fn sigmoid(\u0026self) -\u003e Self {\n        Self {\n            data: B::sigmoid_2d(\u0026self.data),\n            backend: PhantomData,\n        }\n    }\n\n    /// Returns the shape of the tensor as `(rows, columns)`.\n    ///\n    /// # Returns\n    /// Tuple `(m, n)` where `m` is the number of rows and `n` is the number of columns.\n    ///\n    /// # Example\n    /// ```\n    /// use machinelearne_rs::backend::CpuBackend;\n    /// use machinelearne_rs::backend::Tensor2D;\n    ///\n    /// let t = Tensor2D::\u003cCpuBackend\u003e::new(vec![1.0f32, 2.0, 3.0], 3, 1);\n    /// assert_eq!(t.shape(), (3, 1));\n    ///\n    /// let empty = Tensor2D::\u003cCpuBackend\u003e::zeros(0, 5);\n    /// assert_eq!(empty.shape(), (0, 5));\n    /// ```\n    pub fn shape(\u0026self) -\u003e (usize, usize) {\n        B::shape(\u0026self.data)\n    }\n\n    pub fn ravel(\u0026self) -\u003e Tensor1D\u003cB\u003e {\n        Tensor1D::\u003cB\u003e {\n            data: B::ravel_2d(\u0026self.data),\n            backend: PhantomData,\n        }\n    }\n}\n\n#[cfg(test)]\nmod tests {\n    use super::*;\n    use crate::backend::{CpuBackend, Tensor1D};\n\n    #[test]\n    fn test_new_constructor_valid() {\n        // Standard 2x3 matrix\n        let t = Tensor2D::\u003cCpuBackend\u003e::new(vec![1.0f32, 2.0, 3.0, 4.0, 5.0, 6.0], 2, 3);\n        assert_eq!(t.shape(), (2, 3));\n        assert_eq!(t.ravel().to_vec(), vec![1.0, 2.0, 3.0, 4.0, 5.0, 6.0]);\n    }\n\n    #[test]\n    #[should_panic(expected = \"Inconsistent shape\")]\n    fn test_new_constructor_invalid_shape() {\n        // Should panic when data length != rows * cols\n        let _ = Tensor2D::\u003cCpuBackend\u003e::new(vec![1.0f32, 2.0, 3.0], 2, 2); // 3 != 4\n    }\n\n    #[test]\n    fn test_zeros_constructor() {\n        let t = Tensor2D::\u003cCpuBackend\u003e::zeros(3, 4);\n        assert_eq!(t.shape(), (3, 4));\n        assert_eq!(t.ravel().to_vec(), vec![0.0; 12]);\n        assert_eq!(t.mean().to_f64(), 0.0);\n    }\n\n    #[test]\n    fn test_sub_shape_mismatch_panics() {\n        let a = Tensor2D::\u003cCpuBackend\u003e::zeros(2, 3);\n        let b = Tensor2D::\u003cCpuBackend\u003e::zeros(3, 2);\n\n        // Should panic when shapes don't match\n        std::panic::catch_unwind(|| {\n            let _ = a.sub(\u0026b);\n        })\n        .unwrap_err();\n    }\n\n    #[test]\n    fn test_dot_shape_validation() {\n        let mat = Tensor2D::\u003cCpuBackend\u003e::zeros(3, 4); // 3x4 matrix\n        let vec_wrong = Tensor1D::\u003cCpuBackend\u003e::zeros(3); // Should be length 4\n\n        // Should panic: columns (4) != vector length (3)\n        std::panic::catch_unwind(|| {\n            let _ = mat.dot(\u0026vec_wrong);\n        })\n        .unwrap_err();\n    }\n\n    #[test]\n    fn test_tdot_shape_validation() {\n        let mat = Tensor2D::\u003cCpuBackend\u003e::zeros(3, 4); // 3x4 matrix\n        let vec_wrong = Tensor1D::\u003cCpuBackend\u003e::zeros(4); // Should be length 3\n\n        // Should panic: rows (3) != vector length (4)\n        std::panic::catch_unwind(|| {\n            let _ = mat.tdot(\u0026vec_wrong);\n        })\n        .unwrap_err();\n    }\n\n    #[test]\n    fn test_abs_negative_values() {\n        let t = Tensor2D::\u003cCpuBackend\u003e::new(vec![-5.0f32, -0.0, 3.0, -2.5], 2, 2);\n        let abs_t = t.abs();\n\n        let values = abs_t.ravel().to_vec();\n        assert_eq!(values[0], 5.0);\n        assert_eq!(values[1], 0.0); // -0.0 becomes +0.0\n        assert_eq!(values[2], 3.0);\n        assert_eq!(values[3], 2.5);\n    }\n\n    #[test]\n    fn test_sign_edge_cases() {\n        let t = Tensor2D::\u003cCpuBackend\u003e::new(vec![-3.0f32, -0.0, 0.0, 4.2], 2, 2);\n        let sign_t = t.sign();\n\n        let values = sign_t.ravel().to_vec();\n        assert_eq!(values[0], -1.0);\n        assert_eq!(values[1], -0.0); // sign of -0.0 is -0.0 in IEEE 754\n        assert_eq!(values[2], 0.0);\n        assert_eq!(values[3], 1.0);\n    }\n\n    #[test]\n    fn test_len_returns_row_count() {\n        let t = Tensor2D::\u003cCpuBackend\u003e::zeros(7, 3);\n        let len_scalar = t.len();\n        assert_eq!(len_scalar.to_f64(), 7.0); // Returns ROW count, not total elements\n    }\n\n    #[test]\n    fn test_scale_negative_scalar() {\n        let t = Tensor2D::\u003cCpuBackend\u003e::new(vec![1.0f32, 2.0, 3.0, 4.0], 2, 2);\n        let s = Scalar::\u003cCpuBackend\u003e::new(-2.0);\n        let scaled = t.scale(s);\n\n        let values = scaled.ravel().to_vec();\n        assert_eq!(values, vec![-2.0, -4.0, -6.0, -8.0]);\n    }\n\n    #[test]\n    fn test_add_scalar_negative() {\n        let t = Tensor2D::\u003cCpuBackend\u003e::new(vec![5.0f32, -3.0, 0.0, 2.0], 2, 2);\n        let s = Scalar::\u003cCpuBackend\u003e::new(-10.0);\n        let shifted = t.add_scalar(s);\n\n        let values = shifted.ravel().to_vec();\n        assert_eq!(values, vec![-5.0, -13.0, -10.0, -8.0]);\n    }\n\n    #[test]\n    fn test_maximum_elementwise() {\n        let a = Tensor2D::\u003cCpuBackend\u003e::new(vec![1.0f32, 5.0, -2.0, 0.0], 2, 2);\n        let b = Tensor2D::\u003cCpuBackend\u003e::new(vec![3.0f32, 2.0, -5.0, 4.0], 2, 2);\n        let max_ab = a.maximum(b);\n\n        let values = max_ab.ravel().to_vec();\n        assert_eq!(values, vec![3.0, 5.0, -2.0, 4.0]); // Element-wise max\n    }\n\n    #[test]\n    fn test_exp_numerical_behavior() {\n        let t = Tensor2D::\u003cCpuBackend\u003e::new(vec![0.0f32, 1.0, -1.0, 1000.0], 2, 2);\n        let exp_t = t.exp();\n\n        let values = exp_t.ravel().to_vec();\n        assert!((values[0] - 1.0).abs() \u003c 1e-12); // e^0 = 1\n        assert!((values[1] - std::f64::consts::E).abs() \u003c 1e-6); // e^1 â 2.718\n        assert!((values[2] - 0.367879).abs() \u003c 1e-6); // e^-1 â 0.367879\n        assert!(values[3].is_infinite()); // Overflow to INF\n    }\n\n    #[test]\n    fn test_log_zero_returns_negative_infinity() {\n        let t = Tensor2D::\u003cCpuBackend\u003e::new(vec![0.0f32], 1, 1);\n        let result = t.log();\n        let values = result.ravel().to_vec();\n        assert!(\n            values[0].is_infinite(),\n            \"log(0.0) should return -inf, got {}\",\n            values[0]\n        );\n        assert!(values[0] \u003c 0.0, \"log(0.0) should be negative infinity\");\n    }\n\n    #[test]\n    fn test_log_negative_returns_nan() {\n        let t = Tensor2D::\u003cCpuBackend\u003e::new(vec![-1.0f32, -0.001, -100.0], 1, 3);\n        let result = t.log();\n        let values = result.ravel().to_vec();\n\n        assert!(\n            values[0].is_nan(),\n            \"log(-1.0) should return NaN, got {}\",\n            values[0]\n        );\n        assert!(values[1].is_nan(), \"log(-0.001) should return NaN\");\n        assert!(values[2].is_nan(), \"log(-100.0) should return NaN\");\n    }\n\n    #[test]\n    fn test_log_positive_values() {\n        let t = Tensor2D::\u003cCpuBackend\u003e::new(vec![1.0f32, std::f32::consts::E, 10.0], 1, 3);\n        let result = t.log();\n        let values = result.ravel().to_vec();\n\n        assert!((values[0] - 0.0).abs() \u003c 1e-12, \"log(1.0) = 0.0\");\n        assert!((values[1] - 1.0).abs() \u003c 1e-6, \"log(e) = 1.0\");\n        assert!((values[2] - 2.302585).abs() \u003c 1e-6, \"log(10.0) â 2.302585\");\n    }\n\n    #[test]\n    fn test_log_valid_inputs() {\n        let t = Tensor2D::\u003cCpuBackend\u003e::new(vec![1.0f32, std::f32::consts::E, 10.0], 1, 3);\n        let result = t.log();\n        let values = result.ravel().to_vec();\n\n        assert!((values[0] - 0.0).abs() \u003c 1e-12, \"log(1.0) = 0.0\");\n        assert!((values[1] - 1.0).abs() \u003c 1e-6, \"log(e) = 1.0\");\n        assert!((values[2] - 2.302585).abs() \u003c 1e-6, \"log(10.0) â 2.302585\");\n    }\n\n    #[test]\n    fn test_log_small_positive_values() {\n        // Critical for numerical stability in ML (e.g., log-probabilities)\n        let t = Tensor2D::\u003cCpuBackend\u003e::new(vec![1e-20f32, 1e-10, 1e-5], 1, 3);\n        let result = t.log();\n        let values = result.ravel().to_vec();\n\n        // Should not panic or return NaN for tiny positive values\n        assert!(!values[0].is_nan(), \"log(1e-20) should not be NaN\");\n        assert!(!values[1].is_nan(), \"log(1e-10) should not be NaN\");\n        assert!(!values[2].is_nan(), \"log(1e-5) should not be NaN\");\n\n        // Verify approximate values\n        assert!((values[0] + 46.0517).abs() \u003c 0.1, \"log(1e-20) â -46.05\");\n    }\n\n    #[test]\n    fn test_sigmoid_numerical_stability() {\n        let t = Tensor2D::\u003cCpuBackend\u003e::new(vec![-1000.0f32, 0.0, 1000.0], 1, 3);\n        let sig = t.sigmoid();\n\n        let values = sig.ravel().to_vec();\n        assert!(values[0] \u003c 1e-15); // â0.0 for large negative\n        assert!((values[1] - 0.5).abs() \u003c 1e-12); // Exactly 0.5 at zero\n        assert!(values[2] \u003e 1.0 - 1e-15); // â1.0 for large positive\n    }\n\n    #[test]\n    fn test_ravel_row_major_order() {\n        // 3x2 matrix: [[1,2],\n        //              [3,4],\n        //              [5,6]]\n        let t = Tensor2D::\u003cCpuBackend\u003e::new(vec![1.0f32, 2.0, 3.0, 4.0, 5.0, 6.0], 3, 2);\n        let flat = t.ravel();\n\n        // Row-major flattening: [1,2,3,4,5,6]\n        assert_eq!(flat.to_vec(), vec![1.0, 2.0, 3.0, 4.0, 5.0, 6.0]);\n    }\n\n    #[test]\n    fn test_empty_tensor_constructors() {\n        // Valid empty tensors (0 rows or 0 cols)\n        let t1 = Tensor2D::\u003cCpuBackend\u003e::zeros(0, 5);\n        assert_eq!(t1.shape(), (0, 5));\n\n        let t2 = Tensor2D::\u003cCpuBackend\u003e::zeros(5, 0);\n        assert_eq!(t2.shape(), (5, 0));\n\n        let t3 = Tensor2D::\u003cCpuBackend\u003e::zeros(0, 0);\n        assert_eq!(t3.shape(), (0, 0));\n    }\n\n    #[test]\n    #[should_panic(expected = \"empty\")]\n    fn test_mean_empty_tensor_panics() {\n        let t = Tensor2D::\u003cCpuBackend\u003e::zeros(0, 0);\n        let _ = t.mean(); // Should panic on empty tensor\n    }\n\n    #[test]\n    fn test_clone_semantics() {\n        let original = Tensor2D::\u003cCpuBackend\u003e::new(vec![1.0f32, 2.0, 3.0, 4.0], 2, 2);\n        let clone = original.clone();\n\n        // Modify clone via backend operations (simulated by creating new tensor)\n        let modified = clone.sub(\u0026Tensor2D::\u003cCpuBackend\u003e::zeros(2, 2));\n\n        // Original should remain unchanged\n        assert_eq!(original.ravel().to_vec(), vec![1.0, 2.0, 3.0, 4.0]);\n        assert_eq!(modified.ravel().to_vec(), vec![1.0, 2.0, 3.0, 4.0]); // Still same values\n    }\n\n    #[test]\n    fn test_dot_and_tdot_consistency() {\n        // Verify that dot and tdot produce transposed results\n        let a = Tensor2D::\u003cCpuBackend\u003e::new(\n            vec![1.0f32, 2.0, 3.0, 4.0, 5.0, 6.0], // 2x3 matrix\n            2,\n            3,\n        );\n        let x = Tensor1D::\u003cCpuBackend\u003e::new(vec![1.0f32, 0.0, 0.0]); // Length 3\n\n        // A @ x = [1*1 + 2*0 + 3*0, 4*1 + 5*0 + 6*0] = [1, 4]\n        let y = a.dot(\u0026x);\n        assert_eq!(y.to_vec(), vec![1.0, 4.0]);\n\n        // Aáµ @ y should reconstruct x scaled by row norms (not exact inverse, but dimensionally correct)\n        let x_recon = a.tdot(\u0026y);\n        assert_eq!(x_recon.len(), 3); // Should be length 3 (columns of original)\n    }\n}\n","traces":[{"line":87,"address":[2020848],"length":1,"stats":{"Line":1}},{"line":89,"address":[2020871],"length":1,"stats":{"Line":1}},{"line":109,"address":[2021312],"length":1,"stats":{"Line":1}},{"line":111,"address":[2021335],"length":1,"stats":{"Line":1}},{"line":132,"address":[2020912],"length":1,"stats":{"Line":1}},{"line":134,"address":[2020935],"length":1,"stats":{"Line":1}},{"line":158,"address":[2020976],"length":1,"stats":{"Line":1}},{"line":160,"address":[2020981],"length":1,"stats":{"Line":2}},{"line":197,"address":[2020576],"length":1,"stats":{"Line":1}},{"line":199,"address":[2020599],"length":1,"stats":{"Line":1}},{"line":237,"address":[2021056],"length":1,"stats":{"Line":1}},{"line":239,"address":[2021079],"length":1,"stats":{"Line":1}},{"line":256,"address":[2020512],"length":1,"stats":{"Line":1}},{"line":258,"address":[2020531],"length":1,"stats":{"Line":1}},{"line":282,"address":[2020992],"length":1,"stats":{"Line":1}},{"line":284,"address":[2021011],"length":1,"stats":{"Line":1}},{"line":305,"address":[2020720],"length":1,"stats":{"Line":1}},{"line":307,"address":[2020725],"length":1,"stats":{"Line":1}},{"line":328,"address":[2021216],"length":1,"stats":{"Line":1}},{"line":330,"address":[2021240],"length":1,"stats":{"Line":1}},{"line":351,"address":[2020432],"length":1,"stats":{"Line":1}},{"line":353,"address":[2020456],"length":1,"stats":{"Line":1}},{"line":376,"address":[2021477,2021376],"length":1,"stats":{"Line":1}},{"line":378,"address":[2021404],"length":1,"stats":{"Line":1}},{"line":403,"address":[2020656],"length":1,"stats":{"Line":1}},{"line":405,"address":[2020675],"length":1,"stats":{"Line":1}},{"line":438,"address":[2020784],"length":1,"stats":{"Line":2}},{"line":440,"address":[2020803],"length":1,"stats":{"Line":1}},{"line":474,"address":[2021504],"length":1,"stats":{"Line":1}},{"line":476,"address":[2021523],"length":1,"stats":{"Line":1}},{"line":497,"address":[2021296],"length":1,"stats":{"Line":1}},{"line":498,"address":[2021301],"length":1,"stats":{"Line":1}},{"line":501,"address":[2021136],"length":1,"stats":{"Line":1}},{"line":503,"address":[2021155],"length":1,"stats":{"Line":1}}],"covered":34,"coverable":34},{"path":["/","home","victor","ML","machinelearne-rs","lib","src","backend","tensorlike.rs"],"content":"use crate::backend::scalar::Scalar;\nuse std::marker::PhantomData;\n\nuse super::tensor1d::Tensor1D;\nuse super::Backend;\n\n/// Trait for tensor-like structures supporting element-wise arithmetic operations.\n///\n/// Provides a unified interface for performing element-wise operations, aggregations,\n/// and scalar transformations over tensor implementations.\n///\n/// # Design Rationale\n/// - All binary operations (`sub`, `add`, `mul`, `div`) are performed **element-wise**.\n/// - Aggregation methods return scalar values wrapped in [`Scalar\u003cB\u003e`].\n/// - Backend-generic: parameterized over `B: Backend` to abstract away concrete\n///   computation backends (CPU, GPU, etc.).\n/// - Enables generic algorithms: users can write functions that work with any tensor\n///   dimensionality implementing this trait (e.g., `Tensor1D`, `Tensor2D`).\n///\n/// # Example\n/// ```rust\n/// # use machinelearne_rs::backend::{Scalar, Backend, tensorlike::TensorLike};\n/// fn mean_squared_error\u003cT: TensorLike\u003cB\u003e, B: Backend\u003e(\n///     pred: \u0026T,\n///     target: \u0026T,\n/// ) -\u003e Scalar\u003cB\u003e {\n///     let diff = pred.sub(target);\n///     let sq = diff.mul(\u0026diff);\n///     sq.mean_all()  // Returns mean of squared errors\n/// }\n/// ```\npub trait TensorLike\u003cB: Backend\u003e {\n    /// Element-wise subtraction: `self - other`.\n    fn sub(\u0026self, other: \u0026Self) -\u003e Self;\n\n    /// Element-wise addition: `self + other`.\n    fn add(\u0026self, other: \u0026Self) -\u003e Self;\n\n    /// Element-wise multiplication: `self * other`.\n    fn mul(\u0026self, other: \u0026Self) -\u003e Self;\n\n    /// Element-wise division: `self / other`.\n    fn div(\u0026self, other: \u0026Self) -\u003e Self;\n\n    /// Computes the arithmetic mean across all elements of the tensor.\n    ///\n    /// Returns a scalar value:\n    /// ```text\n    /// mean = (Î£ x_i) / N\n    /// ```\n    fn mean_all(\u0026self) -\u003e Scalar\u003cB\u003e;\n\n    /// Sums all elements of the tensor.\n    ///\n    /// Returns a scalar value:\n    /// ```text\n    /// sum = Î£ x_i\n    /// ```\n    fn sum(\u0026self) -\u003e Scalar\u003cB\u003e;\n\n    /// Scales all elements of the tensor by a scalar value.\n    ///\n    /// Equivalent to element-wise multiplication: `self * scalar`.\n    fn scale(\u0026self, other: Scalar\u003cB\u003e) -\u003e Self;\n}\n\nimpl\u003cB: Backend\u003e TensorLike\u003cB\u003e for Tensor1D\u003cB\u003e {\n    fn sub(\u0026self, other: \u0026Self) -\u003e Self {\n        Self {\n            data: B::sub_1d(\u0026self.data, \u0026other.data),\n            backend: PhantomData,\n        }\n    }\n\n    fn add(\u0026self, other: \u0026Self) -\u003e Self {\n        Self {\n            data: B::add_1d(\u0026self.data, \u0026other.data),\n            backend: PhantomData,\n        }\n    }\n\n    fn mul(\u0026self, other: \u0026Self) -\u003e Self {\n        Self {\n            data: B::mul_1d(\u0026self.data, \u0026other.data),\n            backend: PhantomData,\n        }\n    }\n\n    fn div(\u0026self, other: \u0026Self) -\u003e Self {\n        Self {\n            data: B::div_1d(\u0026self.data, \u0026other.data),\n            backend: PhantomData,\n        }\n    }\n\n    fn mean_all(\u0026self) -\u003e Scalar\u003cB\u003e {\n        Scalar {\n            data: B::mean_all_1d(\u0026self.data),\n            backend: PhantomData,\n        }\n    }\n\n    fn scale(\u0026self, other: Scalar\u003cB\u003e) -\u003e Self {\n        Self {\n            data: B::mul_scalar_1d(\u0026self.data, \u0026other.data),\n            backend: PhantomData,\n        }\n    }\n\n    fn sum(\u0026self) -\u003e Scalar\u003cB\u003e {\n        Scalar {\n            data: B::sum_all_1d(\u0026self.data),\n            backend: PhantomData,\n        }\n    }\n}\n\n#[cfg(test)]\nmod tests {\n    use super::*;\n    use crate::backend::tensor1d::Tensor1D;\n    use crate::backend::CpuBackend;\n\n    /// Tests basic `TensorLike` operations for one-dimensional tensors.\n    ///\n    /// Verifies correctness of:\n    /// - Element-wise arithmetic operations (sub, add, mul, div)\n    /// - Aggregation methods (mean_all, sum)\n    /// - Scalar multiplication (scale)\n    #[test]\n    fn test_tensorlike_for_tensor1d() {\n        let a: Tensor1D\u003cCpuBackend\u003e = Tensor1D::new(vec![4.0f32, 6.0]);\n        let b: Tensor1D\u003cCpuBackend\u003e = Tensor1D::new(vec![1.0f32, 2.0]);\n\n        // sub: [4-1, 6-2] = [3, 4]\n        let diff = a.sub(\u0026b);\n        assert_eq!(diff.to_vec(), vec![3.0, 4.0]);\n\n        // add: [4+1, 6+2] = [5, 8]\n        let sum = a.add(\u0026b);\n        assert_eq!(sum.to_vec(), vec![5.0, 8.0]);\n\n        // mul: [4*1, 6*2] = [4, 12]\n        let prod = a.mul(\u0026b);\n        assert_eq!(prod.to_vec(), vec![4.0, 12.0]);\n\n        // div: [4/1, 6/2] = [4, 3]\n        let quot = a.div(\u0026b);\n        assert_eq!(quot.to_vec(), vec![4.0, 3.0]);\n\n        // mean_all: (4 + 6) / 2 = 5.0\n        let mean = a.mean_all();\n        assert!((mean.data - 5.0).abs() \u003c 1e-12);\n\n        // sum: 4 + 6 = 10.0\n        let total = a.sum();\n        assert_eq!(total.data, 10.0);\n\n        // scale: [4*0.5, 6*0.5] = [2, 3]\n        let s = Scalar::\u003cCpuBackend\u003e::new(0.5);\n        let scaled = a.scale(\u0026s);\n        assert_eq!(scaled.to_vec(), vec![2.0, 3.0]);\n    }\n\n    /// Tests a generic function operating on any type implementing `TensorLike`.\n    ///\n    /// Demonstrates the key advantage of the trait: writing algorithms agnostic\n    /// to tensor dimensionality or concrete backend implementation.\n    #[test]\n    fn test_generic_function_over_tensorlike() {\n        fn mean_squared_error\u003cT: TensorLike\u003cCpuBackend\u003e\u003e(\n            pred: \u0026T,\n            target: \u0026T,\n        ) -\u003e Scalar\u003cCpuBackend\u003e {\n            let diff = pred.sub(target);\n            let sq = diff.mul(\u0026diff);\n            sq.mean_all()\n        }\n\n        let pred = Tensor1D::\u003cCpuBackend\u003e::new(vec![3.0f32, 5.0]);\n        let target = Tensor1D::\u003cCpuBackend\u003e::new(vec![1.0f32, 2.0]);\n\n        // MSE = ((3-1)Â² + (5-2)Â²) / 2 = (4 + 9) / 2 = 6.5\n        let mse = mean_squared_error(\u0026pred, \u0026target);\n        assert!((mse.data - 6.5).abs() \u003c 1e-12);\n    }\n}\n","traces":[{"line":68,"address":[2546432],"length":1,"stats":{"Line":1}},{"line":70,"address":[2546455],"length":1,"stats":{"Line":1}},{"line":75,"address":[2546192],"length":1,"stats":{"Line":1}},{"line":77,"address":[2546215],"length":1,"stats":{"Line":1}},{"line":82,"address":[2546352],"length":1,"stats":{"Line":2}},{"line":84,"address":[2546375],"length":1,"stats":{"Line":2}},{"line":89,"address":[2546272],"length":1,"stats":{"Line":1}},{"line":91,"address":[2546295],"length":1,"stats":{"Line":1}},{"line":96,"address":[2546528],"length":1,"stats":{"Line":1}},{"line":98,"address":[2546533],"length":1,"stats":{"Line":1}},{"line":103,"address":[],"length":0,"stats":{"Line":0}},{"line":105,"address":[],"length":0,"stats":{"Line":0}},{"line":110,"address":[2546512],"length":1,"stats":{"Line":1}},{"line":112,"address":[2546517],"length":1,"stats":{"Line":1}}],"covered":12,"coverable":14},{"path":["/","home","victor","ML","machinelearne-rs","lib","src","dataset","memory.rs"],"content":"//! In-memory dataset implementation for tabular data.\n//!\n//! Provides [`InMemoryDataset`], a simple dataset backed by `Vec\u003cVec\u003cf32\u003e\u003e` for features\n//! and `Vec\u003cf32\u003e` for targets. Suitable for small-to-medium datasets that fit entirely in RAM.\n//!\n//! # Design Philosophy\n//!\n//! Following the separation of concerns principle (see ADR: *separate-trainer-losses*):\n//! - Datasets are **pure data containers** without training logic or hyperparameters\n//! - Training loops, optimizers, and loss functions live in separate components\n//! - Fitted models contain only inference parameters (`predict` method), not training state\n//!\n//! # Example\n//!\n//! ```rust\n//! # use machinelearne_rs::dataset::memory::InMemoryDataset;\n//! # use machinelearne_rs::backend::CpuBackend;\n//! # use machinelearne_rs::dataset::Dataset;\n//! # fn main() {\n//! // Create dataset: 3 samples Ã 2 features\n//! let x = vec![\n//!     vec![1.0, 0.0],\n//!     vec![0.0, 1.0],\n//!     vec![1.0, 1.0],\n//! ];\n//! let y = vec![1.0, 0.0, 1.0];\n//!\n//! let dataset = InMemoryDataset::new(x, y).unwrap();\n//!\n//! // Iterate over batches\n//! for batch in dataset.batches::\u003cCpuBackend\u003e(2) {\n//!     let (x_batch, y_batch) = batch.unwrap();\n//!     // ... use tensors for training/inference\n//! }\n//! # }\n//! ```\n\nuse crate::backend::{Backend, Tensor1D, Tensor2D};\nuse crate::dataset::Dataset;\nuse std::ops::Range;\n\n/// An in-memory dataset storing tabular data as nested vectors.\n///\n/// Holds features `X` as `Vec\u003cVec\u003cf32\u003e\u003e` (rows Ã features) and targets `y` as `Vec\u003cf32\u003e`.\n/// Validates structural invariants at construction time:\n/// - Equal length of `X` and `y`\n/// - Non-empty dataset\n/// - Uniform feature dimensionality across all samples\n///\n/// # Memory Layout\n///\n/// Data is stored in row-major order (each inner `Vec` is a sample). When converted\n/// to tensors via [`get_batch`], data is flattened into contiguous column-major layout\n/// required by most ML backends.\n///\n/// # Thread Safety\n///\n/// `InMemoryDataset` is `Send + Sync` (via inherent `Vec` properties) and can be\n/// safely shared across threads for read-only access during training.\n#[derive(Debug, Clone)]\npub struct InMemoryDataset {\n    /// Feature matrix: outer vector = samples, inner vector = features per sample.\n    x: Vec\u003cVec\u003cf32\u003e\u003e,\n    /// Target vector: one value per sample.\n    y: Vec\u003cf32\u003e,\n}\n\nimpl InMemoryDataset {\n    /// Constructs a validated in-memory dataset from feature and target vectors.\n    ///\n    /// # Validation\n    ///\n    /// Returns `Err` if any of the following conditions are violated:\n    /// - `x.len() != y.len()` â mismatched sample counts\n    /// - `x.is_empty()` â empty dataset (no samples)\n    /// - Non-uniform feature dimensions â rows in `x` have different lengths\n    ///\n    /// # Parameters\n    ///\n    /// - `x`: Feature matrix where each inner vector represents one sample's features\n    /// - `y`: Target values corresponding to each sample in `x`\n    ///\n    /// # Returns\n    ///\n    /// - `Ok(InMemoryDataset)` â validated dataset ready for training/inference\n    /// - `Err(String)` â descriptive validation error\n    ///\n    /// # Example\n    ///\n    /// ```rust\n    /// use machinelearne_rs::dataset::memory::InMemoryDataset;\n    ///\n    /// // Valid dataset: 2 samples Ã 2 features\n    /// let x = vec![vec![1.0, 2.0], vec![3.0, 4.0]];\n    /// let y = vec![0.0, 1.0];\n    /// let dataset = InMemoryDataset::new(x, y).unwrap();\n    ///\n    /// // Invalid: mismatched lengths\n    /// let x = vec![vec![1.0]];\n    /// let y = vec![0.0, 1.0];\n    /// assert!(InMemoryDataset::new(x, y).is_err());\n    /// ```\n    pub fn new(x: Vec\u003cVec\u003cf32\u003e\u003e, y: Vec\u003cf32\u003e) -\u003e Result\u003cSelf, String\u003e {\n        if x.len() != y.len() {\n            return Err(\"x and y must have same length\".into());\n        }\n        if x.is_empty() {\n            return Err(\"Dataset is empty\".into());\n        }\n        let n_features = x[0].len();\n        if !x.iter().all(|row| row.len() == n_features) {\n            return Err(\"All rows must have the same number of features\".into());\n        }\n        Ok(Self { x, y })\n    }\n}\n\nimpl Dataset for InMemoryDataset {\n    /// Error type for data access operations.\n    ///\n    /// Uses [`Infallible`](std::convert::Infallible) because:\n    /// - Structural validation happens at construction time\n    /// - Range checks are handled by slice indexing (panics on OOB, not recoverable errors)\n    /// - No I/O operations that could fail at runtime\n    type Error = std::convert::Infallible;\n\n    /// Type of a single dataset item: `(feature_vector, target)`.\n    type Item = (Vec\u003cf32\u003e, f32);\n\n    /// Returns the exact number of samples in the dataset.\n    ///\n    /// Always `Some(n)` since in-memory datasets have known size.\n    fn len(\u0026self) -\u003e Option\u003cusize\u003e {\n        Some(self.x.len())\n    }\n\n    /// Loads a contiguous range of samples as backend-specific tensors.\n    ///\n    /// # Parameters\n    ///\n    /// - `range`: Sample index range `[start, end)` (half-open interval)\n    ///\n    /// # Returns\n    ///\n    /// - `Ok((X, y))` where:\n    ///   - `X` is a `(batch_size, n_features)` tensor in column-major layout\n    ///   - `y` is a `(batch_size,)` tensor\n    ///\n    /// # Panics\n    ///\n    /// Panics if `range` is out of bounds (caller should ensure valid ranges via `len()`).\n    /// This is intentional: boundary checks belong to the iterator layer ([`DatasetBatchIter`]),\n    /// not the dataset implementation itself.\n    ///\n    /// # Implementation Notes\n    ///\n    /// - Flattens row-major `Vec\u003cVec\u003cf32\u003e\u003e` into contiguous column-major buffer\n    /// - Preserves backend abstraction: tensors are constructed generically for any `B: Backend`\n    fn get_batch\u003cB: Backend\u003e(\n        \u0026self,\n        range: Range\u003cusize\u003e,\n    ) -\u003e Result\u003c(Tensor2D\u003cB\u003e, Tensor1D\u003cB\u003e), Self::Error\u003e {\n        let batch_x = \u0026self.x[range.clone()];\n        let batch_y = \u0026self.y[range];\n\n        let batch_size = batch_x.len();\n        let n_features = batch_x[0].len();\n\n        let data = batch_x.iter().flat_map(|row| row.iter()).copied().collect();\n        let x_tensor = Tensor2D::\u003cB\u003e::new(data, batch_size, n_features);\n\n        let y_tensor = Tensor1D::\u003cB\u003e::new(batch_y.to_vec());\n\n        Ok((x_tensor, y_tensor))\n    }\n}\n\n#[cfg(test)]\nmod tests {\n    use super::*;\n    use crate::backend::CpuBackend;\n\n    #[test]\n    fn test_in_memory_dataset_new_success() {\n        let x = vec![vec![1.0, 2.0], vec![3.0, 4.0]];\n        let y = vec![0.0, 1.0];\n        let dataset = InMemoryDataset::new(x, y);\n        assert!(dataset.is_ok());\n    }\n\n    #[test]\n    fn test_in_memory_dataset_new_mismatched_lengths() {\n        let x = vec![vec![1.0, 2.0]];\n        let y = vec![0.0, 1.0];\n        let dataset = InMemoryDataset::new(x, y);\n        assert!(dataset.is_err());\n        assert_eq!(dataset.unwrap_err(), \"x and y must have same length\");\n    }\n\n    #[test]\n    fn test_in_memory_dataset_new_empty() {\n        let x = vec![];\n        let y = vec![];\n        let dataset = InMemoryDataset::new(x, y);\n        assert!(dataset.is_err());\n        assert_eq!(dataset.unwrap_err(), \"Dataset is empty\");\n    }\n\n    #[test]\n    fn test_in_memory_dataset_new_uneven_rows() {\n        let x = vec![vec![1.0, 2.0], vec![3.0]]; // second row shorter\n        let y = vec![0.0, 1.0];\n        let dataset = InMemoryDataset::new(x, y);\n        assert!(dataset.is_err());\n        assert_eq!(\n            dataset.unwrap_err(),\n            \"All rows must have the same number of features\"\n        );\n    }\n\n    #[test]\n    fn test_in_memory_dataset_len() {\n        let x = vec![vec![1.0], vec![2.0]];\n        let y = vec![0.0, 1.0];\n        let dataset = InMemoryDataset::new(x, y).unwrap();\n        assert_eq!(dataset.len(), Some(2));\n    }\n\n    #[test]\n    fn test_in_memory_dataset_batches_integration() {\n        let x = vec![vec![1.0, 0.0], vec![0.0, 1.0], vec![1.0, 1.0]];\n        let y = vec![1.0, 0.0, 1.0];\n        let dataset = InMemoryDataset::new(x, y).unwrap();\n\n        let mut batches = dataset.batches::\u003cCpuBackend\u003e(2);\n        let batch1 = batches.next().unwrap().unwrap();\n        assert_eq!(batch1.0.shape(), (2, 2));\n        assert_eq!(batch1.1.to_vec(), vec![1.0, 0.0]);\n\n        let batch2 = batches.next().unwrap().unwrap();\n        assert_eq!(batch2.0.shape(), (1, 2));\n        assert_eq!(batch2.1.to_vec(), vec![1.0]);\n\n        assert!(batches.next().is_none());\n    }\n}\n","traces":[{"line":103,"address":[3083904,3083040],"length":1,"stats":{"Line":2}},{"line":104,"address":[3083075,3083129],"length":1,"stats":{"Line":4}},{"line":105,"address":[3083804,3083172],"length":1,"stats":{"Line":2}},{"line":107,"address":[3083161,3083210],"length":1,"stats":{"Line":6}},{"line":108,"address":[3083244,3083748],"length":1,"stats":{"Line":2}},{"line":110,"address":[3083221,3083286],"length":1,"stats":{"Line":4}},{"line":111,"address":[1838992,1839024],"length":1,"stats":{"Line":5}},{"line":112,"address":[3083423,3083666],"length":1,"stats":{"Line":2}},{"line":114,"address":[3083475],"length":1,"stats":{"Line":1}},{"line":133,"address":[3083008],"length":1,"stats":{"Line":1}},{"line":134,"address":[3083013],"length":1,"stats":{"Line":1}},{"line":159,"address":[1838931,1838368,1838925],"length":1,"stats":{"Line":1}},{"line":163,"address":[1838408],"length":1,"stats":{"Line":1}},{"line":164,"address":[1838469],"length":1,"stats":{"Line":1}},{"line":166,"address":[1838532],"length":1,"stats":{"Line":1}},{"line":167,"address":[1838540,1838709],"length":1,"stats":{"Line":1}},{"line":169,"address":[1838584,1838969,1838944],"length":1,"stats":{"Line":3}},{"line":170,"address":[1838674],"length":1,"stats":{"Line":1}},{"line":172,"address":[1838697,1838782],"length":1,"stats":{"Line":2}},{"line":174,"address":[1838789],"length":1,"stats":{"Line":1}}],"covered":20,"coverable":20},{"path":["/","home","victor","ML","machinelearne-rs","lib","src","dataset","mod.rs"],"content":"//! Dataset abstractions for machine learning workloads.\n//!\n//! This module provides a generic [`Dataset`] trait for uniform access to training data\n//! and a [`DatasetBatchIter`] iterator for efficient batch loading across different backends.\n//!\n//! # Core Concepts\n//!\n//! - **Dataset** â A source of `(X, y)` pairs where `X` is a feature matrix of shape `(n_samples, n_features)`\n//!   and `y` is a target vector of shape `(n_samples,)`.\n//! - **Backend** â Tensor implementation (`CPU`, `CUDA`, etc.) defined by the [`Backend`] trait.\n//! - **Batch** â A contiguous subset of samples for mini-batch gradient descent.\n//!\n//! # Example\n//!\n//! ```rust\n//! use machinelearne_rs::dataset::{Dataset, InMemoryDataset};\n//! use machinelearne_rs::backend::CpuBackend;\n//!\n//! // Create an in-memory dataset: 2 samples Ã 2 features\n//! let x = vec![vec![1.0], vec![2.0]];\n//! let y = vec![0.0, 1.0];\n//! let dataset = InMemoryDataset::new(x, y).unwrap();\n//!\n//! // Iterate over batches of size 1\n//! for batch in dataset.batches::\u003cCpuBackend\u003e(1) {\n//!     let (x_batch, y_batch) = batch.unwrap();\n//!     // ... train your model\n//! }\n//! ```\n\nuse crate::backend::{Backend, Tensor1D, Tensor2D};\nuse std::{fmt::Debug, ops::Range};\n\npub mod memory;\npub use self::memory::InMemoryDataset;\n\n/// Abstract interface for a machine learning dataset.\n///\n/// Defines a contract for loading data in `(X, y)` format where:\n/// - `X` â Feature matrix with shape `(n_samples, n_features)`\n/// - `y` â Target vector with shape `(n_samples,)`\n///\n/// # Associated Types\n///\n/// - `Error` â Error type returned when accessing data (must implement [`Debug`])\n/// - `Item` â Type of a single dataset item (rarely used directly; often `()` as a placeholder)\n///\n/// # Example Implementation\n///\n/// ```rust\n/// use machinelearne_rs::dataset::Dataset;\n/// use machinelearne_rs::backend::{Backend, Tensor1D, Tensor2D};\n/// use std::ops::Range;\n///\n/// struct MyDataset { /* ... */ }\n///\n/// impl Dataset for MyDataset {\n///     type Error = String;\n///     type Item = ();\n///\n///     fn len(\u0026self) -\u003e Option\u003cusize\u003e {\n///         Some(1000) // or None if size is unknown (e.g., streaming data)\n///     }\n///\n///     fn get_batch\u003cB: Backend\u003e(\n///         \u0026self,\n///         range: Range\u003cusize\u003e,\n///     ) -\u003e Result\u003c(Tensor2D\u003cB\u003e, Tensor1D\u003cB\u003e), Self::Error\u003e {\n///         // Implement range-based data loading\n///         # unimplemented!()\n///     }\n/// }\n/// ```\npub trait Dataset {\n    /// Error type returned when accessing data.\n    type Error: Debug + 'static;\n\n    /// Type of a single dataset item (typically unused directly).\n    type Item: ?Sized;\n\n    /// Returns the total number of samples in the dataset, if known.\n    ///\n    /// # Returns\n    ///\n    /// - `Some(n)` â Exact number of samples\n    /// - `None` â Size is unknown (e.g., infinite streams, lazy-loaded sources)\n    ///\n    /// # Example\n    ///\n    /// ```rust\n    /// # use machinelearne_rs::dataset::Dataset;\n    /// # use machinelearne_rs::backend::{Backend, Tensor1D, Tensor2D};\n    /// # struct MyDataset;\n    /// # impl Dataset for MyDataset {\n    /// #     type Error = ();\n    /// #     type Item = ();\n    /// #     fn len(\u0026self) -\u003e Option\u003cusize\u003e { Some(42) }\n    /// #     fn get_batch\u003cB: machinelearne_rs::backend::Backend\u003e(\u0026self, _: std::ops::Range\u003cusize\u003e) -\u003e Result\u003c(Tensor2D\u003cB\u003e, Tensor1D\u003cB\u003e), Self::Error\u003e { unimplemented!() }\n    /// # }\n    /// let ds = MyDataset;\n    /// assert_eq!(ds.len(), Some(42));\n    /// ```\n    fn len(\u0026self) -\u003e Option\u003cusize\u003e;\n\n    /// Checks whether the dataset is empty.\n    ///\n    /// Default implementation checks if `len() == Some(0)`.\n    ///\n    /// # Example\n    ///\n    /// ```rust\n    /// # use machinelearne_rs::dataset::Dataset;\n    /// # use machinelearne_rs::backend::{Backend, Tensor1D, Tensor2D};\n    /// # struct EmptyDataset;\n    /// # impl Dataset for EmptyDataset {\n    /// #     type Error = ();\n    /// #     type Item = ();\n    /// #     fn len(\u0026self) -\u003e Option\u003cusize\u003e { Some(0) }\n    /// #     fn get_batch\u003cB: Backend\u003e(\u0026self, _: std::ops::Range\u003cusize\u003e) -\u003e Result\u003c(Tensor2D\u003cB\u003e, Tensor1D\u003cB\u003e), Self::Error\u003e { unimplemented!() }\n    /// # }\n    /// let ds = EmptyDataset;\n    /// assert!(ds.is_empty());\n    /// ```\n    fn is_empty(\u0026self) -\u003e bool {\n        self.len() == Some(0)\n    }\n\n    /// Creates an iterator over fixed-size batches.\n    ///\n    /// # Parameters\n    ///\n    /// - `batch_size` â Desired batch size (last batch may be smaller)\n    ///\n    /// # Behavior\n    ///\n    /// - Returns batches of size `batch_size`, except possibly the last one\n    /// - Iterator yields `Result` to propagate errors from [`get_batch`]\n    /// - Requires `Sized` bound because the iterator holds a reference to `self`\n    ///\n    /// # Example\n    ///\n    /// ```rust\n    /// # use machinelearne_rs::dataset::{Dataset, InMemoryDataset};\n    /// # use machinelearne_rs::backend::CpuBackend;\n    /// let x = vec![vec![1.0], vec![2.0]];\n    /// let y = vec![0.0, 1.0];\n    /// let ds = InMemoryDataset::new(x, y).unwrap();\n    ///\n    /// let batches: Vec\u003c_\u003e = ds.batches::\u003cCpuBackend\u003e(1).collect();\n    /// assert_eq!(batches.len(), 2); // 2 batches of 1 sample each\n    /// ```\n    fn batches\u003c'a, B: Backend\u003e(\u0026'a self, batch_size: usize) -\u003e DatasetBatchIter\u003c'a, B, Self\u003e\n    where\n        Self: Sized,\n    {\n        DatasetBatchIter {\n            dataset: self,\n            batch_size,\n            current: 0,\n            _backend: std::marker::PhantomData,\n        }\n    }\n\n    /// Loads a subset of data as tensors for the given index range.\n    ///\n    /// # Parameters\n    ///\n    /// - `range` â Sample index range `[start, end)`\n    ///\n    /// # Returns\n    ///\n    /// - `Ok((X, y))` â Feature matrix `(n, n_features)` and target vector `(n,)`\n    /// - `Err(e)` â Data access error (e.g., out-of-bounds access)\n    ///\n    /// # Panics\n    ///\n    /// Does not panic if implemented correctly. Boundary checks are the implementor's responsibility.\n    ///\n    /// # Example\n    ///\n    /// ```rust\n    /// # use machinelearne_rs::dataset::Dataset;\n    /// # use machinelearne_rs::backend::CpuBackend;\n    /// # use machinelearne_rs::backend::{Backend, Tensor1D, Tensor2D};\n    /// # struct MyDataset;\n    /// # impl Dataset for MyDataset {\n    /// #     type Error = String;\n    /// #     type Item = ();\n    /// #     fn len(\u0026self) -\u003e Option\u003cusize\u003e { Some(10) }\n    /// #     fn get_batch\u003cB: Backend\u003e(\u0026self, range: std::ops::Range\u003cusize\u003e) -\u003e Result\u003c(Tensor2D\u003cB\u003e, Tensor1D\u003cB\u003e), Self::Error\u003e {\n    /// #         Ok((Tensor2D::new(vec![0.0; (range.end - range.start) * 2], range.end - range.start, 2), Tensor1D::new(vec![0.0; range.end - range.start])))\n    /// #     }\n    /// # }\n    /// let ds = MyDataset;\n    /// let (x, y) = ds.get_batch::\u003cCpuBackend\u003e(0..5).unwrap();\n    /// assert_eq!(x.shape().0, 5); // 5 samples\n    /// ```\n    fn get_batch\u003cB: Backend\u003e(\n        \u0026self,\n        range: Range\u003cusize\u003e,\n    ) -\u003e Result\u003c(Tensor2D\u003cB\u003e, Tensor1D\u003cB\u003e), Self::Error\u003e;\n}\n\n/// Iterator over dataset batches.\n///\n/// Created by [`Dataset::batches`], yields consecutive batches of fixed size\n/// (last batch may be smaller than requested).\n///\n/// # Type Parameters\n///\n/// - `'a` â Lifetime of the reference to the dataset\n/// - `B` â Backend type for tensors (implements [`Backend`])\n/// - `D` â Dataset type (implements [`Dataset`])\n///\n/// # Characteristics\n///\n/// - **Lazy loading**: Data is fetched only when `next()` is called\n/// - **Error propagation**: Returns `Result` to forward errors from `get_batch`\n/// - **Partial batches**: Last batch may contain fewer samples than `batch_size`\npub struct DatasetBatchIter\u003c'a, B: Backend, D: ?Sized\u003e {\n    /// Reference to the source dataset.\n    dataset: \u0026'a D,\n    /// Desired batch size (actual size may be smaller for the last batch).\n    batch_size: usize,\n    /// Current position in the dataset (index of next sample to yield).\n    current: usize,\n    /// Phantom marker for the backend type parameter.\n    _backend: std::marker::PhantomData\u003cB\u003e,\n}\n\nimpl\u003c'a, B: Backend, D: Dataset\u003e Iterator for DatasetBatchIter\u003c'a, B, D\u003e {\n    /// Iterator item type: `Result` containing tensor pair or dataset error.\n    type Item = Result\u003c(Tensor2D\u003cB\u003e, Tensor1D\u003cB\u003e), D::Error\u003e;\n\n    /// Returns the next batch or `None` if all samples have been consumed.\n    ///\n    /// # Algorithm\n    ///\n    /// 1. Checks dataset size via `len()`; returns `None` if unknown\n    /// 2. Returns `None` if `current \u003e= total_samples`\n    /// 3. Computes range `[current, min(current + batch_size, total))`\n    /// 4. Calls `dataset.get_batch(range)` and returns the result wrapped in `Some`\n    ///\n    /// # Error Handling\n    ///\n    /// Errors from `get_batch` are propagated as `Some(Err(e))`. The iterator\n    /// terminates only when all samples are consumed (`None`).\n    fn next(\u0026mut self) -\u003e Option\u003cSelf::Item\u003e {\n        let total = self.dataset.len()?;\n        if self.current \u003e= total {\n            return None;\n        }\n\n        let end = (self.current + self.batch_size).min(total);\n        let range = self.current..end;\n        self.current = end;\n\n        // Fetch subset and convert to tensors\n        match self.dataset.get_batch::\u003cB\u003e(range) {\n            Ok((x, y)) =\u003e Some(Ok((x, y))),\n            Err(e) =\u003e Some(Err(e)),\n        }\n    }\n}\n\n#[cfg(test)]\nmod tests {\n    use super::*;\n    use crate::backend::CpuBackend;\n    use std::ops::Range;\n\n    // Mock dataset for iterator logic testing\n    struct MockDataset {\n        len: usize,\n    }\n\n    impl Dataset for MockDataset {\n        type Error = \u0026'static str;\n        type Item = ();\n\n        fn len(\u0026self) -\u003e Option\u003cusize\u003e {\n            Some(self.len)\n        }\n\n        fn get_batch\u003cB: Backend\u003e(\n            \u0026self,\n            range: Range\u003cusize\u003e,\n        ) -\u003e Result\u003c(Tensor2D\u003cB\u003e, Tensor1D\u003cB\u003e), Self::Error\u003e {\n            if range.start \u003e= self.len || range.end \u003e self.len {\n                return Err(\"range out of bounds\");\n            }\n\n            let n = range.len();\n            let start = range.start;\n\n            // X: (n, 2) â unique values per sample: [start*2, start*2+1, ...]\n            let x_data: Vec\u003cf32\u003e = (0..n * 2).map(|i| (start * 2 + i) as f32).collect();\n            let x = Tensor2D::\u003cB\u003e::new(x_data, n, 2);\n\n            // y: (n,) â sequential values starting from `start`\n            let y_data: Vec\u003cf32\u003e = (start..range.end).map(|i| i as f32).collect();\n            let y = Tensor1D::\u003cB\u003e::new(y_data);\n\n            Ok((x, y))\n        }\n    }\n\n    #[test]\n    fn test_dataset_is_empty() {\n        let empty = MockDataset { len: 0 };\n        assert!(empty.is_empty());\n\n        let non_empty = MockDataset { len: 1 };\n        assert!(!non_empty.is_empty());\n    }\n\n    #[test]\n    fn test_batches_full() {\n        let dataset = MockDataset { len: 6 };\n        let mut iter = dataset.batches::\u003cCpuBackend\u003e(2);\n\n        // Should yield 3 full batches\n        for i in 0..3 {\n            let batch = iter.next().unwrap().unwrap();\n            let (x, y) = batch;\n            assert_eq!(x.shape(), (2, 2));\n            assert_eq!(y.to_vec(), vec![i as f64 * 2.0, i as f64 * 2.0 + 1.0]);\n        }\n        assert!(iter.next().is_none());\n    }\n\n    #[test]\n    fn test_batches_partial_last() {\n        let dataset = MockDataset { len: 5 };\n        let mut iter = dataset.batches::\u003cCpuBackend\u003e(2);\n\n        // 2 full batches + 1 partial batch\n        assert_eq!(iter.next().unwrap().unwrap().0.shape(), (2, 2));\n        assert_eq!(iter.next().unwrap().unwrap().0.shape(), (2, 2));\n        assert_eq!(iter.next().unwrap().unwrap().0.shape(), (1, 2));\n        assert!(iter.next().is_none());\n    }\n\n    #[test]\n    fn test_batches_larger_than_dataset() {\n        let dataset = MockDataset { len: 3 };\n        let mut iter = dataset.batches::\u003cCpuBackend\u003e(10);\n\n        // Single batch covering entire dataset\n        let batch = iter.next().unwrap().unwrap();\n        assert_eq!(batch.0.shape(), (3, 2));\n        assert!(iter.next().is_none());\n    }\n\n    #[test]\n    fn test_batches_empty_dataset() {\n        let dataset = MockDataset { len: 0 };\n        let mut iter = dataset.batches::\u003cCpuBackend\u003e(2);\n        assert!(iter.next().is_none());\n    }\n}\n","traces":[{"line":124,"address":[1839088],"length":1,"stats":{"Line":1}},{"line":125,"address":[1839097],"length":1,"stats":{"Line":1}},{"line":152,"address":[1839056],"length":1,"stats":{"Line":2}},{"line":248,"address":[2769376,2768928,2768320],"length":1,"stats":{"Line":3}},{"line":249,"address":[2769406,2768350,2768958],"length":1,"stats":{"Line":2}},{"line":250,"address":[2769485,2768429,2769037],"length":1,"stats":{"Line":4}},{"line":251,"address":[2769528,2768471,2769075],"length":1,"stats":{"Line":2}},{"line":254,"address":[2769048,2769102,2769496,2769556,2768440,2769163,2768498,2768593,2769654],"length":1,"stats":{"Line":5}},{"line":255,"address":[2768520,2769578,2769124],"length":1,"stats":{"Line":2}},{"line":256,"address":[2769144,2769598,2768540],"length":1,"stats":{"Line":3}},{"line":259,"address":[2769148,2768544,2769602],"length":1,"stats":{"Line":2}},{"line":260,"address":[2769176,2768694,2769800],"length":1,"stats":{"Line":3}},{"line":261,"address":[2768611,2769672],"length":1,"stats":{"Line":0}}],"covered":12,"coverable":13},{"path":["/","home","victor","ML","machinelearne-rs","lib","src","lib.rs"],"content":"//! # rust-ml\n//!\n//! A type-safe machine learning library in Rust with pluggable backends and strict\n//! separation between training and inference phases.\n//!\n//! ## Core Design Principles\n//!\n//! - **Stateful Type Safety**: Models carry their training state in the type system\n//!   (`Unfitted` vs `Fitted`), preventing invalid operations at compile time.\n//! - **Training/Inference Separation**: Trained models contain only prediction parameters;\n//!   training logic lives in separate components (losses, optimizers, trainers).\n//! - **Backend Agnosticism**: Abstract `Backend` trait enables CPU/GPU implementations\n//!   without changing model code.\n//! - **Zero-Cost Abstractions**: Generics and traits provide flexibility without runtime overhead.\n//!\n//! ## Quick Start\n//!\n//! ```rust\n//! use machinelearne_rs::backend::CpuBackend;\n//! use machinelearne_rs::model::linear::{LinearModel, Unfitted};\n//! use machinelearne_rs::loss::MSELoss;\n//! use machinelearne_rs::optimizer::SGD;\n//!\n//! // Create an untrained linear model (1 input feature)\n//! let mut model = LinearModel::\u003cCpuBackend, Unfitted\u003e::new(1);\n//!\n//! // Training loop (simplified)\n//! // for epoch in 0..100 {\n//! //     let pred = model.forward(\u0026x_tensor);\n//! //     let grad = MSELoss::grad_wrt_prediction(\u0026pred, \u0026y_tensor);\n//! //     let grads = model.backward(\u0026x_tensor, \u0026grad);\n//! //     let new_params = SGD::new(0.01).step(model.params(), \u0026grads);\n//! //     model.update_params(\u0026new_params);\n//! // }\n//!\n//! // Convert to inference-optimized model\n//! // let fitted = model.into_fitted();\n//! // let prediction = fitted.predict(\u0026input_tensor);\n//! ```\n//!\n//! ## Module Structure\n//!\n//! - `backend` â Tensor abstractions and computation primitives (`Tensor1D`, `Tensor2D`)\n//! - `model` â ML model implementations with stateful type parameters\n//! - `loss` â Differentiable loss functions (MSE, CrossEntropy, etc.)\n//! - `optimizer` â Parameter update algorithms (SGD, Adam)\n//! - `trainer` â High-level training loop orchestration\n//! - `regularizers` â Weight regularization strategies (L1, L2)\n//! - `dataset` â Data loading and preprocessing utilities\n//! - `serialization` â Model persistence formats\n//!\n//! ## Example Projects\n//!\n//! See the `examples/` directory for complete training pipelines demonstrating:\n//! - Linear regression with SGD\n//! - Regularization techniques\n//! - Custom backend integration\n//! - Model serialization workflows\n\npub mod backend;\n\n/// Data loading utilities and dataset abstractions.\npub mod dataset;\n\n/// Data preprocessing transformers for ML pipelines.\npub mod preprocessing;\n\n/// Differentiable loss functions for model training.\npub mod loss;\n\n/// Machine learning models with compile-time state safety.\npub mod model;\n\n/// Optimization algorithms for parameter updates.\npub mod optimizer;\n\n/// Weight regularization strategies to prevent overfitting.\npub mod regularizers;\n\n/// Model persistence and format conversion utilities.\npub mod serialization;\n\n/// High-level training loop orchestration.\npub mod trainer;\n\n/// Re-export of core backend types for convenient usage.\npub use backend::{Backend, CpuBackend, ScalarOps, Tensor1D, Tensor2D};\n\n#[cfg(test)]\nmod tests {\n    use super::*;\n    use crate::backend::CpuBackend;\n    use crate::loss::{Loss, MSELoss};\n    use crate::model::linear::{InferenceModel, LinearModel, TrainableModel, Unfitted};\n    use crate::optimizer::{Optimizer, SGD};\n\n    // Helper function to create (n, 1) matrix from column data\n    fn col_to_tensor2d\u003cB: Backend\u003e(col: \u0026[f32]) -\u003e Tensor2D\u003cB\u003e {\n        let n = col.len();\n        let mut data = vec![0.0; n];\n        data.copy_from_slice(col);\n        Tensor2D::\u003cB\u003e::new(data, n, 1)\n    }\n\n    fn slice_to_tensor1d\u003cB: Backend\u003e(slice: \u0026[f32]) -\u003e Tensor1D\u003cB\u003e {\n        Tensor1D::\u003cB\u003e::new(slice.to_vec())\n    }\n\n    #[test]\n    fn test_linear_regression_identity() {\n        // y = x\n        let x_data: \u0026[f32; 4] = \u0026[1.0, 2.0, 3.0, 4.0];\n        let y_data: \u0026[f32; 4] = \u0026[1.0, 2.0, 3.0, 4.0];\n\n        let x_tensor = col_to_tensor2d(x_data);\n        let y_tensor = slice_to_tensor1d(y_data);\n\n        let mut model = LinearModel::\u003cCpuBackend, Unfitted\u003e::new(1);\n        let loss_fn = MSELoss;\n        let optimizer = SGD::new(0.01);\n\n        for epoch in 0..200 {\n            let pred = model.forward(\u0026x_tensor);\n            let grad_pred = Loss::\u003cCpuBackend\u003e::grad_wrt_prediction(\u0026loss_fn, \u0026pred, \u0026y_tensor);\n            let grads = model.backward(\u0026x_tensor, \u0026grad_pred);\n            let new_params = optimizer.step(\u0026model.params(), \u0026grads);\n            model.update_params(\u0026new_params);\n            if epoch % 5 == 0 {\n                let loss_val = Loss::\u003cCpuBackend\u003e::loss(\u0026loss_fn, \u0026pred, \u0026y_tensor);\n                let w = \u0026model.params().weights.to_vec()[0];\n                let b = model.params().bias.data.to_f64();\n                println!(\n                    \"Epoch {}: loss={:.6}, w={:.4}, b={:.4}, pred={:?}\",\n                    epoch,\n                    loss_val.data.to_f64(),\n                    w,\n                    b,\n                    pred.to_vec()\n                );\n            }\n        }\n\n        let fitted = model.into_fitted();\n        let inp = slice_to_tensor1d::\u003cCpuBackend\u003e(\u0026[2.5]);\n        let pred = fitted.predict(\u0026inp).data.to_f64();\n        assert!((pred - 2.5).abs() \u003c 0.1, \"Expected ~2.5, got {}\", pred);\n    }\n\n    #[test]\n    fn test_linear_regression_with_bias() {\n        // y = 2*x + 1\n        let x_data = \u0026[0.0, 1.0, 2.0, 3.0];\n        let y_data = \u0026[1.0, 3.0, 5.0, 7.0];\n\n        let x_tensor = col_to_tensor2d(x_data);\n        let y_tensor = slice_to_tensor1d(y_data);\n\n        let mut model = LinearModel::\u003cCpuBackend, Unfitted\u003e::new(1);\n\n        let loss_fn = MSELoss;\n        let optimizer = SGD::new(0.01);\n\n        for _ in 0..3000 {\n            let pred = model.forward(\u0026x_tensor);\n            let grad_pred = Loss::\u003cCpuBackend\u003e::grad_wrt_prediction(\u0026loss_fn, \u0026pred, \u0026y_tensor);\n            let grads = model.backward(\u0026x_tensor, \u0026grad_pred);\n            let new_params = optimizer.step(model.params(), \u0026grads);\n            model.update_params(\u0026new_params);\n        }\n\n        let fitted = model.into_fitted();\n        let p0 = fitted.predict(\u0026slice_to_tensor1d::\u003cCpuBackend\u003e(\u0026[0.0]));\n        let p1 = fitted.predict(\u0026slice_to_tensor1d::\u003cCpuBackend\u003e(\u0026[1.0]));\n        let p3 = fitted.predict(\u0026slice_to_tensor1d::\u003cCpuBackend\u003e(\u0026[3.0]));\n\n        assert!(\n            (p0.data.to_f64() - 1.0).abs() \u003c 0.2,\n            \"p0 = {}\",\n            p0.data.to_f64()\n        );\n        assert!(\n            (p1.data.to_f64() - 3.0).abs() \u003c 0.2,\n            \"p1 = {}\",\n            p1.data.to_f64()\n        );\n        assert!(\n            (p3.data.to_f64() - 7.0).abs() \u003c 0.3,\n            \"p3 = {}\",\n            p3.data.to_f64()\n        );\n    }\n}\n","traces":[],"covered":0,"coverable":0},{"path":["/","home","victor","ML","machinelearne-rs","lib","src","loss","mod.rs"],"content":"pub use crate::backend::scalar::{Scalar, ScalarOps};\npub use crate::backend::tensor1d::Tensor1D;\npub use crate::backend::tensorlike::TensorLike;\npub use crate::backend::Backend;\n\npub use crate::model::linear::{LinearModel, LinearParams, Unfitted};\npub use crate::model::TrainableModel;\n\n/// A trait for differentiable loss functions used during model training.\n///\n/// Implementors must define:\n/// - How to compute the scalar loss value (for logging/metrics).\n/// - How to compute the gradient of the loss w.r.t. the model's predictions.\n///\n/// This gradient is passed to the model's `backward()` method to update parameters.\npub trait Loss\u003cB: Backend\u003e {\n    type Prediction: TensorLike\u003cB\u003e;\n    type Target: TensorLike\u003cB\u003e;\n\n    /// Computes the scalar loss value (for logging/metrics).\n    fn loss(\u0026self, prediction: \u0026Self::Prediction, target: \u0026Self::Target) -\u003e Scalar\u003cB\u003e;\n\n    /// Computes the gradient of the loss w.r.t. the prediction: âL/âpred.\n    /// This is what gets passed to `model.backward()`.\n    fn grad_wrt_prediction(\n        \u0026self,\n        prediction: \u0026Self::Prediction,\n        target: \u0026Self::Target,\n    ) -\u003e Self::Prediction;\n}\n/// Mean Squared Error (MSE) loss: `L = (1/n) * Î£(pred_i - target_i)^2`\n///\n/// Gradient w.r.t. prediction: `âL/âpred = (pred - target) / n`\n///\n/// Note: The factor of 2 is omitted, as it can be absorbed into the learning rate.\npub struct MSELoss;\n\nimpl\u003cB: Backend\u003e Loss\u003cB\u003e for MSELoss\nwhere\n    B::Tensor1D: Clone,\n{\n    type Prediction = Tensor1D\u003cB\u003e;\n    type Target = Tensor1D\u003cB\u003e;\n\n    fn loss(\u0026self, pred: \u0026Self::Prediction, target: \u0026Self::Target) -\u003e Scalar\u003cB\u003e {\n        let diff = pred.sub(target);\n        diff.dot(\u0026diff) / Scalar::\u003cB\u003e::new(B::len_1d(\u0026diff.data) as f64)\n    }\n\n    fn grad_wrt_prediction(\u0026self, pred: \u0026Self::Prediction, target: \u0026Self::Target) -\u003e Tensor1D\u003cB\u003e {\n        // d/dp ( (p - y)^2 ) = 2(p - y)\n        // But commonly: MSE = (1/n) * sum(...), so grad = (2/n)(p - y)\n        // However, in practice, we often omit 2 and let LR absorb it.\n        // We'll return (pred - target) â standard in many frameworks.\n        let diff = pred.sub(target);\n        let n = Scalar::\u003cB\u003e::new(1. / pred.len() as f64); // or use backend method\n        diff.scale(\u0026n)\n    }\n}\n\n/// Mean Absolute Error (MAE) loss: `L = (1/n) * Î£|pred_i - target_i|`\n///\n/// Gradient w.r.t. prediction: `âL/âpred = sign(pred - target) / n`\n/// (subgradient is used at zero).\npub struct MAELoss;\n\nimpl\u003cB: Backend\u003e Loss\u003cB\u003e for MAELoss\nwhere\n    B::Tensor1D: Clone,\n{\n    type Prediction = Tensor1D\u003cB\u003e;\n    type Target = Tensor1D\u003cB\u003e;\n\n    fn loss(\u0026self, pred: \u0026Self::Prediction, target: \u0026Self::Target) -\u003e Scalar\u003cB\u003e {\n        pred.sub(target).abs().mean()\n    }\n\n    fn grad_wrt_prediction(\u0026self, pred: \u0026Self::Prediction, target: \u0026Self::Target) -\u003e Tensor1D\u003cB\u003e {\n        let diff = pred.sub(target);\n        let sign = diff.sign();\n        let n = Scalar::\u003cB\u003e::new(1.0 / pred.len() as f64);\n        sign.scale(\u0026n)\n    }\n}\n/// Binary Cross-Entropy loss with logits input (numerically stable).\n///\n/// Computes: `L = -(t * log(Ï(z)) + (1-t) * log(1 - Ï(z)))`\n/// using the stable formulation: `max(z,0) - z*t + log(1 + exp(-|z|))`\n///\n/// Gradient w.r.t. logits: `âL/âz = (Ï(z) - t) / n`\npub struct BCEWithLogitsLoss;\n\nimpl\u003cB: Backend\u003e Loss\u003cB\u003e for BCEWithLogitsLoss\nwhere\n    B::Tensor1D: Clone,\n{\n    type Prediction = Tensor1D\u003cB\u003e;\n    type Target = Tensor1D\u003cB\u003e;\n\n    fn loss(\u0026self, logits: \u0026Self::Prediction, targets: \u0026Self::Target) -\u003e Scalar\u003cB\u003e {\n        // Numerically stable BCE: -(t * log(s(z)) + (1-t) * log(1 - s(z)))\n        // = max(z, 0) - z * t + log(1 + exp(-|z|))\n        let max_logits = logits.maximum(Self::Prediction::zeros(logits.len()));\n        let term2 = logits\n            .abs()\n            .scale(\u0026Scalar::\u003cB\u003e::new(-1.))\n            .exp()\n            .add_scalar(\u0026Scalar::\u003cB\u003e::new(1.))\n            .log();\n\n        let term1 = max_logits.sub(\u0026logits.mul(targets));\n        let total = term1.add(\u0026term2);\n        total.mean()\n    }\n\n    fn grad_wrt_prediction(\n        \u0026self,\n        logits: \u0026Self::Prediction,\n        targets: \u0026Self::Target,\n    ) -\u003e Self::Prediction {\n        // d/dz BCE = sigmoid(z) - t\n        let n = Scalar::\u003cB\u003e::new(1.0 / logits.len() as f64);\n        logits.sigmoid().sub(targets).scale(\u0026n)\n    }\n}\n\n#[cfg(test)]\nmod tests {\n    use super::*;\n    use crate::backend::CpuBackend;\n\n    #[test]\n    fn test_mse_loss() {\n        let pred = Tensor1D::\u003cCpuBackend\u003e::new(vec![3.0f32, 5.0]);\n        let target = Tensor1D::\u003cCpuBackend\u003e::new(vec![1.0f32, 2.0]);\n\n        let mse = MSELoss;\n        let loss_val = mse.loss(\u0026pred, \u0026target);\n        // ((3-1)^2 + (5-2)^2) / 2 = (4 + 9) / 2 = 6.5\n        assert!((loss_val.data - 6.5).abs() \u003c 1e-12);\n\n        let grad = mse.grad_wrt_prediction(\u0026pred, \u0026target);\n        // grad = (pred - target) / n = [2.0, 3.0] / 2 = [1.0, 1.5]\n        let expected_grad = vec![1.0, 1.5];\n        assert_eq!(grad.to_vec(), expected_grad);\n    }\n\n    #[test]\n    fn test_mae_loss() {\n        let pred = Tensor1D::\u003cCpuBackend\u003e::new(vec![3.0f32, -1.0]);\n        let target = Tensor1D::\u003cCpuBackend\u003e::new(vec![1.0f32, 2.0]);\n\n        let mae = MAELoss;\n        let loss_val = mae.loss(\u0026pred, \u0026target);\n        // (|3-1| + |-1-2|) / 2 = (2 + 3) / 2 = 2.5\n        assert!((loss_val.data - 2.5).abs() \u003c 1e-12);\n\n        let grad = mae.grad_wrt_prediction(\u0026pred, \u0026target);\n        // sign(pred - target) / n = [1.0, -1.0] / 2 = [0.5, -0.5]\n        let expected_grad = vec![0.5, -0.5];\n        assert_eq!(grad.to_vec(), expected_grad);\n    }\n\n    #[test]\n    fn test_bce_with_logits_loss() {\n        let logits = Tensor1D::\u003cCpuBackend\u003e::new(vec![0.0f32, 2.0, -2.0]); // sigmoid: [0.5, ~0.88, ~0.12]\n        let targets = Tensor1D::\u003cCpuBackend\u003e::new(vec![1.0f32, 1.0, 0.0]);\n\n        let bce = BCEWithLogitsLoss;\n        let loss_val = bce.loss(\u0026logits, \u0026targets);\n\n        // Expected (computed manually or via PyTorch):\n        // For z=0, t=1: -(0 + log(1+1)) = -log(2) â -0.6931 â but formula gives: max(0,0) - 0*1 + log(1+1) = 0 + 0 + log(2) â 0.6931\n        // For z=2, t=1: max(2,0) - 2*1 + log(1+exp(-2)) = 2 - 2 + log(1+0.135) â 0.127\n        // For z=-2, t=0: max(-2,0)=0 - (-2)*0 + log(1+exp(-2)) = 0 + 0 + log(1.135) â 0.127\n        // Mean â (0.6931 + 0.127 + 0.127) / 3 â 0.3156\n        let expected_loss = 0.3156;\n        assert!((loss_val.data - expected_loss).abs() \u003c 1e-3);\n\n        let grad = bce.grad_wrt_prediction(\u0026logits, \u0026targets);\n        // grad = (sigmoid(z) - t) / n\n        let sig = vec![\n            0.5,\n            1.0 / (1.0 + (-2.0f64).exp()),\n            1.0 / (1.0 + (2.0f64).exp()),\n        ];\n        let expected_grad: Vec\u003cf64\u003e = sig\n            .iter()\n            .zip(targets.to_vec().iter())\n            .map(|(s, t)| (s - t) / 3.0)\n            .collect();\n        for (g, e) in grad.to_vec().iter().zip(expected_grad.iter()) {\n            assert!((g - e).abs() \u003c 1e-5);\n        }\n    }\n\n    #[test]\n    fn test_bce_numerical_stability() {\n        // Large positive and negative logits\n        let logits = Tensor1D::\u003cCpuBackend\u003e::new(vec![100.0f32, -100.0]);\n        let targets = Tensor1D::\u003cCpuBackend\u003e::new(vec![1.0f32, 0.0]);\n\n        let bce = BCEWithLogitsLoss;\n        let loss_val = bce.loss(\u0026logits, \u0026targets);\n        // Should not overflow or produce NaN\n        assert!(loss_val.data.is_finite());\n\n        let grad = bce.grad_wrt_prediction(\u0026logits, \u0026targets);\n        assert!(grad.to_vec().iter().all(|\u0026x| x.is_finite()));\n    }\n}\n","traces":[{"line":45,"address":[2365408,2365612,2365618],"length":1,"stats":{"Line":1}},{"line":46,"address":[2365437],"length":1,"stats":{"Line":1}},{"line":47,"address":[2365447,2365499],"length":1,"stats":{"Line":2}},{"line":50,"address":[2365152,2365386,2365392],"length":1,"stats":{"Line":1}},{"line":55,"address":[2365212],"length":1,"stats":{"Line":1}},{"line":56,"address":[2365222,2365270],"length":1,"stats":{"Line":2}},{"line":57,"address":[2365359],"length":1,"stats":{"Line":1}},{"line":74,"address":[2365138,2364976,2365132],"length":1,"stats":{"Line":1}},{"line":75,"address":[2365005,2365060],"length":1,"stats":{"Line":2}},{"line":78,"address":[2364957,2364963,2364640],"length":1,"stats":{"Line":1}},{"line":79,"address":[2364711],"length":1,"stats":{"Line":1}},{"line":80,"address":[2364726],"length":1,"stats":{"Line":1}},{"line":81,"address":[2364826,2364775],"length":1,"stats":{"Line":2}},{"line":82,"address":[2364915],"length":1,"stats":{"Line":1}},{"line":100,"address":[2365936,2366926,2366920],"length":1,"stats":{"Line":2}},{"line":103,"address":[2365990],"length":1,"stats":{"Line":2}},{"line":104,"address":[],"length":0,"stats":{"Line":0}},{"line":106,"address":[2366098,2366154],"length":1,"stats":{"Line":4}},{"line":108,"address":[2366318,2366262],"length":1,"stats":{"Line":4}},{"line":111,"address":[2366641],"length":1,"stats":{"Line":2}},{"line":112,"address":[2366789],"length":1,"stats":{"Line":2}},{"line":113,"address":[2366804],"length":1,"stats":{"Line":2}},{"line":116,"address":[2365914,2365908,2365632],"length":1,"stats":{"Line":2}},{"line":122,"address":[2365681],"length":1,"stats":{"Line":2}},{"line":123,"address":[2365835,2365763],"length":1,"stats":{"Line":4}}],"covered":24,"coverable":25},{"path":["/","home","victor","ML","machinelearne-rs","lib","src","model","linear.rs"],"content":"//! Linear models for regression and classification.\n//!\n//! This module implements a type-safe linear model with compile-time state tracking:\n//! - [`LinearRegression`] = `LinearModel\u003cUnfitted\u003e` â used during training.\n//! - [`LinearModel\u003cFitted\u003e`] â inference-only, serializable predictor.\n//!\n//! The design follows [ADR-0001: separate-trainer-losses](https://github.com/vzaguskin/machinelearne-rs/issues/1):\n//! \u003e *\"Fitted model is free from training hyperparameters.\"*\n//!\n//! Supports L1/L2 regularization via loss functions and works with any backend implementing [`Backend`].\n//!\npub use crate::backend::scalar::{Scalar, ScalarOps};\npub use crate::backend::tensor1d::Tensor1D;\npub use crate::backend::tensor2d::Tensor2D;\npub use crate::backend::Backend;\nuse crate::loss::TensorLike;\npub use crate::model::{Fitted, InferenceModel, ParamOps, TrainableModel, Unfitted};\nuse std::marker::PhantomData;\n\n/// Trainable parameters of a linear model: weights and bias.\n///\n/// Used internally by both [`TrainableModel`] and [`InferenceModel`] implementations.\n/// Implements [`ParamOps`] to support optimizer updates (e.g., SGD).\n#[derive(Clone)]\npub struct LinearParams\u003cB: Backend\u003e\nwhere\n    Tensor1D\u003cB\u003e: Clone,\n    Scalar\u003cB\u003e: Clone,\n{\n    pub weights: Tensor1D\u003cB\u003e,\n    pub bias: Scalar\u003cB\u003e,\n}\n\n/// Serializable representation of linear model parameters.\n///\n/// Converts internal backend-specific tensors into plain `Vec\u003cf32\u003e` for storage.\n/// Used by [`InferenceModel::save_to_file`] and [`InferenceModel::load_from_file`].\n///\n/// â ï¸ Currently uses `f32` for compactness\n#[cfg(feature = \"serde\")]\n#[derive(serde::Serialize, serde::Deserialize)]\npub struct SerializableLinearParams {\n    pub weights: Vec\u003cf32\u003e,\n    pub bias: f32,\n}\n\nimpl\u003cB: Backend\u003e From\u003c\u0026LinearParams\u003cB\u003e\u003e for SerializableLinearParams {\n    fn from(params: \u0026LinearParams\u003cB\u003e) -\u003e Self {\n        // ÐÑÐµÐ´Ð¿Ð¾Ð»Ð°Ð³Ð°ÐµÐ¼, ÑÑÐ¾ ÑÑÐ¸ Ð¼ÐµÑÐ¾Ð´Ñ ÑÑÑÐµÑÑÐ²ÑÑÑ Ð² Ð²Ð°ÑÐµÐ¼ Ð±ÑÐºÐµÐ½Ð´Ðµ\n        let weights = params\n            .weights\n            .to_vec()\n            .into_iter()\n            .map(|x| x as f32)\n            .collect(); // Vec\u003cf32\u003e\n        let bias = params.bias.data.to_f64() as f32; // f32\n        Self { weights, bias }\n    }\n}\n\nimpl\u003cB: Backend\u003e TryFrom\u003cSerializableLinearParams\u003e for LinearParams\u003cB\u003e {\n    type Error = Box\u003cdyn std::error::Error\u003e;\n\n    fn try_from(value: SerializableLinearParams) -\u003e Result\u003cSelf, Self::Error\u003e {\n        let weights = Tensor1D::\u003cB\u003e::new(value.weights);\n        let bias = Scalar::\u003cB\u003e::new(value.bias as f64);\n        Ok(Self { weights, bias })\n    }\n}\n\nimpl\u003cB\u003e ParamOps\u003cB\u003e for LinearParams\u003cB\u003e\nwhere\n    B: Backend,\n{\n    fn add(\u0026self, other: \u0026Self) -\u003e Self {\n        let w = self.weights.add(\u0026other.weights);\n        let b = self.bias + other.bias;\n        Self {\n            weights: w,\n            bias: b,\n        }\n    }\n    fn scale(\u0026self, scalar: Scalar\u003cB\u003e) -\u003e Self {\n        let w = self.weights.scale(\u0026scalar);\n        let b = self.bias * scalar;\n        Self {\n            weights: w,\n            bias: b,\n        }\n    }\n}\n\n/// A linear model with state encoded at the type level.\n///\n/// - When `S = Unfitted`: implements [`TrainableModel`] â used during training.\n/// - When `S = Fitted`: implements [`InferenceModel`] â used for prediction and serialization.\n///\n/// This enforces, at compile time, that you cannot call `predict()` on an untrained model.\npub struct LinearModel\u003cB: Backend, S\u003e {\n    params: LinearParams\u003cB\u003e,\n    _state: std::marker::PhantomData\u003cS\u003e,\n}\n\nimpl\u003cB: Backend\u003e LinearModel\u003cB, Fitted\u003e {\n    /// Creates a new fitted linear model from trained parameters.\n    ///\n    /// Typically called internally by [`TrainableModel::into_fitted`].\n    /// Useful for manual model construction or loading from external sources.\n    pub fn new(params: LinearParams\u003cB\u003e) -\u003e Self {\n        Self {\n            params,\n            _state: std::marker::PhantomData::\u003cFitted\u003e,\n        }\n    }\n}\n\n/// Implements inference for a trained linear model: `y = w^T x + b`.\n///\n/// - Single-sample input: [`Tensor1D\u003cB\u003e`] â output: [`Scalar\u003cB\u003e`]\n/// - Batch input: [`Tensor2D\u003cB\u003e`] â output: [`Tensor1D\u003cB\u003e`]\n///\n/// Serialization uses [`SerializableLinearParams`] (see `save_to_file`/`load_from_file`).\nimpl\u003cB: Backend\u003e InferenceModel\u003cB\u003e for LinearModel\u003cB, Fitted\u003e {\n    type InputSingle = Tensor1D\u003cB\u003e;\n    type InputBatch = Tensor2D\u003cB\u003e;\n    type OutputSingle = Scalar\u003cB\u003e;\n    type OutputBatch = Tensor1D\u003cB\u003e;\n    type ParamsRepr = SerializableLinearParams;\n\n    /// Predict on a single sample (feature vector).\n    fn predict(\u0026self, input: \u0026Self::InputSingle) -\u003e Self::OutputSingle {\n        self.params.weights.dot(input) + self.params.bias\n    }\n\n    fn predict_batch(\u0026self, input: \u0026Self::InputBatch) -\u003e Self::OutputBatch {\n        input\n            .dot(\u0026self.params.weights)\n            .add_scalar(\u0026self.params.bias)\n    }\n\n    fn extract_params(\u0026self) -\u003e Self::ParamsRepr {\n        (\u0026self.params).into()\n    }\n\n    fn from_params(params: Self::ParamsRepr) -\u003e Result\u003cSelf, Box\u003cdyn std::error::Error\u003e\u003e\n    where\n        Self: Sized,\n    {\n        let internal_params = LinearParams::\u003cB\u003e::try_from(params)?;\n        Ok(Self::new(internal_params))\n    }\n}\n\n/// Implements training interface for linear regression.\n///\n/// Forward pass: `X @ w + b`  \n/// Backward pass: computes gradients âw = X^T Â· grad, âb = sum(grad)\n///\n/// After training, convert to inference model via [`Self::into_fitted`].\nimpl\u003cB: Backend\u003e TrainableModel\u003cB\u003e for LinearModel\u003cB, Unfitted\u003e {\n    type Params = LinearParams\u003cB\u003e;\n    type Gradients = LinearParams\u003cB\u003e;\n    type Prediction = Tensor1D\u003cB\u003e;\n    type Input = Tensor2D\u003cB\u003e;\n    type Output = LinearModel\u003cB, Fitted\u003e;\n\n    fn forward(\u0026self, x: \u0026Self::Input) -\u003e Self::Prediction {\n        x.dot(\u0026self.params.weights).add_scalar(\u0026self.params.bias)\n    }\n\n    fn params(\u0026self) -\u003e \u0026Self::Params {\n        \u0026self.params\n    }\n\n    fn update_params(\u0026mut self, params: \u0026Self::Params) {\n        self.params = params.clone();\n    }\n\n    fn into_fitted(self) -\u003e LinearModel\u003cB, Fitted\u003e {\n        LinearModel::\u003cB, Fitted\u003e::new(self.params)\n    }\n\n    fn backward(\u0026self, x: \u0026Self::Input, grad_output: \u0026Self::Prediction) -\u003e Self::Gradients {\n        let grad_weights = x.tdot(grad_output);\n        let grad_bias = grad_output.sum();\n        LinearParams {\n            weights: grad_weights,\n            bias: grad_bias,\n        }\n    }\n}\n\n/// Alias for an **unfitted** linear regression model.\n///\n/// Equivalent to `LinearModel\u003cUnfitted\u003e`. Use this type when constructing\n/// a model for training with [`Trainer`].\npub type LinearRegression\u003cB\u003e = LinearModel\u003cB, Unfitted\u003e;\n\nimpl\u003cB: Backend\u003e LinearRegression\u003cB\u003e {\n    // Creates a new linear regression model with zero-initialized weights.\n    ///\n    /// # Parameters\n    /// - `n_features`: number of input features (dimensionality of `x`).\n    pub fn new(n_features: usize) -\u003e Self {\n        let params = LinearParams {\n            weights: Tensor1D::\u003cB\u003e::zeros(n_features),\n            bias: Scalar::\u003cB\u003e::new(0.),\n        };\n        Self {\n            params,\n            _state: PhantomData,\n        }\n    }\n\n    /// Constructs a model from explicit parameters (e.g., for testing or warm start).\n    pub fn from_params(params: LinearParams\u003cB\u003e) -\u003e Self {\n        Self {\n            params,\n            _state: PhantomData,\n        }\n    }\n}\n\n// Convenient alias for CPU-based linear regression.\n///\n/// Example:\n/// ```rust\n/// use machinelearne_rs::model::linear::LinearRegressor;\n/// let model = LinearRegressor::new(10); // 10 features\n/// ```\npub type LinearRegressor = LinearRegression\u003ccrate::backend::CpuBackend\u003e;\n\n#[cfg(test)]\nmod tests {\n    use super::*;\n    use crate::backend::CpuBackend;\n\n    // === ParamOps Tests ===\n\n    #[test]\n    fn test_param_ops_add() {\n        let p1 = LinearParams {\n            weights: Tensor1D::\u003cCpuBackend\u003e::new(vec![1.0, 2.0]),\n            bias: Scalar::\u003cCpuBackend\u003e::new(0.5),\n        };\n        let p2 = LinearParams {\n            weights: Tensor1D::\u003cCpuBackend\u003e::new(vec![0.5, 1.0]),\n            bias: Scalar::\u003cCpuBackend\u003e::new(0.5),\n        };\n\n        let result = p1.add(\u0026p2);\n\n        assert_eq!(result.weights.to_vec(), vec![1.5, 3.0]);\n        assert_eq!(result.bias.data.to_f64(), 1.0);\n    }\n\n    #[test]\n    fn test_param_ops_add_negative() {\n        let p1 = LinearParams {\n            weights: Tensor1D::\u003cCpuBackend\u003e::new(vec![1.0]),\n            bias: Scalar::\u003cCpuBackend\u003e::new(1.0),\n        };\n        let p2 = LinearParams {\n            weights: Tensor1D::\u003cCpuBackend\u003e::new(vec![-0.5]),\n            bias: Scalar::\u003cCpuBackend\u003e::new(-2.0),\n        };\n\n        let result = p1.add(\u0026p2);\n\n        assert_eq!(result.weights.to_vec(), vec![0.5]);\n        assert_eq!(result.bias.data.to_f64(), -1.0);\n    }\n\n    #[test]\n    fn test_param_ops_scale() {\n        let p = LinearParams {\n            weights: Tensor1D::\u003cCpuBackend\u003e::new(vec![2.0, 4.0]),\n            bias: Scalar::\u003cCpuBackend\u003e::new(1.0),\n        };\n\n        let result = p.scale(Scalar::\u003cCpuBackend\u003e::new(0.5));\n\n        assert_eq!(result.weights.to_vec(), vec![1.0, 2.0]);\n        assert_eq!(result.bias.data.to_f64(), 0.5);\n    }\n\n    #[test]\n    fn test_param_ops_scale_negative() {\n        let p = LinearParams {\n            weights: Tensor1D::\u003cCpuBackend\u003e::new(vec![2.0]),\n            bias: Scalar::\u003cCpuBackend\u003e::new(1.0),\n        };\n\n        let result = p.scale(Scalar::\u003cCpuBackend\u003e::new(-1.0));\n\n        assert_eq!(result.weights.to_vec(), vec![-2.0]);\n        assert_eq!(result.bias.data.to_f64(), -1.0);\n    }\n\n    #[test]\n    fn test_param_ops_scale_zero() {\n        let p = LinearParams {\n            weights: Tensor1D::\u003cCpuBackend\u003e::new(vec![5.0, 10.0]),\n            bias: Scalar::\u003cCpuBackend\u003e::new(100.0),\n        };\n\n        let result = p.scale(Scalar::\u003cCpuBackend\u003e::new(0.0));\n\n        assert_eq!(result.weights.to_vec(), vec![0.0, 0.0]);\n        assert_eq!(result.bias.data.to_f64(), 0.0);\n    }\n\n    // === LinearRegression (Unfitted) Tests ===\n\n    #[test]\n    fn test_linear_regression_new_zero_initialized() {\n        let model = LinearRegression::\u003cCpuBackend\u003e::new(3);\n\n        let params = model.params();\n        assert_eq!(params.weights.to_vec(), vec![0.0, 0.0, 0.0]);\n        assert_eq!(params.bias.data.to_f64(), 0.0);\n    }\n\n    #[test]\n    fn test_linear_regression_new_single_feature() {\n        let model = LinearRegression::\u003cCpuBackend\u003e::new(1);\n\n        assert_eq!(model.params().weights.to_vec(), vec![0.0]);\n        assert_eq!(model.params().bias.data.to_f64(), 0.0);\n    }\n\n    #[test]\n    fn test_linear_regression_new_large_features() {\n        let n_features = 1000;\n        let model = LinearRegression::\u003cCpuBackend\u003e::new(n_features);\n\n        assert_eq!(model.params().weights.to_vec().len(), n_features);\n        assert!(model.params().weights.to_vec().iter().all(|\u0026x| x == 0.0));\n    }\n\n    #[test]\n    fn test_linear_regression_from_params() {\n        let params = LinearParams {\n            weights: Tensor1D::\u003cCpuBackend\u003e::new(vec![1.0, 2.0, 3.0]),\n            bias: Scalar::\u003cCpuBackend\u003e::new(0.5),\n        };\n\n        let model = LinearRegression::\u003cCpuBackend\u003e::from_params(params.clone());\n\n        assert_eq!(model.params().weights.to_vec(), params.weights.to_vec());\n        assert_eq!(model.params().bias.data.to_f64(), params.bias.data.to_f64());\n    }\n\n    #[test]\n    fn test_linear_regression_update_params() {\n        let mut model = LinearRegression::\u003cCpuBackend\u003e::new(2);\n\n        let new_params = LinearParams {\n            weights: Tensor1D::\u003cCpuBackend\u003e::new(vec![1.0, 2.0]),\n            bias: Scalar::\u003cCpuBackend\u003e::new(0.5),\n        };\n\n        model.update_params(\u0026new_params);\n\n        assert_eq!(model.params().weights.to_vec(), vec![1.0, 2.0]);\n        assert_eq!(model.params().bias.data.to_f64(), 0.5);\n    }\n\n    #[test]\n    fn test_linear_regression_forward_with_zero_params() {\n        let model = LinearRegression::\u003cCpuBackend\u003e::new(2);\n\n        let x = Tensor2D::\u003cCpuBackend\u003e::new(vec![1.0, 2.0, 3.0, 4.0], 2, 2);\n        let pred = model.forward(\u0026x);\n\n        // With zero weights and bias: X @ w + b = 0 + 0 = 0\n        assert_eq!(pred.to_vec(), vec![0.0, 0.0]);\n    }\n\n    #[test]\n    fn test_linear_regression_forward_correctness() {\n        // Create model with known weights\n        let params = LinearParams {\n            weights: Tensor1D::\u003cCpuBackend\u003e::new(vec![2.0, 3.0]),\n            bias: Scalar::\u003cCpuBackend\u003e::new(1.0),\n        };\n        let model = LinearRegression::\u003cCpuBackend\u003e::from_params(params);\n\n        // Input: [[1, 0], [0, 1]]\n        // Expected: [[2*1 + 3*0 + 1], [2*0 + 3*1 + 1]] = [3, 4]\n        let x = Tensor2D::\u003cCpuBackend\u003e::new(vec![1.0, 0.0, 0.0, 1.0], 2, 2);\n        let pred = model.forward(\u0026x);\n\n        assert_eq!(pred.to_vec(), vec![3.0, 4.0]);\n    }\n\n    #[test]\n    fn test_linear_regression_forward_batch() {\n        let params = LinearParams {\n            weights: Tensor1D::\u003cCpuBackend\u003e::new(vec![1.0, 2.0]),\n            bias: Scalar::\u003cCpuBackend\u003e::new(3.0),\n        };\n        let model = LinearRegression::\u003cCpuBackend\u003e::from_params(params);\n\n        // Input: [[1, 1], [2, 2]]\n        // Expected: [[1*1 + 2*1 + 3], [1*2 + 2*2 + 3]] = [6, 9]\n        let x = Tensor2D::\u003cCpuBackend\u003e::new(vec![1.0, 1.0, 2.0, 2.0], 2, 2);\n        let pred = model.forward(\u0026x);\n\n        assert_eq!(pred.to_vec(), vec![6.0, 9.0]);\n    }\n\n    #[test]\n    fn test_linear_regression_backward_single_sample() {\n        let model = LinearRegression::\u003cCpuBackend\u003e::new(1);\n        let x = Tensor2D::\u003cCpuBackend\u003e::new(vec![2.0], 1, 1);\n        let grad_output = Tensor1D::\u003cCpuBackend\u003e::new(vec![0.5]);\n\n        let grads = model.backward(\u0026x, \u0026grad_output);\n\n        // grad_w = X^T @ grad = [2] * [0.5] = 1.0\n        // grad_b = sum(grad) = 0.5\n        assert_eq!(grads.weights.to_vec(), vec![1.0]);\n        assert_eq!(grads.bias.data.to_f64(), 0.5);\n    }\n\n    #[test]\n    fn test_linear_regression_backward_batch() {\n        let model = LinearRegression::\u003cCpuBackend\u003e::new(2);\n        // Input: [[1, 2], [3, 4]]\n        let x = Tensor2D::\u003cCpuBackend\u003e::new(vec![1.0, 2.0, 3.0, 4.0], 2, 2);\n        // grad_output: [0.5, 0.25]\n        let grad_output = Tensor1D::\u003cCpuBackend\u003e::new(vec![0.5, 0.25]);\n\n        let grads = model.backward(\u0026x, \u0026grad_output);\n\n        // grad_w = X^T @ grad = [[1, 3], [2, 4]] @ [0.5, 0.25]^T\n        //       = [1*0.5 + 3*0.25, 2*0.5 + 4*0.25]\n        //       = [0.5 + 0.75, 1.0 + 1.0] = [1.25, 2.0]\n        // grad_b = sum([0.5, 0.25]) = 0.75\n        assert!((grads.weights.to_vec()[0] - 1.25).abs() \u003c 1e-10);\n        assert!((grads.weights.to_vec()[1] - 2.0).abs() \u003c 1e-10);\n        assert!((grads.bias.data.to_f64() - 0.75).abs() \u003c 1e-10);\n    }\n\n    #[test]\n    fn test_linear_regression_backward_negative_gradients() {\n        let model = LinearRegression::\u003cCpuBackend\u003e::new(1);\n        let x = Tensor2D::\u003cCpuBackend\u003e::new(vec![1.0], 1, 1);\n        let grad_output = Tensor1D::\u003cCpuBackend\u003e::new(vec![-0.5]);\n\n        let grads = model.backward(\u0026x, \u0026grad_output);\n\n        assert_eq!(grads.weights.to_vec(), vec![-0.5]);\n        assert_eq!(grads.bias.data.to_f64(), -0.5);\n    }\n\n    #[test]\n    fn test_linear_regression_backward_zero_gradients() {\n        let model = LinearRegression::\u003cCpuBackend\u003e::new(2);\n        let x = Tensor2D::\u003cCpuBackend\u003e::new(vec![1.0, 2.0, 3.0, 4.0], 2, 2);\n        let grad_output = Tensor1D::\u003cCpuBackend\u003e::new(vec![0.0, 0.0]);\n\n        let grads = model.backward(\u0026x, \u0026grad_output);\n\n        assert_eq!(grads.weights.to_vec(), vec![0.0, 0.0]);\n        assert_eq!(grads.bias.data.to_f64(), 0.0);\n    }\n\n    // === Fitted Model Tests ===\n\n    #[test]\n    fn test_linear_model_fitted_new() {\n        let params = LinearParams {\n            weights: Tensor1D::\u003cCpuBackend\u003e::new(vec![1.0, 2.0]),\n            bias: Scalar::\u003cCpuBackend\u003e::new(0.5),\n        };\n\n        let model = LinearModel::\u003cCpuBackend, Fitted\u003e::new(params);\n\n        assert_eq!(model.params.weights.to_vec(), vec![1.0, 2.0]);\n        assert_eq!(model.params.bias.data.to_f64(), 0.5);\n    }\n\n    #[test]\n    fn test_linear_model_predict_single_sample() {\n        let params = LinearParams {\n            weights: Tensor1D::\u003cCpuBackend\u003e::new(vec![2.0, 3.0]),\n            bias: Scalar::\u003cCpuBackend\u003e::new(1.0),\n        };\n        let model = LinearModel::\u003cCpuBackend, Fitted\u003e::new(params);\n\n        let input = Tensor1D::\u003cCpuBackend\u003e::new(vec![1.0, 2.0]);\n        let pred = model.predict(\u0026input);\n\n        // y = 2*1 + 3*2 + 1 = 2 + 6 + 1 = 9\n        assert_eq!(pred.data.to_f64(), 9.0);\n    }\n\n    #[test]\n    fn test_linear_model_predict_batch() {\n        let params = LinearParams {\n            weights: Tensor1D::\u003cCpuBackend\u003e::new(vec![1.0, 2.0]),\n            bias: Scalar::\u003cCpuBackend\u003e::new(3.0),\n        };\n        let model = LinearModel::\u003cCpuBackend, Fitted\u003e::new(params);\n\n        let batch = Tensor2D::\u003cCpuBackend\u003e::new(vec![1.0, 1.0, 2.0, 2.0], 2, 2);\n        let pred = model.predict_batch(\u0026batch);\n\n        // y1 = 1*1 + 2*1 + 3 = 6\n        // y2 = 1*2 + 2*2 + 3 = 9\n        assert_eq!(pred.to_vec(), vec![6.0, 9.0]);\n    }\n\n    #[test]\n    fn test_linear_model_predict_single_feature() {\n        let params = LinearParams {\n            weights: Tensor1D::\u003cCpuBackend\u003e::new(vec![5.0]),\n            bias: Scalar::\u003cCpuBackend\u003e::new(0.0),\n        };\n        let model = LinearModel::\u003cCpuBackend, Fitted\u003e::new(params);\n\n        let input = Tensor1D::\u003cCpuBackend\u003e::new(vec![2.0]);\n        let pred = model.predict(\u0026input);\n\n        assert_eq!(pred.data.to_f64(), 10.0);\n    }\n\n    #[test]\n    fn test_linear_model_predict_zero_bias() {\n        let params = LinearParams {\n            weights: Tensor1D::\u003cCpuBackend\u003e::new(vec![1.0, 1.0]),\n            bias: Scalar::\u003cCpuBackend\u003e::new(0.0),\n        };\n        let model = LinearModel::\u003cCpuBackend, Fitted\u003e::new(params);\n\n        let input = Tensor1D::\u003cCpuBackend\u003e::new(vec![3.0, 4.0]);\n        let pred = model.predict(\u0026input);\n\n        assert_eq!(pred.data.to_f64(), 7.0);\n    }\n\n    #[test]\n    fn test_linear_model_predict_negative_weights() {\n        let params = LinearParams {\n            weights: Tensor1D::\u003cCpuBackend\u003e::new(vec![-1.0, -2.0]),\n            bias: Scalar::\u003cCpuBackend\u003e::new(5.0),\n        };\n        let model = LinearModel::\u003cCpuBackend, Fitted\u003e::new(params);\n\n        let input = Tensor1D::\u003cCpuBackend\u003e::new(vec![1.0, 1.0]);\n        let pred = model.predict(\u0026input);\n\n        // y = -1*1 + -2*1 + 5 = 2\n        assert_eq!(pred.data.to_f64(), 2.0);\n    }\n\n    #[test]\n    fn test_linear_model_extract_params() {\n        let params = LinearParams {\n            weights: Tensor1D::\u003cCpuBackend\u003e::new(vec![1.5, 2.5, 3.5]),\n            bias: Scalar::\u003cCpuBackend\u003e::new(0.25),\n        };\n        let model = LinearModel::\u003cCpuBackend, Fitted\u003e::new(params);\n\n        let extracted = model.extract_params();\n\n        assert_eq!(extracted.weights.len(), 3);\n        assert!((extracted.weights[0] - 1.5).abs() \u003c 1e-6);\n        assert!((extracted.weights[1] - 2.5).abs() \u003c 1e-6);\n        assert!((extracted.weights[2] - 3.5).abs() \u003c 1e-6);\n        assert!((extracted.bias - 0.25).abs() \u003c 1e-6);\n    }\n\n    #[test]\n    fn test_linear_model_from_params() -\u003e Result\u003c(), Box\u003cdyn std::error::Error\u003e\u003e {\n        let serial = SerializableLinearParams {\n            weights: vec![1.0, 2.0, 3.0],\n            bias: 0.5,\n        };\n\n        let model = LinearModel::\u003cCpuBackend, Fitted\u003e::from_params(serial)?;\n\n        assert_eq!(model.params.weights.to_vec(), vec![1.0, 2.0, 3.0]);\n        assert_eq!(model.params.bias.data.to_f64(), 0.5);\n\n        Ok(())\n    }\n\n    #[test]\n    fn test_linear_model_into_fitted() {\n        let params = LinearParams {\n            weights: Tensor1D::\u003cCpuBackend\u003e::new(vec![1.0]),\n            bias: Scalar::\u003cCpuBackend\u003e::new(0.5),\n        };\n        let unfitted_model = LinearRegression::\u003cCpuBackend\u003e::from_params(params);\n\n        let fitted_model: LinearModel\u003cCpuBackend, Fitted\u003e = unfitted_model.into_fitted();\n\n        assert_eq!(fitted_model.params.weights.to_vec(), vec![1.0]);\n        assert_eq!(fitted_model.params.bias.data.to_f64(), 0.5);\n    }\n\n    #[test]\n    fn test_linear_model_predict_does_not_mutate_input() {\n        let params = LinearParams {\n            weights: Tensor1D::\u003cCpuBackend\u003e::new(vec![1.0, 1.0]),\n            bias: Scalar::\u003cCpuBackend\u003e::new(0.0),\n        };\n        let model = LinearModel::\u003cCpuBackend, Fitted\u003e::new(params);\n\n        let input = Tensor1D::\u003cCpuBackend\u003e::new(vec![1.0, 2.0]);\n        let original = input.clone();\n\n        let _ = model.predict(\u0026input);\n\n        // Input should be unchanged\n        assert_eq!(input.to_vec(), original.to_vec());\n    }\n\n    // === Serialization Tests ===\n\n    #[test]\n    fn test_linear_model_save_load() -\u003e Result\u003c(), Box\u003cdyn std::error::Error\u003e\u003e {\n        let weights = vec![1.0, 2.0, 3.0];\n        let bias = 0.5;\n        let serial = SerializableLinearParams { weights, bias };\n        let params = LinearParams::\u003cCpuBackend\u003e::try_from(serial)?;\n        let model = LinearModel::\u003cCpuBackend, Fitted\u003e::new(params);\n\n        // Save\n        let tmp = tempfile::tempdir()?;\n        let path = tmp.path().join(\"model.bin\");\n        model.save_to_file(\u0026path)?;\n\n        // Load\n        let loaded = LinearModel::\u003cCpuBackend, Fitted\u003e::load_from_file(\u0026path)?;\n\n        // Compare\n        let orig_repr = model.extract_params();\n        let loaded_repr = loaded.extract_params();\n        assert_eq!(orig_repr.weights, loaded_repr.weights);\n        assert_eq!(orig_repr.bias, loaded_repr.bias);\n\n        Ok(())\n    }\n\n    #[test]\n    fn test_serialization_params_roundtrip() -\u003e Result\u003c(), Box\u003cdyn std::error::Error\u003e\u003e {\n        let original = LinearParams {\n            weights: Tensor1D::\u003cCpuBackend\u003e::new(vec![0.1, 0.2, 0.3]),\n            bias: Scalar::\u003cCpuBackend\u003e::new(0.05),\n        };\n\n        // Convert to serializable\n        let serial: SerializableLinearParams = (\u0026original).into();\n\n        // Convert back\n        let restored = LinearParams::\u003cCpuBackend\u003e::try_from(serial)?;\n\n        // Verify with tolerance due to f32/f64 conversion precision loss\n        for (orig, rest) in original\n            .weights\n            .to_vec()\n            .iter()\n            .zip(restored.weights.to_vec().iter())\n        {\n            assert!((orig - rest).abs() \u003c 1e-6);\n        }\n        assert!((original.bias.data.to_f64() - restored.bias.data.to_f64()).abs() \u003c 1e-6);\n\n        Ok(())\n    }\n}\n","traces":[{"line":48,"address":[3119904,3120189,3120183],"length":1,"stats":{"Line":1}},{"line":50,"address":[],"length":0,"stats":{"Line":0}},{"line":51,"address":[],"length":0,"stats":{"Line":0}},{"line":54,"address":[3120219,3119987,3120208],"length":1,"stats":{"Line":3}},{"line":56,"address":[3120012,3120083],"length":1,"stats":{"Line":2}},{"line":64,"address":[3119224,3118960],"length":1,"stats":{"Line":1}},{"line":65,"address":[3118982],"length":1,"stats":{"Line":1}},{"line":66,"address":[3119019,3119094],"length":1,"stats":{"Line":3}},{"line":67,"address":[3119103],"length":1,"stats":{"Line":1}},{"line":75,"address":[3118736,3118528,3118730],"length":1,"stats":{"Line":2}},{"line":76,"address":[3118570],"length":1,"stats":{"Line":2}},{"line":77,"address":[3118652,3118585],"length":1,"stats":{"Line":4}},{"line":83,"address":[3118946,3118752,3118940],"length":1,"stats":{"Line":2}},{"line":84,"address":[3118791],"length":1,"stats":{"Line":1}},{"line":85,"address":[3118868,3118801],"length":1,"stats":{"Line":6}},{"line":109,"address":[3124144],"length":1,"stats":{"Line":1}},{"line":131,"address":[3119856],"length":1,"stats":{"Line":1}},{"line":132,"address":[3119874],"length":1,"stats":{"Line":1}},{"line":135,"address":[3119779,3119648,3119785],"length":1,"stats":{"Line":1}},{"line":136,"address":[],"length":0,"stats":{"Line":0}},{"line":137,"address":[3119700],"length":1,"stats":{"Line":1}},{"line":138,"address":[3119720],"length":1,"stats":{"Line":1}},{"line":141,"address":[3119808],"length":1,"stats":{"Line":1}},{"line":142,"address":[3119825],"length":1,"stats":{"Line":1}},{"line":145,"address":[3119248,3119603,3119628],"length":1,"stats":{"Line":1}},{"line":149,"address":[3119386,3119264],"length":1,"stats":{"Line":2}},{"line":150,"address":[3119535,3119440],"length":1,"stats":{"Line":2}},{"line":167,"address":[3120611,3120480,3120617],"length":1,"stats":{"Line":2}},{"line":168,"address":[3120532],"length":1,"stats":{"Line":1}},{"line":171,"address":[3120464],"length":1,"stats":{"Line":1}},{"line":172,"address":[],"length":0,"stats":{"Line":0}},{"line":175,"address":[3120304,3120382],"length":1,"stats":{"Line":1}},{"line":176,"address":[3120327,3120347,3120412],"length":1,"stats":{"Line":2}},{"line":179,"address":[3120224],"length":1,"stats":{"Line":1}},{"line":180,"address":[3120235],"length":1,"stats":{"Line":1}},{"line":183,"address":[3120852,3120640,3120846],"length":1,"stats":{"Line":2}},{"line":184,"address":[3120701],"length":1,"stats":{"Line":1}},{"line":185,"address":[3120768,3120711],"length":1,"stats":{"Line":3}},{"line":204,"address":[3124240,3124409,3124403],"length":1,"stats":{"Line":1}},{"line":206,"address":[3124263],"length":1,"stats":{"Line":2}},{"line":207,"address":[3124271],"length":1,"stats":{"Line":1}},{"line":216,"address":[3124192],"length":1,"stats":{"Line":1}}],"covered":38,"coverable":42},{"path":["/","home","victor","ML","machinelearne-rs","lib","src","model","mod.rs"],"content":"//! Core model abstractions: training vs inference separation.\n//!\n//! This module defines the two central traits:\n//! - [`TrainableModel`]: used during training; owns mutable parameters and computes gradients.\n//! - [`InferenceModel`]: lightweight, serializable, stateless predictor for production use.\n//!\n//! This design ensures that a fitted model carries **only** what is needed for prediction,\n//! with no optimizer state, loss functions, or training hyperparameters.\n//!\n//! See [`linear::LinearRegressor`] for a concrete example.\n\npub mod state;\npub use state::{Fitted, Unfitted};\n\npub mod linear;\npub use crate::backend::scalar::{Scalar, ScalarOps};\npub use crate::backend::Backend;\nuse crate::serialization::SerializableParams;\n\n/// A model that can be trained: it computes forward passes, gradients, and updates its parameters.\n///\n/// This trait is used **only during training**. After training, it is converted into an\n/// [`InferenceModel`] via [`Self::into_fitted`], which strips away all training-related state.\n///\n/// # Type Parameters\n/// - `B`: The backend (e.g., `CpuBackend`) used for computation.\n/// - `Input`: Input data type (e.g., `Tensor1D\u003cB\u003e`).\n/// - `Prediction`: Output of the forward pass (e.g., scalar for regression).\n/// - `Params`: Internal trainable parameters (e.g., weights + bias).\n/// - `Gradients`: Gradient structure matching `Params`.\n/// - `Output`: The corresponding [`InferenceModel`] type.\n///\n/// # Safety \u0026 Invariants\n/// - `backward` must be called **after** `forward` with the same input.\n/// - `update_params` must preserve the shape/structure of parameters.\n///\n/// # Example\n/// See [`linear::LinearRegressor`] for a full implementation.\npub trait TrainableModel\u003cB: Backend\u003e {\n    type Input;\n    type Prediction;\n    type Params;\n    type Gradients;\n    type Output;\n\n    fn forward(\u0026self, input: \u0026Self::Input) -\u003e Self::Prediction;\n    fn backward(\u0026self, input: \u0026Self::Input, grad_output: \u0026Self::Prediction) -\u003e Self::Gradients;\n    fn params(\u0026self) -\u003e \u0026Self::Params;\n    fn update_params(\u0026mut self, new_params: \u0026Self::Params);\n\n    fn into_fitted(self) -\u003e Self::Output;\n}\n/// Operations required to update model parameters during optimization.\n///\n/// Optimizers like SGD rely on these operations to compute weight updates:\n/// ```text\n/// w_new = w_old + (-lr) * grad\n/// ```\n///\n/// Implementations must be **element-wise** and preserve parameter structure.\n///\npub trait ParamOps\u003cB: Backend\u003e: Clone {\n    fn add(\u0026self, other: \u0026Self) -\u003e Self;\n    fn scale(\u0026self, scalar: Scalar\u003cB\u003e) -\u003e Self;\n}\n/// A lightweight, serializable model for inference only.\n///\n/// This trait represents the **final product** of training: it contains no optimizer state,\n/// loss functions, or batch metadata. It is designed for:\n/// - Fast prediction (`predict`, `predict_batch`)\n/// - Serialization (`save_to_file`, `load_from_file`)\n/// - Deployment in production environments\n///\n/// # Type Parameters\n/// - `InputSingle` / `OutputSingle`: types for single-sample prediction.\n/// - `InputBatch` / `OutputBatch`: types for batched prediction (optional optimization).\n/// - `ParamsRepr`: a serializable representation of internal parameters (e.g., struct with `Vec\u003cf64\u003e`).\n///\n/// # Guarantees\n/// - `extract_params()` + `from_params()` is a round-trip.\n/// - `save_to_file` / `load_from_file` are compatible across platforms (uses `bincode` internally).\n///\n/// # Example\n/// ```rust\n/// use machinelearne_rs::{CpuBackend,\n///                         Tensor1D,\n///                         model::linear::LinearModel,\n///                         model::linear::LinearParams,\n///                         model::linear::Fitted,\n///                         model::InferenceModel,\n///                         backend::Scalar};\n/// let weights = Tensor1D::\u003cCpuBackend\u003e::new(vec![3.0f32, 4.0]);\n/// let bias = Scalar::\u003cCpuBackend\u003e::new(1.0);\n/// let params = LinearParams { weights, bias };\n/// let model = LinearModel::\u003cCpuBackend, Fitted\u003e::from_params((\u0026params).into()).unwrap();\n/// let input = Tensor1D::\u003cCpuBackend\u003e::new(vec![3.0, 4.0]);\n/// let pred = model.predict(\u0026input); // â 1*3 + 2*4 + 0.5 = 11.5\n/// ```\npub trait InferenceModel\u003cB: Backend\u003e {\n    type InputSingle;\n    type OutputSingle;\n    type InputBatch;\n    type OutputBatch;\n    type ParamsRepr: SerializableParams;\n    fn predict(\u0026self, input: \u0026Self::InputSingle) -\u003e Self::OutputSingle;\n    fn predict_batch(\u0026self, input: \u0026Self::InputBatch) -\u003e Self::OutputBatch;\n    fn extract_params(\u0026self) -\u003e Self::ParamsRepr;\n    fn from_params(params: Self::ParamsRepr) -\u003e Result\u003cSelf, Box\u003cdyn std::error::Error\u003e\u003e\n    where\n        Self: Sized;\n\n    fn save_to_file\u003cP: AsRef\u003cstd::path::Path\u003e\u003e(\u0026self, path: P) -\u003e std::io::Result\u003c()\u003e {\n        let bytes = self\n            .extract_params()\n            .to_bytes()\n            .map_err(std::io::Error::other)?;\n        std::fs::write(path, bytes)\n    }\n\n    fn load_from_file\u003cP: AsRef\u003cstd::path::Path\u003e\u003e(\n        path: P,\n    ) -\u003e Result\u003cSelf, Box\u003cdyn std::error::Error\u003e\u003e\n    where\n        Self: Sized,\n    {\n        let bytes = std::fs::read(path)?;\n        let params = Self::ParamsRepr::from_bytes(\u0026bytes)\n            .map_err(|e| -\u003e Box\u003cdyn std::error::Error\u003e { Box::new(e) })?;\n        Self::from_params(params)\n    }\n}\n","traces":[{"line":112,"address":[3121369,3121410,3120864],"length":1,"stats":{"Line":1}},{"line":113,"address":[3120898,3121380,3121116,3121062],"length":1,"stats":{"Line":2}},{"line":116,"address":[3121006,3121393,3121103,3121210],"length":1,"stats":{"Line":1}},{"line":117,"address":[3121259],"length":1,"stats":{"Line":1}},{"line":120,"address":[3121440,3122059,3122086],"length":1,"stats":{"Line":1}},{"line":126,"address":[3121465],"length":1,"stats":{"Line":1}},{"line":127,"address":[3121613,3121742,3121696,3121823],"length":1,"stats":{"Line":3}},{"line":128,"address":[2477833,2477824],"length":1,"stats":{"Line":1}},{"line":129,"address":[3121917],"length":1,"stats":{"Line":1}}],"covered":9,"coverable":9},{"path":["/","home","victor","ML","machinelearne-rs","lib","src","model","state.rs"],"content":"/// A marker type indicating that a model is **not yet trained**.\n///\n/// This phantom type is used in generic parameters (e.g., `LinearRegressor\u003cUnfitted\u003e`)\n/// to enforce compile-time guarantees:\n/// - Training methods (like `Trainer::fit`) require an `Unfitted` model.\n/// - Inference methods (`predict`) are **not available** until the model is converted to `Fitted`.\n///\n/// This prevents accidental use of an untrained model for prediction.\npub struct Unfitted;\n\n/// A marker type indicating that a model has been **fully trained**.\n///\n/// After training, a model is converted from `Model\u003cUnfitted\u003e` to `Model\u003cFitted\u003e`,\n/// which implements [`InferenceModel`] and can be serialized or used for prediction.\n///\n/// A `Fitted` model contains **only inference parameters** â no optimizer state,\n/// loss function, or training hyperparameters (per ADR-0001).\npub struct Fitted;\n","traces":[],"covered":0,"coverable":0},{"path":["/","home","victor","ML","machinelearne-rs","lib","src","optimizer","mod.rs"],"content":"use crate::backend::scalar::{Scalar, ScalarOps};\nuse crate::backend::tensor1d::Tensor1D;\nuse crate::backend::Backend;\nuse crate::loss::TensorLike;\nuse crate::model::linear::LinearParams;\n\n/// Trait for gradient-based optimizers.\n///\n/// Optimizers are responsible for updating model parameters based on computed gradients.\n/// The architecture follows separation of concerns principle: training logic (`Trainer`)\n/// is decoupled from parameter update logic. This enables composable design where any\n/// model can be paired with any optimizer while maintaining full type safety without\n/// dynamic dispatch.\n///\n/// # Type Parameters\n/// * `B` â computation backend implementing [`Backend`]\n/// * `P` â model parameters type (e.g., [`LinearParams`])\n///\n/// # Example\n/// ```rust\n/// # use machinelearne_rs::optimizer::{SGD, Optimizer};\n/// # use machinelearne_rs::backend::CpuBackend;\n/// # use machinelearne_rs::model::linear::LinearParams;\n/// # use machinelearne_rs::backend::tensor1d::Tensor1D;\n/// # use machinelearne_rs::backend::scalar::Scalar;\n/// #\n/// # // Mock parameters and gradients for demonstration\n/// # let params = LinearParams {\n/// #     weights: Tensor1D::\u003cCpuBackend\u003e::new(vec![1.0, 2.0, 3.0]),\n/// #     bias: Scalar::\u003cCpuBackend\u003e::new(0.5),\n/// # };\n/// # let gradients = LinearParams {\n/// #     weights: Tensor1D::\u003cCpuBackend\u003e::new(vec![0.1, -0.2, 0.05]),\n/// #     bias: Scalar::\u003cCpuBackend\u003e::new(-0.01),\n/// # };\n/// let sgd = SGD::\u003cCpuBackend\u003e::new(0.01);\n/// let updated_params = sgd.step(\u0026params, \u0026gradients);\n/// ```\npub trait Optimizer\u003cB: Backend, P\u003e {\n    /// Performs an optimization step using the update rule:\n    /// ```text\n    /// params_new = params - learning_rate * gradients\n    /// ```\n    ///\n    /// # Arguments\n    /// * `params` â current model parameters\n    /// * `gradients` â loss gradients w.r.t. parameters (typically computed via backpropagation)\n    ///\n    /// # Returns\n    /// A new owned instance of updated parameters.\n    ///\n    /// # Note\n    /// This method does not mutate inputs â it returns a new value. This functional\n    /// approach simplifies state management in training loops and enables easier\n    /// composition with immutable data structures.\n    fn step(\u0026self, params: \u0026P, gradients: \u0026P) -\u003e P;\n}\n\n/// Stochastic Gradient Descent (SGD) optimizer.\n///\n/// The simplest first-order optimizer that updates parameters according to:\n/// ```text\n/// Î¸ â Î¸ - Î· Â· âL(Î¸)\n/// ```\n/// where `Î·` is the learning rate and `âL(Î¸)` is the loss gradient.\n///\n/// # Design Notes\n/// * Backend-agnostic via [`Backend`] trait bounds\n/// * Type-safe binding to specific parameter structures through trait implementation\n/// * Stateless by design (no momentum, adaptive learning rates) â specialized variants\n///   like Adam or RMSProp should be implemented as separate optimizers\n/// * Immutable update semantics: returns new parameters instead of mutating in place\n///\n/// # Example\n/// ```rust\n/// use machinelearne_rs::optimizer::SGD;\n/// use machinelearne_rs::backend::CpuBackend;\n///\n/// // Create SGD optimizer with learning rate 0.01\n/// let optimizer = SGD::\u003cCpuBackend\u003e::new(0.01);\n/// ```\n#[derive(Clone)]\npub struct SGD\u003cB: Backend\u003e {\n    /// Learning rate (Î·). Stored as a backend scalar to enable type-safe\n    /// arithmetic operations with tensors.\n    lr: Scalar\u003cB\u003e,\n}\n\nimpl\u003cB: Backend\u003e SGD\u003cB\u003e {\n    /// Creates a new SGD optimizer with the specified learning rate.\n    ///\n    /// # Arguments\n    /// * `lr` â learning rate (positive value, typically in range 1e-4 .. 1e-1)\n    ///\n    /// # Panics\n    /// Does not panic directly, but backend implementations may validate `lr`\n    /// during scalar construction (e.g., rejecting NaN or negative values).\n    pub fn new(lr: f64) -\u003e Self {\n        Self {\n            lr: Scalar::\u003cB\u003e::new(lr),\n        }\n    }\n\n    /// Returns the current learning rate.\n    pub fn learning_rate(\u0026self) -\u003e f64 {\n        self.lr.data.to_f64()\n    }\n}\n\nimpl\u003cB: Backend\u003e Optimizer\u003cB, LinearParams\u003cB\u003e\u003e for SGD\u003cB\u003e\nwhere\n    Tensor1D\u003cB\u003e: Clone,\n    Scalar\u003cB\u003e: Clone,\n{\n    fn step(\u0026self, params: \u0026LinearParams\u003cB\u003e, grads: \u0026LinearParams\u003cB\u003e) -\u003e LinearParams\u003cB\u003e {\n        // weights_new = weights - lr * grad_weights\n        // Using (-lr) enables single scaling operation instead of scale + subtract\n        let neg_lr = Scalar::\u003cB\u003e::new(0.0) - self.lr;\n        let scaled_grad = grads.weights.scale(\u0026neg_lr);\n        let weights_update = params.weights.add(\u0026scaled_grad);\n\n        // bias_new = bias - lr * grad_bias\n        // Using explicit backend methods for consistency with tensor operations\n        let scaled_bias_grad = grads.bias * self.lr;\n        let bias_update = params.bias - scaled_bias_grad;\n\n        LinearParams {\n            weights: weights_update,\n            bias: bias_update,\n        }\n    }\n}\n\n#[cfg(test)]\nmod tests {\n    use super::*;\n    use crate::backend::CpuBackend;\n\n    #[test]\n    fn test_sgd_new_initialization() {\n        let lr = 0.01;\n        let sgd = SGD::\u003cCpuBackend\u003e::new(lr);\n\n        assert_eq!(sgd.learning_rate(), lr);\n    }\n\n    #[test]\n    fn test_sgd_learning_rate_accessor() {\n        let sgd = SGD::\u003cCpuBackend\u003e::new(0.001);\n        assert_eq!(sgd.learning_rate(), 0.001);\n\n        let sgd_large = SGD::\u003cCpuBackend\u003e::new(1.0);\n        assert_eq!(sgd_large.learning_rate(), 1.0);\n    }\n\n    #[test]\n    fn test_sgd_step_correctness_weights() {\n        // params_new = params_old - lr * grads\n        let lr = 0.1;\n        let params = LinearParams {\n            weights: Tensor1D::\u003cCpuBackend\u003e::new(vec![2.0, 3.0]),\n            bias: Scalar::\u003cCpuBackend\u003e::new(1.0),\n        };\n        let grads = LinearParams {\n            weights: Tensor1D::\u003cCpuBackend\u003e::new(vec![1.0, -1.0]),\n            bias: Scalar::\u003cCpuBackend\u003e::new(0.5),\n        };\n\n        let sgd = SGD::\u003cCpuBackend\u003e::new(lr);\n        let updated = sgd.step(\u0026params, \u0026grads);\n\n        // weights: [2.0 - 0.1*1.0, 3.0 - 0.1*(-1.0)] = [1.9, 3.1]\n        assert_eq!(updated.weights.to_vec(), vec![1.9, 3.1]);\n    }\n\n    #[test]\n    fn test_sgd_step_correctness_bias() {\n        let lr = 0.05;\n        let params = LinearParams {\n            weights: Tensor1D::\u003cCpuBackend\u003e::new(vec![1.0]),\n            bias: Scalar::\u003cCpuBackend\u003e::new(2.0),\n        };\n        let grads = LinearParams {\n            weights: Tensor1D::\u003cCpuBackend\u003e::new(vec![0.5]),\n            bias: Scalar::\u003cCpuBackend\u003e::new(-1.0),\n        };\n\n        let sgd = SGD::\u003cCpuBackend\u003e::new(lr);\n        let updated = sgd.step(\u0026params, \u0026grads);\n\n        // bias: 2.0 - 0.05*(-1.0) = 2.0 + 0.05 = 2.05\n        assert_eq!(updated.bias.data.to_f64(), 2.05);\n    }\n\n    #[test]\n    fn test_sgd_step_single_parameter() {\n        let sgd = SGD::\u003cCpuBackend\u003e::new(0.1);\n        let params = LinearParams {\n            weights: Tensor1D::\u003cCpuBackend\u003e::new(vec![5.0]),\n            bias: Scalar::\u003cCpuBackend\u003e::new(0.0),\n        };\n        let grads = LinearParams {\n            weights: Tensor1D::\u003cCpuBackend\u003e::new(vec![2.0]),\n            bias: Scalar::\u003cCpuBackend\u003e::new(0.1),\n        };\n\n        let updated = sgd.step(\u0026params, \u0026grads);\n\n        // weight: 5.0 - 0.1*2.0 = 4.8\n        // bias: 0.0 - 0.1*0.1 = -0.01\n        // Use approximate comparison for floating point\n        assert!((updated.weights.to_vec()[0] - 4.8).abs() \u003c 1e-10);\n        assert!((updated.bias.data.to_f64() - (-0.01)).abs() \u003c 1e-10);\n    }\n\n    #[test]\n    fn test_sgd_step_multiple_parameters() {\n        let sgd = SGD::\u003cCpuBackend\u003e::new(0.01);\n        let params = LinearParams {\n            weights: Tensor1D::\u003cCpuBackend\u003e::new(vec![1.0, 2.0, 3.0, 4.0]),\n            bias: Scalar::\u003cCpuBackend\u003e::new(0.5),\n        };\n        let grads = LinearParams {\n            weights: Tensor1D::\u003cCpuBackend\u003e::new(vec![0.5, -0.25, 0.1, -0.2]),\n            bias: Scalar::\u003cCpuBackend\u003e::new(0.01),\n        };\n\n        let updated = sgd.step(\u0026params, \u0026grads);\n\n        // weights: [1.0-0.01*0.5, 2.0-0.01*(-0.25), 3.0-0.01*0.1, 4.0-0.01*(-0.2)]\n        //         = [0.995, 2.0025, 2.999, 4.002]\n        let expected_weights = vec![0.995, 2.0025, 2.999, 4.002];\n        assert!(updated\n            .weights\n            .to_vec()\n            .iter()\n            .zip(expected_weights.iter())\n            .all(|(a, b)| (a - b).abs() \u003c 1e-10));\n\n        // bias: 0.5 - 0.01*0.01 = 0.4999\n        assert!((updated.bias.data.to_f64() - 0.4999).abs() \u003c 1e-5);\n    }\n\n    #[test]\n    fn test_sgd_step_zero_gradients() {\n        // When gradients are zero, parameters should not change\n        let sgd = SGD::\u003cCpuBackend\u003e::new(0.1);\n        let params = LinearParams {\n            weights: Tensor1D::\u003cCpuBackend\u003e::new(vec![1.0, 2.0]),\n            bias: Scalar::\u003cCpuBackend\u003e::new(0.5),\n        };\n        let zero_grads = LinearParams {\n            weights: Tensor1D::\u003cCpuBackend\u003e::new(vec![0.0, 0.0]),\n            bias: Scalar::\u003cCpuBackend\u003e::new(0.0),\n        };\n\n        let updated = sgd.step(\u0026params, \u0026zero_grads);\n\n        assert_eq!(updated.weights.to_vec(), vec![1.0, 2.0]);\n        assert_eq!(updated.bias.data.to_f64(), 0.5);\n    }\n\n    #[test]\n    fn test_sgd_step_zero_learning_rate() {\n        // Zero learning rate should not change parameters\n        let sgd = SGD::\u003cCpuBackend\u003e::new(0.0);\n        let params = LinearParams {\n            weights: Tensor1D::\u003cCpuBackend\u003e::new(vec![1.0, 2.0]),\n            bias: Scalar::\u003cCpuBackend\u003e::new(0.5),\n        };\n        let grads = LinearParams {\n            weights: Tensor1D::\u003cCpuBackend\u003e::new(vec![1.0, 1.0]),\n            bias: Scalar::\u003cCpuBackend\u003e::new(1.0),\n        };\n\n        let updated = sgd.step(\u0026params, \u0026grads);\n\n        // Parameters should remain unchanged\n        assert_eq!(updated.weights.to_vec(), vec![1.0, 2.0]);\n        assert_eq!(updated.bias.data.to_f64(), 0.5);\n    }\n\n    #[test]\n    fn test_sgd_step_negative_learning_rate() {\n        // Negative learning rate should move in opposite direction\n        let sgd = SGD::\u003cCpuBackend\u003e::new(-0.1);\n        let params = LinearParams {\n            weights: Tensor1D::\u003cCpuBackend\u003e::new(vec![1.0]),\n            bias: Scalar::\u003cCpuBackend\u003e::new(0.0),\n        };\n        let grads = LinearParams {\n            weights: Tensor1D::\u003cCpuBackend\u003e::new(vec![1.0]),\n            bias: Scalar::\u003cCpuBackend\u003e::new(0.5),\n        };\n\n        let updated = sgd.step(\u0026params, \u0026grads);\n\n        // With negative LR: params - (-0.1)*grad = params + 0.1*grad\n        // weight: 1.0 + 0.1*1.0 = 1.1\n        // bias: 0.0 + 0.1*0.5 = 0.05\n        assert_eq!(updated.weights.to_vec(), vec![1.1]);\n        assert_eq!(updated.bias.data.to_f64(), 0.05);\n    }\n\n    #[test]\n    fn test_sgd_step_very_large_learning_rate() {\n        let sgd = SGD::\u003cCpuBackend\u003e::new(100.0);\n        let params = LinearParams {\n            weights: Tensor1D::\u003cCpuBackend\u003e::new(vec![1.0]),\n            bias: Scalar::\u003cCpuBackend\u003e::new(0.0),\n        };\n        let grads = LinearParams {\n            weights: Tensor1D::\u003cCpuBackend\u003e::new(vec![0.01]),\n            bias: Scalar::\u003cCpuBackend\u003e::new(0.01),\n        };\n\n        let updated = sgd.step(\u0026params, \u0026grads);\n\n        // weight: 1.0 - 100.0*0.01 = 0.0\n        // bias: 0.0 - 100.0*0.01 = -1.0\n        assert!((updated.weights.to_vec()[0] - 0.0).abs() \u003c 1e-6);\n        assert!((updated.bias.data.to_f64() - (-1.0)).abs() \u003c 1e-6);\n    }\n\n    #[test]\n    fn test_sgd_step_very_large_gradients() {\n        let sgd = SGD::\u003cCpuBackend\u003e::new(0.01);\n        let params = LinearParams {\n            weights: Tensor1D::\u003cCpuBackend\u003e::new(vec![0.0]),\n            bias: Scalar::\u003cCpuBackend\u003e::new(0.0),\n        };\n        let grads = LinearParams {\n            weights: Tensor1D::\u003cCpuBackend\u003e::new(vec![1000.0]),\n            bias: Scalar::\u003cCpuBackend\u003e::new(1000.0),\n        };\n\n        let updated = sgd.step(\u0026params, \u0026grads);\n\n        // weight: 0.0 - 0.01*1000.0 = -10.0\n        // bias: 0.0 - 0.01*1000.0 = -10.0\n        assert_eq!(updated.weights.to_vec(), vec![-10.0]);\n        assert_eq!(updated.bias.data.to_f64(), -10.0);\n    }\n\n    #[test]\n    fn test_sgd_step_negative_gradients() {\n        let sgd = SGD::\u003cCpuBackend\u003e::new(0.1);\n        let params = LinearParams {\n            weights: Tensor1D::\u003cCpuBackend\u003e::new(vec![1.0]),\n            bias: Scalar::\u003cCpuBackend\u003e::new(0.0),\n        };\n        let grads = LinearParams {\n            weights: Tensor1D::\u003cCpuBackend\u003e::new(vec![-1.0]),\n            bias: Scalar::\u003cCpuBackend\u003e::new(-0.5),\n        };\n\n        let updated = sgd.step(\u0026params, \u0026grads);\n\n        // weight: 1.0 - 0.1*(-1.0) = 1.1\n        // bias: 0.0 - 0.1*(-0.5) = 0.05\n        assert_eq!(updated.weights.to_vec(), vec![1.1]);\n        assert_eq!(updated.bias.data.to_f64(), 0.05);\n    }\n\n    #[test]\n    fn test_sgd_step_fractional_values() {\n        let sgd = SGD::\u003cCpuBackend\u003e::new(0.125);\n        let params = LinearParams {\n            weights: Tensor1D::\u003cCpuBackend\u003e::new(vec![0.5]),\n            bias: Scalar::\u003cCpuBackend\u003e::new(0.25),\n        };\n        let grads = LinearParams {\n            weights: Tensor1D::\u003cCpuBackend\u003e::new(vec![0.8]),\n            bias: Scalar::\u003cCpuBackend\u003e::new(0.4),\n        };\n\n        let updated = sgd.step(\u0026params, \u0026grads);\n\n        // weight: 0.5 - 0.125*0.8 = 0.5 - 0.1 = 0.4\n        // bias: 0.25 - 0.125*0.4 = 0.25 - 0.05 = 0.2\n        // Use approximate comparison for floating point\n        assert!((updated.weights.to_vec()[0] - 0.4).abs() \u003c 1e-6);\n        assert!((updated.bias.data.to_f64() - 0.2).abs() \u003c 1e-6);\n    }\n\n    #[test]\n    fn test_sgd_step_does_not_mutate_inputs() {\n        // Verify that step() does not mutate the original params or grads\n        let sgd = SGD::\u003cCpuBackend\u003e::new(0.1);\n        let original_params = LinearParams {\n            weights: Tensor1D::\u003cCpuBackend\u003e::new(vec![1.0, 2.0]),\n            bias: Scalar::\u003cCpuBackend\u003e::new(0.5),\n        };\n        let original_grads = LinearParams {\n            weights: Tensor1D::\u003cCpuBackend\u003e::new(vec![0.5, 0.3]),\n            bias: Scalar::\u003cCpuBackend\u003e::new(0.1),\n        };\n\n        // Clone to compare after step\n        let params_copy = original_params.clone();\n        let grads_copy = original_grads.clone();\n\n        let _ = sgd.step(\u0026original_params, \u0026original_grads);\n\n        // Original should be unchanged\n        assert_eq!(\n            original_params.weights.to_vec(),\n            params_copy.weights.to_vec()\n        );\n        assert_eq!(\n            original_params.bias.data.to_f64(),\n            params_copy.bias.data.to_f64()\n        );\n        assert_eq!(original_grads.weights.to_vec(), grads_copy.weights.to_vec());\n        assert_eq!(\n            original_grads.bias.data.to_f64(),\n            grads_copy.bias.data.to_f64()\n        );\n    }\n\n    #[test]\n    fn test_sgd_clone() {\n        let sgd = SGD::\u003cCpuBackend\u003e::new(0.01);\n        let sgd_clone = sgd.clone();\n\n        assert_eq!(sgd.learning_rate(), sgd_clone.learning_rate());\n\n        // Both should produce same updates\n        let params = LinearParams {\n            weights: Tensor1D::\u003cCpuBackend\u003e::new(vec![1.0]),\n            bias: Scalar::\u003cCpuBackend\u003e::new(0.0),\n        };\n        let grads = LinearParams {\n            weights: Tensor1D::\u003cCpuBackend\u003e::new(vec![0.5]),\n            bias: Scalar::\u003cCpuBackend\u003e::new(0.2),\n        };\n\n        let updated1 = sgd.step(\u0026params, \u0026grads);\n        let updated2 = sgd_clone.step(\u0026params, \u0026grads);\n\n        assert_eq!(updated1.weights.to_vec(), updated2.weights.to_vec());\n        assert_eq!(updated1.bias.data.to_f64(), updated2.bias.data.to_f64());\n    }\n\n    #[test]\n    fn test_sgd_very_small_learning_rate() {\n        let sgd = SGD::\u003cCpuBackend\u003e::new(1e-10);\n        let params = LinearParams {\n            weights: Tensor1D::\u003cCpuBackend\u003e::new(vec![1.0]),\n            bias: Scalar::\u003cCpuBackend\u003e::new(0.5),\n        };\n        let grads = LinearParams {\n            weights: Tensor1D::\u003cCpuBackend\u003e::new(vec![1000.0]),\n            bias: Scalar::\u003cCpuBackend\u003e::new(1000.0),\n        };\n\n        let updated = sgd.step(\u0026params, \u0026grads);\n\n        // With very small LR, parameters should barely change\n        // weight: 1.0 - 1e-10 * 1000.0 = 0.9999999\n        // bias: 0.5 - 1e-10 * 1000.0 = 0.4999999\n        assert!((updated.weights.to_vec()[0] - 0.9999999).abs() \u003c 1e-6);\n        assert!((updated.bias.data.to_f64() - 0.4999999).abs() \u003c 1e-6);\n    }\n}\n","traces":[{"line":98,"address":[3085632],"length":1,"stats":{"Line":2}},{"line":100,"address":[3085638],"length":1,"stats":{"Line":2}},{"line":105,"address":[3085616],"length":1,"stats":{"Line":2}},{"line":106,"address":[3085621],"length":1,"stats":{"Line":3}},{"line":115,"address":[3085586,3085592,3085200],"length":1,"stats":{"Line":1}},{"line":118,"address":[3085259],"length":1,"stats":{"Line":3}},{"line":119,"address":[3085304],"length":1,"stats":{"Line":5}},{"line":120,"address":[3085324],"length":1,"stats":{"Line":6}},{"line":124,"address":[3085378,3085445],"length":1,"stats":{"Line":15}},{"line":125,"address":[3085454],"length":1,"stats":{"Line":8}}],"covered":10,"coverable":10},{"path":["/","home","victor","ML","machinelearne-rs","lib","src","pipeline.rs"],"content":"// src/pipeline.rs\npub trait TrainablePipeline {\n    type Model;\n    type Loss;\n    type Optimizer;\n    type Backend;\n\n    fn fit(\n        trainer: \u0026Trainer\u003cSelf::Model, Self::Loss, Self::Optimizer\u003e,\n        model: Self::Model,\n        x: \u0026\u003cSelf::Backend as Backend\u003e::Tensor2D,\n        y: \u0026\u003cSelf::Backend as Backend\u003e::Tensor1D,\n    ) -\u003e \u003cSelf::Model as Model\u003e::Fitted;\n}\n","traces":[],"covered":0,"coverable":0},{"path":["/","home","victor","ML","machinelearne-rs","lib","src","preprocessing","column_transformer","column_transformer.rs"],"content":"//! ColumnTransformer implementation.\n//!\n//! Applies different transformers to different column subsets and concatenates results.\n\nuse crate::backend::{Backend, Tensor2D};\nuse crate::preprocessing::encoding::{\n    FittedOneHotEncoder, FittedOrdinalEncoder, OneHotEncoder, OneHotEncoderParams, OrdinalEncoder,\n    OrdinalEncoderParams,\n};\nuse crate::preprocessing::error::PreprocessingError;\nuse crate::preprocessing::imputation::{FittedSimpleImputer, SimpleImputer, SimpleImputerParams};\nuse crate::preprocessing::pipeline::pipeline::{FittedPipeline, Pipeline, PipelineStepEnum};\nuse crate::preprocessing::scaling::{\n    FittedMaxAbsScaler, FittedMinMaxScaler, FittedNormalizer, FittedRobustScaler,\n    FittedStandardScaler, MaxAbsScaler, MaxAbsScalerParams, MinMaxScaler, MinMaxScalerParams,\n    Normalizer, NormalizerParams, RobustScaler, RobustScalerParams, StandardScaler,\n    StandardScalerParams,\n};\nuse crate::preprocessing::traits::{FittedTransformer, Transformer};\nuse crate::serialization::SerializableParams;\nuse serde::{Deserialize, Serialize};\nuse std::marker::PhantomData;\nuse std::ops::Range;\n\n/// Specifies which columns a transformer should be applied to.\n#[derive(Clone, Debug)]\npub enum ColumnSpec {\n    /// Apply to specific column indices.\n    Indices(Vec\u003cusize\u003e),\n    /// Apply to a range of columns.\n    Range(Range\u003cusize\u003e),\n    /// Apply to all columns.\n    All,\n}\n\nimpl ColumnSpec {\n    /// Resolve the column spec to actual column indices.\n    fn resolve(\u0026self, n_features: usize) -\u003e Vec\u003cusize\u003e {\n        match self {\n            ColumnSpec::Indices(indices) =\u003e indices.clone(),\n            ColumnSpec::Range(range) =\u003e range.clone().collect(),\n            ColumnSpec::All =\u003e (0..n_features).collect(),\n        }\n    }\n}\n\n/// Enum of unfitted transformers that can be used in a ColumnTransformer.\n#[derive(Clone)]\npub enum ColumnTransformerStep\u003cB: Backend\u003e {\n    StandardScaler(StandardScaler\u003cB\u003e),\n    MinMaxScaler(MinMaxScaler\u003cB\u003e),\n    RobustScaler(RobustScaler\u003cB\u003e),\n    MaxAbsScaler(MaxAbsScaler\u003cB\u003e),\n    Normalizer(Normalizer\u003cB\u003e),\n    SimpleImputer(SimpleImputer\u003cB\u003e),\n    OneHotEncoder(OneHotEncoder\u003cB\u003e),\n    OrdinalEncoder(OrdinalEncoder\u003cB\u003e),\n    Pipeline(Pipeline\u003cB\u003e),\n}\n\n/// Enum of fitted transformers for ColumnTransformer.\n#[derive(Clone)]\npub enum FittedColumnTransformerStep\u003cB: Backend\u003e {\n    StandardScaler(FittedStandardScaler\u003cB\u003e),\n    MinMaxScaler(FittedMinMaxScaler\u003cB\u003e),\n    RobustScaler(FittedRobustScaler\u003cB\u003e),\n    MaxAbsScaler(FittedMaxAbsScaler\u003cB\u003e),\n    Normalizer(FittedNormalizer\u003cB\u003e),\n    SimpleImputer(FittedSimpleImputer\u003cB\u003e),\n    OneHotEncoder(FittedOneHotEncoder\u003cB\u003e),\n    OrdinalEncoder(FittedOrdinalEncoder\u003cB\u003e),\n    Pipeline(FittedPipeline\u003cB\u003e),\n}\n\nimpl\u003cB: Backend\u003e FittedColumnTransformerStep\u003cB\u003e {\n    /// Transform the data.\n    fn transform(\u0026self, data: \u0026Tensor2D\u003cB\u003e) -\u003e Result\u003cTensor2D\u003cB\u003e, PreprocessingError\u003e {\n        match self {\n            FittedColumnTransformerStep::StandardScaler(t) =\u003e t.transform(data),\n            FittedColumnTransformerStep::MinMaxScaler(t) =\u003e t.transform(data),\n            FittedColumnTransformerStep::RobustScaler(t) =\u003e t.transform(data),\n            FittedColumnTransformerStep::MaxAbsScaler(t) =\u003e t.transform(data),\n            FittedColumnTransformerStep::Normalizer(t) =\u003e t.transform(data),\n            FittedColumnTransformerStep::SimpleImputer(t) =\u003e t.transform(data),\n            FittedColumnTransformerStep::OneHotEncoder(t) =\u003e t.transform(data),\n            FittedColumnTransformerStep::OrdinalEncoder(t) =\u003e t.transform(data),\n            FittedColumnTransformerStep::Pipeline(t) =\u003e t.transform(data),\n        }\n    }\n\n    /// Get the step name.\n    fn step_name(\u0026self) -\u003e \u0026'static str {\n        match self {\n            FittedColumnTransformerStep::StandardScaler(_) =\u003e \"StandardScaler\",\n            FittedColumnTransformerStep::MinMaxScaler(_) =\u003e \"MinMaxScaler\",\n            FittedColumnTransformerStep::RobustScaler(_) =\u003e \"RobustScaler\",\n            FittedColumnTransformerStep::MaxAbsScaler(_) =\u003e \"MaxAbsScaler\",\n            FittedColumnTransformerStep::Normalizer(_) =\u003e \"Normalizer\",\n            FittedColumnTransformerStep::SimpleImputer(_) =\u003e \"SimpleImputer\",\n            FittedColumnTransformerStep::OneHotEncoder(_) =\u003e \"OneHotEncoder\",\n            FittedColumnTransformerStep::OrdinalEncoder(_) =\u003e \"OrdinalEncoder\",\n            FittedColumnTransformerStep::Pipeline(_) =\u003e \"Pipeline\",\n        }\n    }\n\n    /// Get the number of output features.\n    fn n_features_out(\u0026self) -\u003e usize {\n        match self {\n            FittedColumnTransformerStep::StandardScaler(t) =\u003e t.n_features_in(),\n            FittedColumnTransformerStep::MinMaxScaler(t) =\u003e t.n_features_in(),\n            FittedColumnTransformerStep::RobustScaler(t) =\u003e t.n_features_in(),\n            FittedColumnTransformerStep::MaxAbsScaler(t) =\u003e t.n_features_in(),\n            FittedColumnTransformerStep::Normalizer(t) =\u003e t.n_features_in(),\n            FittedColumnTransformerStep::SimpleImputer(t) =\u003e t.n_features_in(),\n            FittedColumnTransformerStep::OneHotEncoder(t) =\u003e t.n_features_out(),\n            FittedColumnTransformerStep::OrdinalEncoder(t) =\u003e t.n_features_in(),\n            FittedColumnTransformerStep::Pipeline(t) =\u003e t.n_features_in(),\n        }\n    }\n}\n\n/// Fit a column transformer step from an unfitted step.\nfn fit_step\u003cB: Backend\u003e(\n    step: \u0026ColumnTransformerStep\u003cB\u003e,\n    data: \u0026Tensor2D\u003cB\u003e,\n) -\u003e Result\u003cFittedColumnTransformerStep\u003cB\u003e, PreprocessingError\u003e {\n    match step {\n        ColumnTransformerStep::StandardScaler(t) =\u003e {\n            t.fit(data).map(FittedColumnTransformerStep::StandardScaler)\n        }\n        ColumnTransformerStep::MinMaxScaler(t) =\u003e {\n            t.fit(data).map(FittedColumnTransformerStep::MinMaxScaler)\n        }\n        ColumnTransformerStep::RobustScaler(t) =\u003e {\n            t.fit(data).map(FittedColumnTransformerStep::RobustScaler)\n        }\n        ColumnTransformerStep::MaxAbsScaler(t) =\u003e {\n            t.fit(data).map(FittedColumnTransformerStep::MaxAbsScaler)\n        }\n        ColumnTransformerStep::Normalizer(t) =\u003e {\n            t.fit(data).map(FittedColumnTransformerStep::Normalizer)\n        }\n        ColumnTransformerStep::SimpleImputer(t) =\u003e {\n            t.fit(data).map(FittedColumnTransformerStep::SimpleImputer)\n        }\n        ColumnTransformerStep::OneHotEncoder(t) =\u003e {\n            t.fit(data).map(FittedColumnTransformerStep::OneHotEncoder)\n        }\n        ColumnTransformerStep::OrdinalEncoder(t) =\u003e {\n            t.fit(data).map(FittedColumnTransformerStep::OrdinalEncoder)\n        }\n        ColumnTransformerStep::Pipeline(t) =\u003e {\n            t.fit(data).map(FittedColumnTransformerStep::Pipeline)\n        }\n    }\n}\n\n/// ColumnTransformer applies different transformers to different columns.\n///\n/// This is useful when you have heterogeneous data and want to apply\n/// different preprocessing to different feature subsets (e.g., scale\n/// numerical features, one-hot encode categorical features).\n///\n/// # Example\n/// ```ignore\n/// use machinelearne_rs::preprocessing::{\n///     ColumnTransformer, ColumnSpec, StandardScaler, OneHotEncoder, Transformer\n/// };\n/// use machinelearne_rs::backend::CpuBackend;\n///\n/// // Columns: [age, income, city_code]\n/// // Scale numerical cols [0, 1], one-hot encode col [2]\n/// let ct = ColumnTransformer::\u003cCpuBackend\u003e::new()\n///     .add(StandardScaler::new(), ColumnSpec::Indices(vec![0, 1]))\n///     .add(OneHotEncoder::new(), ColumnSpec::Indices(vec![2]));\n///\n/// let fitted = ct.fit(\u0026data)?;\n/// let transformed = fitted.transform(\u0026data)?;\n/// ```\n#[derive(Clone)]\npub struct ColumnTransformer\u003cB: Backend\u003e {\n    steps: Vec\u003c(ColumnSpec, ColumnTransformerStep\u003cB\u003e)\u003e,\n    _backend: PhantomData\u003cB\u003e,\n}\n\nimpl\u003cB: Backend\u003e Default for ColumnTransformer\u003cB\u003e {\n    fn default() -\u003e Self {\n        Self::new()\n    }\n}\n\nimpl\u003cB: Backend\u003e ColumnTransformer\u003cB\u003e {\n    /// Create a new empty ColumnTransformer.\n    pub fn new() -\u003e Self {\n        Self {\n            steps: Vec::new(),\n            _backend: PhantomData,\n        }\n    }\n\n    /// Add a StandardScaler for specified columns.\n    pub fn add_standard_scaler(mut self, scaler: StandardScaler\u003cB\u003e, spec: ColumnSpec) -\u003e Self {\n        self.steps\n            .push((spec, ColumnTransformerStep::StandardScaler(scaler)));\n        self\n    }\n\n    /// Add a MinMaxScaler for specified columns.\n    pub fn add_minmax_scaler(mut self, scaler: MinMaxScaler\u003cB\u003e, spec: ColumnSpec) -\u003e Self {\n        self.steps\n            .push((spec, ColumnTransformerStep::MinMaxScaler(scaler)));\n        self\n    }\n\n    /// Add a RobustScaler for specified columns.\n    pub fn add_robust_scaler(mut self, scaler: RobustScaler\u003cB\u003e, spec: ColumnSpec) -\u003e Self {\n        self.steps\n            .push((spec, ColumnTransformerStep::RobustScaler(scaler)));\n        self\n    }\n\n    /// Add a MaxAbsScaler for specified columns.\n    pub fn add_maxabs_scaler(mut self, scaler: MaxAbsScaler\u003cB\u003e, spec: ColumnSpec) -\u003e Self {\n        self.steps\n            .push((spec, ColumnTransformerStep::MaxAbsScaler(scaler)));\n        self\n    }\n\n    /// Add a Normalizer for specified columns.\n    pub fn add_normalizer(mut self, normalizer: Normalizer\u003cB\u003e, spec: ColumnSpec) -\u003e Self {\n        self.steps\n            .push((spec, ColumnTransformerStep::Normalizer(normalizer)));\n        self\n    }\n\n    /// Add a SimpleImputer for specified columns.\n    pub fn add_simple_imputer(mut self, imputer: SimpleImputer\u003cB\u003e, spec: ColumnSpec) -\u003e Self {\n        self.steps\n            .push((spec, ColumnTransformerStep::SimpleImputer(imputer)));\n        self\n    }\n\n    /// Add a OneHotEncoder for specified columns.\n    pub fn add_one_hot_encoder(mut self, encoder: OneHotEncoder\u003cB\u003e, spec: ColumnSpec) -\u003e Self {\n        self.steps\n            .push((spec, ColumnTransformerStep::OneHotEncoder(encoder)));\n        self\n    }\n\n    /// Add an OrdinalEncoder for specified columns.\n    pub fn add_ordinal_encoder(mut self, encoder: OrdinalEncoder\u003cB\u003e, spec: ColumnSpec) -\u003e Self {\n        self.steps\n            .push((spec, ColumnTransformerStep::OrdinalEncoder(encoder)));\n        self\n    }\n\n    /// Add a Pipeline for specified columns.\n    pub fn add_pipeline(mut self, pipeline: Pipeline\u003cB\u003e, spec: ColumnSpec) -\u003e Self {\n        self.steps\n            .push((spec, ColumnTransformerStep::Pipeline(pipeline)));\n        self\n    }\n\n    /// Add a generic step.\n    pub fn add(mut self, step: ColumnTransformerStep\u003cB\u003e, spec: ColumnSpec) -\u003e Self {\n        self.steps.push((spec, step));\n        self\n    }\n\n    /// Get the number of transformer steps.\n    pub fn len(\u0026self) -\u003e usize {\n        self.steps.len()\n    }\n\n    /// Check if empty.\n    pub fn is_empty(\u0026self) -\u003e bool {\n        self.steps.is_empty()\n    }\n}\n\n/// Serializable parameters for fitted column transformer step.\n#[derive(Clone, Serialize, Deserialize)]\npub struct StepParams {\n    /// Column indices this step was applied to.\n    pub columns: Vec\u003cusize\u003e,\n    /// Step type name.\n    pub step_type: String,\n    /// Serialized step parameters.\n    pub params: Vec\u003cu8\u003e,\n}\n\n/// Serializable parameters for a fitted ColumnTransformer.\n#[derive(Clone, Serialize, Deserialize)]\npub struct ColumnTransformerParams {\n    /// Number of input features.\n    pub n_features_in: usize,\n    /// Number of output features.\n    pub n_features_out: usize,\n    /// Step parameters.\n    pub steps: Vec\u003cStepParams\u003e,\n}\n\n/// Fitted ColumnTransformer ready for inference.\n#[derive(Clone)]\npub struct FittedColumnTransformer\u003cB: Backend\u003e {\n    /// Fitted steps with their column indices.\n    fitted_steps: Vec\u003c(Vec\u003cusize\u003e, FittedColumnTransformerStep\u003cB\u003e)\u003e,\n    /// Number of input features.\n    n_features_in: usize,\n    /// Number of output features.\n    n_features_out: usize,\n    _backend: PhantomData\u003cB\u003e,\n}\n\nimpl\u003cB: Backend\u003e FittedColumnTransformer\u003cB\u003e {\n    /// Get the number of input features.\n    pub fn n_features_in(\u0026self) -\u003e usize {\n        self.n_features_in\n    }\n\n    /// Get the number of output features.\n    pub fn n_features_out(\u0026self) -\u003e usize {\n        self.n_features_out\n    }\n\n    /// Get step names.\n    pub fn step_names(\u0026self) -\u003e Vec\u003c(\u0026'static str, \u0026[usize])\u003e {\n        self.fitted_steps\n            .iter()\n            .map(|(cols, step)| (step.step_name(), cols.as_slice()))\n            .collect()\n    }\n}\n\nimpl\u003cB: Backend\u003e Transformer\u003cB\u003e for ColumnTransformer\u003cB\u003e {\n    type Input = Tensor2D\u003cB\u003e;\n    type Output = Tensor2D\u003cB\u003e;\n    type Params = ColumnTransformerParams;\n    type Fitted = FittedColumnTransformer\u003cB\u003e;\n\n    fn fit(\u0026self, data: \u0026Self::Input) -\u003e Result\u003cSelf::Fitted, PreprocessingError\u003e {\n        let (rows, cols) = data.shape();\n\n        if rows == 0 {\n            return Err(PreprocessingError::EmptyData(\n                \"Cannot fit ColumnTransformer on empty data\".to_string(),\n            ));\n        }\n\n        if self.steps.is_empty() {\n            return Err(PreprocessingError::InvalidParameter(\n                \"Cannot fit empty ColumnTransformer\".to_string(),\n            ));\n        }\n\n        let mut fitted_steps = Vec::with_capacity(self.steps.len());\n        let mut n_features_out = 0;\n\n        for (spec, step) in \u0026self.steps {\n            let columns = spec.resolve(cols);\n\n            // Validate columns\n            for \u0026col in \u0026columns {\n                if col \u003e= cols {\n                    return Err(PreprocessingError::InvalidParameter(format!(\n                        \"Column index {} out of bounds (max {})\",\n                        col,\n                        cols - 1\n                    )));\n                }\n            }\n\n            // Extract columns\n            let col_data = extract_columns(data, \u0026columns)?;\n\n            // Fit transformer\n            let fitted = fit_step(step, \u0026col_data)?;\n\n            n_features_out += fitted.n_features_out();\n            fitted_steps.push((columns, fitted));\n        }\n\n        Ok(FittedColumnTransformer {\n            fitted_steps,\n            n_features_in: cols,\n            n_features_out,\n            _backend: PhantomData,\n        })\n    }\n\n    fn fit_transform(\u0026self, data: \u0026Self::Input) -\u003e Result\u003cSelf::Output, PreprocessingError\u003e {\n        let fitted = self.fit(data)?;\n        fitted.transform(data)\n    }\n}\n\nimpl\u003cB: Backend\u003e FittedTransformer\u003cB\u003e for FittedColumnTransformer\u003cB\u003e {\n    type Input = Tensor2D\u003cB\u003e;\n    type Output = Tensor2D\u003cB\u003e;\n    type Params = ColumnTransformerParams;\n\n    fn transform(\u0026self, data: \u0026Self::Input) -\u003e Result\u003cSelf::Output, PreprocessingError\u003e {\n        let (rows, cols) = data.shape();\n\n        if cols != self.n_features_in {\n            return Err(PreprocessingError::FeatureMismatch {\n                expected_features: self.n_features_in,\n                got_features: cols,\n            });\n        }\n\n        if rows == 0 {\n            return Ok(Tensor2D::zeros(0, self.n_features_out));\n        }\n\n        // Transform each step and collect outputs\n        let mut transformed_outputs = Vec::with_capacity(self.fitted_steps.len());\n\n        for (columns, step) in \u0026self.fitted_steps {\n            let col_data = extract_columns(data, columns)?;\n            let transformed = step.transform(\u0026col_data)?;\n            transformed_outputs.push(transformed);\n        }\n\n        // Concatenate all outputs horizontally\n        hcat_tensors(\u0026transformed_outputs)\n    }\n\n    fn inverse_transform(\u0026self, _data: \u0026Self::Output) -\u003e Result\u003cSelf::Input, PreprocessingError\u003e {\n        Err(PreprocessingError::InvalidParameter(\n            \"ColumnTransformer does not support inverse_transform\".to_string(),\n        ))\n    }\n\n    fn extract_params(\u0026self) -\u003e Self::Params {\n        let steps = self\n            .fitted_steps\n            .iter()\n            .map(|(columns, step)| {\n                let (step_type, params) = match step {\n                    FittedColumnTransformerStep::StandardScaler(t) =\u003e (\n                        \"StandardScaler\".to_string(),\n                        t.extract_params().to_bytes().unwrap(),\n                    ),\n                    FittedColumnTransformerStep::MinMaxScaler(t) =\u003e (\n                        \"MinMaxScaler\".to_string(),\n                        t.extract_params().to_bytes().unwrap(),\n                    ),\n                    FittedColumnTransformerStep::RobustScaler(t) =\u003e (\n                        \"RobustScaler\".to_string(),\n                        t.extract_params().to_bytes().unwrap(),\n                    ),\n                    FittedColumnTransformerStep::MaxAbsScaler(t) =\u003e (\n                        \"MaxAbsScaler\".to_string(),\n                        t.extract_params().to_bytes().unwrap(),\n                    ),\n                    FittedColumnTransformerStep::Normalizer(t) =\u003e (\n                        \"Normalizer\".to_string(),\n                        t.extract_params().to_bytes().unwrap(),\n                    ),\n                    FittedColumnTransformerStep::SimpleImputer(t) =\u003e (\n                        \"SimpleImputer\".to_string(),\n                        t.extract_params().to_bytes().unwrap(),\n                    ),\n                    FittedColumnTransformerStep::OneHotEncoder(t) =\u003e (\n                        \"OneHotEncoder\".to_string(),\n                        t.extract_params().to_bytes().unwrap(),\n                    ),\n                    FittedColumnTransformerStep::OrdinalEncoder(t) =\u003e (\n                        \"OrdinalEncoder\".to_string(),\n                        t.extract_params().to_bytes().unwrap(),\n                    ),\n                    FittedColumnTransformerStep::Pipeline(t) =\u003e {\n                        // Pipeline needs special handling\n                        let mut step_params = Vec::new();\n                        for s in t.steps() {\n                            let (name, bytes) = match s {\n                                PipelineStepEnum::StandardScaler(st) =\u003e {\n                                    (\"StandardScaler\", st.extract_params().to_bytes().unwrap())\n                                }\n                                PipelineStepEnum::MinMaxScaler(st) =\u003e {\n                                    (\"MinMaxScaler\", st.extract_params().to_bytes().unwrap())\n                                }\n                                PipelineStepEnum::RobustScaler(st) =\u003e {\n                                    (\"RobustScaler\", st.extract_params().to_bytes().unwrap())\n                                }\n                                PipelineStepEnum::MaxAbsScaler(st) =\u003e {\n                                    (\"MaxAbsScaler\", st.extract_params().to_bytes().unwrap())\n                                }\n                                PipelineStepEnum::Normalizer(st) =\u003e {\n                                    (\"Normalizer\", st.extract_params().to_bytes().unwrap())\n                                }\n                                PipelineStepEnum::SimpleImputer(st) =\u003e {\n                                    (\"SimpleImputer\", st.extract_params().to_bytes().unwrap())\n                                }\n                                PipelineStepEnum::OneHotEncoder(st) =\u003e {\n                                    (\"OneHotEncoder\", st.extract_params().to_bytes().unwrap())\n                                }\n                                PipelineStepEnum::OrdinalEncoder(st) =\u003e {\n                                    (\"OrdinalEncoder\", st.extract_params().to_bytes().unwrap())\n                                }\n                            };\n                            step_params.push((name.to_string(), bytes));\n                        }\n                        (\n                            \"Pipeline\".to_string(),\n                            bincode::serialize(\u0026step_params).unwrap(),\n                        )\n                    }\n                };\n                StepParams {\n                    columns: columns.clone(),\n                    step_type,\n                    params,\n                }\n            })\n            .collect();\n\n        ColumnTransformerParams {\n            n_features_in: self.n_features_in,\n            n_features_out: self.n_features_out,\n            steps,\n        }\n    }\n\n    fn from_params(params: Self::Params) -\u003e Result\u003cSelf, PreprocessingError\u003e {\n        let mut fitted_steps = Vec::with_capacity(params.steps.len());\n\n        for step_params in params.steps {\n            let step = match step_params.step_type.as_str() {\n                \"StandardScaler\" =\u003e {\n                    let p: StandardScalerParams = bincode::deserialize(\u0026step_params.params)\n                        .map_err(|e| PreprocessingError::SerializationError(e.to_string()))?;\n                    FittedColumnTransformerStep::StandardScaler(FittedStandardScaler::from_params(\n                        p,\n                    )?)\n                }\n                \"MinMaxScaler\" =\u003e {\n                    let p: MinMaxScalerParams = bincode::deserialize(\u0026step_params.params)\n                        .map_err(|e| PreprocessingError::SerializationError(e.to_string()))?;\n                    FittedColumnTransformerStep::MinMaxScaler(FittedMinMaxScaler::from_params(p)?)\n                }\n                \"RobustScaler\" =\u003e {\n                    let p: RobustScalerParams = bincode::deserialize(\u0026step_params.params)\n                        .map_err(|e| PreprocessingError::SerializationError(e.to_string()))?;\n                    FittedColumnTransformerStep::RobustScaler(FittedRobustScaler::from_params(p)?)\n                }\n                \"MaxAbsScaler\" =\u003e {\n                    let p: MaxAbsScalerParams = bincode::deserialize(\u0026step_params.params)\n                        .map_err(|e| PreprocessingError::SerializationError(e.to_string()))?;\n                    FittedColumnTransformerStep::MaxAbsScaler(FittedMaxAbsScaler::from_params(p)?)\n                }\n                \"Normalizer\" =\u003e {\n                    let p: NormalizerParams = bincode::deserialize(\u0026step_params.params)\n                        .map_err(|e| PreprocessingError::SerializationError(e.to_string()))?;\n                    FittedColumnTransformerStep::Normalizer(FittedNormalizer::from_params(p)?)\n                }\n                \"SimpleImputer\" =\u003e {\n                    let p: SimpleImputerParams = bincode::deserialize(\u0026step_params.params)\n                        .map_err(|e| PreprocessingError::SerializationError(e.to_string()))?;\n                    FittedColumnTransformerStep::SimpleImputer(FittedSimpleImputer::from_params(p)?)\n                }\n                \"OneHotEncoder\" =\u003e {\n                    let p: OneHotEncoderParams = bincode::deserialize(\u0026step_params.params)\n                        .map_err(|e| PreprocessingError::SerializationError(e.to_string()))?;\n                    FittedColumnTransformerStep::OneHotEncoder(FittedOneHotEncoder::from_params(p)?)\n                }\n                \"OrdinalEncoder\" =\u003e {\n                    let p: OrdinalEncoderParams = bincode::deserialize(\u0026step_params.params)\n                        .map_err(|e| PreprocessingError::SerializationError(e.to_string()))?;\n                    FittedColumnTransformerStep::OrdinalEncoder(FittedOrdinalEncoder::from_params(\n                        p,\n                    )?)\n                }\n                \"Pipeline\" =\u003e {\n                    let inner_steps: Vec\u003c(String, Vec\u003cu8\u003e)\u003e =\n                        bincode::deserialize(\u0026step_params.params)\n                            .map_err(|e| PreprocessingError::SerializationError(e.to_string()))?;\n                    let mut steps = Vec::new();\n                    for (name, bytes) in inner_steps {\n                        let step = match name.as_str() {\n                            \"StandardScaler\" =\u003e {\n                                let p: StandardScalerParams = bincode::deserialize(\u0026bytes)\n                                    .map_err(|e| {\n                                        PreprocessingError::SerializationError(e.to_string())\n                                    })?;\n                                PipelineStepEnum::StandardScaler(FittedStandardScaler::from_params(\n                                    p,\n                                )?)\n                            }\n                            \"MinMaxScaler\" =\u003e {\n                                let p: MinMaxScalerParams =\n                                    bincode::deserialize(\u0026bytes).map_err(|e| {\n                                        PreprocessingError::SerializationError(e.to_string())\n                                    })?;\n                                PipelineStepEnum::MinMaxScaler(FittedMinMaxScaler::from_params(p)?)\n                            }\n                            \"RobustScaler\" =\u003e {\n                                let p: RobustScalerParams =\n                                    bincode::deserialize(\u0026bytes).map_err(|e| {\n                                        PreprocessingError::SerializationError(e.to_string())\n                                    })?;\n                                PipelineStepEnum::RobustScaler(FittedRobustScaler::from_params(p)?)\n                            }\n                            \"MaxAbsScaler\" =\u003e {\n                                let p: MaxAbsScalerParams =\n                                    bincode::deserialize(\u0026bytes).map_err(|e| {\n                                        PreprocessingError::SerializationError(e.to_string())\n                                    })?;\n                                PipelineStepEnum::MaxAbsScaler(FittedMaxAbsScaler::from_params(p)?)\n                            }\n                            \"Normalizer\" =\u003e {\n                                let p: NormalizerParams =\n                                    bincode::deserialize(\u0026bytes).map_err(|e| {\n                                        PreprocessingError::SerializationError(e.to_string())\n                                    })?;\n                                PipelineStepEnum::Normalizer(FittedNormalizer::from_params(p)?)\n                            }\n                            \"SimpleImputer\" =\u003e {\n                                let p: SimpleImputerParams =\n                                    bincode::deserialize(\u0026bytes).map_err(|e| {\n                                        PreprocessingError::SerializationError(e.to_string())\n                                    })?;\n                                PipelineStepEnum::SimpleImputer(FittedSimpleImputer::from_params(\n                                    p,\n                                )?)\n                            }\n                            \"OneHotEncoder\" =\u003e {\n                                let p: OneHotEncoderParams =\n                                    bincode::deserialize(\u0026bytes).map_err(|e| {\n                                        PreprocessingError::SerializationError(e.to_string())\n                                    })?;\n                                PipelineStepEnum::OneHotEncoder(FittedOneHotEncoder::from_params(\n                                    p,\n                                )?)\n                            }\n                            \"OrdinalEncoder\" =\u003e {\n                                let p: OrdinalEncoderParams = bincode::deserialize(\u0026bytes)\n                                    .map_err(|e| {\n                                        PreprocessingError::SerializationError(e.to_string())\n                                    })?;\n                                PipelineStepEnum::OrdinalEncoder(FittedOrdinalEncoder::from_params(\n                                    p,\n                                )?)\n                            }\n                            _ =\u003e {\n                                return Err(PreprocessingError::SerializationError(format!(\n                                    \"Unknown step type: {}\",\n                                    name\n                                )))\n                            }\n                        };\n                        steps.push(step);\n                    }\n                    // Get n_features from the first step\n                    let n_features = steps.first().map(|s| s.n_features_in()).unwrap_or(0);\n                    FittedColumnTransformerStep::Pipeline(FittedPipeline::from_steps(\n                        steps, n_features,\n                    ))\n                }\n                _ =\u003e {\n                    return Err(PreprocessingError::SerializationError(format!(\n                        \"Unknown step type: {}\",\n                        step_params.step_type\n                    )))\n                }\n            };\n            fitted_steps.push((step_params.columns, step));\n        }\n\n        Ok(FittedColumnTransformer {\n            fitted_steps,\n            n_features_in: params.n_features_in,\n            n_features_out: params.n_features_out,\n            _backend: PhantomData,\n        })\n    }\n\n    fn n_features_in(\u0026self) -\u003e usize {\n        self.n_features_in\n    }\n}\n\n/// Extract specified columns from a 2D tensor.\nfn extract_columns\u003cB: Backend\u003e(\n    data: \u0026Tensor2D\u003cB\u003e,\n    columns: \u0026[usize],\n) -\u003e Result\u003cTensor2D\u003cB\u003e, PreprocessingError\u003e {\n    if columns.is_empty() {\n        let (rows, _) = data.shape();\n        return Ok(Tensor2D::zeros(rows, 0));\n    }\n\n    // Use the backend method to select columns\n    let inner = B::select_columns_2d(\u0026data.data, columns);\n    Ok(Tensor2D {\n        data: inner,\n        backend: PhantomData,\n    })\n}\n\n/// Horizontally concatenate tensors.\nfn hcat_tensors\u003cB: Backend\u003e(tensors: \u0026[Tensor2D\u003cB\u003e]) -\u003e Result\u003cTensor2D\u003cB\u003e, PreprocessingError\u003e {\n    if tensors.is_empty() {\n        return Err(PreprocessingError::InvalidParameter(\n            \"Cannot concatenate empty slice of tensors\".to_string(),\n        ));\n    }\n\n    let inner_tensors: Vec\u003c_\u003e = tensors.iter().map(|t| t.data.clone()).collect();\n    let result = B::hcat_2d(\u0026inner_tensors)?;\n    Ok(Tensor2D {\n        data: result,\n        backend: PhantomData,\n    })\n}\n\n#[cfg(test)]\nmod tests {\n    use super::*;\n    use crate::backend::CpuBackend;\n\n    #[test]\n    fn test_column_transformer_basic() {\n        // [[1, 10], [2, 20], [3, 30]]\n        let data = Tensor2D::\u003cCpuBackend\u003e::new(vec![1.0f32, 10.0, 2.0, 20.0, 3.0, 30.0], 3, 2);\n\n        let ct = ColumnTransformer::\u003cCpuBackend\u003e::new()\n            .add_standard_scaler(StandardScaler::new(), ColumnSpec::Indices(vec![0]))\n            .add_standard_scaler(StandardScaler::new(), ColumnSpec::Indices(vec![1]));\n\n        let fitted = ct.fit(\u0026data).unwrap();\n        assert_eq!(fitted.n_features_in(), 2);\n        assert_eq!(fitted.n_features_out(), 2);\n\n        let transformed = fitted.transform(\u0026data).unwrap();\n        assert_eq!(transformed.shape(), (3, 2));\n    }\n\n    #[test]\n    fn test_column_transformer_one_hot() {\n        // [[0], [1], [0]]\n        let data = Tensor2D::\u003cCpuBackend\u003e::new(vec![0.0f32, 1.0, 0.0], 3, 1);\n\n        let ct = ColumnTransformer::\u003cCpuBackend\u003e::new()\n            .add_one_hot_encoder(OneHotEncoder::new(), ColumnSpec::All);\n\n        let fitted = ct.fit(\u0026data).unwrap();\n        assert_eq!(fitted.n_features_out(), 2); // 2 categories\n\n        let transformed = fitted.transform(\u0026data).unwrap();\n        assert_eq!(transformed.shape(), (3, 2));\n    }\n\n    #[test]\n    fn test_column_transformer_mixed() {\n        // [[1, 10, 0], [2, 20, 1], [3, 30, 0]] - cols 0,1 numeric, col 2 categorical\n        let data = Tensor2D::\u003cCpuBackend\u003e::new(\n            vec![1.0f32, 10.0, 0.0, 2.0, 20.0, 1.0, 3.0, 30.0, 0.0],\n            3,\n            3,\n        );\n\n        let ct = ColumnTransformer::\u003cCpuBackend\u003e::new()\n            .add_standard_scaler(StandardScaler::new(), ColumnSpec::Range(0..2))\n            .add_one_hot_encoder(OneHotEncoder::new(), ColumnSpec::Indices(vec![2]));\n\n        let fitted = ct.fit(\u0026data).unwrap();\n        // StandardScaler outputs 2 cols, OneHotEncoder outputs 2 cols = 4 total\n        assert_eq!(fitted.n_features_out(), 4);\n\n        let transformed = fitted.transform(\u0026data).unwrap();\n        assert_eq!(transformed.shape(), (3, 4));\n    }\n\n    #[test]\n    fn test_column_transformer_with_pipeline() {\n        let data = Tensor2D::\u003cCpuBackend\u003e::new(vec![1.0f32, 10.0, 2.0, 20.0, 3.0, 30.0], 3, 2);\n\n        let pipeline = Pipeline::\u003cCpuBackend\u003e::new()\n            .add_standard_scaler(StandardScaler::new())\n            .add_minmax_scaler(MinMaxScaler::new().with_range(0.0, 1.0));\n\n        let ct = ColumnTransformer::\u003cCpuBackend\u003e::new().add_pipeline(pipeline, ColumnSpec::All);\n\n        let fitted = ct.fit(\u0026data).unwrap();\n        let transformed = fitted.transform(\u0026data).unwrap();\n\n        // Check values are in [0, 1] range after MinMaxScaler\n        let vals = transformed.ravel().to_vec();\n        for \u0026v in \u0026vals {\n            assert!(v \u003e= -1e-6 \u0026\u0026 v \u003c= 1.0 + 1e-6);\n        }\n    }\n\n    #[test]\n    fn test_column_transformer_serialization() {\n        let data = Tensor2D::\u003cCpuBackend\u003e::new(\n            vec![1.0f32, 10.0, 0.0, 2.0, 20.0, 1.0, 3.0, 30.0, 0.0],\n            3,\n            3,\n        );\n\n        let ct = ColumnTransformer::\u003cCpuBackend\u003e::new()\n            .add_standard_scaler(StandardScaler::new(), ColumnSpec::Indices(vec![0, 1]))\n            .add_one_hot_encoder(OneHotEncoder::new(), ColumnSpec::Indices(vec![2]));\n\n        let fitted = ct.fit(\u0026data).unwrap();\n\n        let temp_file = std::env::temp_dir().join(\"test_column_transformer.bin\");\n        fitted.save_to_file(\u0026temp_file).unwrap();\n\n        let loaded = FittedColumnTransformer::\u003cCpuBackend\u003e::load_from_file(\u0026temp_file).unwrap();\n\n        assert_eq!(loaded.n_features_in(), fitted.n_features_in());\n        assert_eq!(loaded.n_features_out(), fitted.n_features_out());\n\n        // Compare transform results\n        let t1 = fitted.transform(\u0026data).unwrap();\n        let t2 = loaded.transform(\u0026data).unwrap();\n\n        let v1 = t1.ravel().to_vec();\n        let v2 = t2.ravel().to_vec();\n        for (a, b) in v1.iter().zip(v2.iter()) {\n            assert!((a - b).abs() \u003c 1e-6);\n        }\n\n        std::fs::remove_file(temp_file).ok();\n    }\n\n    #[test]\n    fn test_column_transformer_feature_mismatch() {\n        let train = Tensor2D::\u003cCpuBackend\u003e::new(vec![1.0f32, 10.0, 2.0, 20.0], 2, 2);\n        let test = Tensor2D::\u003cCpuBackend\u003e::new(vec![1.0f32, 10.0, 2.0], 1, 3);\n\n        let ct = ColumnTransformer::\u003cCpuBackend\u003e::new()\n            .add_standard_scaler(StandardScaler::new(), ColumnSpec::All);\n\n        let fitted = ct.fit(\u0026train).unwrap();\n\n        let result = fitted.transform(\u0026test);\n        assert!(matches!(\n            result,\n            Err(PreprocessingError::FeatureMismatch { .. })\n        ));\n    }\n\n    #[test]\n    fn test_column_transformer_column_out_of_bounds() {\n        let data = Tensor2D::\u003cCpuBackend\u003e::new(vec![1.0f32, 10.0], 1, 2);\n\n        let ct = ColumnTransformer::\u003cCpuBackend\u003e::new()\n            .add_standard_scaler(StandardScaler::new(), ColumnSpec::Indices(vec![5]));\n\n        let result = ct.fit(\u0026data);\n        assert!(matches!(\n            result,\n            Err(PreprocessingError::InvalidParameter(_))\n        ));\n    }\n}\n","traces":[{"line":38,"address":[2519104],"length":1,"stats":{"Line":6}},{"line":39,"address":[2519137],"length":1,"stats":{"Line":6}},{"line":40,"address":[2519199],"length":1,"stats":{"Line":3}},{"line":41,"address":[2519223],"length":1,"stats":{"Line":1}},{"line":42,"address":[2519261],"length":1,"stats":{"Line":2}},{"line":77,"address":[3373728],"length":1,"stats":{"Line":1}},{"line":78,"address":[3373765],"length":1,"stats":{"Line":1}},{"line":79,"address":[3373839],"length":1,"stats":{"Line":1}},{"line":80,"address":[3373873],"length":1,"stats":{"Line":0}},{"line":81,"address":[3373903],"length":1,"stats":{"Line":0}},{"line":82,"address":[3373937],"length":1,"stats":{"Line":0}},{"line":83,"address":[3373971],"length":1,"stats":{"Line":0}},{"line":84,"address":[3374002],"length":1,"stats":{"Line":0}},{"line":85,"address":[3374033],"length":1,"stats":{"Line":1}},{"line":86,"address":[3374064],"length":1,"stats":{"Line":0}},{"line":87,"address":[3374095],"length":1,"stats":{"Line":1}},{"line":92,"address":[],"length":0,"stats":{"Line":0}},{"line":93,"address":[],"length":0,"stats":{"Line":0}},{"line":94,"address":[],"length":0,"stats":{"Line":0}},{"line":95,"address":[],"length":0,"stats":{"Line":0}},{"line":96,"address":[],"length":0,"stats":{"Line":0}},{"line":97,"address":[],"length":0,"stats":{"Line":0}},{"line":98,"address":[],"length":0,"stats":{"Line":0}},{"line":99,"address":[],"length":0,"stats":{"Line":0}},{"line":100,"address":[],"length":0,"stats":{"Line":0}},{"line":101,"address":[],"length":0,"stats":{"Line":0}},{"line":102,"address":[],"length":0,"stats":{"Line":0}},{"line":107,"address":[3373408],"length":1,"stats":{"Line":1}},{"line":108,"address":[3373421],"length":1,"stats":{"Line":1}},{"line":109,"address":[3373484],"length":1,"stats":{"Line":4}},{"line":110,"address":[3373512],"length":1,"stats":{"Line":0}},{"line":111,"address":[3373536],"length":1,"stats":{"Line":0}},{"line":112,"address":[3373564],"length":1,"stats":{"Line":0}},{"line":113,"address":[3373589],"length":1,"stats":{"Line":0}},{"line":114,"address":[3373614],"length":1,"stats":{"Line":0}},{"line":115,"address":[3373639],"length":1,"stats":{"Line":1}},{"line":116,"address":[3373664],"length":1,"stats":{"Line":0}},{"line":117,"address":[3373689],"length":1,"stats":{"Line":1}},{"line":123,"address":[3374128],"length":1,"stats":{"Line":6}},{"line":127,"address":[3374170],"length":1,"stats":{"Line":6}},{"line":128,"address":[3374210],"length":1,"stats":{"Line":4}},{"line":129,"address":[3374222],"length":1,"stats":{"Line":4}},{"line":131,"address":[3374261],"length":1,"stats":{"Line":0}},{"line":132,"address":[3374273],"length":1,"stats":{"Line":0}},{"line":134,"address":[3374312],"length":1,"stats":{"Line":0}},{"line":135,"address":[3374324],"length":1,"stats":{"Line":0}},{"line":137,"address":[3374369],"length":1,"stats":{"Line":0}},{"line":138,"address":[3374381],"length":1,"stats":{"Line":0}},{"line":140,"address":[3374426],"length":1,"stats":{"Line":0}},{"line":141,"address":[3374438],"length":1,"stats":{"Line":0}},{"line":143,"address":[3374483],"length":1,"stats":{"Line":0}},{"line":144,"address":[3374495],"length":1,"stats":{"Line":0}},{"line":146,"address":[3374540],"length":1,"stats":{"Line":1}},{"line":147,"address":[3374552],"length":1,"stats":{"Line":1}},{"line":149,"address":[3374594],"length":1,"stats":{"Line":0}},{"line":150,"address":[3374606],"length":1,"stats":{"Line":0}},{"line":152,"address":[3374648],"length":1,"stats":{"Line":1}},{"line":153,"address":[3374660],"length":1,"stats":{"Line":1}},{"line":187,"address":[],"length":0,"stats":{"Line":0}},{"line":188,"address":[],"length":0,"stats":{"Line":0}},{"line":194,"address":[3372800],"length":1,"stats":{"Line":1}},{"line":196,"address":[3372813],"length":1,"stats":{"Line":2}},{"line":202,"address":[3372778,3372576],"length":1,"stats":{"Line":5}},{"line":203,"address":[],"length":0,"stats":{"Line":0}},{"line":204,"address":[3372629],"length":1,"stats":{"Line":5}},{"line":205,"address":[3372748],"length":1,"stats":{"Line":5}},{"line":209,"address":[],"length":0,"stats":{"Line":0}},{"line":210,"address":[],"length":0,"stats":{"Line":0}},{"line":211,"address":[],"length":0,"stats":{"Line":0}},{"line":212,"address":[],"length":0,"stats":{"Line":0}},{"line":216,"address":[],"length":0,"stats":{"Line":0}},{"line":217,"address":[],"length":0,"stats":{"Line":0}},{"line":218,"address":[],"length":0,"stats":{"Line":0}},{"line":219,"address":[],"length":0,"stats":{"Line":0}},{"line":223,"address":[],"length":0,"stats":{"Line":0}},{"line":224,"address":[],"length":0,"stats":{"Line":0}},{"line":225,"address":[],"length":0,"stats":{"Line":0}},{"line":226,"address":[],"length":0,"stats":{"Line":0}},{"line":230,"address":[],"length":0,"stats":{"Line":0}},{"line":231,"address":[],"length":0,"stats":{"Line":0}},{"line":232,"address":[],"length":0,"stats":{"Line":0}},{"line":233,"address":[],"length":0,"stats":{"Line":0}},{"line":237,"address":[],"length":0,"stats":{"Line":0}},{"line":238,"address":[],"length":0,"stats":{"Line":0}},{"line":239,"address":[],"length":0,"stats":{"Line":0}},{"line":240,"address":[],"length":0,"stats":{"Line":0}},{"line":244,"address":[3372553,3372368],"length":1,"stats":{"Line":1}},{"line":245,"address":[],"length":0,"stats":{"Line":0}},{"line":246,"address":[3372409],"length":1,"stats":{"Line":1}},{"line":247,"address":[3372523],"length":1,"stats":{"Line":3}},{"line":251,"address":[],"length":0,"stats":{"Line":0}},{"line":252,"address":[],"length":0,"stats":{"Line":0}},{"line":253,"address":[],"length":0,"stats":{"Line":0}},{"line":254,"address":[],"length":0,"stats":{"Line":0}},{"line":258,"address":[3372344,3372160],"length":1,"stats":{"Line":1}},{"line":259,"address":[],"length":0,"stats":{"Line":0}},{"line":260,"address":[3372190],"length":1,"stats":{"Line":1}},{"line":261,"address":[3372314],"length":1,"stats":{"Line":1}},{"line":265,"address":[],"length":0,"stats":{"Line":0}},{"line":266,"address":[],"length":0,"stats":{"Line":0}},{"line":267,"address":[],"length":0,"stats":{"Line":0}},{"line":271,"address":[],"length":0,"stats":{"Line":0}},{"line":272,"address":[],"length":0,"stats":{"Line":0}},{"line":276,"address":[],"length":0,"stats":{"Line":0}},{"line":277,"address":[],"length":0,"stats":{"Line":0}},{"line":317,"address":[3373376],"length":1,"stats":{"Line":1}},{"line":318,"address":[3373381],"length":1,"stats":{"Line":1}},{"line":322,"address":[3373392],"length":1,"stats":{"Line":1}},{"line":323,"address":[3373397],"length":1,"stats":{"Line":1}},{"line":327,"address":[],"length":0,"stats":{"Line":0}},{"line":328,"address":[],"length":0,"stats":{"Line":0}},{"line":330,"address":[],"length":0,"stats":{"Line":0}},{"line":341,"address":[3378653,3376000,3378206],"length":1,"stats":{"Line":3}},{"line":342,"address":[3376051],"length":1,"stats":{"Line":6}},{"line":344,"address":[3376096],"length":1,"stats":{"Line":6}},{"line":345,"address":[3376133],"length":1,"stats":{"Line":0}},{"line":346,"address":[3376102],"length":1,"stats":{"Line":0}},{"line":350,"address":[3376224],"length":1,"stats":{"Line":6}},{"line":351,"address":[3376349],"length":1,"stats":{"Line":0}},{"line":352,"address":[3376318],"length":1,"stats":{"Line":0}},{"line":356,"address":[3376253],"length":1,"stats":{"Line":6}},{"line":357,"address":[3376286],"length":1,"stats":{"Line":6}},{"line":359,"address":[3376488,3376298,3378183],"length":1,"stats":{"Line":13}},{"line":360,"address":[3376625,3376813],"length":1,"stats":{"Line":12}},{"line":363,"address":[3376897,3376829],"length":1,"stats":{"Line":12}},{"line":364,"address":[3377008],"length":1,"stats":{"Line":7}},{"line":365,"address":[3378368,3378304],"length":1,"stats":{"Line":2}},{"line":366,"address":[],"length":0,"stats":{"Line":0}},{"line":367,"address":[],"length":0,"stats":{"Line":0}},{"line":368,"address":[3378347,3378267],"length":1,"stats":{"Line":1}},{"line":374,"address":[3378257,3377024],"length":1,"stats":{"Line":6}},{"line":377,"address":[3377401,3377338],"length":1,"stats":{"Line":7}},{"line":379,"address":[3377789,3378132,3377731],"length":1,"stats":{"Line":2}},{"line":380,"address":[3377830],"length":1,"stats":{"Line":1}},{"line":383,"address":[3376704],"length":1,"stats":{"Line":1}},{"line":384,"address":[3376648],"length":1,"stats":{"Line":1}},{"line":385,"address":[],"length":0,"stats":{"Line":0}},{"line":386,"address":[3376696],"length":1,"stats":{"Line":1}},{"line":387,"address":[],"length":0,"stats":{"Line":0}},{"line":391,"address":[],"length":0,"stats":{"Line":0}},{"line":392,"address":[],"length":0,"stats":{"Line":0}},{"line":393,"address":[],"length":0,"stats":{"Line":0}},{"line":402,"address":[3404719,3403344,3404670],"length":1,"stats":{"Line":1}},{"line":403,"address":[3403395],"length":1,"stats":{"Line":1}},{"line":405,"address":[3403439],"length":1,"stats":{"Line":1}},{"line":406,"address":[3403480],"length":1,"stats":{"Line":1}},{"line":407,"address":[3403476],"length":1,"stats":{"Line":1}},{"line":408,"address":[],"length":0,"stats":{"Line":0}},{"line":412,"address":[3403450],"length":1,"stats":{"Line":1}},{"line":413,"address":[3403536],"length":1,"stats":{"Line":0}},{"line":417,"address":[3403609],"length":1,"stats":{"Line":1}},{"line":419,"address":[3403642,3403722],"length":1,"stats":{"Line":2}},{"line":420,"address":[3404717,3403952,3403843],"length":1,"stats":{"Line":2}},{"line":421,"address":[3404214,3404277],"length":1,"stats":{"Line":2}},{"line":422,"address":[3404503],"length":1,"stats":{"Line":1}},{"line":426,"address":[3403872],"length":1,"stats":{"Line":1}},{"line":429,"address":[],"length":0,"stats":{"Line":0}},{"line":430,"address":[],"length":0,"stats":{"Line":0}},{"line":431,"address":[],"length":0,"stats":{"Line":0}},{"line":435,"address":[3397408],"length":1,"stats":{"Line":1}},{"line":436,"address":[3397438],"length":1,"stats":{"Line":1}},{"line":437,"address":[],"length":0,"stats":{"Line":0}},{"line":439,"address":[3397599,3398764,3397460,3398743,3397552],"length":1,"stats":{"Line":3}},{"line":440,"address":[3398654,3397638],"length":1,"stats":{"Line":2}},{"line":441,"address":[3398537,3397712],"length":1,"stats":{"Line":2}},{"line":442,"address":[3397732],"length":1,"stats":{"Line":1}},{"line":443,"address":[3398507,3397777,3398456],"length":1,"stats":{"Line":3}},{"line":445,"address":[3398915,3397795],"length":1,"stats":{"Line":0}},{"line":446,"address":[3397803],"length":1,"stats":{"Line":0}},{"line":447,"address":[3397848,3398834,3398885],"length":1,"stats":{"Line":0}},{"line":449,"address":[3399188,3397866],"length":1,"stats":{"Line":0}},{"line":450,"address":[3397886],"length":1,"stats":{"Line":0}},{"line":451,"address":[3397931,3399158,3399107],"length":1,"stats":{"Line":0}},{"line":453,"address":[3397949,3399461],"length":1,"stats":{"Line":0}},{"line":454,"address":[3397969],"length":1,"stats":{"Line":0}},{"line":455,"address":[3398014,3399380,3399431],"length":1,"stats":{"Line":0}},{"line":457,"address":[3398032,3399714],"length":1,"stats":{"Line":0}},{"line":458,"address":[3398049],"length":1,"stats":{"Line":0}},{"line":459,"address":[3398083,3399646],"length":1,"stats":{"Line":0}},{"line":461,"address":[3398110,3399956],"length":1,"stats":{"Line":0}},{"line":462,"address":[3398127],"length":1,"stats":{"Line":0}},{"line":463,"address":[3399926,3398169,3399875],"length":1,"stats":{"Line":0}},{"line":465,"address":[3400229,3398187],"length":1,"stats":{"Line":2}},{"line":466,"address":[3398204],"length":1,"stats":{"Line":1}},{"line":467,"address":[3400148,3398246,3400199],"length":1,"stats":{"Line":3}},{"line":469,"address":[3398264,3400502],"length":1,"stats":{"Line":0}},{"line":470,"address":[3398281],"length":1,"stats":{"Line":0}},{"line":471,"address":[3398323,3400421,3400472],"length":1,"stats":{"Line":0}},{"line":473,"address":[3398341],"length":1,"stats":{"Line":0}},{"line":475,"address":[3398366],"length":1,"stats":{"Line":0}},{"line":476,"address":[3403302,3400684,3398376],"length":1,"stats":{"Line":0}},{"line":477,"address":[3400812,3401848],"length":1,"stats":{"Line":0}},{"line":478,"address":[3401409],"length":1,"stats":{"Line":0}},{"line":479,"address":[3401697,3401429,3401744],"length":1,"stats":{"Line":0}},{"line":481,"address":[3401444],"length":1,"stats":{"Line":0}},{"line":482,"address":[3401460,3401958,3402005],"length":1,"stats":{"Line":0}},{"line":484,"address":[3401475],"length":1,"stats":{"Line":0}},{"line":485,"address":[3402175,3402128,3401495],"length":1,"stats":{"Line":0}},{"line":487,"address":[3401510],"length":1,"stats":{"Line":0}},{"line":488,"address":[3401530,3402298,3402345],"length":1,"stats":{"Line":0}},{"line":490,"address":[3401545],"length":1,"stats":{"Line":0}},{"line":491,"address":[3402461,3401557],"length":1,"stats":{"Line":0}},{"line":493,"address":[3401581],"length":1,"stats":{"Line":0}},{"line":494,"address":[3401601,3402672,3402625],"length":1,"stats":{"Line":0}},{"line":496,"address":[3401616],"length":1,"stats":{"Line":0}},{"line":497,"address":[3401636,3402842,3402795],"length":1,"stats":{"Line":0}},{"line":499,"address":[3401651],"length":1,"stats":{"Line":0}},{"line":500,"address":[3402965,3403012,3401671],"length":1,"stats":{"Line":0}},{"line":503,"address":[3403160,3401920],"length":1,"stats":{"Line":0}},{"line":506,"address":[3400869],"length":1,"stats":{"Line":0}},{"line":507,"address":[3400916,3400967],"length":1,"stats":{"Line":0}},{"line":511,"address":[3401267],"length":1,"stats":{"Line":1}},{"line":512,"address":[3398718],"length":1,"stats":{"Line":1}},{"line":513,"address":[3401171],"length":1,"stats":{"Line":1}},{"line":514,"address":[3401219],"length":1,"stats":{"Line":1}},{"line":520,"address":[3397492],"length":1,"stats":{"Line":1}},{"line":521,"address":[3397496],"length":1,"stats":{"Line":1}},{"line":526,"address":[3394896,3378672,3383868],"length":1,"stats":{"Line":1}},{"line":527,"address":[3378759,3378975],"length":1,"stats":{"Line":2}},{"line":529,"address":[3379226,3379005,3394746,3379099],"length":1,"stats":{"Line":4}},{"line":530,"address":[3379351,3379652],"length":1,"stats":{"Line":2}},{"line":531,"address":[3379674],"length":1,"stats":{"Line":1}},{"line":532,"address":[3379755,3394031,3393985,3394131],"length":1,"stats":{"Line":3}},{"line":533,"address":[3394083,3396428,3396400,3394008],"length":1,"stats":{"Line":1}},{"line":534,"address":[3394380,3394544,3394444],"length":1,"stats":{"Line":2}},{"line":535,"address":[3394292],"length":1,"stats":{"Line":1}},{"line":538,"address":[3379800,3379729],"length":1,"stats":{"Line":2}},{"line":539,"address":[3393321,3379848,3393221,3393175],"length":1,"stats":{"Line":0}},{"line":540,"address":[3395244,3395216,3393273,3393198],"length":1,"stats":{"Line":0}},{"line":541,"address":[3393546,3393730],"length":1,"stats":{"Line":0}},{"line":543,"address":[3379822,3379893],"length":1,"stats":{"Line":2}},{"line":544,"address":[3379941,3392437,3392391,3392537],"length":1,"stats":{"Line":0}},{"line":545,"address":[3392414,3395072,3392489,3395100],"length":1,"stats":{"Line":0}},{"line":546,"address":[3392898,3392730],"length":1,"stats":{"Line":0}},{"line":548,"address":[3379986,3379915],"length":1,"stats":{"Line":2}},{"line":549,"address":[3391801,3391655,3391701,3380034],"length":1,"stats":{"Line":0}},{"line":550,"address":[3391678,3391753,3396976,3397004],"length":1,"stats":{"Line":0}},{"line":551,"address":[3391962,3392114],"length":1,"stats":{"Line":0}},{"line":553,"address":[3380008,3380079],"length":1,"stats":{"Line":2}},{"line":554,"address":[3391626,3391197,3391243,3391343,3380127,3391383],"length":1,"stats":{"Line":0}},{"line":555,"address":[3397292,3397264,3391220,3391295],"length":1,"stats":{"Line":0}},{"line":556,"address":[3391621,3391376,3391408],"length":1,"stats":{"Line":0}},{"line":558,"address":[3380101,3380172],"length":1,"stats":{"Line":2}},{"line":559,"address":[3390551,3390597,3380220,3390674],"length":1,"stats":{"Line":0}},{"line":560,"address":[3390574,3395648,3390626,3395676],"length":1,"stats":{"Line":0}},{"line":561,"address":[3390803,3390939],"length":1,"stats":{"Line":0}},{"line":563,"address":[3380265,3380194],"length":1,"stats":{"Line":2}},{"line":564,"address":[3389767,3380313,3389913,3389813],"length":1,"stats":{"Line":3}},{"line":565,"address":[3389790,3394928,3394956,3389865],"length":1,"stats":{"Line":1}},{"line":566,"address":[3390106,3390274],"length":1,"stats":{"Line":2}},{"line":568,"address":[3380287,3380358],"length":1,"stats":{"Line":0}},{"line":569,"address":[3389031,3389177,3380406,3389077],"length":1,"stats":{"Line":0}},{"line":570,"address":[3389129,3396832,3389054,3396860],"length":1,"stats":{"Line":0}},{"line":571,"address":[3389426,3389490,3389590],"length":1,"stats":{"Line":0}},{"line":572,"address":[3389338],"length":1,"stats":{"Line":0}},{"line":575,"address":[3380380,3380451],"length":1,"stats":{"Line":0}},{"line":576,"address":[3389005,3380480,3380898,3380752,3380798],"length":1,"stats":{"Line":0}},{"line":577,"address":[],"length":0,"stats":{"Line":0}},{"line":578,"address":[3380775,3380850,3395968,3395996],"length":1,"stats":{"Line":0}},{"line":579,"address":[3381003],"length":1,"stats":{"Line":0}},{"line":580,"address":[3381287,3381160,3381051],"length":1,"stats":{"Line":0}},{"line":581,"address":[3381396,3382099],"length":1,"stats":{"Line":0}},{"line":582,"address":[3382121],"length":1,"stats":{"Line":0}},{"line":583,"address":[3388099,3382202,3388145,3388245],"length":1,"stats":{"Line":0}},{"line":584,"address":[3396389,3388122,3396256,3396383],"length":1,"stats":{"Line":0}},{"line":585,"address":[3396323,3396284],"length":1,"stats":{"Line":0}},{"line":587,"address":[3388558,3388658,3388494],"length":1,"stats":{"Line":0}},{"line":588,"address":[3388406],"length":1,"stats":{"Line":0}},{"line":591,"address":[3382247,3382176],"length":1,"stats":{"Line":0}},{"line":592,"address":[3395631,3395504,3382295,3395637,3387289,3387435],"length":1,"stats":{"Line":0}},{"line":593,"address":[],"length":0,"stats":{"Line":0}},{"line":594,"address":[3395532,3395571],"length":1,"stats":{"Line":0}},{"line":596,"address":[3387844,3387660],"length":1,"stats":{"Line":0}},{"line":598,"address":[3382340,3382269],"length":1,"stats":{"Line":0}},{"line":599,"address":[3382388,3397120,3397247,3386651,3397253,3386505],"length":1,"stats":{"Line":0}},{"line":600,"address":[],"length":0,"stats":{"Line":0}},{"line":601,"address":[3397148,3397187],"length":1,"stats":{"Line":0}},{"line":603,"address":[3386844,3387012],"length":1,"stats":{"Line":0}},{"line":605,"address":[3382362,3382427],"length":1,"stats":{"Line":0}},{"line":606,"address":[3395957,3385775,3382472,3395824,3385921,3395951],"length":1,"stats":{"Line":0}},{"line":607,"address":[],"length":0,"stats":{"Line":0}},{"line":608,"address":[3395852,3395891],"length":1,"stats":{"Line":0}},{"line":610,"address":[3386082,3386234],"length":1,"stats":{"Line":0}},{"line":612,"address":[3382508,3382449],"length":1,"stats":{"Line":0}},{"line":613,"address":[3396671,3385323,3382553,3385509,3385752,3385469,3396677,3396544],"length":1,"stats":{"Line":0}},{"line":614,"address":[],"length":0,"stats":{"Line":0}},{"line":615,"address":[3396572,3396611],"length":1,"stats":{"Line":0}},{"line":617,"address":[3385747,3385534,3385502],"length":1,"stats":{"Line":0}},{"line":619,"address":[3382530,3382589],"length":1,"stats":{"Line":0}},{"line":620,"address":[3384806,3395360,3382634,3384683,3395487,3395493],"length":1,"stats":{"Line":0}},{"line":621,"address":[],"length":0,"stats":{"Line":0}},{"line":622,"address":[3395388,3395427],"length":1,"stats":{"Line":0}},{"line":624,"address":[3385071,3385148,3385007],"length":1,"stats":{"Line":0}},{"line":625,"address":[3384935],"length":1,"stats":{"Line":0}},{"line":628,"address":[3382670,3382611],"length":1,"stats":{"Line":0}},{"line":629,"address":[3396815,3384051,3396688,3396821,3383905,3382715],"length":1,"stats":{"Line":0}},{"line":630,"address":[],"length":0,"stats":{"Line":0}},{"line":631,"address":[3396716,3396755],"length":1,"stats":{"Line":0}},{"line":633,"address":[3384412,3384348,3384512],"length":1,"stats":{"Line":0}},{"line":634,"address":[3384244],"length":1,"stats":{"Line":0}},{"line":637,"address":[3382751,3382692],"length":1,"stats":{"Line":0}},{"line":638,"address":[3383188,3383042,3382780,3383088],"length":1,"stats":{"Line":0}},{"line":639,"address":[3396245,3396239,3383065,3396112],"length":1,"stats":{"Line":0}},{"line":640,"address":[3396140,3396179],"length":1,"stats":{"Line":0}},{"line":642,"address":[3383501,3383437,3383601],"length":1,"stats":{"Line":0}},{"line":643,"address":[3383349],"length":1,"stats":{"Line":0}},{"line":646,"address":[],"length":0,"stats":{"Line":0}},{"line":647,"address":[3382773,3382811],"length":1,"stats":{"Line":0}},{"line":648,"address":[],"length":0,"stats":{"Line":0}},{"line":649,"address":[],"length":0,"stats":{"Line":0}},{"line":653,"address":[3383708],"length":1,"stats":{"Line":0}},{"line":656,"address":[3395801,3381445,3395792],"length":1,"stats":{"Line":0}},{"line":657,"address":[3381637],"length":1,"stats":{"Line":0}},{"line":658,"address":[3381581],"length":1,"stats":{"Line":0}},{"line":661,"address":[],"length":0,"stats":{"Line":0}},{"line":662,"address":[3380518,3380457],"length":1,"stats":{"Line":0}},{"line":663,"address":[],"length":0,"stats":{"Line":0}},{"line":664,"address":[],"length":0,"stats":{"Line":0}},{"line":668,"address":[3381742],"length":1,"stats":{"Line":1}},{"line":671,"address":[3379472],"length":1,"stats":{"Line":1}},{"line":672,"address":[3379416],"length":1,"stats":{"Line":1}},{"line":673,"address":[3379464],"length":1,"stats":{"Line":1}},{"line":674,"address":[3379468],"length":1,"stats":{"Line":1}},{"line":675,"address":[],"length":0,"stats":{"Line":0}},{"line":679,"address":[],"length":0,"stats":{"Line":0}},{"line":680,"address":[],"length":0,"stats":{"Line":0}},{"line":685,"address":[3366672],"length":1,"stats":{"Line":6}},{"line":689,"address":[3366744],"length":1,"stats":{"Line":6}},{"line":690,"address":[3366853],"length":1,"stats":{"Line":0}},{"line":691,"address":[3366869],"length":1,"stats":{"Line":0}},{"line":695,"address":[3366768],"length":1,"stats":{"Line":6}},{"line":696,"address":[3366778],"length":1,"stats":{"Line":5}},{"line":697,"address":[],"length":0,"stats":{"Line":0}},{"line":698,"address":[],"length":0,"stats":{"Line":0}},{"line":703,"address":[3366584,3366000,3366578],"length":1,"stats":{"Line":1}},{"line":704,"address":[3366059],"length":1,"stats":{"Line":1}},{"line":705,"address":[3366168],"length":1,"stats":{"Line":0}},{"line":706,"address":[3366140],"length":1,"stats":{"Line":0}},{"line":710,"address":[3366078,3366643,3366608],"length":1,"stats":{"Line":3}},{"line":711,"address":[3366294,3366120],"length":1,"stats":{"Line":2}},{"line":712,"address":[3366466],"length":1,"stats":{"Line":1}},{"line":713,"address":[],"length":0,"stats":{"Line":0}},{"line":714,"address":[],"length":0,"stats":{"Line":0}}],"covered":117,"coverable":343},{"path":["/","home","victor","ML","machinelearne-rs","lib","src","preprocessing","column_transformer","mod.rs"],"content":"//! ColumnTransformer for applying different transformers to different columns.\n//!\n//! This module provides the `ColumnTransformer` which allows applying different\n//! preprocessing steps to different subsets of columns in a dataset.\n\n#[allow(clippy::module_inception)]\nmod column_transformer;\n\npub use column_transformer::{\n    ColumnSpec, ColumnTransformer, ColumnTransformerParams, FittedColumnTransformer,\n};\n","traces":[],"covered":0,"coverable":0},{"path":["/","home","victor","ML","machinelearne-rs","lib","src","preprocessing","encoding","label.rs"],"content":"//! Label encoding for 1D target labels.\n//!\n//! Maps target labels to integer indices (0, 1, 2, ...).\n\nuse crate::backend::{Backend, Tensor1D};\nuse crate::preprocessing::error::PreprocessingError;\nuse crate::serialization::SerializableParams;\nuse serde::{Deserialize, Serialize};\nuse std::collections::HashMap;\nuse std::marker::PhantomData;\n\n/// Label encoder for 1D target labels.\n///\n/// Converts target labels to integer indices, suitable for classification\n/// targets. Unlike OrdinalEncoder which works on 2D data, LabelEncoder\n/// works on 1D arrays (a single target column).\n///\n/// # Example\n/// ```ignore\n/// use machinelearne_rs::preprocessing::LabelEncoder;\n/// use machinelearne_rs::backend::CpuBackend;\n///\n/// // Target labels: [2, 0, 1, 0]\n/// let labels = Tensor1D::new(vec![2.0, 0.0, 1.0, 0.0]);\n///\n/// let encoder = LabelEncoder::\u003cCpuBackend\u003e::new();\n/// let fitted = encoder.fit(\u0026labels)?;\n///\n/// // Encoded: [2, 0, 1, 0] (ordinal mapping)\n/// let encoded = fitted.transform(\u0026labels)?;\n/// ```\n#[derive(Clone, Debug)]\npub struct LabelEncoder\u003cB: Backend\u003e {\n    _backend: PhantomData\u003cB\u003e,\n}\n\nimpl\u003cB: Backend\u003e LabelEncoder\u003cB\u003e {\n    /// Create a new LabelEncoder.\n    pub fn new() -\u003e Self {\n        Self {\n            _backend: PhantomData,\n        }\n    }\n\n    /// Fit the encoder to the labels and return the fitted encoder.\n    pub fn fit(\u0026self, labels: \u0026Tensor1D\u003cB\u003e) -\u003e Result\u003cFittedLabelEncoder\u003cB\u003e, PreprocessingError\u003e {\n        let n = labels.len();\n\n        if n == 0 {\n            return Err(PreprocessingError::EmptyData(\n                \"Cannot fit LabelEncoder on empty data\".to_string(),\n            ));\n        }\n\n        let label_vec = labels.to_vec();\n\n        // Find unique classes\n        let mut classes_set: std::collections::BTreeSet\u003ci32\u003e = std::collections::BTreeSet::new();\n        for \u0026val in \u0026label_vec {\n            if !val.is_finite() {\n                return Err(PreprocessingError::InvalidParameter(format!(\n                    \"LabelEncoder expects finite values, got {}\",\n                    val\n                )));\n            }\n            classes_set.insert(val.round() as i32);\n        }\n\n        let classes_: Vec\u003cf32\u003e = classes_set.into_iter().map(|x| x as f32).collect();\n        let n_classes = classes_.len();\n\n        // Create mapping from class to index\n        let mut class_to_idx: HashMap\u003ci32, usize\u003e = HashMap::new();\n        for (idx, \u0026class) in classes_.iter().enumerate() {\n            class_to_idx.insert(class as i32, idx);\n        }\n\n        Ok(FittedLabelEncoder {\n            classes_,\n            class_to_idx,\n            n_classes,\n            _backend: PhantomData,\n        })\n    }\n\n    /// Fit and transform in one step.\n    pub fn fit_transform(\u0026self, labels: \u0026Tensor1D\u003cB\u003e) -\u003e Result\u003cTensor1D\u003cB\u003e, PreprocessingError\u003e {\n        let fitted = self.fit(labels)?;\n        fitted.transform(labels)\n    }\n}\n\nimpl\u003cB: Backend\u003e Default for LabelEncoder\u003cB\u003e {\n    fn default() -\u003e Self {\n        Self::new()\n    }\n}\n\n/// Serializable parameters for a fitted LabelEncoder.\n#[derive(Clone, Serialize, Deserialize)]\npub struct LabelEncoderParams {\n    /// Unique classes in sorted order.\n    pub classes_: Vec\u003cf32\u003e,\n    /// Number of unique classes.\n    pub n_classes: usize,\n}\n\n// Note: We rely on the blanket impl of SerializableParams for Serialize + Deserialize\n\n/// Fitted LabelEncoder ready for inference.\n#[derive(Clone)]\npub struct FittedLabelEncoder\u003cB: Backend\u003e {\n    /// Unique classes in sorted order.\n    classes_: Vec\u003cf32\u003e,\n    /// Mapping from class value to index.\n    class_to_idx: HashMap\u003ci32, usize\u003e,\n    /// Number of classes.\n    n_classes: usize,\n    _backend: PhantomData\u003cB\u003e,\n}\n\nimpl\u003cB: Backend\u003e FittedLabelEncoder\u003cB\u003e {\n    /// Get the unique classes.\n    pub fn classes(\u0026self) -\u003e \u0026[f32] {\n        \u0026self.classes_\n    }\n\n    /// Get the number of classes.\n    pub fn n_classes(\u0026self) -\u003e usize {\n        self.n_classes\n    }\n\n    /// Transform labels to encoded indices.\n    pub fn transform(\u0026self, labels: \u0026Tensor1D\u003cB\u003e) -\u003e Result\u003cTensor1D\u003cB\u003e, PreprocessingError\u003e {\n        let n = labels.len();\n        if n == 0 {\n            return Ok(Tensor1D::zeros(0));\n        }\n\n        let label_vec = labels.to_vec();\n        let mut result = Vec::with_capacity(n);\n\n        for \u0026val in \u0026label_vec {\n            let key = val.round() as i32;\n            match self.class_to_idx.get(\u0026key) {\n                Some(\u0026idx) =\u003e result.push(idx as f64),\n                None =\u003e {\n                    return Err(PreprocessingError::InvalidParameter(format!(\n                        \"Unknown label value: {}\",\n                        val\n                    )));\n                }\n            }\n        }\n\n        Ok(Tensor1D::new(result.iter().map(|\u0026x| x as f32).collect()))\n    }\n\n    /// Inverse transform encoded indices back to original labels.\n    pub fn inverse_transform(\n        \u0026self,\n        indices: \u0026Tensor1D\u003cB\u003e,\n    ) -\u003e Result\u003cTensor1D\u003cB\u003e, PreprocessingError\u003e {\n        let n = indices.len();\n        if n == 0 {\n            return Ok(Tensor1D::zeros(0));\n        }\n\n        let idx_vec = indices.to_vec();\n        let mut result = Vec::with_capacity(n);\n\n        for \u0026idx in \u0026idx_vec {\n            let idx_usize = idx.round() as usize;\n            if idx_usize \u003e= self.n_classes {\n                return Err(PreprocessingError::InvalidParameter(format!(\n                    \"Index {} out of bounds (max {})\",\n                    idx_usize,\n                    self.n_classes - 1\n                )));\n            }\n            result.push(self.classes_[idx_usize] as f64);\n        }\n\n        Ok(Tensor1D::new(result.iter().map(|\u0026x| x as f32).collect()))\n    }\n\n    /// Extract parameters for serialization.\n    pub fn extract_params(\u0026self) -\u003e LabelEncoderParams {\n        LabelEncoderParams {\n            classes_: self.classes_.clone(),\n            n_classes: self.n_classes,\n        }\n    }\n\n    /// Reconstruct from parameters.\n    pub fn from_params(params: LabelEncoderParams) -\u003e Result\u003cSelf, PreprocessingError\u003e {\n        let mut class_to_idx = HashMap::new();\n        for (idx, \u0026class) in params.classes_.iter().enumerate() {\n            class_to_idx.insert(class as i32, idx);\n        }\n\n        Ok(FittedLabelEncoder {\n            classes_: params.classes_,\n            class_to_idx,\n            n_classes: params.n_classes,\n            _backend: PhantomData,\n        })\n    }\n\n    /// Save to file.\n    pub fn save_to_file\u003cP: AsRef\u003cstd::path::Path\u003e\u003e(\u0026self, path: P) -\u003e std::io::Result\u003c()\u003e {\n        let params = self.extract_params();\n        let bytes = params.to_bytes().map_err(std::io::Error::other)?;\n        std::fs::write(path, bytes)\n    }\n\n    /// Load from file.\n    pub fn load_from_file\u003cP: AsRef\u003cstd::path::Path\u003e\u003e(path: P) -\u003e Result\u003cSelf, PreprocessingError\u003e {\n        let bytes = std::fs::read(path)?;\n        let params = LabelEncoderParams::from_bytes(\u0026bytes)\n            .map_err(|e| PreprocessingError::SerializationError(e.to_string()))?;\n        Self::from_params(params)\n    }\n}\n\n#[cfg(test)]\nmod tests {\n    use super::*;\n    use crate::backend::CpuBackend;\n\n    #[test]\n    fn test_label_encoder_basic() {\n        let labels = Tensor1D::\u003cCpuBackend\u003e::new(vec![2.0f32, 0.0, 1.0, 0.0]);\n\n        let encoder = LabelEncoder::\u003cCpuBackend\u003e::new();\n        let fitted = encoder.fit(\u0026labels).unwrap();\n\n        assert_eq!(fitted.n_classes(), 3);\n        assert_eq!(fitted.classes(), \u0026[0.0f32, 1.0, 2.0]);\n\n        let encoded = fitted.transform(\u0026labels).unwrap();\n        let vals = encoded.to_vec();\n\n        // Classes: [0, 1, 2] -\u003e indices: 0-\u003e0, 1-\u003e1, 2-\u003e2\n        assert!((vals[0] - 2.0).abs() \u003c 1e-6); // 2 -\u003e idx 2\n        assert!((vals[1] - 0.0).abs() \u003c 1e-6); // 0 -\u003e idx 0\n        assert!((vals[2] - 1.0).abs() \u003c 1e-6); // 1 -\u003e idx 1\n        assert!((vals[3] - 0.0).abs() \u003c 1e-6); // 0 -\u003e idx 0\n    }\n\n    #[test]\n    fn test_label_encoder_non_contiguous() {\n        let labels = Tensor1D::\u003cCpuBackend\u003e::new(vec![10.0f32, 5.0, 0.0]);\n\n        let encoder = LabelEncoder::\u003cCpuBackend\u003e::new();\n        let fitted = encoder.fit(\u0026labels).unwrap();\n\n        assert_eq!(fitted.classes(), \u0026[0.0f32, 5.0, 10.0]);\n\n        let encoded = fitted.transform(\u0026labels).unwrap();\n        let vals = encoded.to_vec();\n\n        assert!((vals[0] - 2.0).abs() \u003c 1e-6); // 10 -\u003e idx 2\n        assert!((vals[1] - 1.0).abs() \u003c 1e-6); // 5 -\u003e idx 1\n        assert!((vals[2] - 0.0).abs() \u003c 1e-6); // 0 -\u003e idx 0\n    }\n\n    #[test]\n    fn test_label_encoder_inverse() {\n        let labels = Tensor1D::\u003cCpuBackend\u003e::new(vec![2.0f32, 0.0, 1.0, 0.0]);\n\n        let encoder = LabelEncoder::\u003cCpuBackend\u003e::new();\n        let fitted = encoder.fit(\u0026labels).unwrap();\n\n        let encoded = fitted.transform(\u0026labels).unwrap();\n        let recovered = fitted.inverse_transform(\u0026encoded).unwrap();\n\n        let orig = labels.to_vec();\n        let rec = recovered.to_vec();\n\n        for (o, r) in orig.iter().zip(rec.iter()) {\n            assert!((o - r).abs() \u003c 1e-6);\n        }\n    }\n\n    #[test]\n    fn test_label_encoder_unknown_error() {\n        let train = Tensor1D::\u003cCpuBackend\u003e::new(vec![0.0f32, 1.0]);\n        let test = Tensor1D::\u003cCpuBackend\u003e::new(vec![2.0f32]);\n\n        let encoder = LabelEncoder::\u003cCpuBackend\u003e::new();\n        let fitted = encoder.fit(\u0026train).unwrap();\n\n        let result = fitted.transform(\u0026test);\n        assert!(result.is_err());\n    }\n\n    #[test]\n    fn test_label_encoder_serialization() {\n        let labels = Tensor1D::\u003cCpuBackend\u003e::new(vec![2.0f32, 0.0, 1.0, 0.0]);\n\n        let encoder = LabelEncoder::\u003cCpuBackend\u003e::new();\n        let fitted = encoder.fit(\u0026labels).unwrap();\n\n        let temp_file = std::env::temp_dir().join(\"test_label.bin\");\n        fitted.save_to_file(\u0026temp_file).unwrap();\n\n        let loaded = FittedLabelEncoder::\u003cCpuBackend\u003e::load_from_file(\u0026temp_file).unwrap();\n\n        assert_eq!(loaded.n_classes(), fitted.n_classes());\n        assert_eq!(loaded.classes(), fitted.classes());\n\n        let e1 = fitted.transform(\u0026labels).unwrap();\n        let e2 = loaded.transform(\u0026labels).unwrap();\n\n        let v1 = e1.to_vec();\n        let v2 = e2.to_vec();\n        for (a, b) in v1.iter().zip(v2.iter()) {\n            assert!((a - b).abs() \u003c 1e-6);\n        }\n\n        std::fs::remove_file(temp_file).ok();\n    }\n}\n","traces":[{"line":39,"address":[],"length":0,"stats":{"Line":0}},{"line":46,"address":[2753434,2752160,2753829],"length":1,"stats":{"Line":3}},{"line":47,"address":[2752206],"length":1,"stats":{"Line":3}},{"line":49,"address":[2752227],"length":1,"stats":{"Line":3}},{"line":50,"address":[2752268],"length":1,"stats":{"Line":0}},{"line":51,"address":[2752237],"length":1,"stats":{"Line":0}},{"line":55,"address":[2752383],"length":1,"stats":{"Line":3}},{"line":58,"address":[2752396],"length":1,"stats":{"Line":3}},{"line":59,"address":[2752537,2752456],"length":1,"stats":{"Line":6}},{"line":60,"address":[2752642,2753444],"length":1,"stats":{"Line":6}},{"line":61,"address":[2753501,2753450],"length":1,"stats":{"Line":0}},{"line":62,"address":[],"length":0,"stats":{"Line":0}},{"line":63,"address":[],"length":0,"stats":{"Line":0}},{"line":66,"address":[2753477,2753759],"length":1,"stats":{"Line":8}},{"line":69,"address":[2753856,2752665,2753865],"length":1,"stats":{"Line":12}},{"line":70,"address":[2752839,2752782],"length":1,"stats":{"Line":8}},{"line":73,"address":[2752855],"length":1,"stats":{"Line":4}},{"line":74,"address":[2752934,2752870],"length":1,"stats":{"Line":5}},{"line":75,"address":[2753166,2753429],"length":1,"stats":{"Line":5}},{"line":78,"address":[2753289],"length":1,"stats":{"Line":1}},{"line":79,"address":[2753215],"length":1,"stats":{"Line":1}},{"line":80,"address":[2753263],"length":1,"stats":{"Line":4}},{"line":81,"address":[],"length":0,"stats":{"Line":0}},{"line":82,"address":[],"length":0,"stats":{"Line":0}},{"line":87,"address":[],"length":0,"stats":{"Line":0}},{"line":88,"address":[],"length":0,"stats":{"Line":0}},{"line":89,"address":[],"length":0,"stats":{"Line":0}},{"line":94,"address":[],"length":0,"stats":{"Line":0}},{"line":95,"address":[],"length":0,"stats":{"Line":0}},{"line":124,"address":[2757312],"length":1,"stats":{"Line":1}},{"line":125,"address":[2757317],"length":1,"stats":{"Line":1}},{"line":129,"address":[2757328],"length":1,"stats":{"Line":1}},{"line":130,"address":[2757333],"length":1,"stats":{"Line":1}},{"line":134,"address":[2757344,2758464,2758470],"length":1,"stats":{"Line":1}},{"line":135,"address":[2757395],"length":1,"stats":{"Line":1}},{"line":136,"address":[2757416],"length":1,"stats":{"Line":1}},{"line":137,"address":[2757422],"length":1,"stats":{"Line":0}},{"line":140,"address":[2757508],"length":1,"stats":{"Line":1}},{"line":141,"address":[2757521],"length":1,"stats":{"Line":3}},{"line":143,"address":[2757604,2757668],"length":1,"stats":{"Line":4}},{"line":144,"address":[2758019,2757773],"length":1,"stats":{"Line":4}},{"line":145,"address":[2758058],"length":1,"stats":{"Line":1}},{"line":146,"address":[2758126,2758459],"length":1,"stats":{"Line":6}},{"line":147,"address":[],"length":0,"stats":{"Line":0}},{"line":148,"address":[2758209],"length":1,"stats":{"Line":1}},{"line":149,"address":[],"length":0,"stats":{"Line":0}},{"line":150,"address":[],"length":0,"stats":{"Line":0}},{"line":156,"address":[2757806,2758496,2758506],"length":1,"stats":{"Line":9}},{"line":160,"address":[2757246,2756032,2757252],"length":1,"stats":{"Line":1}},{"line":164,"address":[2756086],"length":1,"stats":{"Line":1}},{"line":165,"address":[2756107],"length":1,"stats":{"Line":1}},{"line":166,"address":[2756113],"length":1,"stats":{"Line":0}},{"line":169,"address":[2756199],"length":1,"stats":{"Line":1}},{"line":170,"address":[2756212],"length":1,"stats":{"Line":1}},{"line":172,"address":[2756298,2756362],"length":1,"stats":{"Line":2}},{"line":173,"address":[2756467,2756704],"length":1,"stats":{"Line":2}},{"line":174,"address":[2756781],"length":1,"stats":{"Line":1}},{"line":175,"address":[2756980,2756917],"length":1,"stats":{"Line":0}},{"line":176,"address":[],"length":0,"stats":{"Line":0}},{"line":177,"address":[],"length":0,"stats":{"Line":0}},{"line":178,"address":[2756832,2756901,2756960],"length":1,"stats":{"Line":0}},{"line":181,"address":[2756861,2756800],"length":1,"stats":{"Line":2}},{"line":184,"address":[2757280,2757290,2756491],"length":1,"stats":{"Line":3}},{"line":188,"address":[2755104],"length":1,"stats":{"Line":1}},{"line":190,"address":[2755127],"length":1,"stats":{"Line":1}},{"line":191,"address":[2755158],"length":1,"stats":{"Line":1}},{"line":196,"address":[2753888,2754512],"length":1,"stats":{"Line":1}},{"line":197,"address":[2753915],"length":1,"stats":{"Line":1}},{"line":198,"address":[2754025,2753964],"length":1,"stats":{"Line":2}},{"line":199,"address":[2754257,2754507],"length":1,"stats":{"Line":2}},{"line":202,"address":[2754380],"length":1,"stats":{"Line":1}},{"line":203,"address":[2754308],"length":1,"stats":{"Line":1}},{"line":204,"address":[2754343],"length":1,"stats":{"Line":1}},{"line":205,"address":[2754371],"length":1,"stats":{"Line":1}},{"line":206,"address":[],"length":0,"stats":{"Line":0}},{"line":211,"address":[2755081,2754544,2755043],"length":1,"stats":{"Line":1}},{"line":212,"address":[2754578],"length":1,"stats":{"Line":1}},{"line":213,"address":[2754668,2754722,2755054],"length":1,"stats":{"Line":2}},{"line":214,"address":[2754986,2754884],"length":1,"stats":{"Line":2}},{"line":218,"address":[2755843,2755200,2755870],"length":1,"stats":{"Line":1}},{"line":219,"address":[2755225],"length":1,"stats":{"Line":1}},{"line":220,"address":[2755373,2755599,2755456,2755502],"length":1,"stats":{"Line":3}},{"line":221,"address":[2755888,2755479,2755916,2755551],"length":1,"stats":{"Line":1}},{"line":222,"address":[2755701],"length":1,"stats":{"Line":1}}],"covered":61,"coverable":84},{"path":["/","home","victor","ML","machinelearne-rs","lib","src","preprocessing","encoding","mod.rs"],"content":"//! Categorical feature encoding transformers.\n//!\n//! This module provides encoders for converting categorical features to numerical\n//! representations that can be used by machine learning models.\n//!\n//! # Available Encoders\n//!\n//! ## OneHotEncoder\n//! Converts categorical integer values to one-hot (dummy) encoding.\n//!\n//! ```ignore\n//! // Input: [[0], [1], [2]]  (3 samples, 1 categorical feature)\n//! // Output: [[1,0,0], [0,1,0], [0,0,1]]  (3 samples, 3 binary features)\n//! ```\n//!\n//! ## OrdinalEncoder\n//! Maps categorical values to integer ordinals (0, 1, 2, ...).\n//!\n//! ## LabelEncoder\n//! Encodes 1D target labels to integers (for classification targets).\n//!\n//! # Design Notes\n//!\n//! All encoders work with `f32` tensor representations, where categorical values\n//! are pre-mapped to integers by the user. This design:\n//! - Maintains consistency with the tensor-based API\n//! - Avoids string handling in the core library\n//! - Allows users to handle their own category-to-integer mapping\n\nmod label;\nmod one_hot;\nmod ordinal;\n\npub use label::{FittedLabelEncoder, LabelEncoder, LabelEncoderParams};\npub use one_hot::{FittedOneHotEncoder, OneHotEncoder, OneHotEncoderParams, OneHotOutput};\npub use ordinal::{FittedOrdinalEncoder, OrdinalEncoder, OrdinalEncoderParams};\n\n/// Strategy for handling unknown categories during transform.\n#[derive(Clone, Copy, Debug, Default, PartialEq, serde::Serialize, serde::Deserialize)]\npub enum HandleUnknown {\n    /// Raise an error when unknown categories are encountered.\n    #[default]\n    Error,\n    /// Ignore unknown categories (output zeros for one-hot, NaN for ordinal).\n    Ignore,\n}\n","traces":[],"covered":0,"coverable":0},{"path":["/","home","victor","ML","machinelearne-rs","lib","src","preprocessing","encoding","one_hot.rs"],"content":"//! One-hot encoding for categorical features.\n//!\n//! Transforms categorical integer values to one-hot (dummy) encoded vectors.\n\nuse crate::backend::{Backend, Tensor2D};\nuse crate::preprocessing::encoding::HandleUnknown;\nuse crate::preprocessing::error::PreprocessingError;\nuse crate::preprocessing::traits::{FittedTransformer, Transformer};\nuse serde::{Deserialize, Serialize};\nuse std::collections::HashSet;\nuse std::marker::PhantomData;\n\n/// One-hot encoder for categorical features.\n///\n/// Converts integer categories to one-hot encoded vectors. Each input column\n/// is treated as a categorical feature, and the encoder learns the unique\n/// values (categories) present in each column during fitting.\n///\n/// # Example\n/// ```ignore\n/// use machinelearne_rs::preprocessing::{OneHotEncoder, Transformer};\n/// use machinelearne_rs::backend::CpuBackend;\n///\n/// // Input: 3 samples with 1 categorical feature each\n/// // Categories: 0, 1, 2 (e.g., \"red\", \"green\", \"blue\")\n/// let data = Tensor2D::new(vec![0.0, 1.0, 2.0], 3, 1);\n///\n/// let encoder = OneHotEncoder::\u003cCpuBackend\u003e::new();\n/// let fitted = encoder.fit(\u0026data)?;\n///\n/// // Output: 3x3 one-hot matrix\n/// let encoded = fitted.transform(\u0026data)?;\n/// // [[1, 0, 0],\n/// //  [0, 1, 0],\n/// //  [0, 0, 1]]\n/// ```\n#[derive(Clone, Debug)]\npub struct OneHotEncoder\u003cB: Backend\u003e {\n    /// How to handle unknown categories during transform.\n    handle_unknown: HandleUnknown,\n    _backend: PhantomData\u003cB\u003e,\n}\n\nimpl\u003cB: Backend\u003e OneHotEncoder\u003cB\u003e {\n    /// Create a new OneHotEncoder with default settings.\n    pub fn new() -\u003e Self {\n        Self {\n            handle_unknown: HandleUnknown::default(),\n            _backend: PhantomData,\n        }\n    }\n\n    /// Set the strategy for handling unknown categories.\n    pub fn with_handle_unknown(mut self, strategy: HandleUnknown) -\u003e Self {\n        self.handle_unknown = strategy;\n        self\n    }\n}\n\nimpl\u003cB: Backend\u003e Default for OneHotEncoder\u003cB\u003e {\n    fn default() -\u003e Self {\n        Self::new()\n    }\n}\n\n/// Output configuration for one-hot encoding.\n#[derive(Clone, Debug, Default)]\npub struct OneHotOutput;\n\n/// Serializable parameters for a fitted OneHotEncoder.\n#[derive(Clone, Serialize, Deserialize)]\npub struct OneHotEncoderParams {\n    /// Categories (unique values) for each input column.\n    pub categories_: Vec\u003cVec\u003cf32\u003e\u003e,\n    /// Number of categories per column.\n    pub n_values_: Vec\u003cusize\u003e,\n    /// Total number of output features.\n    pub n_features_out: usize,\n    /// Number of input features.\n    pub n_features_in: usize,\n    /// Handle unknown strategy.\n    pub handle_unknown: HandleUnknown,\n}\n\n// Note: We rely on the blanket impl of SerializableParams for Serialize + Deserialize\n\n/// Fitted OneHotEncoder ready for inference.\n#[derive(Clone)]\npub struct FittedOneHotEncoder\u003cB: Backend\u003e {\n    /// Categories (unique sorted values) for each input column.\n    categories_: Vec\u003cVec\u003cf32\u003e\u003e,\n    /// Number of categories per column.\n    n_values_: Vec\u003cusize\u003e,\n    /// Total number of output features (sum of n_values_).\n    n_features_out: usize,\n    /// Number of input features.\n    n_features_in: usize,\n    /// Handle unknown strategy.\n    handle_unknown: HandleUnknown,\n    _backend: PhantomData\u003cB\u003e,\n}\n\nimpl\u003cB: Backend\u003e FittedOneHotEncoder\u003cB\u003e {\n    /// Get the categories learned for each feature.\n    pub fn categories(\u0026self) -\u003e \u0026[Vec\u003cf32\u003e] {\n        \u0026self.categories_\n    }\n\n    /// Get the number of output features.\n    pub fn n_features_out(\u0026self) -\u003e usize {\n        self.n_features_out\n    }\n\n    /// Get the number of categories per input feature.\n    pub fn n_values(\u0026self) -\u003e \u0026[usize] {\n        \u0026self.n_values_\n    }\n}\n\nimpl\u003cB: Backend\u003e Transformer\u003cB\u003e for OneHotEncoder\u003cB\u003e {\n    type Input = Tensor2D\u003cB\u003e;\n    type Output = Tensor2D\u003cB\u003e;\n    type Params = OneHotEncoderParams;\n    type Fitted = FittedOneHotEncoder\u003cB\u003e;\n\n    fn fit(\u0026self, data: \u0026Self::Input) -\u003e Result\u003cSelf::Fitted, PreprocessingError\u003e {\n        let (rows, cols) = data.shape();\n\n        if rows == 0 {\n            return Err(PreprocessingError::EmptyData(\n                \"Cannot fit OneHotEncoder on empty data\".to_string(),\n            ));\n        }\n\n        let data_vec = data.ravel().to_vec();\n\n        // Find unique categories for each column\n        let mut categories_: Vec\u003cVec\u003cf32\u003e\u003e = Vec::with_capacity(cols);\n        let mut n_values_: Vec\u003cusize\u003e = Vec::with_capacity(cols);\n\n        for col in 0..cols {\n            let mut col_cats: HashSet\u003ci32\u003e = HashSet::new();\n            for row in 0..rows {\n                let val = data_vec[row * cols + col];\n                if !val.is_finite() || val \u003c 0.0 || val.fract() != 0.0 {\n                    return Err(PreprocessingError::InvalidParameter(format!(\n                        \"OneHotEncoder expects non-negative integer values, got {} at ({}, {})\",\n                        val, row, col\n                    )));\n                }\n                col_cats.insert(val as i32);\n            }\n\n            // Sort categories\n            let mut sorted_cats: Vec\u003cf32\u003e = col_cats.into_iter().map(|x| x as f32).collect();\n            sorted_cats.sort_by(|a, b| a.partial_cmp(b).unwrap());\n\n            n_values_.push(sorted_cats.len());\n            categories_.push(sorted_cats);\n        }\n\n        let n_features_out: usize = n_values_.iter().sum();\n\n        Ok(FittedOneHotEncoder {\n            categories_,\n            n_values_,\n            n_features_out,\n            n_features_in: cols,\n            handle_unknown: self.handle_unknown,\n            _backend: PhantomData,\n        })\n    }\n\n    fn fit_transform(\u0026self, data: \u0026Self::Input) -\u003e Result\u003cSelf::Output, PreprocessingError\u003e {\n        let fitted = self.fit(data)?;\n        fitted.transform(data)\n    }\n}\n\nimpl\u003cB: Backend\u003e FittedTransformer\u003cB\u003e for FittedOneHotEncoder\u003cB\u003e {\n    type Input = Tensor2D\u003cB\u003e;\n    type Output = Tensor2D\u003cB\u003e;\n    type Params = OneHotEncoderParams;\n\n    fn transform(\u0026self, data: \u0026Self::Input) -\u003e Result\u003cSelf::Output, PreprocessingError\u003e {\n        let (rows, cols) = data.shape();\n\n        if cols != self.n_features_in {\n            return Err(PreprocessingError::FeatureMismatch {\n                expected_features: self.n_features_in,\n                got_features: cols,\n            });\n        }\n\n        if rows == 0 {\n            return Ok(Tensor2D::zeros(0, self.n_features_out));\n        }\n\n        let data_vec = data.ravel().to_vec();\n        let mut result = vec![0.0f64; rows * self.n_features_out];\n\n        for row in 0..rows {\n            let mut out_col_offset = 0;\n            for col in 0..cols {\n                let val = data_vec[row * cols + col];\n                let cats = \u0026self.categories_[col];\n\n                // Find category index\n                let cat_idx = cats.iter().position(|\u0026c| (c - val as f32).abs() \u003c 1e-6);\n\n                match cat_idx {\n                    Some(idx) =\u003e {\n                        result[row * self.n_features_out + out_col_offset + idx] = 1.0;\n                    }\n                    None =\u003e {\n                        if self.handle_unknown == HandleUnknown::Error {\n                            return Err(PreprocessingError::InvalidParameter(format!(\n                                \"Unknown category {} in column {}\",\n                                val, col\n                            )));\n                        }\n                        // With Ignore, leave as zeros\n                    }\n                }\n\n                out_col_offset += self.n_values_[col];\n            }\n        }\n\n        Ok(Tensor2D::new(\n            result.iter().map(|\u0026x| x as f32).collect(),\n            rows,\n            self.n_features_out,\n        ))\n    }\n\n    fn inverse_transform(\u0026self, data: \u0026Self::Output) -\u003e Result\u003cSelf::Input, PreprocessingError\u003e {\n        let (rows, out_cols) = data.shape();\n\n        if out_cols != self.n_features_out {\n            return Err(PreprocessingError::FeatureMismatch {\n                expected_features: self.n_features_out,\n                got_features: out_cols,\n            });\n        }\n\n        let data_vec = data.ravel().to_vec();\n        let mut result = vec![0.0f64; rows * self.n_features_in];\n\n        for row in 0..rows {\n            let mut in_col_offset = 0;\n            for col in 0..self.n_features_in {\n                let n_cats = self.n_values_[col];\n                let cats = \u0026self.categories_[col];\n\n                // Find the index of the 1 in this column's one-hot section\n                let mut found = false;\n                for (i, \u0026cat) in cats.iter().enumerate() {\n                    let val = data_vec[row * out_cols + in_col_offset + i];\n                    if val \u003e 0.5 {\n                        result[row * self.n_features_in + col] = cat as f64;\n                        found = true;\n                        break;\n                    }\n                }\n\n                if !found {\n                    // No category found - this shouldn't happen for valid one-hot data\n                    result[row * self.n_features_in + col] = f64::NAN;\n                }\n\n                in_col_offset += n_cats;\n            }\n        }\n\n        Ok(Tensor2D::new(\n            result.iter().map(|\u0026x| x as f32).collect(),\n            rows,\n            self.n_features_in,\n        ))\n    }\n\n    fn extract_params(\u0026self) -\u003e Self::Params {\n        OneHotEncoderParams {\n            categories_: self.categories_.clone(),\n            n_values_: self.n_values_.clone(),\n            n_features_out: self.n_features_out,\n            n_features_in: self.n_features_in,\n            handle_unknown: self.handle_unknown,\n        }\n    }\n\n    fn from_params(params: Self::Params) -\u003e Result\u003cSelf, PreprocessingError\u003e {\n        Ok(FittedOneHotEncoder {\n            categories_: params.categories_,\n            n_values_: params.n_values_,\n            n_features_out: params.n_features_out,\n            n_features_in: params.n_features_in,\n            handle_unknown: params.handle_unknown,\n            _backend: PhantomData,\n        })\n    }\n\n    fn n_features_in(\u0026self) -\u003e usize {\n        self.n_features_in\n    }\n}\n\n#[cfg(test)]\nmod tests {\n    use super::*;\n    use crate::backend::CpuBackend;\n\n    #[test]\n    fn test_one_hot_encoder_single_column() {\n        // Input: [[0], [1], [2]]\n        let data = Tensor2D::\u003cCpuBackend\u003e::new(vec![0.0f32, 1.0, 2.0], 3, 1);\n\n        let encoder = OneHotEncoder::\u003cCpuBackend\u003e::new();\n        let fitted = encoder.fit(\u0026data).unwrap();\n\n        assert_eq!(fitted.n_features_in(), 1);\n        assert_eq!(fitted.n_features_out(), 3);\n        assert_eq!(fitted.categories()[0], vec![0.0f32, 1.0, 2.0]);\n\n        let transformed = fitted.transform(\u0026data).unwrap();\n        let vals = transformed.ravel().to_vec();\n\n        // Expected: [[1,0,0], [0,1,0], [0,0,1]]\n        assert!((vals[0] - 1.0).abs() \u003c 1e-6);\n        assert!((vals[1] - 0.0).abs() \u003c 1e-6);\n        assert!((vals[2] - 0.0).abs() \u003c 1e-6);\n        assert!((vals[3] - 0.0).abs() \u003c 1e-6);\n        assert!((vals[4] - 1.0).abs() \u003c 1e-6);\n        assert!((vals[5] - 0.0).abs() \u003c 1e-6);\n    }\n\n    #[test]\n    fn test_one_hot_encoder_missing_category() {\n        // Input: [[0], [2]] - category 1 is missing\n        let data = Tensor2D::\u003cCpuBackend\u003e::new(vec![0.0f32, 2.0], 2, 1);\n\n        let encoder = OneHotEncoder::\u003cCpuBackend\u003e::new();\n        let fitted = encoder.fit(\u0026data).unwrap();\n\n        // Should only have 2 categories\n        assert_eq!(fitted.n_features_out(), 2);\n        assert_eq!(fitted.categories()[0], vec![0.0f32, 2.0]);\n\n        let transformed = fitted.transform(\u0026data).unwrap();\n        // [[1,0], [0,1]]\n        let vals = transformed.ravel().to_vec();\n        assert!((vals[0] - 1.0).abs() \u003c 1e-6);\n        assert!((vals[1] - 0.0).abs() \u003c 1e-6);\n        assert!((vals[2] - 0.0).abs() \u003c 1e-6);\n        assert!((vals[3] - 1.0).abs() \u003c 1e-6);\n    }\n\n    #[test]\n    fn test_one_hot_encoder_multiple_columns() {\n        // Input: [[0, 1], [1, 0]]\n        // Column 0 has categories [0, 1]\n        // Column 1 has categories [0, 1]\n        let data = Tensor2D::\u003cCpuBackend\u003e::new(vec![0.0f32, 1.0, 1.0, 0.0], 2, 2);\n\n        let encoder = OneHotEncoder::\u003cCpuBackend\u003e::new();\n        let fitted = encoder.fit(\u0026data).unwrap();\n\n        assert_eq!(fitted.n_features_out(), 4); // 2 + 2\n\n        let transformed = fitted.transform(\u0026data).unwrap();\n        // Expected: [[1,0, 0,1], [0,1, 1,0]]\n        let vals = transformed.ravel().to_vec();\n        assert!((vals[0] - 1.0).abs() \u003c 1e-6); // row 0, col 0 -\u003e [1,0]\n        assert!((vals[1] - 0.0).abs() \u003c 1e-6);\n        assert!((vals[2] - 0.0).abs() \u003c 1e-6); // row 0, col 1 -\u003e [0,1]\n        assert!((vals[3] - 1.0).abs() \u003c 1e-6);\n    }\n\n    #[test]\n    fn test_one_hot_encoder_unknown_error() {\n        let train = Tensor2D::\u003cCpuBackend\u003e::new(vec![0.0f32, 1.0], 2, 1);\n        let test = Tensor2D::\u003cCpuBackend\u003e::new(vec![2.0f32], 1, 1); // unknown category\n\n        let encoder = OneHotEncoder::\u003cCpuBackend\u003e::new();\n        let fitted = encoder.fit(\u0026train).unwrap();\n\n        let result = fitted.transform(\u0026test);\n        assert!(result.is_err());\n    }\n\n    #[test]\n    fn test_one_hot_encoder_unknown_ignore() {\n        let train = Tensor2D::\u003cCpuBackend\u003e::new(vec![0.0f32, 1.0], 2, 1);\n        let test = Tensor2D::\u003cCpuBackend\u003e::new(vec![2.0f32], 1, 1); // unknown category\n\n        let encoder = OneHotEncoder::\u003cCpuBackend\u003e::new().with_handle_unknown(HandleUnknown::Ignore);\n        let fitted = encoder.fit(\u0026train).unwrap();\n\n        let transformed = fitted.transform(\u0026test).unwrap();\n        // Should output [0, 0] for unknown\n        let vals = transformed.ravel().to_vec();\n        assert!((vals[0] - 0.0).abs() \u003c 1e-6);\n        assert!((vals[1] - 0.0).abs() \u003c 1e-6);\n    }\n\n    #[test]\n    fn test_one_hot_encoder_inverse() {\n        let data = Tensor2D::\u003cCpuBackend\u003e::new(vec![0.0f32, 1.0, 2.0], 3, 1);\n\n        let encoder = OneHotEncoder::\u003cCpuBackend\u003e::new();\n        let fitted = encoder.fit(\u0026data).unwrap();\n\n        let transformed = fitted.transform(\u0026data).unwrap();\n        let recovered = fitted.inverse_transform(\u0026transformed).unwrap();\n\n        let orig = data.ravel().to_vec();\n        let rec = recovered.ravel().to_vec();\n\n        for (o, r) in orig.iter().zip(rec.iter()) {\n            assert!((o - r).abs() \u003c 1e-6);\n        }\n    }\n\n    #[test]\n    fn test_one_hot_encoder_serialization() {\n        let data = Tensor2D::\u003cCpuBackend\u003e::new(vec![0.0f32, 1.0, 2.0], 3, 1);\n\n        let encoder = OneHotEncoder::\u003cCpuBackend\u003e::new();\n        let fitted = encoder.fit(\u0026data).unwrap();\n\n        let temp_file = std::env::temp_dir().join(\"test_onehot.bin\");\n        fitted.save_to_file(\u0026temp_file).unwrap();\n\n        let loaded = FittedOneHotEncoder::\u003cCpuBackend\u003e::load_from_file(\u0026temp_file).unwrap();\n\n        assert_eq!(loaded.n_features_in(), fitted.n_features_in());\n        assert_eq!(loaded.n_features_out(), fitted.n_features_out());\n        assert_eq!(loaded.categories(), fitted.categories());\n\n        std::fs::remove_file(temp_file).ok();\n    }\n}\n","traces":[{"line":46,"address":[3474544],"length":1,"stats":{"Line":1}},{"line":48,"address":[3474545],"length":1,"stats":{"Line":1}},{"line":54,"address":[3474496],"length":1,"stats":{"Line":1}},{"line":55,"address":[3474518],"length":1,"stats":{"Line":1}},{"line":56,"address":[3474524],"length":1,"stats":{"Line":1}},{"line":61,"address":[],"length":0,"stats":{"Line":0}},{"line":62,"address":[],"length":0,"stats":{"Line":0}},{"line":105,"address":[3474560],"length":1,"stats":{"Line":1}},{"line":106,"address":[3474565],"length":1,"stats":{"Line":1}},{"line":110,"address":[3474576],"length":1,"stats":{"Line":1}},{"line":111,"address":[3474581],"length":1,"stats":{"Line":1}},{"line":115,"address":[],"length":0,"stats":{"Line":0}},{"line":116,"address":[],"length":0,"stats":{"Line":0}},{"line":126,"address":[3461616,3464105,3463374],"length":1,"stats":{"Line":1}},{"line":127,"address":[3461682],"length":1,"stats":{"Line":1}},{"line":129,"address":[3461735],"length":1,"stats":{"Line":1}},{"line":130,"address":[3461779],"length":1,"stats":{"Line":0}},{"line":131,"address":[3461745],"length":1,"stats":{"Line":0}},{"line":135,"address":[3461914,3461966],"length":1,"stats":{"Line":1}},{"line":138,"address":[3462061],"length":1,"stats":{"Line":1}},{"line":139,"address":[3462091],"length":1,"stats":{"Line":1}},{"line":141,"address":[3462162,3462250,3463351],"length":1,"stats":{"Line":6}},{"line":142,"address":[3462371],"length":1,"stats":{"Line":1}},{"line":143,"address":[3462775,3462859],"length":1,"stats":{"Line":2}},{"line":144,"address":[3463385,3462968],"length":1,"stats":{"Line":3}},{"line":145,"address":[3463496,3463601],"length":1,"stats":{"Line":3}},{"line":146,"address":[3463723,3463534],"length":1,"stats":{"Line":0}},{"line":147,"address":[],"length":0,"stats":{"Line":0}},{"line":148,"address":[],"length":0,"stats":{"Line":0}},{"line":151,"address":[3463654],"length":1,"stats":{"Line":3}},{"line":155,"address":[3462998,3464217,3464208],"length":1,"stats":{"Line":6}},{"line":156,"address":[3463207,3464128,3463139,3464171],"length":1,"stats":{"Line":10}},{"line":158,"address":[3463222],"length":1,"stats":{"Line":4}},{"line":159,"address":[3463265],"length":1,"stats":{"Line":1}},{"line":162,"address":[3462381],"length":1,"stats":{"Line":1}},{"line":164,"address":[3462610],"length":1,"stats":{"Line":1}},{"line":165,"address":[3462512],"length":1,"stats":{"Line":4}},{"line":166,"address":[3462560],"length":1,"stats":{"Line":1}},{"line":167,"address":[],"length":0,"stats":{"Line":0}},{"line":168,"address":[],"length":0,"stats":{"Line":0}},{"line":169,"address":[3462608],"length":1,"stats":{"Line":4}},{"line":170,"address":[],"length":0,"stats":{"Line":0}},{"line":174,"address":[],"length":0,"stats":{"Line":0}},{"line":175,"address":[],"length":0,"stats":{"Line":0}},{"line":176,"address":[],"length":0,"stats":{"Line":0}},{"line":185,"address":[3466896,3469180,3469186],"length":1,"stats":{"Line":1}},{"line":186,"address":[3466962],"length":1,"stats":{"Line":2}},{"line":188,"address":[3467007],"length":1,"stats":{"Line":2}},{"line":189,"address":[3467060],"length":1,"stats":{"Line":0}},{"line":190,"address":[3467056],"length":1,"stats":{"Line":0}},{"line":191,"address":[],"length":0,"stats":{"Line":0}},{"line":195,"address":[3467021],"length":1,"stats":{"Line":2}},{"line":196,"address":[3467125],"length":1,"stats":{"Line":0}},{"line":199,"address":[3467223,3467275],"length":1,"stats":{"Line":1}},{"line":200,"address":[3467378],"length":1,"stats":{"Line":2}},{"line":202,"address":[3467456,3467543],"length":1,"stats":{"Line":3}},{"line":203,"address":[3467672],"length":1,"stats":{"Line":1}},{"line":204,"address":[3469154,3467684,3468007],"length":1,"stats":{"Line":4}},{"line":205,"address":[3468140],"length":1,"stats":{"Line":2}},{"line":206,"address":[3468293],"length":1,"stats":{"Line":1}},{"line":209,"address":[3468333,3469213,3469200],"length":1,"stats":{"Line":6}},{"line":211,"address":[3468462],"length":1,"stats":{"Line":1}},{"line":212,"address":[3468492],"length":1,"stats":{"Line":3}},{"line":213,"address":[3468513,3468943],"length":1,"stats":{"Line":4}},{"line":215,"address":[],"length":0,"stats":{"Line":0}},{"line":216,"address":[3468544],"length":1,"stats":{"Line":1}},{"line":217,"address":[3468626],"length":1,"stats":{"Line":1}},{"line":218,"address":[],"length":0,"stats":{"Line":0}},{"line":219,"address":[],"length":0,"stats":{"Line":0}},{"line":226,"address":[3469159,3468584,3469116],"length":1,"stats":{"Line":4}},{"line":230,"address":[3467894],"length":1,"stats":{"Line":3}},{"line":231,"address":[3469280,3469290,3467726],"length":1,"stats":{"Line":7}},{"line":232,"address":[],"length":0,"stats":{"Line":0}},{"line":233,"address":[3467874],"length":1,"stats":{"Line":1}},{"line":237,"address":[3466845,3464656,3466839],"length":1,"stats":{"Line":1}},{"line":238,"address":[3464722],"length":1,"stats":{"Line":1}},{"line":240,"address":[3464767],"length":1,"stats":{"Line":1}},{"line":241,"address":[3464853],"length":1,"stats":{"Line":0}},{"line":242,"address":[3464849],"length":1,"stats":{"Line":0}},{"line":243,"address":[],"length":0,"stats":{"Line":0}},{"line":247,"address":[3464918,3464797],"length":1,"stats":{"Line":1}},{"line":248,"address":[3465021],"length":1,"stats":{"Line":1}},{"line":250,"address":[3465186,3465099],"length":1,"stats":{"Line":2}},{"line":251,"address":[3465315],"length":1,"stats":{"Line":1}},{"line":252,"address":[3466813,3465665,3465327],"length":1,"stats":{"Line":3}},{"line":253,"address":[3465798],"length":1,"stats":{"Line":1}},{"line":254,"address":[3465867],"length":1,"stats":{"Line":1}},{"line":257,"address":[3465905],"length":1,"stats":{"Line":1}},{"line":258,"address":[3465913],"length":1,"stats":{"Line":1}},{"line":259,"address":[3466256,3466217],"length":1,"stats":{"Line":2}},{"line":260,"address":[3466420],"length":1,"stats":{"Line":1}},{"line":261,"address":[3466454],"length":1,"stats":{"Line":1}},{"line":262,"address":[3466592],"length":1,"stats":{"Line":1}},{"line":263,"address":[],"length":0,"stats":{"Line":0}},{"line":267,"address":[3466795,3466232],"length":1,"stats":{"Line":1}},{"line":269,"address":[3466688,3466621],"length":1,"stats":{"Line":0}},{"line":272,"address":[3466805,3466645,3466818],"length":1,"stats":{"Line":2}},{"line":276,"address":[3465541],"length":1,"stats":{"Line":1}},{"line":277,"address":[3465373,3466864,3466874],"length":1,"stats":{"Line":3}},{"line":278,"address":[],"length":0,"stats":{"Line":0}},{"line":279,"address":[3465521],"length":1,"stats":{"Line":1}},{"line":283,"address":[3464432,3464633,3464627],"length":1,"stats":{"Line":1}},{"line":285,"address":[3464456],"length":1,"stats":{"Line":1}},{"line":286,"address":[3464478],"length":1,"stats":{"Line":1}},{"line":287,"address":[3464544],"length":1,"stats":{"Line":1}},{"line":288,"address":[3464548],"length":1,"stats":{"Line":1}},{"line":289,"address":[3464552],"length":1,"stats":{"Line":1}},{"line":293,"address":[3464224],"length":1,"stats":{"Line":1}},{"line":294,"address":[3464306],"length":1,"stats":{"Line":1}},{"line":295,"address":[3464239],"length":1,"stats":{"Line":1}},{"line":296,"address":[3464265],"length":1,"stats":{"Line":1}},{"line":297,"address":[3464295],"length":1,"stats":{"Line":1}},{"line":298,"address":[3464299],"length":1,"stats":{"Line":1}},{"line":299,"address":[3464303],"length":1,"stats":{"Line":1}},{"line":300,"address":[],"length":0,"stats":{"Line":0}},{"line":304,"address":[3464416],"length":1,"stats":{"Line":1}},{"line":305,"address":[3464421],"length":1,"stats":{"Line":1}}],"covered":87,"coverable":117},{"path":["/","home","victor","ML","machinelearne-rs","lib","src","preprocessing","encoding","ordinal.rs"],"content":"//! Ordinal encoding for categorical features.\n//!\n//! Maps categorical values to integer ordinals (0, 1, 2, ...).\n\nuse crate::backend::{Backend, Tensor2D};\nuse crate::preprocessing::encoding::HandleUnknown;\nuse crate::preprocessing::error::PreprocessingError;\nuse crate::preprocessing::traits::{FittedTransformer, Transformer};\nuse serde::{Deserialize, Serialize};\nuse std::collections::HashMap;\nuse std::marker::PhantomData;\n\n/// Ordinal encoder for categorical features.\n///\n/// Maps each unique category to an integer ordinal (0, 1, 2, ...).\n/// The mapping is learned from the training data, with categories\n/// sorted in ascending order.\n///\n/// # Example\n/// ```ignore\n/// use machinelearne_rs::preprocessing::{OrdinalEncoder, Transformer};\n/// use machinelearne_rs::backend::CpuBackend;\n///\n/// // Input: [[0], [2], [1]]  (categories: 0, 1, 2)\n/// let data = Tensor2D::new(vec![0.0, 2.0, 1.0], 3, 1);\n///\n/// let encoder = OrdinalEncoder::\u003cCpuBackend\u003e::new();\n/// let fitted = encoder.fit(\u0026data)?;\n///\n/// // Output: [[0], [2], [1]]  (ordinal mapping)\n/// let encoded = fitted.transform(\u0026data)?;\n/// ```\n#[derive(Clone, Debug)]\npub struct OrdinalEncoder\u003cB: Backend\u003e {\n    /// How to handle unknown categories during transform.\n    handle_unknown: HandleUnknown,\n    _backend: PhantomData\u003cB\u003e,\n}\n\nimpl\u003cB: Backend\u003e OrdinalEncoder\u003cB\u003e {\n    /// Create a new OrdinalEncoder with default settings.\n    pub fn new() -\u003e Self {\n        Self {\n            handle_unknown: HandleUnknown::default(),\n            _backend: PhantomData,\n        }\n    }\n\n    /// Set the strategy for handling unknown categories.\n    pub fn with_handle_unknown(mut self, strategy: HandleUnknown) -\u003e Self {\n        self.handle_unknown = strategy;\n        self\n    }\n}\n\nimpl\u003cB: Backend\u003e Default for OrdinalEncoder\u003cB\u003e {\n    fn default() -\u003e Self {\n        Self::new()\n    }\n}\n\n/// Serializable parameters for a fitted OrdinalEncoder.\n#[derive(Clone, Serialize, Deserialize)]\npub struct OrdinalEncoderParams {\n    /// Categories (unique sorted values) for each input column.\n    pub categories_: Vec\u003cVec\u003cf32\u003e\u003e,\n    /// Mapping from category value to ordinal for each column.\n    /// Stored as (category, ordinal) pairs for serialization.\n    pub mappings_: Vec\u003cVec\u003c(f32, usize)\u003e\u003e,\n    /// Number of input features.\n    pub n_features_in: usize,\n    /// Handle unknown strategy.\n    pub handle_unknown: HandleUnknown,\n}\n\n// Note: We rely on the blanket impl of SerializableParams for Serialize + Deserialize\n\n/// Fitted OrdinalEncoder ready for inference.\n#[derive(Clone)]\npub struct FittedOrdinalEncoder\u003cB: Backend\u003e {\n    /// Categories (unique sorted values) for each input column.\n    categories_: Vec\u003cVec\u003cf32\u003e\u003e,\n    /// Mapping from category value to ordinal index for each column.\n    mappings_: Vec\u003cHashMap\u003ci32, usize\u003e\u003e,\n    /// Number of input features.\n    n_features_in: usize,\n    /// Handle unknown strategy.\n    handle_unknown: HandleUnknown,\n    _backend: PhantomData\u003cB\u003e,\n}\n\nimpl\u003cB: Backend\u003e FittedOrdinalEncoder\u003cB\u003e {\n    /// Get the categories learned for each feature.\n    pub fn categories(\u0026self) -\u003e \u0026[Vec\u003cf32\u003e] {\n        \u0026self.categories_\n    }\n\n    /// Get the mapping (category -\u003e ordinal) for a specific feature.\n    pub fn mapping(\u0026self, feature_idx: usize) -\u003e Option\u003c\u0026HashMap\u003ci32, usize\u003e\u003e {\n        self.mappings_.get(feature_idx)\n    }\n}\n\nimpl\u003cB: Backend\u003e Transformer\u003cB\u003e for OrdinalEncoder\u003cB\u003e {\n    type Input = Tensor2D\u003cB\u003e;\n    type Output = Tensor2D\u003cB\u003e;\n    type Params = OrdinalEncoderParams;\n    type Fitted = FittedOrdinalEncoder\u003cB\u003e;\n\n    fn fit(\u0026self, data: \u0026Self::Input) -\u003e Result\u003cSelf::Fitted, PreprocessingError\u003e {\n        let (rows, cols) = data.shape();\n\n        if rows == 0 {\n            return Err(PreprocessingError::EmptyData(\n                \"Cannot fit OrdinalEncoder on empty data\".to_string(),\n            ));\n        }\n\n        let data_vec = data.ravel().to_vec();\n\n        // Find unique categories for each column\n        let mut categories_: Vec\u003cVec\u003cf32\u003e\u003e = Vec::with_capacity(cols);\n        let mut mappings_: Vec\u003cHashMap\u003ci32, usize\u003e\u003e = Vec::with_capacity(cols);\n\n        for col in 0..cols {\n            let mut col_cats: std::collections::HashSet\u003ci32\u003e = std::collections::HashSet::new();\n            for row in 0..rows {\n                let val = data_vec[row * cols + col];\n                if !val.is_finite() {\n                    return Err(PreprocessingError::InvalidParameter(format!(\n                        \"OrdinalEncoder expects finite values, got {} at ({}, {})\",\n                        val, row, col\n                    )));\n                }\n                // For ordinal encoding, we allow non-integer values too\n                // (they'll be rounded to i32 for the mapping key)\n                col_cats.insert(val.round() as i32);\n            }\n\n            // Sort categories\n            let mut sorted_cats: Vec\u003ci32\u003e = col_cats.into_iter().collect();\n            sorted_cats.sort();\n\n            // Create mapping\n            let mut mapping = HashMap::new();\n            for (idx, \u0026cat) in sorted_cats.iter().enumerate() {\n                mapping.insert(cat, idx);\n            }\n\n            categories_.push(sorted_cats.into_iter().map(|x| x as f32).collect());\n            mappings_.push(mapping);\n        }\n\n        Ok(FittedOrdinalEncoder {\n            categories_,\n            mappings_,\n            n_features_in: cols,\n            handle_unknown: self.handle_unknown,\n            _backend: PhantomData,\n        })\n    }\n\n    fn fit_transform(\u0026self, data: \u0026Self::Input) -\u003e Result\u003cSelf::Output, PreprocessingError\u003e {\n        let fitted = self.fit(data)?;\n        fitted.transform(data)\n    }\n}\n\nimpl\u003cB: Backend\u003e FittedTransformer\u003cB\u003e for FittedOrdinalEncoder\u003cB\u003e {\n    type Input = Tensor2D\u003cB\u003e;\n    type Output = Tensor2D\u003cB\u003e;\n    type Params = OrdinalEncoderParams;\n\n    fn transform(\u0026self, data: \u0026Self::Input) -\u003e Result\u003cSelf::Output, PreprocessingError\u003e {\n        let (rows, cols) = data.shape();\n\n        if cols != self.n_features_in {\n            return Err(PreprocessingError::FeatureMismatch {\n                expected_features: self.n_features_in,\n                got_features: cols,\n            });\n        }\n\n        if rows == 0 {\n            return Ok(Tensor2D::zeros(0, cols));\n        }\n\n        let data_vec = data.ravel().to_vec();\n        let mut result = vec![0.0f64; rows * cols];\n\n        for row in 0..rows {\n            for col in 0..cols {\n                let val = data_vec[row * cols + col];\n                let key = val.round() as i32;\n\n                match self.mappings_[col].get(\u0026key) {\n                    Some(\u0026ordinal) =\u003e {\n                        result[row * cols + col] = ordinal as f64;\n                    }\n                    None =\u003e {\n                        if self.handle_unknown == HandleUnknown::Error {\n                            return Err(PreprocessingError::InvalidParameter(format!(\n                                \"Unknown category {} in column {}\",\n                                val, col\n                            )));\n                        }\n                        // With Ignore, output NaN\n                        result[row * cols + col] = f64::NAN;\n                    }\n                }\n            }\n        }\n\n        Ok(Tensor2D::new(\n            result.iter().map(|\u0026x| x as f32).collect(),\n            rows,\n            cols,\n        ))\n    }\n\n    fn inverse_transform(\u0026self, data: \u0026Self::Output) -\u003e Result\u003cSelf::Input, PreprocessingError\u003e {\n        let (rows, cols) = data.shape();\n\n        if cols != self.n_features_in {\n            return Err(PreprocessingError::FeatureMismatch {\n                expected_features: self.n_features_in,\n                got_features: cols,\n            });\n        }\n\n        let data_vec = data.ravel().to_vec();\n        let mut result = vec![0.0f64; rows * cols];\n\n        for row in 0..rows {\n            for col in 0..cols {\n                let ordinal = data_vec[row * cols + col] as usize;\n                if ordinal \u003c self.categories_[col].len() {\n                    result[row * cols + col] = self.categories_[col][ordinal] as f64;\n                } else {\n                    result[row * cols + col] = f64::NAN;\n                }\n            }\n        }\n\n        Ok(Tensor2D::new(\n            result.iter().map(|\u0026x| x as f32).collect(),\n            rows,\n            cols,\n        ))\n    }\n\n    fn extract_params(\u0026self) -\u003e Self::Params {\n        // Convert HashMaps to Vec\u003c(f32, usize)\u003e for serialization\n        let mappings_: Vec\u003cVec\u003c(f32, usize)\u003e\u003e = self\n            .mappings_\n            .iter()\n            .map(|m| {\n                let mut pairs: Vec\u003c(f32, usize)\u003e = m.iter().map(|(\u0026k, \u0026v)| (k as f32, v)).collect();\n                pairs.sort_by_key(|\u0026(_, v)| v);\n                pairs\n            })\n            .collect();\n\n        OrdinalEncoderParams {\n            categories_: self.categories_.clone(),\n            mappings_,\n            n_features_in: self.n_features_in,\n            handle_unknown: self.handle_unknown,\n        }\n    }\n\n    fn from_params(params: Self::Params) -\u003e Result\u003cSelf, PreprocessingError\u003e {\n        // Convert Vec\u003c(f32, usize)\u003e back to HashMap\n        let mappings_: Vec\u003cHashMap\u003ci32, usize\u003e\u003e = params\n            .mappings_\n            .iter()\n            .map(|pairs| pairs.iter().map(|\u0026(k, v)| (k.round() as i32, v)).collect())\n            .collect();\n\n        Ok(FittedOrdinalEncoder {\n            categories_: params.categories_,\n            mappings_,\n            n_features_in: params.n_features_in,\n            handle_unknown: params.handle_unknown,\n            _backend: PhantomData,\n        })\n    }\n\n    fn n_features_in(\u0026self) -\u003e usize {\n        self.n_features_in\n    }\n}\n\n#[cfg(test)]\nmod tests {\n    use super::*;\n    use crate::backend::CpuBackend;\n\n    #[test]\n    fn test_ordinal_encoder_basic() {\n        // Input: [[0], [2], [1]]\n        let data = Tensor2D::\u003cCpuBackend\u003e::new(vec![0.0f32, 2.0, 1.0], 3, 1);\n\n        let encoder = OrdinalEncoder::\u003cCpuBackend\u003e::new();\n        let fitted = encoder.fit(\u0026data).unwrap();\n\n        assert_eq!(fitted.n_features_in(), 1);\n        assert_eq!(fitted.categories()[0], vec![0.0f32, 1.0, 2.0]);\n\n        let transformed = fitted.transform(\u0026data).unwrap();\n        let vals = transformed.ravel().to_vec();\n\n        // Categories sorted: 0-\u003e0, 1-\u003e1, 2-\u003e2\n        assert!((vals[0] - 0.0).abs() \u003c 1e-6); // 0 -\u003e 0\n        assert!((vals[1] - 2.0).abs() \u003c 1e-6); // 2 -\u003e 2\n        assert!((vals[2] - 1.0).abs() \u003c 1e-6); // 1 -\u003e 1\n    }\n\n    #[test]\n    fn test_ordinal_encoder_non_contiguous() {\n        // Input: [[0], [5], [10]]\n        let data = Tensor2D::\u003cCpuBackend\u003e::new(vec![0.0f32, 5.0, 10.0], 3, 1);\n\n        let encoder = OrdinalEncoder::\u003cCpuBackend\u003e::new();\n        let fitted = encoder.fit(\u0026data).unwrap();\n\n        assert_eq!(fitted.categories()[0], vec![0.0f32, 5.0, 10.0]);\n\n        let transformed = fitted.transform(\u0026data).unwrap();\n        let vals = transformed.ravel().to_vec();\n\n        // Ordinals: 0-\u003e0, 5-\u003e1, 10-\u003e2\n        assert!((vals[0] - 0.0).abs() \u003c 1e-6);\n        assert!((vals[1] - 1.0).abs() \u003c 1e-6);\n        assert!((vals[2] - 2.0).abs() \u003c 1e-6);\n    }\n\n    #[test]\n    fn test_ordinal_encoder_inverse() {\n        let data = Tensor2D::\u003cCpuBackend\u003e::new(vec![0.0f32, 5.0, 10.0], 3, 1);\n\n        let encoder = OrdinalEncoder::\u003cCpuBackend\u003e::new();\n        let fitted = encoder.fit(\u0026data).unwrap();\n\n        let transformed = fitted.transform(\u0026data).unwrap();\n        let recovered = fitted.inverse_transform(\u0026transformed).unwrap();\n\n        let orig = data.ravel().to_vec();\n        let rec = recovered.ravel().to_vec();\n\n        for (o, r) in orig.iter().zip(rec.iter()) {\n            assert!((o - r).abs() \u003c 1e-6);\n        }\n    }\n\n    #[test]\n    fn test_ordinal_encoder_unknown_error() {\n        let train = Tensor2D::\u003cCpuBackend\u003e::new(vec![0.0f32, 1.0], 2, 1);\n        let test = Tensor2D::\u003cCpuBackend\u003e::new(vec![2.0f32], 1, 1);\n\n        let encoder = OrdinalEncoder::\u003cCpuBackend\u003e::new();\n        let fitted = encoder.fit(\u0026train).unwrap();\n\n        let result = fitted.transform(\u0026test);\n        assert!(result.is_err());\n    }\n\n    #[test]\n    fn test_ordinal_encoder_unknown_ignore() {\n        let train = Tensor2D::\u003cCpuBackend\u003e::new(vec![0.0f32, 1.0], 2, 1);\n        let test = Tensor2D::\u003cCpuBackend\u003e::new(vec![2.0f32], 1, 1);\n\n        let encoder =\n            OrdinalEncoder::\u003cCpuBackend\u003e::new().with_handle_unknown(HandleUnknown::Ignore);\n        let fitted = encoder.fit(\u0026train).unwrap();\n\n        let transformed = fitted.transform(\u0026test).unwrap();\n        let vals = transformed.ravel().to_vec();\n        assert!(vals[0].is_nan());\n    }\n\n    #[test]\n    fn test_ordinal_encoder_serialization() {\n        let data = Tensor2D::\u003cCpuBackend\u003e::new(vec![0.0f32, 5.0, 10.0], 3, 1);\n\n        let encoder = OrdinalEncoder::\u003cCpuBackend\u003e::new();\n        let fitted = encoder.fit(\u0026data).unwrap();\n\n        let temp_file = std::env::temp_dir().join(\"test_ordinal.bin\");\n        fitted.save_to_file(\u0026temp_file).unwrap();\n\n        let loaded = FittedOrdinalEncoder::\u003cCpuBackend\u003e::load_from_file(\u0026temp_file).unwrap();\n\n        assert_eq!(loaded.n_features_in(), fitted.n_features_in());\n        assert_eq!(loaded.categories(), fitted.categories());\n\n        // Verify transform gives same result\n        let t1 = fitted.transform(\u0026data).unwrap();\n        let t2 = loaded.transform(\u0026data).unwrap();\n        let v1 = t1.ravel().to_vec();\n        let v2 = t2.ravel().to_vec();\n        for (a, b) in v1.iter().zip(v2.iter()) {\n            assert!((a - b).abs() \u003c 1e-6);\n        }\n\n        std::fs::remove_file(temp_file).ok();\n    }\n}\n","traces":[{"line":42,"address":[2004144],"length":1,"stats":{"Line":1}},{"line":44,"address":[2004145],"length":1,"stats":{"Line":1}},{"line":50,"address":[2004096],"length":1,"stats":{"Line":1}},{"line":51,"address":[2004118],"length":1,"stats":{"Line":1}},{"line":52,"address":[2004124],"length":1,"stats":{"Line":1}},{"line":57,"address":[],"length":0,"stats":{"Line":0}},{"line":58,"address":[],"length":0,"stats":{"Line":0}},{"line":94,"address":[2004160],"length":1,"stats":{"Line":1}},{"line":95,"address":[2004165],"length":1,"stats":{"Line":1}},{"line":99,"address":[],"length":0,"stats":{"Line":0}},{"line":100,"address":[],"length":0,"stats":{"Line":0}},{"line":110,"address":[1993451,1994165,1991360],"length":1,"stats":{"Line":1}},{"line":111,"address":[1991426],"length":1,"stats":{"Line":2}},{"line":113,"address":[1991487],"length":1,"stats":{"Line":1}},{"line":114,"address":[1991531],"length":1,"stats":{"Line":0}},{"line":115,"address":[1991497],"length":1,"stats":{"Line":0}},{"line":119,"address":[1991718,1991666],"length":1,"stats":{"Line":3}},{"line":122,"address":[1991813],"length":1,"stats":{"Line":1}},{"line":123,"address":[1991843],"length":1,"stats":{"Line":3}},{"line":125,"address":[1991994,1991910,1993423],"length":1,"stats":{"Line":5}},{"line":126,"address":[1992115],"length":1,"stats":{"Line":1}},{"line":127,"address":[1992404,1992500],"length":1,"stats":{"Line":4}},{"line":128,"address":[1993480,1992609],"length":1,"stats":{"Line":4}},{"line":129,"address":[1993591],"length":1,"stats":{"Line":3}},{"line":130,"address":[1993721,1993629],"length":1,"stats":{"Line":0}},{"line":131,"address":[],"length":0,"stats":{"Line":0}},{"line":132,"address":[],"length":0,"stats":{"Line":0}},{"line":137,"address":[1993696,1994095],"length":1,"stats":{"Line":4}},{"line":141,"address":[1992639],"length":1,"stats":{"Line":1}},{"line":142,"address":[1992757,1992825],"length":1,"stats":{"Line":4}},{"line":145,"address":[1992840],"length":1,"stats":{"Line":3}},{"line":146,"address":[1992847,1992928],"length":1,"stats":{"Line":4}},{"line":147,"address":[1993164,1993428],"length":1,"stats":{"Line":4}},{"line":150,"address":[1993174,1994192,1994201],"length":1,"stats":{"Line":5}},{"line":151,"address":[1993313],"length":1,"stats":{"Line":3}},{"line":154,"address":[1992247],"length":1,"stats":{"Line":1}},{"line":155,"address":[1992149],"length":1,"stats":{"Line":3}},{"line":156,"address":[1992197],"length":1,"stats":{"Line":1}},{"line":157,"address":[],"length":0,"stats":{"Line":0}},{"line":158,"address":[1992245],"length":1,"stats":{"Line":4}},{"line":159,"address":[],"length":0,"stats":{"Line":0}},{"line":163,"address":[],"length":0,"stats":{"Line":0}},{"line":164,"address":[],"length":0,"stats":{"Line":0}},{"line":165,"address":[],"length":0,"stats":{"Line":0}},{"line":174,"address":[1997216,1999486,1999492],"length":1,"stats":{"Line":2}},{"line":175,"address":[1997282],"length":1,"stats":{"Line":2}},{"line":177,"address":[1997327],"length":1,"stats":{"Line":3}},{"line":178,"address":[1997380],"length":1,"stats":{"Line":0}},{"line":179,"address":[1997376],"length":1,"stats":{"Line":0}},{"line":180,"address":[],"length":0,"stats":{"Line":0}},{"line":184,"address":[1997341],"length":1,"stats":{"Line":1}},{"line":185,"address":[1997445],"length":1,"stats":{"Line":0}},{"line":188,"address":[1997591,1997539],"length":1,"stats":{"Line":3}},{"line":189,"address":[1997694],"length":1,"stats":{"Line":1}},{"line":191,"address":[1997771,1997858],"length":1,"stats":{"Line":4}},{"line":192,"address":[1997987,1998306],"length":1,"stats":{"Line":4}},{"line":193,"address":[1998433],"length":1,"stats":{"Line":3}},{"line":194,"address":[1998566],"length":1,"stats":{"Line":1}},{"line":196,"address":[1998649],"length":1,"stats":{"Line":3}},{"line":197,"address":[1998765],"length":1,"stats":{"Line":1}},{"line":198,"address":[1998789,1999331],"length":1,"stats":{"Line":3}},{"line":200,"address":[1999052],"length":1,"stats":{"Line":1}},{"line":201,"address":[1998819],"length":1,"stats":{"Line":1}},{"line":202,"address":[1998890,1999057],"length":1,"stats":{"Line":2}},{"line":203,"address":[],"length":0,"stats":{"Line":0}},{"line":204,"address":[],"length":0,"stats":{"Line":0}},{"line":208,"address":[1998938,1998867],"length":1,"stats":{"Line":2}},{"line":214,"address":[1998193],"length":1,"stats":{"Line":1}},{"line":215,"address":[1998029,1999530,1999520],"length":1,"stats":{"Line":5}},{"line":216,"address":[],"length":0,"stats":{"Line":0}},{"line":217,"address":[],"length":0,"stats":{"Line":0}},{"line":221,"address":[1995376,1997170,1997164],"length":1,"stats":{"Line":1}},{"line":222,"address":[1995442],"length":1,"stats":{"Line":1}},{"line":224,"address":[1995487],"length":1,"stats":{"Line":1}},{"line":225,"address":[1995573],"length":1,"stats":{"Line":0}},{"line":226,"address":[1995569],"length":1,"stats":{"Line":0}},{"line":227,"address":[],"length":0,"stats":{"Line":0}},{"line":231,"address":[1995638,1995517],"length":1,"stats":{"Line":1}},{"line":232,"address":[1995741],"length":1,"stats":{"Line":1}},{"line":234,"address":[1995905,1995818],"length":1,"stats":{"Line":2}},{"line":235,"address":[1996034,1996364],"length":1,"stats":{"Line":2}},{"line":236,"address":[1996502],"length":1,"stats":{"Line":1}},{"line":237,"address":[1996962,1996729,1997159],"length":1,"stats":{"Line":2}},{"line":238,"address":[1996977,1996824],"length":1,"stats":{"Line":2}},{"line":240,"address":[1996796,1996853],"length":1,"stats":{"Line":0}},{"line":245,"address":[1996240],"length":1,"stats":{"Line":1}},{"line":246,"address":[1996076,1997194,1997184],"length":1,"stats":{"Line":3}},{"line":247,"address":[],"length":0,"stats":{"Line":0}},{"line":248,"address":[],"length":0,"stats":{"Line":0}},{"line":252,"address":[1995039,1995033,1994784],"length":1,"stats":{"Line":1}},{"line":254,"address":[1994815],"length":1,"stats":{"Line":1}},{"line":255,"address":[],"length":0,"stats":{"Line":0}},{"line":257,"address":[1994841,1995275,1995281,1995056],"length":1,"stats":{"Line":2}},{"line":258,"address":[1995113,1995343,1995328],"length":1,"stats":{"Line":3}},{"line":259,"address":[1995296,1995163,1995224,1995306],"length":1,"stats":{"Line":4}},{"line":260,"address":[1995241],"length":1,"stats":{"Line":1}},{"line":265,"address":[1994864],"length":1,"stats":{"Line":1}},{"line":267,"address":[1994958],"length":1,"stats":{"Line":1}},{"line":268,"address":[1994962],"length":1,"stats":{"Line":1}},{"line":272,"address":[1994556,1994208],"length":1,"stats":{"Line":1}},{"line":274,"address":[1994238],"length":1,"stats":{"Line":1}},{"line":275,"address":[],"length":0,"stats":{"Line":0}},{"line":277,"address":[1994611,1994576,1994672,1994333,1994686],"length":1,"stats":{"Line":5}},{"line":280,"address":[1994422],"length":1,"stats":{"Line":1}},{"line":281,"address":[1994380],"length":1,"stats":{"Line":1}},{"line":282,"address":[],"length":0,"stats":{"Line":0}},{"line":283,"address":[1994415],"length":1,"stats":{"Line":1}},{"line":284,"address":[1994419],"length":1,"stats":{"Line":1}},{"line":285,"address":[],"length":0,"stats":{"Line":0}},{"line":289,"address":[1994768],"length":1,"stats":{"Line":1}},{"line":290,"address":[1994773],"length":1,"stats":{"Line":1}}],"covered":79,"coverable":111},{"path":["/","home","victor","ML","machinelearne-rs","lib","src","preprocessing","error.rs"],"content":"//! Error types for preprocessing operations.\n\nuse std::fmt;\n\n/// Error type for preprocessing operations.\n#[derive(Debug)]\npub enum PreprocessingError {\n    /// Shape mismatch between expected and actual tensor dimensions.\n    InvalidShape { expected: String, got: String },\n    /// Numerical computation error (overflow, underflow, etc.).\n    NumericalError(String),\n    /// Data contains missing values (NaN) when not expected.\n    MissingValues(String),\n    /// Invalid hyperparameter value.\n    InvalidParameter(String),\n    /// Serialization or deserialization error.\n    SerializationError(String),\n    /// I/O error during file operations.\n    IoError(String),\n    /// Empty data provided where non-empty was required.\n    EmptyData(String),\n    /// Feature dimension mismatch.\n    FeatureMismatch {\n        expected_features: usize,\n        got_features: usize,\n    },\n}\n\nimpl fmt::Display for PreprocessingError {\n    fn fmt(\u0026self, f: \u0026mut fmt::Formatter\u003c'_\u003e) -\u003e fmt::Result {\n        match self {\n            PreprocessingError::InvalidShape { expected, got } =\u003e {\n                write!(f, \"Invalid shape: expected {}, got {}\", expected, got)\n            }\n            PreprocessingError::NumericalError(msg) =\u003e {\n                write!(f, \"Numerical error: {}\", msg)\n            }\n            PreprocessingError::MissingValues(msg) =\u003e {\n                write!(f, \"Missing values: {}\", msg)\n            }\n            PreprocessingError::InvalidParameter(msg) =\u003e {\n                write!(f, \"Invalid parameter: {}\", msg)\n            }\n            PreprocessingError::SerializationError(msg) =\u003e {\n                write!(f, \"Serialization error: {}\", msg)\n            }\n            PreprocessingError::IoError(msg) =\u003e {\n                write!(f, \"I/O error: {}\", msg)\n            }\n            PreprocessingError::EmptyData(msg) =\u003e {\n                write!(f, \"Empty data: {}\", msg)\n            }\n            PreprocessingError::FeatureMismatch {\n                expected_features,\n                got_features,\n            } =\u003e {\n                write!(\n                    f,\n                    \"Feature mismatch: expected {} features, got {}\",\n                    expected_features, got_features\n                )\n            }\n        }\n    }\n}\n\nimpl std::error::Error for PreprocessingError {}\n\nimpl From\u003cstd::io::Error\u003e for PreprocessingError {\n    fn from(err: std::io::Error) -\u003e Self {\n        PreprocessingError::IoError(err.to_string())\n    }\n}\n\nimpl From\u003cbincode::Error\u003e for PreprocessingError {\n    fn from(err: bincode::Error) -\u003e Self {\n        PreprocessingError::SerializationError(err.to_string())\n    }\n}\n","traces":[{"line":30,"address":[3364608],"length":1,"stats":{"Line":0}},{"line":31,"address":[3364641],"length":1,"stats":{"Line":0}},{"line":32,"address":[3364700],"length":1,"stats":{"Line":0}},{"line":33,"address":[3364714],"length":1,"stats":{"Line":0}},{"line":35,"address":[3364887],"length":1,"stats":{"Line":0}},{"line":36,"address":[3364899],"length":1,"stats":{"Line":0}},{"line":38,"address":[3365016],"length":1,"stats":{"Line":0}},{"line":39,"address":[3365028],"length":1,"stats":{"Line":0}},{"line":41,"address":[3365145],"length":1,"stats":{"Line":0}},{"line":42,"address":[3365157],"length":1,"stats":{"Line":0}},{"line":44,"address":[3365274],"length":1,"stats":{"Line":0}},{"line":45,"address":[3365286],"length":1,"stats":{"Line":0}},{"line":47,"address":[3365403],"length":1,"stats":{"Line":0}},{"line":48,"address":[3365415],"length":1,"stats":{"Line":0}},{"line":50,"address":[3365532],"length":1,"stats":{"Line":0}},{"line":51,"address":[3365544],"length":1,"stats":{"Line":0}},{"line":53,"address":[3365676],"length":1,"stats":{"Line":0}},{"line":57,"address":[3365688],"length":1,"stats":{"Line":0}},{"line":70,"address":[3363552,3363686,3363692],"length":1,"stats":{"Line":0}},{"line":71,"address":[3363625,3363581],"length":1,"stats":{"Line":0}},{"line":76,"address":[3363847,3363712,3363841],"length":1,"stats":{"Line":0}},{"line":77,"address":[3363741,3363781],"length":1,"stats":{"Line":0}}],"covered":0,"coverable":22},{"path":["/","home","victor","ML","machinelearne-rs","lib","src","preprocessing","feature_engineering","mod.rs"],"content":"//! Feature engineering transformers.\n//!\n//! This module provides transformers for generating new features from existing data.\n\nmod polynomial;\n\npub use polynomial::{FittedPolynomialFeatures, PolynomialFeatures, PolynomialFeaturesParams};\n","traces":[],"covered":0,"coverable":0},{"path":["/","home","victor","ML","machinelearne-rs","lib","src","preprocessing","feature_engineering","polynomial.rs"],"content":"//! Polynomial feature generation.\n//!\n//! Generates polynomial and interaction features from input data.\n\nuse crate::backend::{Backend, Tensor2D};\nuse crate::preprocessing::error::PreprocessingError;\nuse crate::preprocessing::traits::{FittedTransformer, Transformer};\nuse serde::{Deserialize, Serialize};\nuse std::marker::PhantomData;\n\n/// PolynomialFeatures transformer for generating polynomial and interaction features.\n///\n/// Generates a new feature matrix consisting of all polynomial combinations\n/// of the features with degree less than or equal to the specified degree.\n///\n/// For example, if an input sample is two dimensional and of the form [a, b],\n/// the degree-2 polynomial features are [1, a, b, a^2, ab, b^2].\n///\n/// # Example\n/// ```ignore\n/// use machinelearne_rs::preprocessing::{PolynomialFeatures, Transformer};\n/// use machinelearne_rs::backend::CpuBackend;\n///\n/// // Input: [[1, 2], [3, 4]]\n/// let data = Tensor2D::new(vec![1.0, 2.0, 3.0, 4.0], 2, 2);\n///\n/// let poly = PolynomialFeatures::\u003cCpuBackend\u003e::new().with_degree(2);\n/// let fitted = poly.fit(\u0026data)?;\n///\n/// // Output: [[1, 1, 2, 1, 2, 4], [1, 3, 4, 9, 12, 16]]\n/// let transformed = fitted.transform(\u0026data)?;\n/// ```\n#[derive(Clone, Debug)]\npub struct PolynomialFeatures\u003cB: Backend\u003e {\n    /// Maximum degree of polynomial features.\n    degree: usize,\n    /// If True, include a bias column (feature of all 1s).\n    include_bias: bool,\n    /// If True, only produce interaction features (products of distinct features).\n    interaction_only: bool,\n    _backend: PhantomData\u003cB\u003e,\n}\n\nimpl\u003cB: Backend\u003e Default for PolynomialFeatures\u003cB\u003e {\n    fn default() -\u003e Self {\n        Self::new()\n    }\n}\n\nimpl\u003cB: Backend\u003e PolynomialFeatures\u003cB\u003e {\n    /// Create a new PolynomialFeatures with default settings (degree=2).\n    pub fn new() -\u003e Self {\n        Self {\n            degree: 2,\n            include_bias: true,\n            interaction_only: false,\n            _backend: PhantomData,\n        }\n    }\n\n    /// Set the maximum degree of polynomial features.\n    pub fn with_degree(mut self, degree: usize) -\u003e Self {\n        self.degree = degree;\n        self\n    }\n\n    /// Set whether to include a bias column (all 1s).\n    pub fn with_include_bias(mut self, include_bias: bool) -\u003e Self {\n        self.include_bias = include_bias;\n        self\n    }\n\n    /// Set whether to only produce interaction features.\n    pub fn with_interaction_only(mut self, interaction_only: bool) -\u003e Self {\n        self.interaction_only = interaction_only;\n        self\n    }\n}\n\n/// Serializable parameters for fitted PolynomialFeatures.\n#[derive(Clone, Serialize, Deserialize)]\npub struct PolynomialFeaturesParams {\n    /// Maximum degree of polynomial features.\n    pub degree: usize,\n    /// If True, include a bias column.\n    pub include_bias: bool,\n    /// If True, only produce interaction features.\n    pub interaction_only: bool,\n    /// Number of input features.\n    pub n_features_in: usize,\n    /// Number of output features.\n    pub n_features_out: usize,\n    /// List of (degree, feature indices) combinations for output features.\n    /// For example, [(0, [])] for bias, [(1, [0])] for first input feature,\n    /// [(2, [0, 0])] for square of first feature, [(2, [0, 1])] for interaction.\n    pub output_combinations: Vec\u003c(usize, Vec\u003cusize\u003e)\u003e,\n}\n\n/// Fitted PolynomialFeatures ready for inference.\n#[derive(Clone)]\npub struct FittedPolynomialFeatures\u003cB: Backend\u003e {\n    /// Maximum degree of polynomial features.\n    degree: usize,\n    /// If True, include a bias column.\n    include_bias: bool,\n    /// If True, only produce interaction features.\n    interaction_only: bool,\n    /// Number of input features.\n    n_features_in: usize,\n    /// Number of output features.\n    n_features_out: usize,\n    /// Output feature combinations (degree, indices).\n    output_combinations: Vec\u003c(usize, Vec\u003cusize\u003e)\u003e,\n    _backend: PhantomData\u003cB\u003e,\n}\n\nimpl\u003cB: Backend\u003e FittedPolynomialFeatures\u003cB\u003e {\n    /// Get the number of output features.\n    pub fn n_features_out(\u0026self) -\u003e usize {\n        self.n_features_out\n    }\n\n    /// Get the output feature combinations.\n    pub fn output_combinations(\u0026self) -\u003e \u0026[(usize, Vec\u003cusize\u003e)] {\n        \u0026self.output_combinations\n    }\n}\n\nimpl\u003cB: Backend\u003e Transformer\u003cB\u003e for PolynomialFeatures\u003cB\u003e {\n    type Input = Tensor2D\u003cB\u003e;\n    type Output = Tensor2D\u003cB\u003e;\n    type Params = PolynomialFeaturesParams;\n    type Fitted = FittedPolynomialFeatures\u003cB\u003e;\n\n    fn fit(\u0026self, data: \u0026Self::Input) -\u003e Result\u003cSelf::Fitted, PreprocessingError\u003e {\n        let (rows, cols) = data.shape();\n\n        if rows == 0 {\n            return Err(PreprocessingError::EmptyData(\n                \"Cannot fit PolynomialFeatures on empty data\".to_string(),\n            ));\n        }\n\n        if cols == 0 {\n            return Err(PreprocessingError::InvalidParameter(\n                \"Cannot fit PolynomialFeatures on data with no features\".to_string(),\n            ));\n        }\n\n        // Generate output combinations\n        let output_combinations = generate_polynomial_combinations(\n            cols,\n            self.degree,\n            self.include_bias,\n            self.interaction_only,\n        );\n\n        let n_features_out = output_combinations.len();\n\n        Ok(FittedPolynomialFeatures {\n            degree: self.degree,\n            include_bias: self.include_bias,\n            interaction_only: self.interaction_only,\n            n_features_in: cols,\n            n_features_out,\n            output_combinations,\n            _backend: PhantomData,\n        })\n    }\n\n    fn fit_transform(\u0026self, data: \u0026Self::Input) -\u003e Result\u003cSelf::Output, PreprocessingError\u003e {\n        let fitted = self.fit(data)?;\n        fitted.transform(data)\n    }\n}\n\nimpl\u003cB: Backend\u003e FittedTransformer\u003cB\u003e for FittedPolynomialFeatures\u003cB\u003e {\n    type Input = Tensor2D\u003cB\u003e;\n    type Output = Tensor2D\u003cB\u003e;\n    type Params = PolynomialFeaturesParams;\n\n    fn transform(\u0026self, data: \u0026Self::Input) -\u003e Result\u003cSelf::Output, PreprocessingError\u003e {\n        let (rows, cols) = data.shape();\n\n        if cols != self.n_features_in {\n            return Err(PreprocessingError::FeatureMismatch {\n                expected_features: self.n_features_in,\n                got_features: cols,\n            });\n        }\n\n        if rows == 0 {\n            return Ok(Tensor2D::zeros(0, self.n_features_out));\n        }\n\n        let data_vec = data.ravel().to_vec();\n        let mut result = Vec::with_capacity(rows * self.n_features_out);\n\n        for row in 0..rows {\n            let row_start = row * cols;\n            let row_data: Vec\u003cf64\u003e = (0..cols).map(|c| data_vec[row_start + c]).collect();\n\n            for (_degree, indices) in \u0026self.output_combinations {\n                let mut val = 1.0f64;\n                for \u0026idx in indices {\n                    val *= row_data[idx];\n                }\n                result.push(val);\n            }\n        }\n\n        Ok(Tensor2D::new(\n            result.iter().map(|\u0026x| x as f32).collect(),\n            rows,\n            self.n_features_out,\n        ))\n    }\n\n    fn inverse_transform(\u0026self, _data: \u0026Self::Output) -\u003e Result\u003cSelf::Input, PreprocessingError\u003e {\n        Err(PreprocessingError::InvalidParameter(\n            \"PolynomialFeatures does not support inverse_transform\".to_string(),\n        ))\n    }\n\n    fn extract_params(\u0026self) -\u003e Self::Params {\n        PolynomialFeaturesParams {\n            degree: self.degree,\n            include_bias: self.include_bias,\n            interaction_only: self.interaction_only,\n            n_features_in: self.n_features_in,\n            n_features_out: self.n_features_out,\n            output_combinations: self.output_combinations.clone(),\n        }\n    }\n\n    fn from_params(params: Self::Params) -\u003e Result\u003cSelf, PreprocessingError\u003e {\n        Ok(FittedPolynomialFeatures {\n            degree: params.degree,\n            include_bias: params.include_bias,\n            interaction_only: params.interaction_only,\n            n_features_in: params.n_features_in,\n            n_features_out: params.n_features_out,\n            output_combinations: params.output_combinations,\n            _backend: PhantomData,\n        })\n    }\n\n    fn n_features_in(\u0026self) -\u003e usize {\n        self.n_features_in\n    }\n}\n\n/// Generate all polynomial combinations up to given degree.\nfn generate_polynomial_combinations(\n    n_features: usize,\n    degree: usize,\n    include_bias: bool,\n    interaction_only: bool,\n) -\u003e Vec\u003c(usize, Vec\u003cusize\u003e)\u003e {\n    let mut combinations = Vec::new();\n\n    // Bias term (degree 0)\n    if include_bias {\n        combinations.push((0, Vec::new()));\n    }\n\n    // Generate combinations for degrees 1 to max_degree\n    for d in 1..=degree {\n        generate_degree_combinations(\n            n_features,\n            d,\n            d,\n            interaction_only,\n            \u0026mut Vec::new(),\n            \u0026mut combinations,\n        );\n    }\n\n    combinations\n}\n\n/// Recursively generate combinations for a specific degree.\nfn generate_degree_combinations(\n    n_features: usize,\n    target_degree: usize,\n    remaining_degree: usize,\n    interaction_only: bool,\n    current: \u0026mut Vec\u003cusize\u003e,\n    result: \u0026mut Vec\u003c(usize, Vec\u003cusize\u003e)\u003e,\n) {\n    if remaining_degree == 0 {\n        // Check interaction_only constraint\n        if interaction_only {\n            // All indices must be distinct for interaction_only\n            let mut sorted = current.clone();\n            sorted.sort();\n            sorted.dedup();\n            if sorted.len() != current.len() {\n                return; // Has duplicates, skip\n            }\n        }\n        result.push((target_degree, current.clone()));\n        return;\n    }\n\n    // Determine starting index to avoid duplicates and maintain ordering\n    let start = if current.is_empty() {\n        0\n    } else {\n        *current.last().unwrap()\n    };\n\n    for i in start..n_features {\n        current.push(i);\n        generate_degree_combinations(\n            n_features,\n            target_degree,\n            remaining_degree - 1,\n            interaction_only,\n            current,\n            result,\n        );\n        current.pop();\n    }\n}\n\n#[cfg(test)]\nmod tests {\n    use super::*;\n    use crate::backend::CpuBackend;\n\n    #[test]\n    fn test_polynomial_features_basic() {\n        // Input: [[1, 2]] -\u003e [1, a, b, a^2, ab, b^2]\n        let data = Tensor2D::\u003cCpuBackend\u003e::new(vec![1.0f32, 2.0], 1, 2);\n\n        let poly = PolynomialFeatures::\u003cCpuBackend\u003e::new()\n            .with_degree(2)\n            .with_include_bias(true);\n        let fitted = poly.fit(\u0026data).unwrap();\n\n        assert_eq!(fitted.n_features_in(), 2);\n        assert_eq!(fitted.n_features_out(), 6); // bias + 2 linear + 3 quadratic\n\n        let transformed = fitted.transform(\u0026data).unwrap();\n        let vals = transformed.ravel().to_vec();\n\n        // [1, 1, 2, 1, 2, 4]\n        assert!((vals[0] - 1.0).abs() \u003c 1e-6); // bias\n        assert!((vals[1] - 1.0).abs() \u003c 1e-6); // a\n        assert!((vals[2] - 2.0).abs() \u003c 1e-6); // b\n        assert!((vals[3] - 1.0).abs() \u003c 1e-6); // a^2\n        assert!((vals[4] - 2.0).abs() \u003c 1e-6); // ab\n        assert!((vals[5] - 4.0).abs() \u003c 1e-6); // b^2\n    }\n\n    #[test]\n    fn test_polynomial_features_no_bias() {\n        // Input: [[1, 2]] -\u003e [a, b, a^2, ab, b^2]\n        let data = Tensor2D::\u003cCpuBackend\u003e::new(vec![1.0f32, 2.0], 1, 2);\n\n        let poly = PolynomialFeatures::\u003cCpuBackend\u003e::new()\n            .with_degree(2)\n            .with_include_bias(false);\n        let fitted = poly.fit(\u0026data).unwrap();\n\n        assert_eq!(fitted.n_features_out(), 5); // 2 linear + 3 quadratic\n\n        let transformed = fitted.transform(\u0026data).unwrap();\n        let vals = transformed.ravel().to_vec();\n\n        // [1, 2, 1, 2, 4]\n        assert!((vals[0] - 1.0).abs() \u003c 1e-6); // a\n        assert!((vals[1] - 2.0).abs() \u003c 1e-6); // b\n        assert!((vals[2] - 1.0).abs() \u003c 1e-6); // a^2\n        assert!((vals[3] - 2.0).abs() \u003c 1e-6); // ab\n        assert!((vals[4] - 4.0).abs() \u003c 1e-6); // b^2\n    }\n\n    #[test]\n    fn test_polynomial_features_interaction_only() {\n        // Input: [[1, 2, 3]] -\u003e [a, b, c, ab, ac, bc]\n        let data = Tensor2D::\u003cCpuBackend\u003e::new(vec![1.0f32, 2.0, 3.0], 1, 3);\n\n        let poly = PolynomialFeatures::\u003cCpuBackend\u003e::new()\n            .with_degree(2)\n            .with_include_bias(false)\n            .with_interaction_only(true);\n        let fitted = poly.fit(\u0026data).unwrap();\n\n        // 3 linear + 3 interaction (no a^2, b^2, c^2)\n        assert_eq!(fitted.n_features_out(), 6);\n\n        let transformed = fitted.transform(\u0026data).unwrap();\n        let vals = transformed.ravel().to_vec();\n\n        // [1, 2, 3, 2, 3, 6]\n        assert!((vals[0] - 1.0).abs() \u003c 1e-6); // a\n        assert!((vals[1] - 2.0).abs() \u003c 1e-6); // b\n        assert!((vals[2] - 3.0).abs() \u003c 1e-6); // c\n        assert!((vals[3] - 2.0).abs() \u003c 1e-6); // ab\n        assert!((vals[4] - 3.0).abs() \u003c 1e-6); // ac\n        assert!((vals[5] - 6.0).abs() \u003c 1e-6); // bc\n    }\n\n    #[test]\n    fn test_polynomial_features_degree_3() {\n        // Input: [[1, 2]]\n        let data = Tensor2D::\u003cCpuBackend\u003e::new(vec![1.0f32, 2.0], 1, 2);\n\n        let poly = PolynomialFeatures::\u003cCpuBackend\u003e::new()\n            .with_degree(3)\n            .with_include_bias(true);\n        let fitted = poly.fit(\u0026data).unwrap();\n\n        // bias + 2 linear + 3 quadratic + 4 cubic = 10\n        assert_eq!(fitted.n_features_out(), 10);\n\n        let transformed = fitted.transform(\u0026data).unwrap();\n        let vals = transformed.ravel().to_vec();\n\n        // [1, 1, 2, 1, 2, 4, 1, 2, 4, 8]\n        // degree 0: 1\n        // degree 1: a, b\n        // degree 2: a^2, ab, b^2\n        // degree 3: a^3, a^2*b, a*b^2, b^3\n        assert!((vals[0] - 1.0).abs() \u003c 1e-6); // bias\n        assert!((vals[6] - 1.0).abs() \u003c 1e-6); // a^3\n        assert!((vals[9] - 8.0).abs() \u003c 1e-6); // b^3\n    }\n\n    #[test]\n    fn test_polynomial_features_multiple_rows() {\n        let data = Tensor2D::\u003cCpuBackend\u003e::new(vec![1.0f32, 2.0, 3.0, 4.0], 2, 2);\n\n        let poly = PolynomialFeatures::\u003cCpuBackend\u003e::new()\n            .with_degree(2)\n            .with_include_bias(true);\n        let fitted = poly.fit(\u0026data).unwrap();\n\n        let transformed = fitted.transform(\u0026data).unwrap();\n        assert_eq!(transformed.shape(), (2, 6));\n\n        let vals = transformed.ravel().to_vec();\n\n        // Row 0: [1, 1, 2, 1, 2, 4]\n        assert!((vals[0] - 1.0).abs() \u003c 1e-6);\n        assert!((vals[4] - 2.0).abs() \u003c 1e-6);\n        assert!((vals[5] - 4.0).abs() \u003c 1e-6);\n\n        // Row 1: [1, 3, 4, 9, 12, 16]\n        assert!((vals[6] - 1.0).abs() \u003c 1e-6);\n        assert!((vals[10] - 12.0).abs() \u003c 1e-6);\n        assert!((vals[11] - 16.0).abs() \u003c 1e-6);\n    }\n\n    #[test]\n    fn test_polynomial_features_serialization() {\n        let data = Tensor2D::\u003cCpuBackend\u003e::new(vec![1.0f32, 2.0], 1, 2);\n\n        let poly = PolynomialFeatures::\u003cCpuBackend\u003e::new()\n            .with_degree(2)\n            .with_include_bias(true);\n        let fitted = poly.fit(\u0026data).unwrap();\n\n        let temp_file = std::env::temp_dir().join(\"test_poly.bin\");\n        fitted.save_to_file(\u0026temp_file).unwrap();\n\n        let loaded = FittedPolynomialFeatures::\u003cCpuBackend\u003e::load_from_file(\u0026temp_file).unwrap();\n\n        assert_eq!(loaded.n_features_in(), fitted.n_features_in());\n        assert_eq!(loaded.n_features_out(), fitted.n_features_out());\n\n        let t1 = fitted.transform(\u0026data).unwrap();\n        let t2 = loaded.transform(\u0026data).unwrap();\n\n        let v1 = t1.ravel().to_vec();\n        let v2 = t2.ravel().to_vec();\n        for (a, b) in v1.iter().zip(v2.iter()) {\n            assert!((a - b).abs() \u003c 1e-6);\n        }\n\n        std::fs::remove_file(temp_file).ok();\n    }\n}\n","traces":[{"line":45,"address":[],"length":0,"stats":{"Line":0}},{"line":46,"address":[],"length":0,"stats":{"Line":0}},{"line":52,"address":[2817040],"length":1,"stats":{"Line":1}},{"line":62,"address":[2816912],"length":1,"stats":{"Line":2}},{"line":63,"address":[2816920],"length":1,"stats":{"Line":2}},{"line":64,"address":[2816923],"length":1,"stats":{"Line":1}},{"line":68,"address":[2816944],"length":1,"stats":{"Line":2}},{"line":69,"address":[2816958],"length":1,"stats":{"Line":1}},{"line":70,"address":[2816964],"length":1,"stats":{"Line":2}},{"line":74,"address":[2816992],"length":1,"stats":{"Line":1}},{"line":75,"address":[2817006],"length":1,"stats":{"Line":1}},{"line":76,"address":[2817012],"length":1,"stats":{"Line":1}},{"line":119,"address":[2817072],"length":1,"stats":{"Line":1}},{"line":120,"address":[2817077],"length":1,"stats":{"Line":2}},{"line":124,"address":[],"length":0,"stats":{"Line":0}},{"line":125,"address":[],"length":0,"stats":{"Line":0}},{"line":135,"address":[2819112,2818448,2819118],"length":1,"stats":{"Line":2}},{"line":136,"address":[2818499],"length":1,"stats":{"Line":3}},{"line":138,"address":[2818525],"length":1,"stats":{"Line":3}},{"line":139,"address":[2818559],"length":1,"stats":{"Line":0}},{"line":140,"address":[2818531],"length":1,"stats":{"Line":0}},{"line":144,"address":[2818648],"length":1,"stats":{"Line":1}},{"line":145,"address":[2818703],"length":1,"stats":{"Line":0}},{"line":146,"address":[2818672],"length":1,"stats":{"Line":0}},{"line":152,"address":[],"length":0,"stats":{"Line":0}},{"line":153,"address":[2818824],"length":1,"stats":{"Line":4}},{"line":154,"address":[2818827],"length":1,"stats":{"Line":1}},{"line":155,"address":[2818831],"length":1,"stats":{"Line":4}},{"line":158,"address":[2818857,2818929],"length":1,"stats":{"Line":6}},{"line":160,"address":[2818995],"length":1,"stats":{"Line":5}},{"line":161,"address":[2818937],"length":1,"stats":{"Line":5}},{"line":162,"address":[2818940],"length":1,"stats":{"Line":1}},{"line":163,"address":[2818944],"length":1,"stats":{"Line":5}},{"line":164,"address":[],"length":0,"stats":{"Line":0}},{"line":165,"address":[],"length":0,"stats":{"Line":0}},{"line":166,"address":[2818947],"length":1,"stats":{"Line":1}},{"line":167,"address":[],"length":0,"stats":{"Line":0}},{"line":171,"address":[],"length":0,"stats":{"Line":0}},{"line":172,"address":[],"length":0,"stats":{"Line":0}},{"line":173,"address":[],"length":0,"stats":{"Line":0}},{"line":182,"address":[2821050,2819488,2821056],"length":1,"stats":{"Line":1}},{"line":183,"address":[2819554],"length":1,"stats":{"Line":1}},{"line":185,"address":[2819599],"length":1,"stats":{"Line":3}},{"line":186,"address":[2819652],"length":1,"stats":{"Line":0}},{"line":187,"address":[2819648],"length":1,"stats":{"Line":0}},{"line":188,"address":[],"length":0,"stats":{"Line":0}},{"line":192,"address":[2819613],"length":1,"stats":{"Line":1}},{"line":193,"address":[2819717],"length":1,"stats":{"Line":0}},{"line":196,"address":[2819867,2819815],"length":1,"stats":{"Line":4}},{"line":197,"address":[2819970],"length":1,"stats":{"Line":1}},{"line":199,"address":[2820132,2820045],"length":1,"stats":{"Line":5}},{"line":200,"address":[2820241,2820513,2820556],"length":1,"stats":{"Line":5}},{"line":201,"address":[2820549,2820593,2821118,2821104],"length":1,"stats":{"Line":10}},{"line":203,"address":[2820675,2820608],"length":1,"stats":{"Line":5}},{"line":204,"address":[2820801],"length":1,"stats":{"Line":4}},{"line":205,"address":[2821045,2820809,2820854],"length":1,"stats":{"Line":6}},{"line":206,"address":[2821023,2820957],"length":1,"stats":{"Line":2}},{"line":208,"address":[2820983],"length":1,"stats":{"Line":3}},{"line":212,"address":[2820403],"length":1,"stats":{"Line":4}},{"line":213,"address":[2821082,2820271,2821072],"length":1,"stats":{"Line":9}},{"line":214,"address":[],"length":0,"stats":{"Line":0}},{"line":215,"address":[2820383],"length":1,"stats":{"Line":1}},{"line":219,"address":[],"length":0,"stats":{"Line":0}},{"line":220,"address":[],"length":0,"stats":{"Line":0}},{"line":221,"address":[],"length":0,"stats":{"Line":0}},{"line":225,"address":[2819312],"length":1,"stats":{"Line":1}},{"line":227,"address":[2819331],"length":1,"stats":{"Line":1}},{"line":228,"address":[2819340],"length":1,"stats":{"Line":1}},{"line":229,"address":[2819347],"length":1,"stats":{"Line":1}},{"line":230,"address":[2819354],"length":1,"stats":{"Line":1}},{"line":231,"address":[2819363],"length":1,"stats":{"Line":1}},{"line":232,"address":[2819372],"length":1,"stats":{"Line":1}},{"line":236,"address":[2819136],"length":1,"stats":{"Line":1}},{"line":237,"address":[2819196],"length":1,"stats":{"Line":1}},{"line":238,"address":[2819150],"length":1,"stats":{"Line":1}},{"line":239,"address":[2819154],"length":1,"stats":{"Line":1}},{"line":240,"address":[2819158],"length":1,"stats":{"Line":1}},{"line":241,"address":[2819162],"length":1,"stats":{"Line":1}},{"line":242,"address":[2819166],"length":1,"stats":{"Line":1}},{"line":243,"address":[2819170],"length":1,"stats":{"Line":1}},{"line":244,"address":[],"length":0,"stats":{"Line":0}},{"line":248,"address":[2819296],"length":1,"stats":{"Line":1}},{"line":249,"address":[2819301],"length":1,"stats":{"Line":1}},{"line":254,"address":[3139295,3138736,3139301],"length":1,"stats":{"Line":1}},{"line":260,"address":[3138809],"length":1,"stats":{"Line":4}},{"line":263,"address":[3138823],"length":1,"stats":{"Line":1}},{"line":264,"address":[3138908,3138864],"length":1,"stats":{"Line":6}},{"line":268,"address":[3138988,3138832],"length":1,"stats":{"Line":2}},{"line":274,"address":[3139136],"length":1,"stats":{"Line":1}},{"line":279,"address":[3139153],"length":1,"stats":{"Line":5}},{"line":283,"address":[3138441,3138447,3137984],"length":1,"stats":{"Line":1}},{"line":291,"address":[3138071],"length":1,"stats":{"Line":4}},{"line":293,"address":[3138081],"length":1,"stats":{"Line":1}},{"line":295,"address":[3138235],"length":1,"stats":{"Line":1}},{"line":296,"address":[3138262,3138330],"length":1,"stats":{"Line":2}},{"line":297,"address":[3138342],"length":1,"stats":{"Line":1}},{"line":298,"address":[3138349],"length":1,"stats":{"Line":1}},{"line":302,"address":[3138119],"length":1,"stats":{"Line":4}},{"line":307,"address":[3138096,3138508],"length":1,"stats":{"Line":5}},{"line":308,"address":[3138510],"length":1,"stats":{"Line":4}},{"line":310,"address":[3138465],"length":1,"stats":{"Line":4}},{"line":313,"address":[3138557,3138527],"length":1,"stats":{"Line":5}},{"line":314,"address":[3138626],"length":1,"stats":{"Line":1}},{"line":318,"address":[3138717,3138644],"length":1,"stats":{"Line":4}},{"line":323,"address":[3138707],"length":1,"stats":{"Line":4}}],"covered":81,"coverable":105},{"path":["/","home","victor","ML","machinelearne-rs","lib","src","preprocessing","imputation","mod.rs"],"content":"//! Imputation transformers for handling missing values.\n//!\n//! This module provides transformers for imputing (filling in) missing values\n//! in datasets.\n//!\n//! # Available Transformers\n//!\n//! | Transformer | Description |\n//! |-------------|-------------|\n//! | [`SimpleImputer`] | Impute with mean, median, most_frequent, or constant |\n//!\n//! # Example\n//!\n//! ```ignore\n//! use machinelearne_rs::preprocessing::imputation::SimpleImputer;\n//! use machinelearne_rs::preprocessing::{Transformer, ImputeStrategy};\n//! use machinelearne_rs::backend::CpuBackend;\n//!\n//! let imputer = SimpleImputer::\u003cCpuBackend\u003e::new(ImputeStrategy::Mean);\n//! let fitted = imputer.fit(\u0026data)?;\n//! let imputed = fitted.transform(\u0026new_data)?;\n//! ```\n\npub mod simple;\n\npub use simple::{FittedSimpleImputer, ImputeStrategy, SimpleImputer, SimpleImputerParams};\n","traces":[],"covered":0,"coverable":0},{"path":["/","home","victor","ML","machinelearne-rs","lib","src","preprocessing","imputation","simple.rs"],"content":"//! Simple Imputer.\n//!\n//! Imputation transformer for completing missing values.\n//! Supports mean, median, most_frequent, and constant strategies.\n//!\n//! Note: This implementation treats NaN as missing values.\n//!\n//! # Example\n//! ```ignore\n//! use machinelearne_rs::preprocessing::{Transformer, SimpleImputer, ImputeStrategy};\n//! use machinelearne_rs::backend::CpuBackend;\n//!\n//! let imputer = SimpleImputer::\u003cCpuBackend\u003e::new(ImputeStrategy::Mean);\n//! let fitted = imputer.fit(\u0026data)?;\n//! let imputed = fitted.transform(\u0026data)?;\n//! ```\n\nuse crate::backend::{Backend, Tensor1D, Tensor2D};\nuse crate::preprocessing::error::PreprocessingError;\nuse crate::preprocessing::traits::{FittedTransformer, Transformer};\nuse serde::{Deserialize, Serialize};\nuse std::marker::PhantomData;\n\n/// Strategy for imputing missing values.\n#[derive(Clone, Debug, Default, Serialize, Deserialize)]\npub enum ImputeStrategy {\n    /// Replace missing values with the mean of each column.\n    #[default]\n    Mean,\n    /// Replace missing values with the median of each column.\n    Median,\n    /// Replace missing values with the most frequent value of each column.\n    MostFrequent,\n    /// Replace missing values with a constant value.\n    Constant(f64),\n}\n\n/// Serializable parameters for a fitted SimpleImputer.\n#[derive(Clone, Serialize, Deserialize)]\npub struct SimpleImputerParams {\n    /// Strategy used for imputation.\n    pub strategy: ImputeStrategy,\n    /// Statistics (fill values) for each feature.\n    pub statistics_: Vec\u003cf64\u003e,\n    /// Number of features seen during fit.\n    pub n_features: usize,\n}\n\n/// SimpleImputer transformer (unfitted).\n///\n/// Imputation transformer for completing missing values.\n#[derive(Clone)]\npub struct SimpleImputer\u003cB: Backend\u003e {\n    strategy: ImputeStrategy,\n    _backend: PhantomData\u003cB\u003e,\n}\n\nimpl\u003cB: Backend\u003e Default for SimpleImputer\u003cB\u003e {\n    fn default() -\u003e Self {\n        Self::new(ImputeStrategy::default())\n    }\n}\n\nimpl\u003cB: Backend\u003e SimpleImputer\u003cB\u003e {\n    /// Create a new SimpleImputer with the specified strategy.\n    pub fn new(strategy: ImputeStrategy) -\u003e Self {\n        Self {\n            strategy,\n            _backend: PhantomData,\n        }\n    }\n}\n\n/// Compute statistics for imputation, ignoring NaN values.\nfn compute_statistics(\n    data: \u0026[f64],\n    rows: usize,\n    cols: usize,\n    strategy: \u0026ImputeStrategy,\n) -\u003e Vec\u003cf64\u003e {\n    let mut stats = vec![0.0; cols];\n\n    for col in 0..cols {\n        // Collect non-NaN values for this column\n        let column_values: Vec\u003cf64\u003e = (0..rows)\n            .map(|row| data[row * cols + col])\n            .filter(|\u0026v| !v.is_nan())\n            .collect();\n\n        stats[col] = if column_values.is_empty() {\n            0.0 // Default to 0 if all values are missing\n        } else {\n            match strategy {\n                ImputeStrategy::Mean =\u003e {\n                    column_values.iter().sum::\u003cf64\u003e() / column_values.len() as f64\n                }\n                ImputeStrategy::Median =\u003e {\n                    let mut sorted = column_values.clone();\n                    sorted.sort_by(|a, b| a.partial_cmp(b).unwrap_or(std::cmp::Ordering::Equal));\n                    let n = sorted.len();\n                    if n.is_multiple_of(2) {\n                        (sorted[n / 2 - 1] + sorted[n / 2]) / 2.0\n                    } else {\n                        sorted[n / 2]\n                    }\n                }\n                ImputeStrategy::MostFrequent =\u003e {\n                    // Find the most common value\n                    let mut counts = std::collections::HashMap::new();\n                    for \u0026v in \u0026column_values {\n                        *counts.entry(v.to_bits()).or_insert(0) += 1;\n                    }\n                    counts\n                        .into_iter()\n                        .max_by_key(|\u0026(_, count)| count)\n                        .map(|(bits, _)| f64::from_bits(bits))\n                        .unwrap_or(0.0)\n                }\n                ImputeStrategy::Constant(val) =\u003e *val,\n            }\n        };\n    }\n\n    stats\n}\n\nimpl\u003cB: Backend\u003e Transformer\u003cB\u003e for SimpleImputer\u003cB\u003e {\n    type Input = Tensor2D\u003cB\u003e;\n    type Output = Tensor2D\u003cB\u003e;\n    type Params = SimpleImputerParams;\n    type Fitted = FittedSimpleImputer\u003cB\u003e;\n\n    fn fit(\u0026self, data: \u0026Self::Input) -\u003e Result\u003cSelf::Fitted, PreprocessingError\u003e {\n        let (rows, cols) = data.shape();\n\n        if rows == 0 {\n            return Err(PreprocessingError::EmptyData(\n                \"Cannot fit SimpleImputer on empty data\".to_string(),\n            ));\n        }\n\n        let flat_data = B::to_vec_1d(\u0026B::ravel_2d(\u0026data.data));\n        let statistics_ = compute_statistics(\u0026flat_data, rows, cols, \u0026self.strategy);\n\n        let statistics_tensor = Tensor1D {\n            data: B::from_vec_1d(statistics_.iter().map(|\u0026x| x as f32).collect()),\n            backend: PhantomData::\u003cB\u003e,\n        };\n\n        Ok(FittedSimpleImputer {\n            strategy: self.strategy.clone(),\n            statistics_: statistics_tensor,\n            n_features: cols,\n            _backend: PhantomData,\n        })\n    }\n\n    fn fit_transform(\u0026self, data: \u0026Self::Input) -\u003e Result\u003cSelf::Output, PreprocessingError\u003e {\n        let fitted = self.fit(data)?;\n        fitted.transform(data)\n    }\n}\n\n/// Fitted SimpleImputer ready for inference.\n#[derive(Clone)]\npub struct FittedSimpleImputer\u003cB: Backend\u003e {\n    strategy: ImputeStrategy,\n    statistics_: Tensor1D\u003cB\u003e,\n    n_features: usize,\n    _backend: PhantomData\u003cB\u003e,\n}\n\nimpl\u003cB: Backend\u003e FittedSimpleImputer\u003cB\u003e {\n    /// Get the imputation statistics (fill values) for each feature.\n    pub fn statistics(\u0026self) -\u003e \u0026Tensor1D\u003cB\u003e {\n        \u0026self.statistics_\n    }\n}\n\nimpl\u003cB: Backend\u003e FittedTransformer\u003cB\u003e for FittedSimpleImputer\u003cB\u003e {\n    type Input = Tensor2D\u003cB\u003e;\n    type Output = Tensor2D\u003cB\u003e;\n    type Params = SimpleImputerParams;\n\n    fn transform(\u0026self, data: \u0026Self::Input) -\u003e Result\u003cSelf::Output, PreprocessingError\u003e {\n        let (rows, cols) = data.shape();\n\n        if cols != self.n_features {\n            return Err(PreprocessingError::FeatureMismatch {\n                expected_features: self.n_features,\n                got_features: cols,\n            });\n        }\n\n        let flat_data = B::to_vec_1d(\u0026B::ravel_2d(\u0026data.data));\n        let stats = self.statistics_.to_vec();\n\n        // Replace NaN values with statistics\n        let mut result = Vec::with_capacity(rows * cols);\n        for row in 0..rows {\n            for col in 0..cols {\n                let val = flat_data[row * cols + col];\n                result.push(if val.is_nan() { stats[col] } else { val });\n            }\n        }\n\n        Ok(Tensor2D {\n            data: B::from_vec_2d(result.iter().map(|\u0026x| x as f32).collect(), rows, cols),\n            backend: PhantomData::\u003cB\u003e,\n        })\n    }\n\n    fn inverse_transform(\u0026self, _data: \u0026Self::Output) -\u003e Result\u003cSelf::Input, PreprocessingError\u003e {\n        Err(PreprocessingError::InvalidParameter(\n            \"SimpleImputer does not support inverse_transform (missing value information is lost)\"\n                .to_string(),\n        ))\n    }\n\n    fn extract_params(\u0026self) -\u003e Self::Params {\n        SimpleImputerParams {\n            strategy: self.strategy.clone(),\n            statistics_: self.statistics_.to_vec(),\n            n_features: self.n_features,\n        }\n    }\n\n    fn from_params(params: Self::Params) -\u003e Result\u003cSelf, PreprocessingError\u003e {\n        let statistics_ = Tensor1D {\n            data: B::from_vec_1d(params.statistics_.iter().map(|\u0026x| x as f32).collect()),\n            backend: PhantomData::\u003cB\u003e,\n        };\n\n        Ok(Self {\n            strategy: params.strategy,\n            statistics_,\n            n_features: params.n_features,\n            _backend: PhantomData,\n        })\n    }\n\n    fn n_features_in(\u0026self) -\u003e usize {\n        self.n_features\n    }\n\n    fn save_to_file\u003cP: AsRef\u003cstd::path::Path\u003e\u003e(\u0026self, path: P) -\u003e std::io::Result\u003c()\u003e {\n        let params = self.extract_params();\n        let bytes = bincode::serialize(\u0026params).map_err(std::io::Error::other)?;\n        std::fs::write(path, bytes)\n    }\n\n    fn load_from_file\u003cP: AsRef\u003cstd::path::Path\u003e\u003e(path: P) -\u003e Result\u003cSelf, PreprocessingError\u003e\n    where\n        Self: Sized,\n    {\n        let bytes = std::fs::read(path)?;\n        let params: SimpleImputerParams = bincode::deserialize(\u0026bytes)\n            .map_err(|e| PreprocessingError::SerializationError(e.to_string()))?;\n        Self::from_params(params)\n    }\n}\n\n#[cfg(test)]\nmod tests {\n    use super::*;\n    use crate::backend::CpuBackend;\n\n    fn create_test_data_with_missing() -\u003e Tensor2D\u003cCpuBackend\u003e {\n        // [[1, NaN], [3, 4], [5, 6]]\n        Tensor2D::new(vec![1.0f32, f32::NAN, 3.0, 4.0, 5.0, 6.0], 3, 2)\n    }\n\n    #[test]\n    fn test_simple_imputer_mean() {\n        let data = create_test_data_with_missing();\n        let imputer = SimpleImputer::\u003cCpuBackend\u003e::new(ImputeStrategy::Mean);\n        let fitted = imputer.fit(\u0026data).unwrap();\n\n        let stats = fitted.statistics().to_vec();\n        // Column 0: mean of [1, 3, 5] = 3\n        // Column 1: mean of [4, 6] = 5 (NaN excluded)\n        assert!((stats[0] - 3.0).abs() \u003c 1e-6);\n        assert!((stats[1] - 5.0).abs() \u003c 1e-6);\n\n        let imputed = fitted.transform(\u0026data).unwrap();\n        let values = imputed.ravel().to_vec();\n\n        // Column 0 unchanged: [1, 3, 5]\n        assert!((values[0] - 1.0).abs() \u003c 1e-6);\n        assert!((values[2] - 3.0).abs() \u003c 1e-6);\n        assert!((values[4] - 5.0).abs() \u003c 1e-6);\n\n        // Column 1: [5, 4, 6] (NaN replaced with 5)\n        assert!((values[1] - 5.0).abs() \u003c 1e-6);\n        assert!((values[3] - 4.0).abs() \u003c 1e-6);\n        assert!((values[5] - 6.0).abs() \u003c 1e-6);\n    }\n\n    #[test]\n    fn test_simple_imputer_median() {\n        let data = create_test_data_with_missing();\n        let imputer = SimpleImputer::\u003cCpuBackend\u003e::new(ImputeStrategy::Median);\n        let fitted = imputer.fit(\u0026data).unwrap();\n\n        let stats = fitted.statistics().to_vec();\n        // Column 0: median of [1, 3, 5] = 3\n        // Column 1: median of [4, 6] = 5\n        assert!((stats[0] - 3.0).abs() \u003c 1e-6);\n        assert!((stats[1] - 5.0).abs() \u003c 1e-6);\n    }\n\n    #[test]\n    fn test_simple_imputer_constant() {\n        let data = create_test_data_with_missing();\n        let imputer = SimpleImputer::\u003cCpuBackend\u003e::new(ImputeStrategy::Constant(-1.0));\n        let fitted = imputer.fit(\u0026data).unwrap();\n\n        let stats = fitted.statistics().to_vec();\n        assert!((stats[0] - (-1.0)).abs() \u003c 1e-6);\n        assert!((stats[1] - (-1.0)).abs() \u003c 1e-6);\n\n        let imputed = fitted.transform(\u0026data).unwrap();\n        let values = imputed.ravel().to_vec();\n\n        // NaN replaced with -1\n        assert!((values[1] - (-1.0)).abs() \u003c 1e-6);\n    }\n\n    #[test]\n    fn test_simple_imputer_inverse_not_supported() {\n        let data = create_test_data_with_missing();\n        let imputer = SimpleImputer::\u003cCpuBackend\u003e::new(ImputeStrategy::Mean);\n        let fitted = imputer.fit(\u0026data).unwrap();\n\n        let imputed = fitted.transform(\u0026data).unwrap();\n        let result = fitted.inverse_transform(\u0026imputed);\n\n        assert!(matches!(\n            result,\n            Err(PreprocessingError::InvalidParameter(_))\n        ));\n    }\n\n    #[test]\n    fn test_simple_imputer_serialization() {\n        let data = create_test_data_with_missing();\n        let imputer = SimpleImputer::\u003cCpuBackend\u003e::new(ImputeStrategy::Mean);\n        let fitted = imputer.fit(\u0026data).unwrap();\n\n        let params = fitted.extract_params();\n        let restored = FittedSimpleImputer::\u003cCpuBackend\u003e::from_params(params).unwrap();\n\n        let imputed1 = fitted.transform(\u0026data).unwrap();\n        let imputed2 = restored.transform(\u0026data).unwrap();\n\n        let i1 = imputed1.ravel().to_vec();\n        let i2 = imputed2.ravel().to_vec();\n\n        for (a, b) in i1.iter().zip(i2.iter()) {\n            assert!((a - b).abs() \u003c 1e-6);\n        }\n    }\n\n    #[test]\n    fn test_simple_imputer_feature_mismatch() {\n        let data = create_test_data_with_missing(); // 2 features\n        let imputer = SimpleImputer::\u003cCpuBackend\u003e::new(ImputeStrategy::Mean);\n        let fitted = imputer.fit(\u0026data).unwrap();\n\n        let wrong_data = Tensor2D::\u003cCpuBackend\u003e::new(vec![1.0f32, 2.0, 3.0], 1, 3); // 3 features\n        let result = fitted.transform(\u0026wrong_data);\n\n        assert!(matches!(\n            result,\n            Err(PreprocessingError::FeatureMismatch {\n                expected_features: 2,\n                got_features: 3\n            })\n        ));\n    }\n}\n","traces":[{"line":59,"address":[],"length":0,"stats":{"Line":0}},{"line":60,"address":[],"length":0,"stats":{"Line":0}},{"line":66,"address":[2411888],"length":1,"stats":{"Line":1}},{"line":75,"address":[2469984,2471392,2471947],"length":1,"stats":{"Line":1}},{"line":81,"address":[2470079],"length":1,"stats":{"Line":1}},{"line":83,"address":[2470213,2470118],"length":1,"stats":{"Line":2}},{"line":86,"address":[2408144,2408166],"length":1,"stats":{"Line":3}},{"line":87,"address":[2408333,2408320],"length":1,"stats":{"Line":3}},{"line":90,"address":[2470538,2470660,2470599,2470954,2471916],"length":1,"stats":{"Line":5}},{"line":91,"address":[2470648],"length":1,"stats":{"Line":0}},{"line":93,"address":[2470613],"length":1,"stats":{"Line":1}},{"line":95,"address":[2470807,2470675],"length":1,"stats":{"Line":2}},{"line":98,"address":[2470698],"length":1,"stats":{"Line":1}},{"line":99,"address":[2471003,2471082],"length":1,"stats":{"Line":4}},{"line":100,"address":[2471097],"length":1,"stats":{"Line":1}},{"line":101,"address":[2471387,2471127],"length":1,"stats":{"Line":2}},{"line":102,"address":[2471228,2471150],"length":1,"stats":{"Line":2}},{"line":104,"address":[2471157,2471148],"length":1,"stats":{"Line":2}},{"line":109,"address":[2470739],"length":1,"stats":{"Line":0}},{"line":110,"address":[2471482,2471864,2471398],"length":1,"stats":{"Line":0}},{"line":111,"address":[2471587,2471869,2471806],"length":1,"stats":{"Line":0}},{"line":113,"address":[2471763,2471602],"length":1,"stats":{"Line":0}},{"line":114,"address":[2471674],"length":1,"stats":{"Line":0}},{"line":115,"address":[2408058,2408048],"length":1,"stats":{"Line":0}},{"line":116,"address":[2408368,2408381],"length":1,"stats":{"Line":0}},{"line":117,"address":[2471744],"length":1,"stats":{"Line":0}},{"line":119,"address":[2470757],"length":1,"stats":{"Line":1}},{"line":124,"address":[2470441],"length":1,"stats":{"Line":2}},{"line":133,"address":[2404864,2405774,2405768],"length":1,"stats":{"Line":1}},{"line":134,"address":[2404915],"length":1,"stats":{"Line":1}},{"line":136,"address":[2404952],"length":1,"stats":{"Line":1}},{"line":137,"address":[2404993],"length":1,"stats":{"Line":0}},{"line":138,"address":[2404962],"length":1,"stats":{"Line":0}},{"line":142,"address":[2405113,2405159],"length":1,"stats":{"Line":1}},{"line":143,"address":[2405254],"length":1,"stats":{"Line":1}},{"line":146,"address":[2405792,2405325,2405392,2405802],"length":1,"stats":{"Line":8}},{"line":150,"address":[2405635],"length":1,"stats":{"Line":2}},{"line":151,"address":[2405521],"length":1,"stats":{"Line":2}},{"line":152,"address":[2405603],"length":1,"stats":{"Line":3}},{"line":153,"address":[],"length":0,"stats":{"Line":0}},{"line":154,"address":[],"length":0,"stats":{"Line":0}},{"line":158,"address":[],"length":0,"stats":{"Line":0}},{"line":159,"address":[],"length":0,"stats":{"Line":0}},{"line":160,"address":[],"length":0,"stats":{"Line":0}},{"line":175,"address":[2411904],"length":1,"stats":{"Line":2}},{"line":176,"address":[2411912],"length":1,"stats":{"Line":2}},{"line":185,"address":[2406528,2407995,2408001],"length":1,"stats":{"Line":2}},{"line":186,"address":[2406594],"length":1,"stats":{"Line":2}},{"line":188,"address":[2406639],"length":1,"stats":{"Line":2}},{"line":189,"address":[2406725],"length":1,"stats":{"Line":1}},{"line":190,"address":[2406721],"length":1,"stats":{"Line":1}},{"line":191,"address":[],"length":0,"stats":{"Line":0}},{"line":195,"address":[2406669,2406790],"length":1,"stats":{"Line":1}},{"line":196,"address":[2406885],"length":1,"stats":{"Line":2}},{"line":199,"address":[2406920],"length":1,"stats":{"Line":1}},{"line":200,"address":[2407118,2407034],"length":1,"stats":{"Line":3}},{"line":201,"address":[2407247,2407604],"length":1,"stats":{"Line":2}},{"line":202,"address":[2407730],"length":1,"stats":{"Line":2}},{"line":203,"address":[2407869],"length":1,"stats":{"Line":1}},{"line":207,"address":[2407432],"length":1,"stats":{"Line":1}},{"line":208,"address":[2407289,2408026,2408016],"length":1,"stats":{"Line":5}},{"line":209,"address":[],"length":0,"stats":{"Line":0}},{"line":213,"address":[2406400],"length":1,"stats":{"Line":1}},{"line":214,"address":[2406450],"length":1,"stats":{"Line":1}},{"line":215,"address":[],"length":0,"stats":{"Line":0}},{"line":216,"address":[2406423],"length":1,"stats":{"Line":1}},{"line":220,"address":[2406256],"length":1,"stats":{"Line":1}},{"line":222,"address":[2406286],"length":1,"stats":{"Line":1}},{"line":223,"address":[2406306],"length":1,"stats":{"Line":1}},{"line":224,"address":[2406345],"length":1,"stats":{"Line":1}},{"line":228,"address":[2405824,2406184],"length":1,"stats":{"Line":1}},{"line":230,"address":[2405922,2406208,2405854,2406218],"length":1,"stats":{"Line":4}},{"line":234,"address":[2406063],"length":1,"stats":{"Line":1}},{"line":235,"address":[2406051],"length":1,"stats":{"Line":1}},{"line":236,"address":[],"length":0,"stats":{"Line":0}},{"line":237,"address":[2406059],"length":1,"stats":{"Line":1}},{"line":238,"address":[],"length":0,"stats":{"Line":0}},{"line":242,"address":[2406240],"length":1,"stats":{"Line":0}},{"line":243,"address":[2406245],"length":1,"stats":{"Line":0}},{"line":246,"address":[],"length":0,"stats":{"Line":0}},{"line":247,"address":[],"length":0,"stats":{"Line":0}},{"line":248,"address":[],"length":0,"stats":{"Line":0}},{"line":249,"address":[],"length":0,"stats":{"Line":0}},{"line":256,"address":[],"length":0,"stats":{"Line":0}},{"line":257,"address":[],"length":0,"stats":{"Line":0}},{"line":258,"address":[],"length":0,"stats":{"Line":0}},{"line":259,"address":[],"length":0,"stats":{"Line":0}}],"covered":54,"coverable":87},{"path":["/","home","victor","ML","machinelearne-rs","lib","src","preprocessing","mod.rs"],"content":"//! Data preprocessing transformers for machine learning pipelines.\n//!\n//! This module provides a comprehensive set of data preprocessing transformers\n//! following the same type-state pattern as models in this library.\n//!\n//! # Design Philosophy\n//!\n//! - **Type Safety**: Transformers use phantom types to distinguish fitted/unfitted states\n//! - **Backend Agnostic**: All transformers work with any `Backend` implementation\n//! - **Serializable**: Fitted transformers can be saved and loaded\n//! - **sklearn-compatible**: API familiar to users of scikit-learn\n//!\n//! # Core Traits\n//!\n//! - [`Transformer`]: Unfitted transformer with hyperparameters\n//! - [`FittedTransformer`]: Fitted transformer ready for inference\n//!\n//! # Available Transformers\n//!\n//! ## Scaling\n//! - [`StandardScaler`]: Z-score normalization\n//! - [`MinMaxScaler`]: Scale to [0, 1] or custom range\n//! - [`RobustScaler`]: Use median and IQR (robust to outliers)\n//! - [`MaxAbsScaler`]: Scale by maximum absolute value\n//! - [`Normalizer`]: Scale individual samples to unit norm\n//!\n//! ## Imputation\n//! - [`SimpleImputer`]: Fill missing values with mean, median, most_frequent, or constant\n//!\n//! ## Encoding\n//! - [`OneHotEncoder`]: One-hot (dummy) encoding for categorical features\n//! - [`OrdinalEncoder`]: Ordinal (integer) encoding for categorical features\n//! - [`LabelEncoder`]: Label encoding for 1D classification targets\n//!\n//! ## Feature Engineering\n//! - [`PolynomialFeatures`]: Generate polynomial and interaction features\n//!\n//! ## Pipeline\n//! - [`Pipeline`]: Chain multiple transformers together\n//!\n//! ## Column Transformer\n//! - [`ColumnTransformer`]: Apply different transformers to different columns\n//!\n//! # Example\n//!\n//! ```ignore\n//! use machinelearne_rs::preprocessing::{Transformer, StandardScaler};\n//! use machinelearne_rs::backend::CpuBackend;\n//!\n//! // Create and fit a scaler\n//! let scaler = StandardScaler::\u003cCpuBackend\u003e::new()\n//!     .with_mean(true)\n//!     .with_std(true);\n//!\n//! let fitted = scaler.fit(\u0026training_data)?;\n//!\n//! // Transform training data\n//! let scaled_train = fitted.transform(\u0026training_data)?;\n//!\n//! // Save for later use\n//! fitted.save_to_file(\"scaler.bin\")?;\n//!\n//! // Later, load and transform new data\n//! let loaded = FittedStandardScaler::load_from_file(\"scaler.bin\")?;\n//! let scaled_test = loaded.transform(\u0026test_data)?;\n//! ```\n\npub mod column_transformer;\npub mod encoding;\npub mod error;\npub mod feature_engineering;\npub mod imputation;\npub mod pipeline;\npub mod predictive_pipeline;\npub mod scaling;\npub mod traits;\n\n// Re-export main types\npub use column_transformer::{\n    ColumnSpec, ColumnTransformer, ColumnTransformerParams, FittedColumnTransformer,\n};\npub use encoding::{\n    FittedLabelEncoder, FittedOneHotEncoder, FittedOrdinalEncoder, HandleUnknown, LabelEncoder,\n    LabelEncoderParams, OneHotEncoder, OneHotEncoderParams, OneHotOutput, OrdinalEncoder,\n    OrdinalEncoderParams,\n};\npub use error::PreprocessingError;\npub use feature_engineering::{\n    FittedPolynomialFeatures, PolynomialFeatures, PolynomialFeaturesParams,\n};\npub use imputation::{FittedSimpleImputer, ImputeStrategy, SimpleImputer, SimpleImputerParams};\npub use pipeline::{FittedPipeline, Pipeline, PipelineParams, PipelineStep, PipelineStepEnum};\npub use predictive_pipeline::{PredictivePipeline, PredictivePipelineParams};\npub use scaling::{\n    FittedMaxAbsScaler, FittedMinMaxScaler, FittedNormalizer, FittedRobustScaler,\n    FittedStandardScaler, MaxAbsScaler, MaxAbsScalerParams, MinMaxScaler, MinMaxScalerConfig,\n    MinMaxScalerParams, NormType, Normalizer, NormalizerParams, RobustScaler, RobustScalerConfig,\n    RobustScalerParams, StandardScaler, StandardScalerConfig, StandardScalerParams,\n};\npub use traits::{FittedTransformer, Transformer};\n","traces":[],"covered":0,"coverable":0},{"path":["/","home","victor","ML","machinelearne-rs","lib","src","preprocessing","pipeline","mod.rs"],"content":"//! Pipeline utilities for chaining transformers.\n//!\n//! This module provides tools for combining multiple transformers into\n//! a single pipeline that can be fitted and used for inference.\n//!\n//! # Available Components\n//!\n//! | Component | Description |\n//! |-----------|-------------|\n//! | [`Pipeline`] | Chain transformers sequentially |\n//!\n//! # Example\n//!\n//! ```ignore\n//! use machinelearne_rs::preprocessing::pipeline::Pipeline;\n//! use machinelearne_rs::preprocessing::{StandardScaler, MinMaxScaler, Transformer};\n//! use machinelearne_rs::backend::CpuBackend;\n//!\n//! let pipeline = Pipeline::\u003cCpuBackend\u003e::new()\n//!     .add_standard_scaler(StandardScaler::new())\n//!     .add_minmax_scaler(MinMaxScaler::new());\n//!\n//! let fitted = pipeline.fit(\u0026data)?;\n//! let transformed = fitted.transform(\u0026new_data)?;\n//! ```\n\n#[allow(clippy::module_inception)]\npub mod pipeline;\n\npub use pipeline::{FittedPipeline, Pipeline, PipelineParams, PipelineStep, PipelineStepEnum};\n","traces":[],"covered":0,"coverable":0},{"path":["/","home","victor","ML","machinelearne-rs","lib","src","preprocessing","pipeline","pipeline.rs"],"content":"//! Pipeline for chaining transformers.\n//!\n//! A Pipeline allows chaining multiple transformers together, where the output\n//! of one transformer becomes the input to the next.\n//!\n//! # Example\n//! ```ignore\n//! use machinelearne_rs::preprocessing::{\n//!     Pipeline, StandardScaler, MinMaxScaler, Transformer\n//! };\n//! use machinelearne_rs::backend::CpuBackend;\n//!\n//! let pipeline = Pipeline::\u003cCpuBackend\u003e::new()\n//!     .add(StandardScaler::new())\n//!     .add(MinMaxScaler::new().with_range(0.0, 1.0));\n//!\n//! let fitted = pipeline.fit(\u0026data)?;\n//! let transformed = fitted.transform(\u0026data)?;\n//! ```\n\nuse crate::backend::{Backend, Tensor2D};\nuse crate::preprocessing::encoding::{\n    FittedOneHotEncoder, FittedOrdinalEncoder, OneHotEncoder, OneHotEncoderParams, OrdinalEncoder,\n    OrdinalEncoderParams,\n};\nuse crate::preprocessing::error::PreprocessingError;\nuse crate::preprocessing::imputation::{FittedSimpleImputer, SimpleImputer, SimpleImputerParams};\nuse crate::preprocessing::scaling::{\n    FittedMaxAbsScaler, FittedMinMaxScaler, FittedNormalizer, FittedRobustScaler,\n    FittedStandardScaler, MaxAbsScaler, MaxAbsScalerParams, MinMaxScaler, MinMaxScalerParams,\n    Normalizer, NormalizerParams, RobustScaler, RobustScalerParams, StandardScaler,\n    StandardScalerParams,\n};\nuse crate::preprocessing::traits::{FittedTransformer, Transformer};\nuse serde::{Deserialize, Serialize};\nuse std::marker::PhantomData;\n\n/// A trait for fitted transformers that can be part of a pipeline.\n///\n/// This trait is automatically implemented for any fitted transformer\n/// that works with Tensor2D input/output.\npub trait PipelineStep\u003cB: Backend\u003e: Clone {\n    /// Transform the data.\n    fn transform_step(\u0026self, data: \u0026Tensor2D\u003cB\u003e) -\u003e Result\u003cTensor2D\u003cB\u003e, PreprocessingError\u003e;\n    /// Inverse transform the data (if supported).\n    fn inverse_transform_step(\u0026self, data: \u0026Tensor2D\u003cB\u003e)\n        -\u003e Result\u003cTensor2D\u003cB\u003e, PreprocessingError\u003e;\n    /// Get the step name for debugging.\n    fn step_name(\u0026self) -\u003e \u0026'static str;\n}\n\n/// Serializable representation of a fitted pipeline.\n#[derive(Clone, Serialize, Deserialize)]\npub struct PipelineParams {\n    /// Number of steps in the pipeline.\n    pub n_steps: usize,\n    /// Number of features seen during fit.\n    pub n_features: usize,\n}\n\n/// A step in the pipeline that can be serialized.\n#[derive(Clone)]\npub enum PipelineStepEnum\u003cB: Backend\u003e {\n    /// StandardScaler step.\n    StandardScaler(FittedStandardScaler\u003cB\u003e),\n    /// MinMaxScaler step.\n    MinMaxScaler(FittedMinMaxScaler\u003cB\u003e),\n    /// RobustScaler step.\n    RobustScaler(FittedRobustScaler\u003cB\u003e),\n    /// MaxAbsScaler step.\n    MaxAbsScaler(FittedMaxAbsScaler\u003cB\u003e),\n    /// Normalizer step.\n    Normalizer(FittedNormalizer\u003cB\u003e),\n    /// SimpleImputer step.\n    SimpleImputer(FittedSimpleImputer\u003cB\u003e),\n    /// OneHotEncoder step.\n    OneHotEncoder(FittedOneHotEncoder\u003cB\u003e),\n    /// OrdinalEncoder step.\n    OrdinalEncoder(FittedOrdinalEncoder\u003cB\u003e),\n}\n\nimpl\u003cB: Backend\u003e PipelineStep\u003cB\u003e for PipelineStepEnum\u003cB\u003e {\n    fn transform_step(\u0026self, data: \u0026Tensor2D\u003cB\u003e) -\u003e Result\u003cTensor2D\u003cB\u003e, PreprocessingError\u003e {\n        match self {\n            PipelineStepEnum::StandardScaler(t) =\u003e t.transform(data),\n            PipelineStepEnum::MinMaxScaler(t) =\u003e t.transform(data),\n            PipelineStepEnum::RobustScaler(t) =\u003e t.transform(data),\n            PipelineStepEnum::MaxAbsScaler(t) =\u003e t.transform(data),\n            PipelineStepEnum::Normalizer(t) =\u003e t.transform(data),\n            PipelineStepEnum::SimpleImputer(t) =\u003e t.transform(data),\n            PipelineStepEnum::OneHotEncoder(t) =\u003e t.transform(data),\n            PipelineStepEnum::OrdinalEncoder(t) =\u003e t.transform(data),\n        }\n    }\n\n    fn inverse_transform_step(\n        \u0026self,\n        data: \u0026Tensor2D\u003cB\u003e,\n    ) -\u003e Result\u003cTensor2D\u003cB\u003e, PreprocessingError\u003e {\n        match self {\n            PipelineStepEnum::StandardScaler(t) =\u003e t.inverse_transform(data),\n            PipelineStepEnum::MinMaxScaler(t) =\u003e t.inverse_transform(data),\n            PipelineStepEnum::RobustScaler(t) =\u003e t.inverse_transform(data),\n            PipelineStepEnum::MaxAbsScaler(t) =\u003e t.inverse_transform(data),\n            PipelineStepEnum::Normalizer(t) =\u003e t.inverse_transform(data),\n            PipelineStepEnum::SimpleImputer(t) =\u003e t.inverse_transform(data),\n            PipelineStepEnum::OneHotEncoder(t) =\u003e t.inverse_transform(data),\n            PipelineStepEnum::OrdinalEncoder(t) =\u003e t.inverse_transform(data),\n        }\n    }\n\n    fn step_name(\u0026self) -\u003e \u0026'static str {\n        match self {\n            PipelineStepEnum::StandardScaler(_) =\u003e \"StandardScaler\",\n            PipelineStepEnum::MinMaxScaler(_) =\u003e \"MinMaxScaler\",\n            PipelineStepEnum::RobustScaler(_) =\u003e \"RobustScaler\",\n            PipelineStepEnum::MaxAbsScaler(_) =\u003e \"MaxAbsScaler\",\n            PipelineStepEnum::Normalizer(_) =\u003e \"Normalizer\",\n            PipelineStepEnum::SimpleImputer(_) =\u003e \"SimpleImputer\",\n            PipelineStepEnum::OneHotEncoder(_) =\u003e \"OneHotEncoder\",\n            PipelineStepEnum::OrdinalEncoder(_) =\u003e \"OrdinalEncoder\",\n        }\n    }\n}\n\nimpl\u003cB: Backend\u003e PipelineStepEnum\u003cB\u003e {\n    /// Get the number of input features for this step.\n    pub fn n_features_in(\u0026self) -\u003e usize {\n        use crate::preprocessing::traits::FittedTransformer;\n        match self {\n            PipelineStepEnum::StandardScaler(t) =\u003e t.n_features_in(),\n            PipelineStepEnum::MinMaxScaler(t) =\u003e t.n_features_in(),\n            PipelineStepEnum::RobustScaler(t) =\u003e t.n_features_in(),\n            PipelineStepEnum::MaxAbsScaler(t) =\u003e t.n_features_in(),\n            PipelineStepEnum::Normalizer(t) =\u003e t.n_features_in(),\n            PipelineStepEnum::SimpleImputer(t) =\u003e t.n_features_in(),\n            PipelineStepEnum::OneHotEncoder(t) =\u003e t.n_features_in(),\n            PipelineStepEnum::OrdinalEncoder(t) =\u003e t.n_features_in(),\n        }\n    }\n}\n\n/// Builder for a fitted step (used during pipeline construction).\ntrait FittedStepBuilder\u003cB: Backend\u003e: Clone {\n    type Fitted: PipelineStep\u003cB\u003e;\n    fn fit(\u0026self, data: \u0026Tensor2D\u003cB\u003e) -\u003e Result\u003cSelf::Fitted, PreprocessingError\u003e;\n}\n\n/// A step in the unfitted pipeline.\n#[derive(Clone)]\npub enum UnfittedStepEnum\u003cB: Backend\u003e {\n    StandardScaler(StandardScaler\u003cB\u003e),\n    MinMaxScaler(MinMaxScaler\u003cB\u003e),\n    RobustScaler(RobustScaler\u003cB\u003e),\n    MaxAbsScaler(MaxAbsScaler\u003cB\u003e),\n    Normalizer(Normalizer\u003cB\u003e),\n    SimpleImputer(SimpleImputer\u003cB\u003e),\n    OneHotEncoder(OneHotEncoder\u003cB\u003e),\n    OrdinalEncoder(OrdinalEncoder\u003cB\u003e),\n}\n\nimpl\u003cB: Backend\u003e FittedStepBuilder\u003cB\u003e for UnfittedStepEnum\u003cB\u003e {\n    type Fitted = PipelineStepEnum\u003cB\u003e;\n\n    fn fit(\u0026self, data: \u0026Tensor2D\u003cB\u003e) -\u003e Result\u003cSelf::Fitted, PreprocessingError\u003e {\n        match self {\n            UnfittedStepEnum::StandardScaler(t) =\u003e {\n                t.fit(data).map(PipelineStepEnum::StandardScaler)\n            }\n            UnfittedStepEnum::MinMaxScaler(t) =\u003e t.fit(data).map(PipelineStepEnum::MinMaxScaler),\n            UnfittedStepEnum::RobustScaler(t) =\u003e t.fit(data).map(PipelineStepEnum::RobustScaler),\n            UnfittedStepEnum::MaxAbsScaler(t) =\u003e t.fit(data).map(PipelineStepEnum::MaxAbsScaler),\n            UnfittedStepEnum::Normalizer(t) =\u003e t.fit(data).map(PipelineStepEnum::Normalizer),\n            UnfittedStepEnum::SimpleImputer(t) =\u003e t.fit(data).map(PipelineStepEnum::SimpleImputer),\n            UnfittedStepEnum::OneHotEncoder(t) =\u003e t.fit(data).map(PipelineStepEnum::OneHotEncoder),\n            UnfittedStepEnum::OrdinalEncoder(t) =\u003e {\n                t.fit(data).map(PipelineStepEnum::OrdinalEncoder)\n            }\n        }\n    }\n}\n\n/// Pipeline transformer (unfitted).\n///\n/// Chains multiple transformers together.\n#[derive(Clone)]\npub struct Pipeline\u003cB: Backend\u003e {\n    steps: Vec\u003cUnfittedStepEnum\u003cB\u003e\u003e,\n    _backend: PhantomData\u003cB\u003e,\n}\n\nimpl\u003cB: Backend\u003e Default for Pipeline\u003cB\u003e {\n    fn default() -\u003e Self {\n        Self::new()\n    }\n}\n\nimpl\u003cB: Backend\u003e Pipeline\u003cB\u003e {\n    /// Create a new empty pipeline.\n    pub fn new() -\u003e Self {\n        Self {\n            steps: Vec::new(),\n            _backend: PhantomData,\n        }\n    }\n\n    /// Add a StandardScaler to the pipeline.\n    pub fn add_standard_scaler(mut self, scaler: StandardScaler\u003cB\u003e) -\u003e Self {\n        self.steps.push(UnfittedStepEnum::StandardScaler(scaler));\n        self\n    }\n\n    /// Add a MinMaxScaler to the pipeline.\n    pub fn add_minmax_scaler(mut self, scaler: MinMaxScaler\u003cB\u003e) -\u003e Self {\n        self.steps.push(UnfittedStepEnum::MinMaxScaler(scaler));\n        self\n    }\n\n    /// Add a RobustScaler to the pipeline.\n    pub fn add_robust_scaler(mut self, scaler: RobustScaler\u003cB\u003e) -\u003e Self {\n        self.steps.push(UnfittedStepEnum::RobustScaler(scaler));\n        self\n    }\n\n    /// Add a MaxAbsScaler to the pipeline.\n    pub fn add_maxabs_scaler(mut self, scaler: MaxAbsScaler\u003cB\u003e) -\u003e Self {\n        self.steps.push(UnfittedStepEnum::MaxAbsScaler(scaler));\n        self\n    }\n\n    /// Add a Normalizer to the pipeline.\n    pub fn add_normalizer(mut self, normalizer: Normalizer\u003cB\u003e) -\u003e Self {\n        self.steps.push(UnfittedStepEnum::Normalizer(normalizer));\n        self\n    }\n\n    /// Add a SimpleImputer to the pipeline.\n    pub fn add_simple_imputer(mut self, imputer: SimpleImputer\u003cB\u003e) -\u003e Self {\n        self.steps.push(UnfittedStepEnum::SimpleImputer(imputer));\n        self\n    }\n\n    /// Add a OneHotEncoder to the pipeline.\n    pub fn add_one_hot_encoder(mut self, encoder: OneHotEncoder\u003cB\u003e) -\u003e Self {\n        self.steps.push(UnfittedStepEnum::OneHotEncoder(encoder));\n        self\n    }\n\n    /// Add an OrdinalEncoder to the pipeline.\n    pub fn add_ordinal_encoder(mut self, encoder: OrdinalEncoder\u003cB\u003e) -\u003e Self {\n        self.steps.push(UnfittedStepEnum::OrdinalEncoder(encoder));\n        self\n    }\n\n    /// Get the number of steps in the pipeline.\n    pub fn len(\u0026self) -\u003e usize {\n        self.steps.len()\n    }\n\n    /// Check if the pipeline is empty.\n    pub fn is_empty(\u0026self) -\u003e bool {\n        self.steps.is_empty()\n    }\n}\n\nimpl\u003cB: Backend\u003e Transformer\u003cB\u003e for Pipeline\u003cB\u003e {\n    type Input = Tensor2D\u003cB\u003e;\n    type Output = Tensor2D\u003cB\u003e;\n    type Params = PipelineParams;\n    type Fitted = FittedPipeline\u003cB\u003e;\n\n    fn fit(\u0026self, data: \u0026Self::Input) -\u003e Result\u003cSelf::Fitted, PreprocessingError\u003e {\n        if self.steps.is_empty() {\n            return Err(PreprocessingError::InvalidParameter(\n                \"Cannot fit an empty pipeline\".to_string(),\n            ));\n        }\n\n        let (rows, cols) = data.shape();\n        if rows == 0 {\n            return Err(PreprocessingError::EmptyData(\n                \"Cannot fit pipeline on empty data\".to_string(),\n            ));\n        }\n\n        let mut fitted_steps = Vec::with_capacity(self.steps.len());\n        let mut current_data = data.clone();\n\n        for step in \u0026self.steps {\n            let fitted = step.fit(\u0026current_data)?;\n            current_data = fitted.transform_step(\u0026current_data)?;\n            fitted_steps.push(fitted);\n        }\n\n        Ok(FittedPipeline {\n            steps: fitted_steps,\n            n_features: cols,\n            _backend: PhantomData,\n        })\n    }\n\n    fn fit_transform(\u0026self, data: \u0026Self::Input) -\u003e Result\u003cSelf::Output, PreprocessingError\u003e {\n        let fitted = self.fit(data)?;\n        fitted.transform(data)\n    }\n}\n\n/// Fitted Pipeline ready for inference.\n#[derive(Clone)]\npub struct FittedPipeline\u003cB: Backend\u003e {\n    steps: Vec\u003cPipelineStepEnum\u003cB\u003e\u003e,\n    n_features: usize,\n    _backend: PhantomData\u003cB\u003e,\n}\n\nimpl\u003cB: Backend\u003e FittedPipeline\u003cB\u003e {\n    /// Get the number of steps in the pipeline.\n    pub fn len(\u0026self) -\u003e usize {\n        self.steps.len()\n    }\n\n    /// Check if the pipeline is empty.\n    pub fn is_empty(\u0026self) -\u003e bool {\n        self.steps.is_empty()\n    }\n\n    /// Get the names of all steps in the pipeline.\n    pub fn step_names(\u0026self) -\u003e Vec\u003c\u0026'static str\u003e {\n        self.steps.iter().map(|s| s.step_name()).collect()\n    }\n\n    /// Get a reference to the pipeline steps.\n    pub fn steps(\u0026self) -\u003e \u0026[PipelineStepEnum\u003cB\u003e] {\n        \u0026self.steps\n    }\n\n    /// Create a FittedPipeline from steps (for deserialization).\n    pub fn from_steps(steps: Vec\u003cPipelineStepEnum\u003cB\u003e\u003e, n_features: usize) -\u003e Self {\n        Self {\n            steps,\n            n_features,\n            _backend: PhantomData,\n        }\n    }\n}\n\nimpl\u003cB: Backend\u003e FittedTransformer\u003cB\u003e for FittedPipeline\u003cB\u003e {\n    type Input = Tensor2D\u003cB\u003e;\n    type Output = Tensor2D\u003cB\u003e;\n    type Params = PipelineParams;\n\n    fn transform(\u0026self, data: \u0026Self::Input) -\u003e Result\u003cSelf::Output, PreprocessingError\u003e {\n        let (_, cols) = data.shape();\n\n        if cols != self.n_features {\n            return Err(PreprocessingError::FeatureMismatch {\n                expected_features: self.n_features,\n                got_features: cols,\n            });\n        }\n\n        let mut result = data.clone();\n        for step in \u0026self.steps {\n            result = step.transform_step(\u0026result)?;\n        }\n        Ok(result)\n    }\n\n    fn inverse_transform(\u0026self, data: \u0026Self::Output) -\u003e Result\u003cSelf::Input, PreprocessingError\u003e {\n        let (_, cols) = data.shape();\n\n        if cols != self.n_features {\n            return Err(PreprocessingError::FeatureMismatch {\n                expected_features: self.n_features,\n                got_features: cols,\n            });\n        }\n\n        let mut result = data.clone();\n        // Apply inverse transforms in reverse order\n        for step in self.steps.iter().rev() {\n            result = step.inverse_transform_step(\u0026result)?;\n        }\n        Ok(result)\n    }\n\n    fn extract_params(\u0026self) -\u003e Self::Params {\n        PipelineParams {\n            n_steps: self.steps.len(),\n            n_features: self.n_features,\n        }\n    }\n\n    fn from_params(_params: Self::Params) -\u003e Result\u003cSelf, PreprocessingError\u003e {\n        Err(PreprocessingError::InvalidParameter(\n            \"Pipeline does not support from_params - use save_to_file/load_from_file instead\"\n                .to_string(),\n        ))\n    }\n\n    fn n_features_in(\u0026self) -\u003e usize {\n        self.n_features\n    }\n\n    fn save_to_file\u003cP: AsRef\u003cstd::path::Path\u003e\u003e(\u0026self, path: P) -\u003e std::io::Result\u003c()\u003e {\n        // Save each step's params in sequence\n        let mut step_params = Vec::new();\n        for step in \u0026self.steps {\n            let (name, bytes) = match step {\n                PipelineStepEnum::StandardScaler(t) =\u003e (\n                    \"StandardScaler\",\n                    bincode::serialize(\u0026t.extract_params()).map_err(std::io::Error::other)?,\n                ),\n                PipelineStepEnum::MinMaxScaler(t) =\u003e (\n                    \"MinMaxScaler\",\n                    bincode::serialize(\u0026t.extract_params()).map_err(std::io::Error::other)?,\n                ),\n                PipelineStepEnum::RobustScaler(t) =\u003e (\n                    \"RobustScaler\",\n                    bincode::serialize(\u0026t.extract_params()).map_err(std::io::Error::other)?,\n                ),\n                PipelineStepEnum::MaxAbsScaler(t) =\u003e (\n                    \"MaxAbsScaler\",\n                    bincode::serialize(\u0026t.extract_params()).map_err(std::io::Error::other)?,\n                ),\n                PipelineStepEnum::Normalizer(t) =\u003e (\n                    \"Normalizer\",\n                    bincode::serialize(\u0026t.extract_params()).map_err(std::io::Error::other)?,\n                ),\n                PipelineStepEnum::SimpleImputer(t) =\u003e (\n                    \"SimpleImputer\",\n                    bincode::serialize(\u0026t.extract_params()).map_err(std::io::Error::other)?,\n                ),\n                PipelineStepEnum::OneHotEncoder(t) =\u003e (\n                    \"OneHotEncoder\",\n                    bincode::serialize(\u0026t.extract_params()).map_err(std::io::Error::other)?,\n                ),\n                PipelineStepEnum::OrdinalEncoder(t) =\u003e (\n                    \"OrdinalEncoder\",\n                    bincode::serialize(\u0026t.extract_params()).map_err(std::io::Error::other)?,\n                ),\n            };\n            step_params.push((name.to_string(), bytes));\n        }\n\n        let serialized =\n            bincode::serialize(\u0026(self.n_features, step_params)).map_err(std::io::Error::other)?;\n        std::fs::write(path, serialized)\n    }\n\n    fn load_from_file\u003cP: AsRef\u003cstd::path::Path\u003e\u003e(path: P) -\u003e Result\u003cSelf, PreprocessingError\u003e\n    where\n        Self: Sized,\n    {\n        let bytes = std::fs::read(path)?;\n        let (n_features, step_params): (usize, Vec\u003c(String, Vec\u003cu8\u003e)\u003e) =\n            bincode::deserialize(\u0026bytes)\n                .map_err(|e| PreprocessingError::SerializationError(e.to_string()))?;\n\n        let mut steps = Vec::new();\n        for (name, step_bytes) in step_params {\n            let step = match name.as_str() {\n                \"StandardScaler\" =\u003e {\n                    let params: StandardScalerParams = bincode::deserialize(\u0026step_bytes)\n                        .map_err(|e| PreprocessingError::SerializationError(e.to_string()))?;\n                    PipelineStepEnum::StandardScaler(FittedStandardScaler::from_params(params)?)\n                }\n                \"MinMaxScaler\" =\u003e {\n                    let params: MinMaxScalerParams = bincode::deserialize(\u0026step_bytes)\n                        .map_err(|e| PreprocessingError::SerializationError(e.to_string()))?;\n                    PipelineStepEnum::MinMaxScaler(FittedMinMaxScaler::from_params(params)?)\n                }\n                \"RobustScaler\" =\u003e {\n                    let params: RobustScalerParams = bincode::deserialize(\u0026step_bytes)\n                        .map_err(|e| PreprocessingError::SerializationError(e.to_string()))?;\n                    PipelineStepEnum::RobustScaler(FittedRobustScaler::from_params(params)?)\n                }\n                \"MaxAbsScaler\" =\u003e {\n                    let params: MaxAbsScalerParams = bincode::deserialize(\u0026step_bytes)\n                        .map_err(|e| PreprocessingError::SerializationError(e.to_string()))?;\n                    PipelineStepEnum::MaxAbsScaler(FittedMaxAbsScaler::from_params(params)?)\n                }\n                \"Normalizer\" =\u003e {\n                    let params: NormalizerParams = bincode::deserialize(\u0026step_bytes)\n                        .map_err(|e| PreprocessingError::SerializationError(e.to_string()))?;\n                    PipelineStepEnum::Normalizer(FittedNormalizer::from_params(params)?)\n                }\n                \"SimpleImputer\" =\u003e {\n                    let params: SimpleImputerParams = bincode::deserialize(\u0026step_bytes)\n                        .map_err(|e| PreprocessingError::SerializationError(e.to_string()))?;\n                    PipelineStepEnum::SimpleImputer(FittedSimpleImputer::from_params(params)?)\n                }\n                \"OneHotEncoder\" =\u003e {\n                    let params: OneHotEncoderParams = bincode::deserialize(\u0026step_bytes)\n                        .map_err(|e| PreprocessingError::SerializationError(e.to_string()))?;\n                    PipelineStepEnum::OneHotEncoder(FittedOneHotEncoder::from_params(params)?)\n                }\n                \"OrdinalEncoder\" =\u003e {\n                    let params: OrdinalEncoderParams = bincode::deserialize(\u0026step_bytes)\n                        .map_err(|e| PreprocessingError::SerializationError(e.to_string()))?;\n                    PipelineStepEnum::OrdinalEncoder(FittedOrdinalEncoder::from_params(params)?)\n                }\n                _ =\u003e {\n                    return Err(PreprocessingError::SerializationError(format!(\n                        \"Unknown step type: {}\",\n                        name\n                    )))\n                }\n            };\n            steps.push(step);\n        }\n\n        Ok(Self {\n            steps,\n            n_features,\n            _backend: PhantomData,\n        })\n    }\n}\n\n#[cfg(test)]\nmod tests {\n    use super::*;\n    use crate::backend::CpuBackend;\n    use crate::preprocessing::scaling::NormType;\n\n    fn create_test_data() -\u003e Tensor2D\u003cCpuBackend\u003e {\n        // [[0, 1], [0, 1], [1, 3]]\n        Tensor2D::new(vec![0.0f32, 1.0, 0.0, 1.0, 1.0, 3.0], 3, 2)\n    }\n\n    #[test]\n    fn test_pipeline_single_step() {\n        let data = create_test_data();\n        let pipeline = Pipeline::\u003cCpuBackend\u003e::new().add_standard_scaler(StandardScaler::new());\n\n        let fitted = pipeline.fit(\u0026data).unwrap();\n        let transformed = fitted.transform(\u0026data).unwrap();\n\n        // Should be standardized\n        let values = transformed.ravel().to_vec();\n        assert!(values.iter().all(|\u0026v| v.is_finite()));\n    }\n\n    #[test]\n    fn test_pipeline_multiple_steps() {\n        let data = create_test_data();\n        let pipeline = Pipeline::\u003cCpuBackend\u003e::new()\n            .add_standard_scaler(StandardScaler::new())\n            .add_minmax_scaler(MinMaxScaler::new().with_range(0.0, 1.0));\n\n        let fitted = pipeline.fit(\u0026data).unwrap();\n        let transformed = fitted.transform(\u0026data).unwrap();\n\n        // After StandardScaler + MinMaxScaler, values should be in [0, 1]\n        let values = transformed.ravel().to_vec();\n        assert!(values.iter().all(|\u0026v| v \u003e= -1e-6 \u0026\u0026 v \u003c= 1.0 + 1e-6));\n    }\n\n    #[test]\n    fn test_pipeline_inverse_transform() {\n        let data = create_test_data();\n        let pipeline = Pipeline::\u003cCpuBackend\u003e::new()\n            .add_standard_scaler(StandardScaler::new())\n            .add_minmax_scaler(MinMaxScaler::new().with_range(0.0, 1.0));\n\n        let fitted = pipeline.fit(\u0026data).unwrap();\n        let transformed = fitted.transform(\u0026data).unwrap();\n        let recovered = fitted.inverse_transform(\u0026transformed).unwrap();\n\n        let original = data.ravel().to_vec();\n        let recovered_vec = recovered.ravel().to_vec();\n\n        for (o, r) in original.iter().zip(recovered_vec.iter()) {\n            assert!((o - r).abs() \u003c 1e-5, \"Expected {}, got {}\", o, r);\n        }\n    }\n\n    #[test]\n    fn test_pipeline_with_normalizer() {\n        let data = create_test_data();\n        let pipeline = Pipeline::\u003cCpuBackend\u003e::new().add_normalizer(Normalizer::new(NormType::L2));\n\n        let fitted = pipeline.fit(\u0026data).unwrap();\n        let transformed = fitted.transform(\u0026data).unwrap();\n\n        // Each row should have L2 norm = 1\n        let values = transformed.ravel().to_vec();\n        for row in 0..3 {\n            let row_norm = (values[row * 2].powi(2) + values[row * 2 + 1].powi(2)).sqrt();\n            assert!(\n                (row_norm - 1.0).abs() \u003c 1e-5,\n                \"Row {} L2 norm: {}\",\n                row,\n                row_norm\n            );\n        }\n    }\n\n    #[test]\n    fn test_pipeline_serialization() {\n        let data = create_test_data();\n        let pipeline = Pipeline::\u003cCpuBackend\u003e::new()\n            .add_standard_scaler(StandardScaler::new())\n            .add_minmax_scaler(MinMaxScaler::new().with_range(0.0, 1.0));\n\n        let fitted = pipeline.fit(\u0026data).unwrap();\n\n        // Save to file\n        let temp_file = std::env::temp_dir().join(\"test_pipeline.bin\");\n        fitted.save_to_file(\u0026temp_file).unwrap();\n\n        // Load from file\n        let loaded = FittedPipeline::\u003cCpuBackend\u003e::load_from_file(\u0026temp_file).unwrap();\n\n        // Compare results\n        let transformed1 = fitted.transform(\u0026data).unwrap();\n        let transformed2 = loaded.transform(\u0026data).unwrap();\n\n        let t1 = transformed1.ravel().to_vec();\n        let t2 = transformed2.ravel().to_vec();\n\n        for (a, b) in t1.iter().zip(t2.iter()) {\n            assert!((a - b).abs() \u003c 1e-6);\n        }\n\n        // Clean up\n        std::fs::remove_file(temp_file).ok();\n    }\n\n    #[test]\n    fn test_pipeline_feature_mismatch() {\n        let data = create_test_data(); // 2 features\n        let pipeline = Pipeline::\u003cCpuBackend\u003e::new().add_standard_scaler(StandardScaler::new());\n\n        let fitted = pipeline.fit(\u0026data).unwrap();\n\n        let wrong_data = Tensor2D::\u003cCpuBackend\u003e::new(vec![1.0f32, 2.0, 3.0], 1, 3); // 3 features\n        let result = fitted.transform(\u0026wrong_data);\n\n        assert!(matches!(\n            result,\n            Err(PreprocessingError::FeatureMismatch {\n                expected_features: 2,\n                got_features: 3\n            })\n        ));\n    }\n\n    #[test]\n    fn test_pipeline_empty() {\n        let pipeline = Pipeline::\u003cCpuBackend\u003e::new();\n        let data = create_test_data();\n\n        let result = pipeline.fit(\u0026data);\n        assert!(matches!(\n            result,\n            Err(PreprocessingError::InvalidParameter(_))\n        ));\n    }\n\n    #[test]\n    fn test_pipeline_step_names() {\n        let pipeline = Pipeline::\u003cCpuBackend\u003e::new()\n            .add_standard_scaler(StandardScaler::new())\n            .add_minmax_scaler(MinMaxScaler::new());\n\n        let data = create_test_data();\n        let fitted = pipeline.fit(\u0026data).unwrap();\n\n        let names = fitted.step_names();\n        assert_eq!(names, vec![\"StandardScaler\", \"MinMaxScaler\"]);\n    }\n}\n","traces":[{"line":83,"address":[3188560],"length":1,"stats":{"Line":1}},{"line":84,"address":[3188593],"length":1,"stats":{"Line":1}},{"line":85,"address":[3188666],"length":1,"stats":{"Line":1}},{"line":86,"address":[3188699],"length":1,"stats":{"Line":2}},{"line":87,"address":[3188728],"length":1,"stats":{"Line":0}},{"line":88,"address":[3188761],"length":1,"stats":{"Line":0}},{"line":89,"address":[3188791],"length":1,"stats":{"Line":1}},{"line":90,"address":[3188821],"length":1,"stats":{"Line":0}},{"line":91,"address":[3188851],"length":1,"stats":{"Line":0}},{"line":92,"address":[3188881],"length":1,"stats":{"Line":0}},{"line":96,"address":[3188912],"length":1,"stats":{"Line":1}},{"line":100,"address":[3188945],"length":1,"stats":{"Line":1}},{"line":101,"address":[3189018],"length":1,"stats":{"Line":1}},{"line":102,"address":[3189051],"length":1,"stats":{"Line":1}},{"line":103,"address":[3189080],"length":1,"stats":{"Line":0}},{"line":104,"address":[3189113],"length":1,"stats":{"Line":0}},{"line":105,"address":[3189143],"length":1,"stats":{"Line":0}},{"line":106,"address":[3189173],"length":1,"stats":{"Line":0}},{"line":107,"address":[3189203],"length":1,"stats":{"Line":0}},{"line":108,"address":[3189233],"length":1,"stats":{"Line":0}},{"line":112,"address":[3189264],"length":1,"stats":{"Line":1}},{"line":113,"address":[3189269],"length":1,"stats":{"Line":1}},{"line":114,"address":[3189328],"length":1,"stats":{"Line":1}},{"line":115,"address":[3189354],"length":1,"stats":{"Line":1}},{"line":116,"address":[3189380],"length":1,"stats":{"Line":0}},{"line":117,"address":[3189403],"length":1,"stats":{"Line":0}},{"line":118,"address":[3189426],"length":1,"stats":{"Line":0}},{"line":119,"address":[3189449],"length":1,"stats":{"Line":0}},{"line":120,"address":[3189472],"length":1,"stats":{"Line":0}},{"line":121,"address":[3189495],"length":1,"stats":{"Line":0}},{"line":128,"address":[3188256],"length":1,"stats":{"Line":0}},{"line":130,"address":[3188270],"length":1,"stats":{"Line":0}},{"line":131,"address":[3188334],"length":1,"stats":{"Line":0}},{"line":132,"address":[3188363],"length":1,"stats":{"Line":0}},{"line":133,"address":[3188388],"length":1,"stats":{"Line":0}},{"line":134,"address":[3188417],"length":1,"stats":{"Line":0}},{"line":135,"address":[3188443],"length":1,"stats":{"Line":0}},{"line":136,"address":[3188469],"length":1,"stats":{"Line":0}},{"line":137,"address":[3188495],"length":1,"stats":{"Line":0}},{"line":138,"address":[3188521],"length":1,"stats":{"Line":0}},{"line":165,"address":[3189536],"length":1,"stats":{"Line":1}},{"line":166,"address":[3189579],"length":1,"stats":{"Line":1}},{"line":167,"address":[3189639],"length":1,"stats":{"Line":1}},{"line":168,"address":[3189647],"length":1,"stats":{"Line":1}},{"line":170,"address":[3189687],"length":1,"stats":{"Line":1}},{"line":171,"address":[3189735],"length":1,"stats":{"Line":0}},{"line":172,"address":[3189789],"length":1,"stats":{"Line":0}},{"line":173,"address":[3189843],"length":1,"stats":{"Line":1}},{"line":174,"address":[3189897],"length":1,"stats":{"Line":0}},{"line":175,"address":[3189948],"length":1,"stats":{"Line":0}},{"line":176,"address":[3189999],"length":1,"stats":{"Line":0}},{"line":177,"address":[3190007],"length":1,"stats":{"Line":0}},{"line":193,"address":[],"length":0,"stats":{"Line":0}},{"line":194,"address":[],"length":0,"stats":{"Line":0}},{"line":200,"address":[3187984],"length":1,"stats":{"Line":1}},{"line":202,"address":[3187997],"length":1,"stats":{"Line":1}},{"line":208,"address":[3187824,3187966],"length":1,"stats":{"Line":1}},{"line":209,"address":[3187866],"length":1,"stats":{"Line":1}},{"line":210,"address":[3187939],"length":1,"stats":{"Line":1}},{"line":214,"address":[3187793,3187648],"length":1,"stats":{"Line":1}},{"line":215,"address":[3187687],"length":1,"stats":{"Line":1}},{"line":216,"address":[3187766],"length":1,"stats":{"Line":1}},{"line":220,"address":[],"length":0,"stats":{"Line":0}},{"line":221,"address":[],"length":0,"stats":{"Line":0}},{"line":222,"address":[],"length":0,"stats":{"Line":0}},{"line":226,"address":[],"length":0,"stats":{"Line":0}},{"line":227,"address":[],"length":0,"stats":{"Line":0}},{"line":228,"address":[],"length":0,"stats":{"Line":0}},{"line":232,"address":[3187631,3187504],"length":1,"stats":{"Line":1}},{"line":233,"address":[3187535],"length":1,"stats":{"Line":1}},{"line":234,"address":[3187604],"length":1,"stats":{"Line":1}},{"line":238,"address":[],"length":0,"stats":{"Line":0}},{"line":239,"address":[],"length":0,"stats":{"Line":0}},{"line":240,"address":[],"length":0,"stats":{"Line":0}},{"line":244,"address":[],"length":0,"stats":{"Line":0}},{"line":245,"address":[],"length":0,"stats":{"Line":0}},{"line":246,"address":[],"length":0,"stats":{"Line":0}},{"line":250,"address":[],"length":0,"stats":{"Line":0}},{"line":251,"address":[],"length":0,"stats":{"Line":0}},{"line":252,"address":[],"length":0,"stats":{"Line":0}},{"line":256,"address":[],"length":0,"stats":{"Line":0}},{"line":257,"address":[],"length":0,"stats":{"Line":0}},{"line":261,"address":[],"length":0,"stats":{"Line":0}},{"line":262,"address":[],"length":0,"stats":{"Line":0}},{"line":272,"address":[3171550,3169840,3171584],"length":1,"stats":{"Line":1}},{"line":273,"address":[3169891],"length":1,"stats":{"Line":1}},{"line":274,"address":[3169986],"length":1,"stats":{"Line":1}},{"line":275,"address":[3169958],"length":1,"stats":{"Line":1}},{"line":279,"address":[3169921],"length":1,"stats":{"Line":1}},{"line":280,"address":[3169947],"length":1,"stats":{"Line":1}},{"line":281,"address":[3170088],"length":1,"stats":{"Line":0}},{"line":282,"address":[3170057],"length":1,"stats":{"Line":0}},{"line":286,"address":[3170179],"length":1,"stats":{"Line":1}},{"line":287,"address":[3170228],"length":1,"stats":{"Line":1}},{"line":289,"address":[3170358,3171489,3170294],"length":1,"stats":{"Line":5}},{"line":290,"address":[3170470,3170651,3171556],"length":1,"stats":{"Line":2}},{"line":291,"address":[3170997,3171246,3171061],"length":1,"stats":{"Line":3}},{"line":292,"address":[3171347],"length":1,"stats":{"Line":3}},{"line":295,"address":[3170530],"length":1,"stats":{"Line":1}},{"line":296,"address":[3170490],"length":1,"stats":{"Line":1}},{"line":297,"address":[],"length":0,"stats":{"Line":0}},{"line":298,"address":[],"length":0,"stats":{"Line":0}},{"line":302,"address":[],"length":0,"stats":{"Line":0}},{"line":303,"address":[],"length":0,"stats":{"Line":0}},{"line":304,"address":[],"length":0,"stats":{"Line":0}},{"line":318,"address":[],"length":0,"stats":{"Line":0}},{"line":319,"address":[],"length":0,"stats":{"Line":0}},{"line":323,"address":[],"length":0,"stats":{"Line":0}},{"line":324,"address":[],"length":0,"stats":{"Line":0}},{"line":328,"address":[3188096],"length":1,"stats":{"Line":1}},{"line":329,"address":[3188128,3188192,3188217],"length":1,"stats":{"Line":3}},{"line":333,"address":[3188240],"length":1,"stats":{"Line":0}},{"line":334,"address":[3188245],"length":1,"stats":{"Line":0}},{"line":338,"address":[3188048],"length":1,"stats":{"Line":0}},{"line":352,"address":[3186304,3187018,3187024],"length":1,"stats":{"Line":1}},{"line":353,"address":[3186355],"length":1,"stats":{"Line":1}},{"line":355,"address":[3186378],"length":1,"stats":{"Line":1}},{"line":356,"address":[3186440],"length":1,"stats":{"Line":1}},{"line":357,"address":[3186436],"length":1,"stats":{"Line":1}},{"line":358,"address":[],"length":0,"stats":{"Line":0}},{"line":362,"address":[3186394],"length":1,"stats":{"Line":1}},{"line":363,"address":[3186532,3186998,3186404],"length":1,"stats":{"Line":3}},{"line":364,"address":[3186925,3186743,3186639],"length":1,"stats":{"Line":2}},{"line":366,"address":[3186646],"length":1,"stats":{"Line":1}},{"line":369,"address":[3185456,3186278,3186284],"length":1,"stats":{"Line":1}},{"line":370,"address":[3185507],"length":1,"stats":{"Line":1}},{"line":372,"address":[3185530],"length":1,"stats":{"Line":1}},{"line":373,"address":[3185595],"length":1,"stats":{"Line":0}},{"line":374,"address":[3185591],"length":1,"stats":{"Line":0}},{"line":375,"address":[],"length":0,"stats":{"Line":0}},{"line":379,"address":[3185549],"length":1,"stats":{"Line":1}},{"line":381,"address":[3185693,3185559,3186255],"length":1,"stats":{"Line":3}},{"line":382,"address":[3185991,3186176,3185884],"length":1,"stats":{"Line":2}},{"line":384,"address":[3185891],"length":1,"stats":{"Line":1}},{"line":387,"address":[],"length":0,"stats":{"Line":0}},{"line":389,"address":[],"length":0,"stats":{"Line":0}},{"line":390,"address":[],"length":0,"stats":{"Line":0}},{"line":394,"address":[],"length":0,"stats":{"Line":0}},{"line":395,"address":[],"length":0,"stats":{"Line":0}},{"line":396,"address":[],"length":0,"stats":{"Line":0}},{"line":397,"address":[],"length":0,"stats":{"Line":0}},{"line":401,"address":[3175856],"length":1,"stats":{"Line":1}},{"line":402,"address":[3175861],"length":1,"stats":{"Line":1}},{"line":405,"address":[3175831,3172547,3171616],"length":1,"stats":{"Line":1}},{"line":407,"address":[3171655],"length":1,"stats":{"Line":1}},{"line":408,"address":[3175728,3171759,3171838],"length":1,"stats":{"Line":3}},{"line":409,"address":[3171939,3173181],"length":1,"stats":{"Line":2}},{"line":410,"address":[3172615,3173107],"length":1,"stats":{"Line":2}},{"line":411,"address":[],"length":0,"stats":{"Line":0}},{"line":412,"address":[3172635,3172903,3173280,3172966],"length":1,"stats":{"Line":3}},{"line":414,"address":[3173523,3172650],"length":1,"stats":{"Line":2}},{"line":415,"address":[],"length":0,"stats":{"Line":0}},{"line":416,"address":[3172666,3173605,3173385,3173322],"length":1,"stats":{"Line":3}},{"line":418,"address":[3172681,3173848],"length":1,"stats":{"Line":0}},{"line":419,"address":[],"length":0,"stats":{"Line":0}},{"line":420,"address":[3173710,3173930,3173647,3172701],"length":1,"stats":{"Line":0}},{"line":422,"address":[3172716,3174173],"length":1,"stats":{"Line":0}},{"line":423,"address":[],"length":0,"stats":{"Line":0}},{"line":424,"address":[3173972,3172736,3174255,3174035],"length":1,"stats":{"Line":0}},{"line":426,"address":[3172751,3174501],"length":1,"stats":{"Line":0}},{"line":427,"address":[],"length":0,"stats":{"Line":0}},{"line":428,"address":[3174586,3174290,3172763],"length":1,"stats":{"Line":0}},{"line":430,"address":[3174816,3172787],"length":1,"stats":{"Line":0}},{"line":431,"address":[],"length":0,"stats":{"Line":0}},{"line":432,"address":[3172807,3174615,3174898,3174678],"length":1,"stats":{"Line":0}},{"line":434,"address":[3172822,3175141],"length":1,"stats":{"Line":0}},{"line":435,"address":[],"length":0,"stats":{"Line":0}},{"line":436,"address":[3175003,3172842,3174940,3175223],"length":1,"stats":{"Line":0}},{"line":438,"address":[3175468,3172857],"length":1,"stats":{"Line":0}},{"line":439,"address":[],"length":0,"stats":{"Line":0}},{"line":440,"address":[3175265,3172877,3175759,3175328],"length":1,"stats":{"Line":0}},{"line":443,"address":[3175586,3173253],"length":1,"stats":{"Line":2}},{"line":446,"address":[3172558,3172004],"length":1,"stats":{"Line":1}},{"line":447,"address":[],"length":0,"stats":{"Line":0}},{"line":448,"address":[3172412],"length":1,"stats":{"Line":1}},{"line":451,"address":[3184144,3179014,3175872],"length":1,"stats":{"Line":1}},{"line":455,"address":[3175918],"length":1,"stats":{"Line":1}},{"line":456,"address":[3176307,3176407,3184142,3176158,3176261],"length":1,"stats":{"Line":3}},{"line":457,"address":[],"length":0,"stats":{"Line":0}},{"line":458,"address":[3176359,3176284,3184476,3184448],"length":1,"stats":{"Line":1}},{"line":460,"address":[3176536],"length":1,"stats":{"Line":1}},{"line":461,"address":[3176684,3176584,3176811],"length":1,"stats":{"Line":3}},{"line":462,"address":[3177240,3176920],"length":1,"stats":{"Line":2}},{"line":463,"address":[3177262],"length":1,"stats":{"Line":1}},{"line":464,"address":[3177343,3183294,3183248,3183394],"length":1,"stats":{"Line":3}},{"line":465,"address":[3185340,3183271,3183346,3185312],"length":1,"stats":{"Line":1}},{"line":466,"address":[3183555,3183707],"length":1,"stats":{"Line":2}},{"line":468,"address":[3177317,3177388],"length":1,"stats":{"Line":2}},{"line":469,"address":[3182584,3182484,3177436,3182438],"length":1,"stats":{"Line":3}},{"line":470,"address":[3185168,3182536,3185196,3182461],"length":1,"stats":{"Line":1}},{"line":471,"address":[3182993,3182809],"length":1,"stats":{"Line":2}},{"line":473,"address":[3177481,3177410],"length":1,"stats":{"Line":0}},{"line":474,"address":[3181800,3181654,3177529,3181700],"length":1,"stats":{"Line":0}},{"line":475,"address":[3184908,3181677,3184880,3181752],"length":1,"stats":{"Line":0}},{"line":476,"address":[3181993,3182161],"length":1,"stats":{"Line":0}},{"line":478,"address":[3177571,3177503],"length":1,"stats":{"Line":0}},{"line":479,"address":[3180921,3180967,3181067,3177616],"length":1,"stats":{"Line":0}},{"line":480,"address":[3184764,3180944,3181019,3184736],"length":1,"stats":{"Line":0}},{"line":481,"address":[3181228,3181380],"length":1,"stats":{"Line":0}},{"line":483,"address":[3177593,3177652],"length":1,"stats":{"Line":0}},{"line":484,"address":[3180655,3180515,3180469,3177697,3180615,3180898],"length":1,"stats":{"Line":0}},{"line":485,"address":[3185024,3180492,3180567,3185052],"length":1,"stats":{"Line":0}},{"line":486,"address":[3180648,3180680,3180893],"length":1,"stats":{"Line":0}},{"line":488,"address":[3177733,3177674],"length":1,"stats":{"Line":0}},{"line":489,"address":[3179829,3179875,3179952,3177778],"length":1,"stats":{"Line":0}},{"line":490,"address":[3179852,3179904,3184620,3184592],"length":1,"stats":{"Line":0}},{"line":491,"address":[3180081,3180217],"length":1,"stats":{"Line":0}},{"line":493,"address":[3177814,3177755],"length":1,"stats":{"Line":0}},{"line":494,"address":[3177859,3179197,3179051,3179097],"length":1,"stats":{"Line":0}},{"line":495,"address":[3184188,3184160,3179149,3179074],"length":1,"stats":{"Line":0}},{"line":496,"address":[3179390,3179558],"length":1,"stats":{"Line":0}},{"line":498,"address":[3177895,3177836],"length":1,"stats":{"Line":0}},{"line":499,"address":[3178334,3178188,3178234,3177924],"length":1,"stats":{"Line":0}},{"line":500,"address":[3178211,3178286,3184304,3184332],"length":1,"stats":{"Line":0}},{"line":501,"address":[3178647,3178495],"length":1,"stats":{"Line":0}},{"line":503,"address":[],"length":0,"stats":{"Line":0}},{"line":504,"address":[3177917,3177956],"length":1,"stats":{"Line":0}},{"line":505,"address":[],"length":0,"stats":{"Line":0}},{"line":506,"address":[],"length":0,"stats":{"Line":0}},{"line":510,"address":[3178854],"length":1,"stats":{"Line":1}},{"line":513,"address":[3177025],"length":1,"stats":{"Line":1}},{"line":514,"address":[3176977],"length":1,"stats":{"Line":1}},{"line":515,"address":[],"length":0,"stats":{"Line":0}},{"line":516,"address":[],"length":0,"stats":{"Line":0}}],"covered":93,"coverable":224},{"path":["/","home","victor","ML","machinelearne-rs","lib","src","preprocessing","predictive_pipeline.rs"],"content":"//! Predictive pipeline combining preprocessing and model inference.\n//!\n//! This module provides a unified wrapper for a complete ML pipeline that\n//! combines preprocessing transformers with a trained model.\n\nuse crate::backend::{Backend, Tensor1D, Tensor2D};\nuse crate::model::InferenceModel;\nuse crate::preprocessing::column_transformer::FittedColumnTransformer;\nuse crate::preprocessing::error::PreprocessingError;\nuse crate::preprocessing::feature_engineering::FittedPolynomialFeatures;\nuse crate::preprocessing::traits::FittedTransformer;\nuse crate::serialization::SerializableParams;\nuse serde::{Deserialize, Serialize};\nuse std::io;\nuse std::marker::PhantomData;\n\n/// Serializable parameters for the predictive pipeline.\n#[derive(Clone, Serialize, Deserialize)]\npub struct PredictivePipelineParams {\n    /// Preprocessor parameters.\n    pub preprocessor: Vec\u003cu8\u003e,\n    /// Polynomial features parameters (optional).\n    pub poly: Option\u003cVec\u003cu8\u003e\u003e,\n    /// Model parameters.\n    pub model: Vec\u003cu8\u003e,\n    /// Number of input features.\n    pub n_features_in: usize,\n}\n\n/// Predictive pipeline combining preprocessing and model inference.\n///\n/// This provides a unified interface for:\n/// 1. Preprocessing new data using a fitted ColumnTransformer\n/// 2. Optionally generating polynomial features\n/// 3. Making predictions with a trained model\n///\n/// The entire pipeline can be serialized and loaded for deployment.\npub struct PredictivePipeline\u003c\n    B: Backend,\n    M: InferenceModel\u003cB, InputBatch = Tensor2D\u003cB\u003e, OutputBatch = Tensor1D\u003cB\u003e\u003e,\n\u003e {\n    /// Fitted column transformer for preprocessing.\n    preprocessor: FittedColumnTransformer\u003cB\u003e,\n    /// Optional polynomial features transformer.\n    poly: Option\u003cFittedPolynomialFeatures\u003cB\u003e\u003e,\n    /// Trained model for inference.\n    model: M,\n    /// Number of input features expected.\n    n_features_in: usize,\n    _backend: PhantomData\u003cB\u003e,\n}\n\nimpl\u003cB: Backend, M: InferenceModel\u003cB, InputBatch = Tensor2D\u003cB\u003e, OutputBatch = Tensor1D\u003cB\u003e\u003e\u003e\n    PredictivePipeline\u003cB, M\u003e\n{\n    /// Create a new predictive pipeline.\n    pub fn new(\n        preprocessor: FittedColumnTransformer\u003cB\u003e,\n        poly: Option\u003cFittedPolynomialFeatures\u003cB\u003e\u003e,\n        model: M,\n    ) -\u003e Self {\n        let n_features_in = preprocessor.n_features_in();\n        Self {\n            preprocessor,\n            poly,\n            model,\n            n_features_in,\n            _backend: PhantomData,\n        }\n    }\n\n    /// Get the number of input features.\n    pub fn n_features_in(\u0026self) -\u003e usize {\n        self.n_features_in\n    }\n\n    /// Preprocess data (transform using preprocessor and polynomial features).\n    pub fn preprocess(\u0026self, data: \u0026Tensor2D\u003cB\u003e) -\u003e Result\u003cTensor2D\u003cB\u003e, PreprocessingError\u003e {\n        let mut processed = self.preprocessor.transform(data)?;\n\n        if let Some(ref poly) = self.poly {\n            processed = poly.transform(\u0026processed)?;\n        }\n\n        Ok(processed)\n    }\n\n    /// Make predictions on new data.\n    pub fn predict(\u0026self, data: \u0026Tensor2D\u003cB\u003e) -\u003e Result\u003cTensor1D\u003cB\u003e, PreprocessingError\u003e {\n        let (_, cols) = data.shape();\n\n        if cols != self.n_features_in {\n            return Err(PreprocessingError::FeatureMismatch {\n                expected_features: self.n_features_in,\n                got_features: cols,\n            });\n        }\n\n        let processed = self.preprocess(data)?;\n        let predictions = self.model.predict_batch(\u0026processed);\n\n        Ok(predictions)\n    }\n\n    /// Save the entire pipeline to a file.\n    pub fn save_to_file\u003cP: AsRef\u003cstd::path::Path\u003e\u003e(\u0026self, path: P) -\u003e io::Result\u003c()\u003e {\n        let params = self.extract_params();\n        let bytes = params.to_bytes().map_err(io::Error::other)?;\n        std::fs::write(path, bytes)\n    }\n\n    /// Load a pipeline from a file.\n    pub fn load_from_file\u003cP: AsRef\u003cstd::path::Path\u003e\u003e(\n        path: P,\n        model_loader: impl FnOnce(\u0026[u8]) -\u003e Result\u003cM, PreprocessingError\u003e,\n    ) -\u003e Result\u003cSelf, PreprocessingError\u003e {\n        let bytes = std::fs::read(path).map_err(|e| {\n            PreprocessingError::SerializationError(format!(\"Failed to read file: {}\", e))\n        })?;\n        let params = PredictivePipelineParams::from_bytes(\u0026bytes)\n            .map_err(|e| PreprocessingError::SerializationError(e.to_string()))?;\n        Self::from_params(params, model_loader)\n    }\n\n    /// Extract parameters for serialization.\n    pub fn extract_params(\u0026self) -\u003e PredictivePipelineParams {\n        let preprocessor_bytes = self.preprocessor.extract_params().to_bytes().unwrap();\n        let poly_bytes = self\n            .poly\n            .as_ref()\n            .map(|p| p.extract_params().to_bytes().unwrap());\n        let model_bytes = self.model.extract_params().to_bytes().unwrap();\n\n        PredictivePipelineParams {\n            preprocessor: preprocessor_bytes,\n            poly: poly_bytes,\n            model: model_bytes,\n            n_features_in: self.n_features_in,\n        }\n    }\n\n    /// Reconstruct from parameters.\n    pub fn from_params(\n        params: PredictivePipelineParams,\n        model_loader: impl FnOnce(\u0026[u8]) -\u003e Result\u003cM, PreprocessingError\u003e,\n    ) -\u003e Result\u003cSelf, PreprocessingError\u003e {\n        let preprocessor_params =\n            crate::preprocessing::column_transformer::ColumnTransformerParams::from_bytes(\n                \u0026params.preprocessor,\n            )\n            .map_err(|e| PreprocessingError::SerializationError(e.to_string()))?;\n        let preprocessor = FittedColumnTransformer::from_params(preprocessor_params)?;\n\n        let poly = if let Some(poly_bytes) = params.poly {\n            let poly_params =\n                crate::preprocessing::feature_engineering::PolynomialFeaturesParams::from_bytes(\n                    \u0026poly_bytes,\n                )\n                .map_err(|e| PreprocessingError::SerializationError(e.to_string()))?;\n            Some(FittedPolynomialFeatures::from_params(poly_params)?)\n        } else {\n            None\n        };\n\n        let model = model_loader(\u0026params.model)?;\n\n        Ok(Self {\n            preprocessor,\n            poly,\n            model,\n            n_features_in: params.n_features_in,\n            _backend: PhantomData,\n        })\n    }\n}\n\n#[cfg(test)]\nmod tests {\n    use super::*;\n    use crate::backend::CpuBackend;\n    use crate::model::linear::{LinearModel, LinearParams, SerializableLinearParams};\n    use crate::model::state::Fitted;\n    use crate::preprocessing::{\n        ColumnSpec, ColumnTransformer, PolynomialFeatures, StandardScaler, Transformer,\n    };\n\n    // Helper to create a simple fitted model for testing\n    fn create_test_model(n_features: usize) -\u003e LinearModel\u003cCpuBackend, Fitted\u003e {\n        let serial_params = SerializableLinearParams {\n            weights: vec![1.0; n_features],\n            bias: 0.0,\n        };\n        let params = LinearParams::try_from(serial_params).expect(\"Failed to convert params\");\n        \u003cLinearModel\u003cCpuBackend, Fitted\u003e\u003e::new(params)\n    }\n\n    #[test]\n    fn test_predictive_pipeline_basic() {\n        // Create simple data\n        let data = Tensor2D::\u003cCpuBackend\u003e::new(vec![1.0f32, 2.0, 3.0, 4.0], 2, 2);\n\n        // Build preprocessor\n        let ct = ColumnTransformer::\u003cCpuBackend\u003e::new()\n            .add_standard_scaler(StandardScaler::new(), ColumnSpec::All);\n        let fitted_ct = ct.fit(\u0026data).unwrap();\n\n        // Create model\n        let model = create_test_model(2);\n\n        // Build pipeline\n        let pipeline = PredictivePipeline::new(fitted_ct, None, model);\n\n        // Predict\n        let predictions = pipeline.predict(\u0026data).unwrap();\n        assert_eq!(predictions.len(), 2);\n    }\n\n    #[test]\n    fn test_predictive_pipeline_with_polynomial() {\n        // Create simple data\n        let data = Tensor2D::\u003cCpuBackend\u003e::new(vec![1.0f32, 2.0, 3.0, 4.0], 2, 2);\n\n        // Build preprocessor\n        let ct = ColumnTransformer::\u003cCpuBackend\u003e::new()\n            .add_standard_scaler(StandardScaler::new(), ColumnSpec::All);\n        let fitted_ct = ct.fit(\u0026data).unwrap();\n\n        // Transform to get intermediate data for poly fitting\n        let preprocessed = fitted_ct.transform(\u0026data).unwrap();\n\n        // Build polynomial features\n        let poly = PolynomialFeatures::\u003cCpuBackend\u003e::new()\n            .with_degree(2)\n            .with_include_bias(false);\n        let fitted_poly = poly.fit(\u0026preprocessed).unwrap();\n\n        // Create model with correct number of features\n        let n_poly_features = fitted_poly.n_features_out();\n        let model = create_test_model(n_poly_features);\n\n        // Build pipeline\n        let pipeline = PredictivePipeline::new(fitted_ct, Some(fitted_poly), model);\n\n        // Predict\n        let predictions = pipeline.predict(\u0026data).unwrap();\n        assert_eq!(predictions.len(), 2);\n    }\n\n    #[test]\n    fn test_predictive_pipeline_preprocess() {\n        // Create simple data\n        let data = Tensor2D::\u003cCpuBackend\u003e::new(vec![1.0f32, 2.0, 3.0, 4.0], 2, 2);\n\n        // Build preprocessor\n        let ct = ColumnTransformer::\u003cCpuBackend\u003e::new()\n            .add_standard_scaler(StandardScaler::new(), ColumnSpec::All);\n        let fitted_ct = ct.fit(\u0026data).unwrap();\n\n        // Create model\n        let model = create_test_model(2);\n\n        // Build pipeline\n        let pipeline = PredictivePipeline::new(fitted_ct, None, model);\n\n        // Preprocess\n        let processed = pipeline.preprocess(\u0026data).unwrap();\n        assert_eq!(processed.shape(), (2, 2));\n    }\n\n    #[test]\n    fn test_predictive_pipeline_feature_mismatch() {\n        // Create data with 2 features\n        let data = Tensor2D::\u003cCpuBackend\u003e::new(vec![1.0f32, 2.0, 3.0, 4.0], 2, 2);\n\n        // Build preprocessor for 2 features\n        let ct = ColumnTransformer::\u003cCpuBackend\u003e::new()\n            .add_standard_scaler(StandardScaler::new(), ColumnSpec::All);\n        let fitted_ct = ct.fit(\u0026data).unwrap();\n\n        // Create model\n        let model = create_test_model(2);\n\n        // Build pipeline\n        let pipeline = PredictivePipeline::new(fitted_ct, None, model);\n\n        // Try to predict with wrong number of features\n        let bad_data = Tensor2D::\u003cCpuBackend\u003e::new(vec![1.0f32, 2.0, 3.0], 1, 3);\n        let result = pipeline.predict(\u0026bad_data);\n\n        assert!(matches!(\n            result,\n            Err(PreprocessingError::FeatureMismatch { .. })\n        ));\n    }\n\n    #[test]\n    fn test_predictive_pipeline_serialization() {\n        // Create simple data\n        let data = Tensor2D::\u003cCpuBackend\u003e::new(vec![1.0f32, 2.0, 3.0, 4.0], 2, 2);\n\n        // Build preprocessor\n        let ct = ColumnTransformer::\u003cCpuBackend\u003e::new()\n            .add_standard_scaler(StandardScaler::new(), ColumnSpec::All);\n        let fitted_ct = ct.fit(\u0026data).unwrap();\n\n        // Create model\n        let model = create_test_model(2);\n\n        // Build pipeline\n        let pipeline = PredictivePipeline::new(fitted_ct, None, model);\n\n        // Save\n        let temp_file = std::env::temp_dir().join(\"test_predictive_pipeline.bin\");\n        pipeline.save_to_file(\u0026temp_file).unwrap();\n\n        // Load with model loader\n        let loaded =\n            PredictivePipeline::\u003cCpuBackend, LinearModel\u003cCpuBackend, Fitted\u003e\u003e::load_from_file(\n                \u0026temp_file,\n                |bytes| {\n                    let serial_params: SerializableLinearParams = bincode::deserialize(bytes)\n                        .map_err(|e| PreprocessingError::SerializationError(e.to_string()))?;\n                    let params = LinearParams::try_from(serial_params)\n                        .map_err(|e| PreprocessingError::SerializationError(e.to_string()))?;\n                    Ok(\u003cLinearModel\u003cCpuBackend, Fitted\u003e\u003e::new(params))\n                },\n            )\n            .unwrap();\n\n        // Verify\n        assert_eq!(loaded.n_features_in(), pipeline.n_features_in());\n\n        let p1 = pipeline.predict(\u0026data).unwrap();\n        let p2 = loaded.predict(\u0026data).unwrap();\n\n        let v1 = p1.to_vec();\n        let v2 = p2.to_vec();\n        for (a, b) in v1.iter().zip(v2.iter()) {\n            assert!((a - b).abs() \u003c 1e-6);\n        }\n\n        std::fs::remove_file(temp_file).ok();\n    }\n}\n","traces":[{"line":57,"address":[2045911,2045568,2045881],"length":1,"stats":{"Line":1}},{"line":62,"address":[2045607,2045665],"length":1,"stats":{"Line":4}},{"line":73,"address":[2043280],"length":1,"stats":{"Line":1}},{"line":74,"address":[2043285],"length":1,"stats":{"Line":1}},{"line":78,"address":[2038768,2039452,2039458],"length":1,"stats":{"Line":1}},{"line":79,"address":[2038806],"length":1,"stats":{"Line":1}},{"line":81,"address":[2039424,2038973],"length":1,"stats":{"Line":2}},{"line":82,"address":[2039169,2039351,2039036],"length":1,"stats":{"Line":2}},{"line":85,"address":[2039043],"length":1,"stats":{"Line":1}},{"line":89,"address":[2045936,2046455,2046461],"length":1,"stats":{"Line":2}},{"line":90,"address":[2045987],"length":1,"stats":{"Line":2}},{"line":92,"address":[2046010],"length":1,"stats":{"Line":2}},{"line":93,"address":[2046124],"length":1,"stats":{"Line":1}},{"line":94,"address":[2046117],"length":1,"stats":{"Line":1}},{"line":95,"address":[],"length":0,"stats":{"Line":0}},{"line":99,"address":[2046169,2046029],"length":1,"stats":{"Line":1}},{"line":100,"address":[2046320],"length":1,"stats":{"Line":1}},{"line":102,"address":[2046383],"length":1,"stats":{"Line":1}},{"line":106,"address":[2042688,2043252,2043214],"length":1,"stats":{"Line":1}},{"line":107,"address":[2042722],"length":1,"stats":{"Line":1}},{"line":108,"address":[2042812,2042869,2043225],"length":1,"stats":{"Line":2}},{"line":109,"address":[2043157,2043052],"length":1,"stats":{"Line":2}},{"line":113,"address":[2045134,2044176,2045100],"length":1,"stats":{"Line":1}},{"line":117,"address":[2045543,2044201,2044396,2044294,2045132,2045296,2045537],"length":1,"stats":{"Line":2}},{"line":118,"address":[2045383,2045318],"length":1,"stats":{"Line":0}},{"line":120,"address":[2044699,2044602,2044473,2044556],"length":1,"stats":{"Line":3}},{"line":121,"address":[2044651,2045152,2045180,2044579],"length":1,"stats":{"Line":1}},{"line":122,"address":[2044897],"length":1,"stats":{"Line":1}},{"line":126,"address":[2043296,2043997,2044003],"length":1,"stats":{"Line":1}},{"line":127,"address":[2043336],"length":1,"stats":{"Line":1}},{"line":128,"address":[2043477],"length":1,"stats":{"Line":1}},{"line":129,"address":[],"length":0,"stats":{"Line":0}},{"line":131,"address":[2043504,2044016,2044047],"length":1,"stats":{"Line":1}},{"line":132,"address":[2043516,2043588],"length":1,"stats":{"Line":2}},{"line":138,"address":[2043871],"length":1,"stats":{"Line":1}},{"line":143,"address":[2039472,2041250,2041967],"length":1,"stats":{"Line":1}},{"line":147,"address":[2039502],"length":1,"stats":{"Line":1}},{"line":149,"address":[2039558],"length":1,"stats":{"Line":1}},{"line":151,"address":[2042400,2039727,2042428,2039661],"length":1,"stats":{"Line":1}},{"line":152,"address":[2039892,2040019,2042023],"length":1,"stats":{"Line":2}},{"line":154,"address":[2040319,2040202],"length":1,"stats":{"Line":2}},{"line":155,"address":[],"length":0,"stats":{"Line":0}},{"line":157,"address":[2040272],"length":1,"stats":{"Line":0}},{"line":159,"address":[2042544,2042572,2040409,2040481],"length":1,"stats":{"Line":0}},{"line":160,"address":[2040842,2040690],"length":1,"stats":{"Line":0}},{"line":162,"address":[2040301],"length":1,"stats":{"Line":1}},{"line":165,"address":[2041284,2041164,2041426,2042018],"length":1,"stats":{"Line":2}},{"line":167,"address":[2041699],"length":1,"stats":{"Line":1}},{"line":168,"address":[2041583],"length":1,"stats":{"Line":1}},{"line":169,"address":[2041631],"length":1,"stats":{"Line":1}},{"line":170,"address":[],"length":0,"stats":{"Line":0}},{"line":171,"address":[2041695],"length":1,"stats":{"Line":1}},{"line":172,"address":[],"length":0,"stats":{"Line":0}}],"covered":44,"coverable":53},{"path":["/","home","victor","ML","machinelearne-rs","lib","src","preprocessing","scaling","maxabs.rs"],"content":"//! Max-Abs Scaler.\n//!\n//! Scales each feature by its maximum absolute value.\n//! This estimator scales and translates each feature individually such that\n//! the maximal absolute value of each feature in the training set will be 1.0.\n//!\n//! It does not shift/center the data, and thus does not destroy any sparsity.\n//!\n//! # Example\n//! ```ignore\n//! use machinelearne_rs::preprocessing::{Transformer, MaxAbsScaler};\n//! use machinelearne_rs::backend::CpuBackend;\n//!\n//! let scaler = MaxAbsScaler::\u003cCpuBackend\u003e::new();\n//! let fitted = scaler.fit(\u0026data)?;\n//! let scaled = fitted.transform(\u0026data)?;\n//! ```\n\nuse crate::backend::{Backend, Tensor1D, Tensor2D};\nuse crate::preprocessing::error::PreprocessingError;\nuse crate::preprocessing::traits::{FittedTransformer, Transformer};\nuse serde::{Deserialize, Serialize};\nuse std::marker::PhantomData;\n\n/// Serializable parameters for a fitted MaxAbsScaler.\n#[derive(Clone, Serialize, Deserialize)]\npub struct MaxAbsScalerParams {\n    /// Maximum absolute value for each feature.\n    pub max_abs_: Vec\u003cf64\u003e,\n    /// Scale factor for each feature (1.0 / max_abs).\n    pub scale_: Vec\u003cf64\u003e,\n    /// Number of features seen during fit.\n    pub n_features: usize,\n}\n\n/// MaxAbsScaler transformer (unfitted).\n///\n/// Scales each feature by its maximum absolute value.\n#[derive(Clone, Default)]\npub struct MaxAbsScaler\u003cB: Backend\u003e {\n    _backend: PhantomData\u003cB\u003e,\n}\n\nimpl\u003cB: Backend\u003e MaxAbsScaler\u003cB\u003e {\n    /// Create a new MaxAbsScaler.\n    pub fn new() -\u003e Self {\n        Self {\n            _backend: PhantomData,\n        }\n    }\n}\n\nimpl\u003cB: Backend\u003e Transformer\u003cB\u003e for MaxAbsScaler\u003cB\u003e {\n    type Input = Tensor2D\u003cB\u003e;\n    type Output = Tensor2D\u003cB\u003e;\n    type Params = MaxAbsScalerParams;\n    type Fitted = FittedMaxAbsScaler\u003cB\u003e;\n\n    fn fit(\u0026self, data: \u0026Self::Input) -\u003e Result\u003cSelf::Fitted, PreprocessingError\u003e {\n        let (rows, cols) = data.shape();\n\n        if rows == 0 {\n            return Err(PreprocessingError::EmptyData(\n                \"Cannot fit MaxAbsScaler on empty data\".to_string(),\n            ));\n        }\n\n        // Get absolute values of data\n        let abs_data = B::abs_2d(\u0026data.data);\n\n        // Find max absolute value per column\n        let max_abs_raw = B::col_max_2d(\u0026abs_data);\n        let max_abs_vals = B::to_vec_1d(\u0026max_abs_raw);\n\n        // Compute scale: 1.0 / max_abs (handle zero max)\n        let scale_vals: Vec\u003cf64\u003e = max_abs_vals\n            .iter()\n            .map(|\u0026m| if m == 0.0 { 1.0 } else { 1.0 / m })\n            .collect();\n\n        let max_abs_ = Tensor1D {\n            data: B::from_vec_1d(max_abs_vals.iter().map(|\u0026x| x as f32).collect()),\n            backend: PhantomData::\u003cB\u003e,\n        };\n\n        let scale_ = Tensor1D {\n            data: B::from_vec_1d(scale_vals.iter().map(|\u0026x| x as f32).collect()),\n            backend: PhantomData::\u003cB\u003e,\n        };\n\n        Ok(FittedMaxAbsScaler {\n            max_abs_,\n            scale_,\n            n_features: cols,\n            _backend: PhantomData,\n        })\n    }\n\n    fn fit_transform(\u0026self, data: \u0026Self::Input) -\u003e Result\u003cSelf::Output, PreprocessingError\u003e {\n        let fitted = self.fit(data)?;\n        fitted.transform(data)\n    }\n}\n\n/// Fitted MaxAbsScaler ready for inference.\n#[derive(Clone)]\npub struct FittedMaxAbsScaler\u003cB: Backend\u003e {\n    max_abs_: Tensor1D\u003cB\u003e,\n    scale_: Tensor1D\u003cB\u003e,\n    n_features: usize,\n    _backend: PhantomData\u003cB\u003e,\n}\n\nimpl\u003cB: Backend\u003e FittedMaxAbsScaler\u003cB\u003e {\n    /// Get the maximum absolute values for each feature.\n    pub fn max_abs(\u0026self) -\u003e \u0026Tensor1D\u003cB\u003e {\n        \u0026self.max_abs_\n    }\n\n    /// Get the scale factor for each feature.\n    pub fn scale(\u0026self) -\u003e \u0026Tensor1D\u003cB\u003e {\n        \u0026self.scale_\n    }\n}\n\nimpl\u003cB: Backend\u003e FittedTransformer\u003cB\u003e for FittedMaxAbsScaler\u003cB\u003e {\n    type Input = Tensor2D\u003cB\u003e;\n    type Output = Tensor2D\u003cB\u003e;\n    type Params = MaxAbsScalerParams;\n\n    fn transform(\u0026self, data: \u0026Self::Input) -\u003e Result\u003cSelf::Output, PreprocessingError\u003e {\n        let (_, cols) = data.shape();\n\n        if cols != self.n_features {\n            return Err(PreprocessingError::FeatureMismatch {\n                expected_features: self.n_features,\n                got_features: cols,\n            });\n        }\n\n        // X_scaled = X * scale_\n        let result = B::broadcast_mul_1d_to_2d_rows(\u0026data.data, \u0026self.scale_.data);\n\n        Ok(Tensor2D {\n            data: result,\n            backend: PhantomData::\u003cB\u003e,\n        })\n    }\n\n    fn inverse_transform(\u0026self, data: \u0026Self::Output) -\u003e Result\u003cSelf::Input, PreprocessingError\u003e {\n        let (_, cols) = data.shape();\n\n        if cols != self.n_features {\n            return Err(PreprocessingError::FeatureMismatch {\n                expected_features: self.n_features,\n                got_features: cols,\n            });\n        }\n\n        // X = X_scaled * max_abs_\n        let result = B::broadcast_mul_1d_to_2d_rows(\u0026data.data, \u0026self.max_abs_.data);\n\n        Ok(Tensor2D {\n            data: result,\n            backend: PhantomData::\u003cB\u003e,\n        })\n    }\n\n    fn extract_params(\u0026self) -\u003e Self::Params {\n        MaxAbsScalerParams {\n            max_abs_: self.max_abs_.to_vec(),\n            scale_: self.scale_.to_vec(),\n            n_features: self.n_features,\n        }\n    }\n\n    fn from_params(params: Self::Params) -\u003e Result\u003cSelf, PreprocessingError\u003e {\n        let max_abs_ = Tensor1D {\n            data: B::from_vec_1d(params.max_abs_.iter().map(|\u0026x| x as f32).collect()),\n            backend: PhantomData::\u003cB\u003e,\n        };\n        let scale_ = Tensor1D {\n            data: B::from_vec_1d(params.scale_.iter().map(|\u0026x| x as f32).collect()),\n            backend: PhantomData::\u003cB\u003e,\n        };\n\n        Ok(Self {\n            max_abs_,\n            scale_,\n            n_features: params.n_features,\n            _backend: PhantomData,\n        })\n    }\n\n    fn n_features_in(\u0026self) -\u003e usize {\n        self.n_features\n    }\n\n    fn save_to_file\u003cP: AsRef\u003cstd::path::Path\u003e\u003e(\u0026self, path: P) -\u003e std::io::Result\u003c()\u003e {\n        let params = self.extract_params();\n        let bytes = bincode::serialize(\u0026params).map_err(std::io::Error::other)?;\n        std::fs::write(path, bytes)\n    }\n\n    fn load_from_file\u003cP: AsRef\u003cstd::path::Path\u003e\u003e(path: P) -\u003e Result\u003cSelf, PreprocessingError\u003e\n    where\n        Self: Sized,\n    {\n        let bytes = std::fs::read(path)?;\n        let params: MaxAbsScalerParams = bincode::deserialize(\u0026bytes)\n            .map_err(|e| PreprocessingError::SerializationError(e.to_string()))?;\n        Self::from_params(params)\n    }\n}\n\n#[cfg(test)]\nmod tests {\n    use super::*;\n    use crate::backend::CpuBackend;\n\n    fn create_test_data() -\u003e Tensor2D\u003cCpuBackend\u003e {\n        // [[-1, 2], [0, 4], [1, -6]]  -- max abs: [1, 6]\n        Tensor2D::new(vec![-1.0f32, 2.0, 0.0, 4.0, 1.0, -6.0], 3, 2)\n    }\n\n    #[test]\n    fn test_maxabs_scaler_fit() {\n        let data = create_test_data();\n        let scaler = MaxAbsScaler::\u003cCpuBackend\u003e::new();\n        let fitted = scaler.fit(\u0026data).unwrap();\n\n        let max_abs = fitted.max_abs().to_vec();\n        assert!((max_abs[0] - 1.0).abs() \u003c 1e-6);\n        assert!((max_abs[1] - 6.0).abs() \u003c 1e-6);\n\n        let scale = fitted.scale().to_vec();\n        assert!((scale[0] - 1.0).abs() \u003c 1e-6);\n        assert!((scale[1] - 1.0 / 6.0).abs() \u003c 1e-6);\n    }\n\n    #[test]\n    fn test_maxabs_scaler_transform() {\n        let data = create_test_data();\n        let scaler = MaxAbsScaler::\u003cCpuBackend\u003e::new();\n        let fitted = scaler.fit(\u0026data).unwrap();\n\n        let transformed = fitted.transform(\u0026data).unwrap();\n        let values = transformed.ravel().to_vec();\n\n        // First column: [-1, 0, 1] * 1.0 = [-1, 0, 1]\n        assert!((values[0] - (-1.0)).abs() \u003c 1e-6);\n        assert!((values[2] - 0.0).abs() \u003c 1e-6);\n        assert!((values[4] - 1.0).abs() \u003c 1e-6);\n\n        // Second column: [2, 4, -6] * (1/6) = [0.333, 0.667, -1]\n        assert!((values[1] - (2.0 / 6.0)).abs() \u003c 1e-6);\n        assert!((values[3] - (4.0 / 6.0)).abs() \u003c 1e-6);\n        assert!((values[5] - (-1.0)).abs() \u003c 1e-6);\n    }\n\n    #[test]\n    fn test_maxabs_scaler_inverse_transform() {\n        let data = create_test_data();\n        let scaler = MaxAbsScaler::\u003cCpuBackend\u003e::new();\n        let fitted = scaler.fit(\u0026data).unwrap();\n\n        let transformed = fitted.transform(\u0026data).unwrap();\n        let recovered = fitted.inverse_transform(\u0026transformed).unwrap();\n\n        let original = data.ravel().to_vec();\n        let recovered_vec = recovered.ravel().to_vec();\n\n        for (o, r) in original.iter().zip(recovered_vec.iter()) {\n            assert!((o - r).abs() \u003c 1e-6, \"Expected {}, got {}\", o, r);\n        }\n    }\n\n    #[test]\n    fn test_maxabs_scaler_serialization() {\n        let data = create_test_data();\n        let scaler = MaxAbsScaler::\u003cCpuBackend\u003e::new();\n        let fitted = scaler.fit(\u0026data).unwrap();\n\n        let params = fitted.extract_params();\n        let restored = FittedMaxAbsScaler::\u003cCpuBackend\u003e::from_params(params).unwrap();\n\n        let transformed1 = fitted.transform(\u0026data).unwrap();\n        let transformed2 = restored.transform(\u0026data).unwrap();\n\n        let t1 = transformed1.ravel().to_vec();\n        let t2 = transformed2.ravel().to_vec();\n\n        for (i, (a, b)) in t1.iter().zip(t2.iter()).enumerate() {\n            assert!(\n                (a - b).abs() \u003c 1e-6,\n                \"Mismatch at index {}: {} vs {}\",\n                i,\n                a,\n                b\n            );\n        }\n    }\n\n    #[test]\n    fn test_maxabs_scaler_feature_mismatch() {\n        let data = create_test_data(); // 2 features\n        let scaler = MaxAbsScaler::\u003cCpuBackend\u003e::new();\n        let fitted = scaler.fit(\u0026data).unwrap();\n\n        let wrong_data = Tensor2D::\u003cCpuBackend\u003e::new(vec![1.0f32, 2.0, 3.0], 1, 3); // 3 features\n        let result = fitted.transform(\u0026wrong_data);\n\n        assert!(matches!(\n            result,\n            Err(PreprocessingError::FeatureMismatch {\n                expected_features: 2,\n                got_features: 3\n            })\n        ));\n    }\n}\n","traces":[{"line":46,"address":[],"length":0,"stats":{"Line":0}},{"line":59,"address":[3452341,3452347,3451152],"length":1,"stats":{"Line":2}},{"line":60,"address":[3451210],"length":1,"stats":{"Line":2}},{"line":62,"address":[3451239],"length":1,"stats":{"Line":3}},{"line":63,"address":[3451283],"length":1,"stats":{"Line":0}},{"line":64,"address":[3451249],"length":1,"stats":{"Line":0}},{"line":69,"address":[3451418],"length":1,"stats":{"Line":3}},{"line":72,"address":[3451439],"length":1,"stats":{"Line":1}},{"line":73,"address":[3451518],"length":1,"stats":{"Line":3}},{"line":76,"address":[3451570],"length":1,"stats":{"Line":1}},{"line":78,"address":[3452400,3451673,3452410],"length":1,"stats":{"Line":7}},{"line":82,"address":[3452496,3452506,3451723,3451790],"length":1,"stats":{"Line":8}},{"line":87,"address":[3451989,3452368,3451922,3452378],"length":1,"stats":{"Line":8}},{"line":91,"address":[3452159],"length":1,"stats":{"Line":3}},{"line":92,"address":[3452127],"length":1,"stats":{"Line":1}},{"line":93,"address":[],"length":0,"stats":{"Line":0}},{"line":94,"address":[],"length":0,"stats":{"Line":0}},{"line":95,"address":[],"length":0,"stats":{"Line":0}},{"line":99,"address":[],"length":0,"stats":{"Line":0}},{"line":100,"address":[],"length":0,"stats":{"Line":0}},{"line":101,"address":[],"length":0,"stats":{"Line":0}},{"line":116,"address":[3456624],"length":1,"stats":{"Line":1}},{"line":117,"address":[],"length":0,"stats":{"Line":0}},{"line":121,"address":[3456608],"length":1,"stats":{"Line":1}},{"line":122,"address":[3456616],"length":1,"stats":{"Line":1}},{"line":131,"address":[3453728],"length":1,"stats":{"Line":2}},{"line":132,"address":[3453779],"length":1,"stats":{"Line":2}},{"line":134,"address":[3453802],"length":1,"stats":{"Line":2}},{"line":135,"address":[3453921],"length":1,"stats":{"Line":1}},{"line":136,"address":[3453917],"length":1,"stats":{"Line":1}},{"line":137,"address":[],"length":0,"stats":{"Line":0}},{"line":142,"address":[3453818],"length":1,"stats":{"Line":1}},{"line":144,"address":[3453832],"length":1,"stats":{"Line":2}},{"line":145,"address":[],"length":0,"stats":{"Line":0}},{"line":146,"address":[],"length":0,"stats":{"Line":0}},{"line":150,"address":[3453472],"length":1,"stats":{"Line":1}},{"line":151,"address":[3453523],"length":1,"stats":{"Line":1}},{"line":153,"address":[3453546],"length":1,"stats":{"Line":1}},{"line":154,"address":[3453661],"length":1,"stats":{"Line":0}},{"line":155,"address":[3453657],"length":1,"stats":{"Line":0}},{"line":156,"address":[],"length":0,"stats":{"Line":0}},{"line":161,"address":[3453562],"length":1,"stats":{"Line":1}},{"line":163,"address":[3453572],"length":1,"stats":{"Line":1}},{"line":164,"address":[],"length":0,"stats":{"Line":0}},{"line":165,"address":[],"length":0,"stats":{"Line":0}},{"line":169,"address":[3453450,3453280,3453444],"length":1,"stats":{"Line":1}},{"line":171,"address":[3453309],"length":1,"stats":{"Line":1}},{"line":172,"address":[3453319],"length":1,"stats":{"Line":1}},{"line":173,"address":[3453378],"length":1,"stats":{"Line":1}},{"line":177,"address":[3452528,3453169],"length":1,"stats":{"Line":1}},{"line":179,"address":[3453210,3452558,3453200,3452622],"length":1,"stats":{"Line":4}},{"line":183,"address":[3452751,3453232,3453242,3452822],"length":1,"stats":{"Line":4}},{"line":187,"address":[3453024],"length":1,"stats":{"Line":1}},{"line":188,"address":[3452972],"length":1,"stats":{"Line":1}},{"line":189,"address":[],"length":0,"stats":{"Line":0}},{"line":190,"address":[3453020],"length":1,"stats":{"Line":1}},{"line":191,"address":[],"length":0,"stats":{"Line":0}},{"line":195,"address":[3453264],"length":1,"stats":{"Line":0}},{"line":196,"address":[3453269],"length":1,"stats":{"Line":0}},{"line":199,"address":[],"length":0,"stats":{"Line":0}},{"line":200,"address":[],"length":0,"stats":{"Line":0}},{"line":201,"address":[],"length":0,"stats":{"Line":0}},{"line":202,"address":[],"length":0,"stats":{"Line":0}},{"line":209,"address":[],"length":0,"stats":{"Line":0}},{"line":210,"address":[],"length":0,"stats":{"Line":0}},{"line":211,"address":[],"length":0,"stats":{"Line":0}},{"line":212,"address":[],"length":0,"stats":{"Line":0}}],"covered":37,"coverable":67},{"path":["/","home","victor","ML","machinelearne-rs","lib","src","preprocessing","scaling","minmax.rs"],"content":"//! Min-Max Scaler.\n//!\n//! Transforms features by scaling each feature to a given range (default [0, 1]).\n//!\n//! The transformation is given by:\n//! ```text\n//! X_scaled = (X - X_min) / (X_max - X_min) * (max - min) + min\n//! ```\n//!\n//! # Example\n//! ```ignore\n//! use machinelearne_rs::preprocessing::{Transformer, MinMaxScaler};\n//! use machinelearne_rs::backend::CpuBackend;\n//!\n//! let scaler = MinMaxScaler::\u003cCpuBackend\u003e::new()\n//!     .with_range(0.0, 1.0);\n//!\n//! let fitted = scaler.fit(\u0026data)?;\n//! let scaled = fitted.transform(\u0026data)?;\n//! ```\n\nuse crate::backend::{Backend, Tensor1D, Tensor2D};\nuse crate::preprocessing::error::PreprocessingError;\nuse crate::preprocessing::traits::{FittedTransformer, Transformer};\nuse serde::{Deserialize, Serialize};\nuse std::marker::PhantomData;\n\n/// Configuration for MinMaxScaler.\n#[derive(Clone, Serialize, Deserialize)]\npub struct MinMaxScalerConfig {\n    /// Minimum value of the target range.\n    pub min: f64,\n    /// Maximum value of the target range.\n    pub max: f64,\n}\n\nimpl Default for MinMaxScalerConfig {\n    fn default() -\u003e Self {\n        Self { min: 0.0, max: 1.0 }\n    }\n}\n\n/// Serializable parameters for a fitted MinMaxScaler.\n#[derive(Clone, Serialize, Deserialize)]\npub struct MinMaxScalerParams {\n    /// Configuration options.\n    pub config: MinMaxScalerConfig,\n    /// Minimum of each feature.\n    pub min_: Vec\u003cf64\u003e,\n    /// Maximum of each feature.\n    pub max_: Vec\u003cf64\u003e,\n    /// Scale factor for each feature: (max - min) / (feature_max - feature_min).\n    pub scale_: Vec\u003cf64\u003e,\n    /// Number of features seen during fit.\n    pub n_features: usize,\n}\n\n/// MinMaxScaler transformer (unfitted).\n///\n/// Transforms features by scaling each feature to a given range.\n#[derive(Clone)]\npub struct MinMaxScaler\u003cB: Backend\u003e {\n    config: MinMaxScalerConfig,\n    _backend: PhantomData\u003cB\u003e,\n}\n\nimpl\u003cB: Backend\u003e Default for MinMaxScaler\u003cB\u003e {\n    fn default() -\u003e Self {\n        Self::new()\n    }\n}\n\nimpl\u003cB: Backend\u003e MinMaxScaler\u003cB\u003e {\n    /// Create a new MinMaxScaler with default range [0, 1].\n    pub fn new() -\u003e Self {\n        Self {\n            config: MinMaxScalerConfig::default(),\n            _backend: PhantomData,\n        }\n    }\n\n    /// Set the target range for scaling.\n    pub fn with_range(mut self, min: f64, max: f64) -\u003e Self {\n        assert!(max \u003e min, \"max must be greater than min\");\n        self.config.min = min;\n        self.config.max = max;\n        self\n    }\n}\n\nimpl\u003cB: Backend\u003e Transformer\u003cB\u003e for MinMaxScaler\u003cB\u003e {\n    type Input = Tensor2D\u003cB\u003e;\n    type Output = Tensor2D\u003cB\u003e;\n    type Params = MinMaxScalerParams;\n    type Fitted = FittedMinMaxScaler\u003cB\u003e;\n\n    fn fit(\u0026self, data: \u0026Self::Input) -\u003e Result\u003cSelf::Fitted, PreprocessingError\u003e {\n        let (rows, cols) = data.shape();\n\n        if rows == 0 {\n            return Err(PreprocessingError::EmptyData(\n                \"Cannot fit MinMaxScaler on empty data\".to_string(),\n            ));\n        }\n\n        let min_vec = B::col_min_2d(\u0026data.data);\n        let max_vec = B::col_max_2d(\u0026data.data);\n\n        let min_vals = B::to_vec_1d(\u0026min_vec);\n        let max_vals = B::to_vec_1d(\u0026max_vec);\n\n        // Compute scale: (target_max - target_min) / (feature_max - feature_min)\n        let target_range = self.config.max - self.config.min;\n        let scale_vals: Vec\u003cf64\u003e = min_vals\n            .iter()\n            .zip(max_vals.iter())\n            .map(|(\u0026min, \u0026max)| {\n                let range = max - min;\n                if range == 0.0 {\n                    1.0 // Constant feature: scale by 1 to avoid division by zero\n                } else {\n                    target_range / range\n                }\n            })\n            .collect();\n\n        let scale_ = Tensor1D {\n            data: B::from_vec_1d(scale_vals.iter().map(|\u0026x| x as f32).collect()),\n            backend: PhantomData::\u003cB\u003e,\n        };\n\n        let min_ = Tensor1D {\n            data: min_vec,\n            backend: PhantomData::\u003cB\u003e,\n        };\n\n        Ok(FittedMinMaxScaler {\n            config: self.config.clone(),\n            min_,\n            max_: max_vec,\n            scale_,\n            n_features: cols,\n            _backend: PhantomData,\n        })\n    }\n\n    fn fit_transform(\u0026self, data: \u0026Self::Input) -\u003e Result\u003cSelf::Output, PreprocessingError\u003e {\n        let fitted = self.fit(data)?;\n        fitted.transform(data)\n    }\n}\n\n/// Fitted MinMaxScaler ready for inference.\n#[derive(Clone)]\npub struct FittedMinMaxScaler\u003cB: Backend\u003e {\n    config: MinMaxScalerConfig,\n    min_: Tensor1D\u003cB\u003e,\n    max_: B::Tensor1D,\n    scale_: Tensor1D\u003cB\u003e,\n    n_features: usize,\n    _backend: PhantomData\u003cB\u003e,\n}\n\nimpl\u003cB: Backend\u003e FittedMinMaxScaler\u003cB\u003e {\n    /// Get the minimum values for each feature.\n    pub fn min(\u0026self) -\u003e \u0026Tensor1D\u003cB\u003e {\n        \u0026self.min_\n    }\n\n    /// Get the scale factor for each feature.\n    pub fn scale(\u0026self) -\u003e \u0026Tensor1D\u003cB\u003e {\n        \u0026self.scale_\n    }\n\n    /// Get the data range (max - min) for each feature.\n    pub fn data_range(\u0026self) -\u003e Tensor1D\u003cB\u003e {\n        let max_tensor = Tensor1D {\n            data: self.max_.clone(),\n            backend: PhantomData::\u003cB\u003e,\n        };\n        Tensor1D {\n            data: B::sub_1d(\u0026max_tensor.data, \u0026self.min_.data),\n            backend: PhantomData::\u003cB\u003e,\n        }\n    }\n}\n\nimpl\u003cB: Backend\u003e FittedTransformer\u003cB\u003e for FittedMinMaxScaler\u003cB\u003e {\n    type Input = Tensor2D\u003cB\u003e;\n    type Output = Tensor2D\u003cB\u003e;\n    type Params = MinMaxScalerParams;\n\n    fn transform(\u0026self, data: \u0026Self::Input) -\u003e Result\u003cSelf::Output, PreprocessingError\u003e {\n        let (_, cols) = data.shape();\n\n        if cols != self.n_features {\n            return Err(PreprocessingError::FeatureMismatch {\n                expected_features: self.n_features,\n                got_features: cols,\n            });\n        }\n\n        // X_scaled = (X - X_min) * scale_ + target_min\n        // which equals: (X - X_min) / (X_max - X_min) * (target_max - target_min) + target_min\n        let centered = B::broadcast_sub_1d_to_2d_rows(\u0026data.data, \u0026self.min_.data);\n        let scaled = B::broadcast_mul_1d_to_2d_rows(\u0026centered, \u0026self.scale_.data);\n        let result = B::add_scalar_2d(\u0026scaled, \u0026B::scalar_f64(self.config.min));\n\n        Ok(Tensor2D {\n            data: result,\n            backend: PhantomData::\u003cB\u003e,\n        })\n    }\n\n    fn inverse_transform(\u0026self, data: \u0026Self::Output) -\u003e Result\u003cSelf::Input, PreprocessingError\u003e {\n        let (_, cols) = data.shape();\n\n        if cols != self.n_features {\n            return Err(PreprocessingError::FeatureMismatch {\n                expected_features: self.n_features,\n                got_features: cols,\n            });\n        }\n\n        // X = (X_scaled - target_min) / scale_ + X_min\n        let centered = B::add_scalar_2d(\u0026data.data, \u0026B::scalar_f64(-self.config.min));\n        let unscaled = B::broadcast_div_1d_to_2d_rows(\u0026centered, \u0026self.scale_.data);\n        let result = B::broadcast_add_1d_to_2d_rows(\u0026unscaled, \u0026self.min_.data);\n\n        Ok(Tensor2D {\n            data: result,\n            backend: PhantomData::\u003cB\u003e,\n        })\n    }\n\n    fn extract_params(\u0026self) -\u003e Self::Params {\n        MinMaxScalerParams {\n            config: self.config.clone(),\n            min_: self.min_.to_vec(),\n            max_: B::to_vec_1d(\u0026self.max_),\n            scale_: self.scale_.to_vec(),\n            n_features: self.n_features,\n        }\n    }\n\n    fn from_params(params: Self::Params) -\u003e Result\u003cSelf, PreprocessingError\u003e {\n        let min_ = Tensor1D {\n            data: B::from_vec_1d(params.min_.iter().map(|\u0026x| x as f32).collect()),\n            backend: PhantomData::\u003cB\u003e,\n        };\n        let max_ = B::from_vec_1d(params.max_.iter().map(|\u0026x| x as f32).collect());\n        let scale_ = Tensor1D {\n            data: B::from_vec_1d(params.scale_.iter().map(|\u0026x| x as f32).collect()),\n            backend: PhantomData::\u003cB\u003e,\n        };\n\n        Ok(Self {\n            config: params.config,\n            min_,\n            max_,\n            scale_,\n            n_features: params.n_features,\n            _backend: PhantomData,\n        })\n    }\n\n    fn n_features_in(\u0026self) -\u003e usize {\n        self.n_features\n    }\n\n    fn save_to_file\u003cP: AsRef\u003cstd::path::Path\u003e\u003e(\u0026self, path: P) -\u003e std::io::Result\u003c()\u003e {\n        let params = self.extract_params();\n        let bytes = bincode::serialize(\u0026params).map_err(std::io::Error::other)?;\n        std::fs::write(path, bytes)\n    }\n\n    fn load_from_file\u003cP: AsRef\u003cstd::path::Path\u003e\u003e(path: P) -\u003e Result\u003cSelf, PreprocessingError\u003e\n    where\n        Self: Sized,\n    {\n        let bytes = std::fs::read(path)?;\n        let params: MinMaxScalerParams = bincode::deserialize(\u0026bytes)\n            .map_err(|e| PreprocessingError::SerializationError(e.to_string()))?;\n        Self::from_params(params)\n    }\n}\n\n#[cfg(test)]\nmod tests {\n    use super::*;\n    use crate::backend::CpuBackend;\n\n    fn create_test_data() -\u003e Tensor2D\u003cCpuBackend\u003e {\n        // [[0, 1], [0, 1], [1, 3]]\n        Tensor2D::new(vec![0.0f32, 1.0, 0.0, 1.0, 1.0, 3.0], 3, 2)\n    }\n\n    #[test]\n    fn test_minmax_scaler_fit() {\n        let data = create_test_data();\n        let scaler = MinMaxScaler::\u003cCpuBackend\u003e::new();\n        let fitted = scaler.fit(\u0026data).unwrap();\n\n        // Min: [0, 1], Max: [1, 3]\n        let min = fitted.min().to_vec();\n        assert_eq!(min[0], 0.0);\n        assert_eq!(min[1], 1.0);\n\n        // Scale: (1 - 0) / (1 - 0) = 1, (1 - 0) / (3 - 1) = 0.5\n        let scale = fitted.scale().to_vec();\n        assert!((scale[0] - 1.0).abs() \u003c 1e-10);\n        assert!((scale[1] - 0.5).abs() \u003c 1e-10);\n    }\n\n    #[test]\n    fn test_minmax_scaler_transform() {\n        let data = create_test_data();\n        let scaler = MinMaxScaler::\u003cCpuBackend\u003e::new();\n        let fitted = scaler.fit(\u0026data).unwrap();\n\n        let transformed = fitted.transform(\u0026data).unwrap();\n        let values = transformed.ravel().to_vec();\n\n        // First column: [0, 0, 1] -\u003e [0, 0, 1]\n        assert!((values[0] - 0.0).abs() \u003c 1e-6);\n        assert!((values[2] - 0.0).abs() \u003c 1e-6);\n        assert!((values[4] - 1.0).abs() \u003c 1e-6);\n\n        // Second column: [1, 1, 3] -\u003e [0, 0, 1]\n        assert!((values[1] - 0.0).abs() \u003c 1e-6);\n        assert!((values[3] - 0.0).abs() \u003c 1e-6);\n        assert!((values[5] - 1.0).abs() \u003c 1e-6);\n    }\n\n    #[test]\n    fn test_minmax_scaler_inverse_transform() {\n        let data = create_test_data();\n        let scaler = MinMaxScaler::\u003cCpuBackend\u003e::new();\n        let fitted = scaler.fit(\u0026data).unwrap();\n\n        let transformed = fitted.transform(\u0026data).unwrap();\n        let recovered = fitted.inverse_transform(\u0026transformed).unwrap();\n\n        let original = data.ravel().to_vec();\n        let recovered_vec = recovered.ravel().to_vec();\n\n        for (o, r) in original.iter().zip(recovered_vec.iter()) {\n            assert!((o - r).abs() \u003c 1e-6, \"Expected {}, got {}\", o, r);\n        }\n    }\n\n    #[test]\n    fn test_minmax_scaler_custom_range() {\n        let data = create_test_data();\n        let scaler = MinMaxScaler::\u003cCpuBackend\u003e::new().with_range(-1.0, 1.0);\n        let fitted = scaler.fit(\u0026data).unwrap();\n\n        let transformed = fitted.transform(\u0026data).unwrap();\n        let values = transformed.ravel().to_vec();\n\n        // First column: [0, 0, 1] -\u003e [-1, -1, 1]\n        assert!((values[0] - (-1.0)).abs() \u003c 1e-6);\n        assert!((values[2] - (-1.0)).abs() \u003c 1e-6);\n        assert!((values[4] - 1.0).abs() \u003c 1e-6);\n\n        // Second column: [1, 1, 3] -\u003e [-1, -1, 1]\n        assert!((values[1] - (-1.0)).abs() \u003c 1e-6);\n        assert!((values[3] - (-1.0)).abs() \u003c 1e-6);\n        assert!((values[5] - 1.0).abs() \u003c 1e-6);\n    }\n\n    #[test]\n    fn test_minmax_scaler_serialization() {\n        let data = create_test_data();\n        let scaler = MinMaxScaler::\u003cCpuBackend\u003e::new();\n        let fitted = scaler.fit(\u0026data).unwrap();\n\n        let params = fitted.extract_params();\n        let restored = FittedMinMaxScaler::\u003cCpuBackend\u003e::from_params(params).unwrap();\n\n        let transformed1 = fitted.transform(\u0026data).unwrap();\n        let transformed2 = restored.transform(\u0026data).unwrap();\n\n        let t1 = transformed1.ravel().to_vec();\n        let t2 = transformed2.ravel().to_vec();\n\n        for (i, (a, b)) in t1.iter().zip(t2.iter()).enumerate() {\n            assert!(\n                (a - b).abs() \u003c 1e-6,\n                \"Mismatch at index {}: {} vs {}\",\n                i,\n                a,\n                b\n            );\n        }\n    }\n\n    #[test]\n    fn test_minmax_scaler_feature_mismatch() {\n        let data = create_test_data(); // 2 features\n        let scaler = MinMaxScaler::\u003cCpuBackend\u003e::new();\n        let fitted = scaler.fit(\u0026data).unwrap();\n\n        let wrong_data = Tensor2D::\u003cCpuBackend\u003e::new(vec![1.0f32, 2.0, 3.0], 1, 3); // 3 features\n        let result = fitted.transform(\u0026wrong_data);\n\n        assert!(matches!(\n            result,\n            Err(PreprocessingError::FeatureMismatch {\n                expected_features: 2,\n                got_features: 3\n            })\n        ));\n    }\n}\n","traces":[{"line":38,"address":[1949872],"length":1,"stats":{"Line":1}},{"line":68,"address":[],"length":0,"stats":{"Line":0}},{"line":69,"address":[],"length":0,"stats":{"Line":0}},{"line":75,"address":[1851744],"length":1,"stats":{"Line":1}},{"line":77,"address":[1851745],"length":1,"stats":{"Line":1}},{"line":83,"address":[1851616],"length":1,"stats":{"Line":1}},{"line":84,"address":[1851656],"length":1,"stats":{"Line":1}},{"line":85,"address":[1851710],"length":1,"stats":{"Line":1}},{"line":86,"address":[1851716],"length":1,"stats":{"Line":1}},{"line":87,"address":[1851722],"length":1,"stats":{"Line":1}},{"line":97,"address":[1843115,1843157,1841584],"length":1,"stats":{"Line":1}},{"line":98,"address":[1841650],"length":1,"stats":{"Line":2}},{"line":100,"address":[1841695],"length":1,"stats":{"Line":2}},{"line":101,"address":[1841739],"length":1,"stats":{"Line":0}},{"line":102,"address":[1841705],"length":1,"stats":{"Line":0}},{"line":106,"address":[1841858],"length":1,"stats":{"Line":2}},{"line":107,"address":[1841959,1841895],"length":1,"stats":{"Line":4}},{"line":109,"address":[1841983],"length":1,"stats":{"Line":3}},{"line":110,"address":[1842044],"length":1,"stats":{"Line":2}},{"line":113,"address":[1842096],"length":1,"stats":{"Line":3}},{"line":114,"address":[1842126],"length":1,"stats":{"Line":2}},{"line":116,"address":[1842253],"length":1,"stats":{"Line":3}},{"line":117,"address":[1842356,1843252,1843232],"length":1,"stats":{"Line":7}},{"line":118,"address":[1843272],"length":1,"stats":{"Line":3}},{"line":119,"address":[1843329,1843288],"length":1,"stats":{"Line":4}},{"line":120,"address":[1843331],"length":1,"stats":{"Line":0}},{"line":122,"address":[1843312],"length":1,"stats":{"Line":3}},{"line":128,"address":[1843200,1842461,1843210,1842394],"length":1,"stats":{"Line":10}},{"line":137,"address":[1842856],"length":1,"stats":{"Line":3}},{"line":138,"address":[1842665],"length":1,"stats":{"Line":3}},{"line":139,"address":[1842752],"length":1,"stats":{"Line":2}},{"line":140,"address":[1842784],"length":1,"stats":{"Line":3}},{"line":141,"address":[1842824],"length":1,"stats":{"Line":2}},{"line":142,"address":[],"length":0,"stats":{"Line":0}},{"line":143,"address":[],"length":0,"stats":{"Line":0}},{"line":147,"address":[],"length":0,"stats":{"Line":0}},{"line":148,"address":[],"length":0,"stats":{"Line":0}},{"line":149,"address":[],"length":0,"stats":{"Line":0}},{"line":166,"address":[1851760],"length":1,"stats":{"Line":1}},{"line":167,"address":[],"length":0,"stats":{"Line":0}},{"line":171,"address":[1851776],"length":1,"stats":{"Line":1}},{"line":172,"address":[1851784],"length":1,"stats":{"Line":1}},{"line":176,"address":[],"length":0,"stats":{"Line":0}},{"line":178,"address":[],"length":0,"stats":{"Line":0}},{"line":182,"address":[],"length":0,"stats":{"Line":0}},{"line":193,"address":[1845312,1845793,1845799],"length":1,"stats":{"Line":3}},{"line":194,"address":[1845363],"length":1,"stats":{"Line":2}},{"line":196,"address":[1845386],"length":1,"stats":{"Line":3}},{"line":197,"address":[1845465],"length":1,"stats":{"Line":1}},{"line":198,"address":[1845461],"length":1,"stats":{"Line":1}},{"line":199,"address":[],"length":0,"stats":{"Line":0}},{"line":205,"address":[1845412],"length":1,"stats":{"Line":2}},{"line":206,"address":[1845427],"length":1,"stats":{"Line":4}},{"line":207,"address":[1845552,1845613],"length":1,"stats":{"Line":6}},{"line":209,"address":[1845658],"length":1,"stats":{"Line":2}},{"line":210,"address":[],"length":0,"stats":{"Line":0}},{"line":211,"address":[],"length":0,"stats":{"Line":0}},{"line":215,"address":[1844816,1845292,1845298],"length":1,"stats":{"Line":1}},{"line":216,"address":[1844867],"length":1,"stats":{"Line":1}},{"line":218,"address":[1844890],"length":1,"stats":{"Line":1}},{"line":219,"address":[1845005],"length":1,"stats":{"Line":0}},{"line":220,"address":[1845001],"length":1,"stats":{"Line":0}},{"line":221,"address":[],"length":0,"stats":{"Line":0}},{"line":226,"address":[1844901],"length":1,"stats":{"Line":1}},{"line":227,"address":[1844967],"length":1,"stats":{"Line":1}},{"line":228,"address":[1845108],"length":1,"stats":{"Line":1}},{"line":230,"address":[1845157],"length":1,"stats":{"Line":1}},{"line":231,"address":[],"length":0,"stats":{"Line":0}},{"line":232,"address":[],"length":0,"stats":{"Line":0}},{"line":236,"address":[1844480,1844789,1844795],"length":1,"stats":{"Line":1}},{"line":238,"address":[1844514],"length":1,"stats":{"Line":1}},{"line":239,"address":[1844545],"length":1,"stats":{"Line":1}},{"line":240,"address":[1844555],"length":1,"stats":{"Line":1}},{"line":241,"address":[1844610],"length":1,"stats":{"Line":1}},{"line":242,"address":[1844683],"length":1,"stats":{"Line":1}},{"line":246,"address":[1844344,1843360],"length":1,"stats":{"Line":1}},{"line":248,"address":[1844400,1843481,1843402,1844410],"length":1,"stats":{"Line":4}},{"line":251,"address":[1843690,1844368,1844378,1843619],"length":1,"stats":{"Line":4}},{"line":253,"address":[1843790,1844442,1844432,1843861],"length":1,"stats":{"Line":4}},{"line":257,"address":[1844127],"length":1,"stats":{"Line":1}},{"line":258,"address":[1844017],"length":1,"stats":{"Line":1}},{"line":259,"address":[1844027],"length":1,"stats":{"Line":1}},{"line":260,"address":[1844075],"length":1,"stats":{"Line":1}},{"line":261,"address":[],"length":0,"stats":{"Line":0}},{"line":262,"address":[1844123],"length":1,"stats":{"Line":1}},{"line":263,"address":[],"length":0,"stats":{"Line":0}},{"line":267,"address":[1844464],"length":1,"stats":{"Line":0}},{"line":268,"address":[1844469],"length":1,"stats":{"Line":0}},{"line":271,"address":[],"length":0,"stats":{"Line":0}},{"line":272,"address":[],"length":0,"stats":{"Line":0}},{"line":273,"address":[],"length":0,"stats":{"Line":0}},{"line":274,"address":[],"length":0,"stats":{"Line":0}},{"line":281,"address":[],"length":0,"stats":{"Line":0}},{"line":282,"address":[],"length":0,"stats":{"Line":0}},{"line":283,"address":[],"length":0,"stats":{"Line":0}},{"line":284,"address":[],"length":0,"stats":{"Line":0}}],"covered":62,"coverable":96},{"path":["/","home","victor","ML","machinelearne-rs","lib","src","preprocessing","scaling","mod.rs"],"content":"//! Scaling transformers for feature normalization.\n//!\n//! This module provides transformers that scale features to a common range\n//! or distribution, which is essential for many machine learning algorithms.\n//!\n//! # Available Transformers\n//!\n//! | Transformer | Description | Use Case |\n//! |-------------|-------------|----------|\n//! | [`StandardScaler`] | Z-score normalization (mean=0, std=1) | Default choice for most algorithms |\n//! | [`MinMaxScaler`] | Scale to [0, 1] or custom range | When bounded output is needed |\n//! | [`RobustScaler`] | Use median and IQR | Data with outliers |\n//! | [`MaxAbsScaler`] | Scale by max absolute value | Sparse data |\n//! | [`Normalizer`] | Scale individual samples to unit norm | Text classification, clustering |\n//!\n//! # Example\n//!\n//! ```ignore\n//! use machinelearne_rs::preprocessing::scaling::StandardScaler;\n//! use machinelearne_rs::preprocessing::Transformer;\n//! use machinelearne_rs::backend::CpuBackend;\n//!\n//! let scaler = StandardScaler::\u003cCpuBackend\u003e::new();\n//! let fitted = scaler.fit(\u0026data)?;\n//! let scaled = fitted.transform(\u0026new_data)?;\n//! ```\n\npub mod maxabs;\npub mod minmax;\npub mod normalizer;\npub mod robust;\npub mod standard;\n\npub use maxabs::{FittedMaxAbsScaler, MaxAbsScaler, MaxAbsScalerParams};\npub use minmax::{FittedMinMaxScaler, MinMaxScaler, MinMaxScalerConfig, MinMaxScalerParams};\npub use normalizer::{FittedNormalizer, NormType, Normalizer, NormalizerParams};\npub use robust::{FittedRobustScaler, RobustScaler, RobustScalerConfig, RobustScalerParams};\npub use standard::{\n    FittedStandardScaler, StandardScaler, StandardScalerConfig, StandardScalerParams,\n};\n","traces":[],"covered":0,"coverable":0},{"path":["/","home","victor","ML","machinelearne-rs","lib","src","preprocessing","scaling","normalizer.rs"],"content":"//! Normalizer - scales individual samples to unit norm.\n//!\n//! Normalizes samples individually to unit norm. Each sample (row) is\n//! rescaled independently of other samples.\n//!\n//! Supports three norm types:\n//! - L1: Sum of absolute values = 1\n//! - L2 (default): Sum of squares = 1 (Euclidean norm)\n//! - Max: Maximum absolute value = 1\n//!\n//! # Example\n//! ```ignore\n//! use machinelearne_rs::preprocessing::{Transformer, Normalizer, NormType};\n//! use machinelearne_rs::backend::CpuBackend;\n//!\n//! let normalizer = Normalizer::\u003cCpuBackend\u003e::new(NormType::L2);\n//! let normalized = normalizer.fit_transform(\u0026data)?;\n//! ```\n\nuse crate::backend::{Backend, Tensor2D};\nuse crate::preprocessing::error::PreprocessingError;\nuse crate::preprocessing::traits::{FittedTransformer, Transformer};\nuse serde::{Deserialize, Serialize};\nuse std::marker::PhantomData;\n\n/// Type of normalization to apply.\n#[derive(Clone, Copy, Debug, Default, Serialize, Deserialize)]\npub enum NormType {\n    /// L1 norm: sum of absolute values = 1\n    L1,\n    /// L2 norm (Euclidean): sum of squares = 1\n    #[default]\n    L2,\n    /// Max norm: maximum absolute value = 1\n    Max,\n}\n\n/// Serializable parameters for a fitted Normalizer (trivial - just norm type).\n#[derive(Clone, Serialize, Deserialize)]\npub struct NormalizerParams {\n    /// The norm type used.\n    pub norm: NormType,\n    /// Number of features seen during fit.\n    pub n_features: usize,\n}\n\n/// Normalizer transformer.\n///\n/// Scales individual samples to unit norm.\n#[derive(Clone)]\npub struct Normalizer\u003cB: Backend\u003e {\n    norm: NormType,\n    _backend: PhantomData\u003cB\u003e,\n}\n\nimpl\u003cB: Backend\u003e Default for Normalizer\u003cB\u003e {\n    fn default() -\u003e Self {\n        Self::new(NormType::default())\n    }\n}\n\nimpl\u003cB: Backend\u003e Normalizer\u003cB\u003e {\n    /// Create a new Normalizer with the specified norm type.\n    pub fn new(norm: NormType) -\u003e Self {\n        Self {\n            norm,\n            _backend: PhantomData,\n        }\n    }\n}\n\nimpl\u003cB: Backend\u003e Transformer\u003cB\u003e for Normalizer\u003cB\u003e {\n    type Input = Tensor2D\u003cB\u003e;\n    type Output = Tensor2D\u003cB\u003e;\n    type Params = NormalizerParams;\n    type Fitted = FittedNormalizer\u003cB\u003e;\n\n    fn fit(\u0026self, data: \u0026Self::Input) -\u003e Result\u003cSelf::Fitted, PreprocessingError\u003e {\n        let (rows, cols) = data.shape();\n\n        if rows == 0 {\n            return Err(PreprocessingError::EmptyData(\n                \"Cannot fit Normalizer on empty data\".to_string(),\n            ));\n        }\n\n        Ok(FittedNormalizer {\n            norm: self.norm,\n            n_features: cols,\n            _backend: PhantomData,\n        })\n    }\n\n    fn fit_transform(\u0026self, data: \u0026Self::Input) -\u003e Result\u003cSelf::Output, PreprocessingError\u003e {\n        // Normalizer is stateless - just transform directly\n        let (rows, cols) = data.shape();\n\n        if rows == 0 {\n            return Err(PreprocessingError::EmptyData(\n                \"Cannot normalize empty data\".to_string(),\n            ));\n        }\n\n        let flat_data = B::to_vec_1d(\u0026B::ravel_2d(\u0026data.data));\n        let mut result = vec![0.0; rows * cols];\n\n        for row in 0..rows {\n            // Extract row data\n            let row_start = row * cols;\n            let row_data: Vec\u003cf64\u003e = flat_data[row_start..row_start + cols].to_vec();\n\n            // Compute norm\n            let norm = match self.norm {\n                NormType::L1 =\u003e row_data.iter().map(|x| x.abs()).sum::\u003cf64\u003e(),\n                NormType::L2 =\u003e (row_data.iter().map(|x| x * x).sum::\u003cf64\u003e()).sqrt(),\n                NormType::Max =\u003e row_data.iter().map(|x| x.abs()).fold(0.0_f64, f64::max),\n            };\n\n            // Normalize\n            let scale = if norm == 0.0 { 1.0 } else { 1.0 / norm };\n            for col in 0..cols {\n                result[row_start + col] = flat_data[row_start + col] * scale;\n            }\n        }\n\n        Ok(Tensor2D {\n            data: B::from_vec_2d(result.iter().map(|\u0026x| x as f32).collect(), rows, cols),\n            backend: PhantomData::\u003cB\u003e,\n        })\n    }\n}\n\n/// Fitted Normalizer ready for inference.\n#[derive(Clone)]\npub struct FittedNormalizer\u003cB: Backend\u003e {\n    norm: NormType,\n    n_features: usize,\n    _backend: PhantomData\u003cB\u003e,\n}\n\nimpl\u003cB: Backend\u003e FittedNormalizer\u003cB\u003e {\n    /// Get the norm type used.\n    pub fn norm(\u0026self) -\u003e NormType {\n        self.norm\n    }\n}\n\nimpl\u003cB: Backend\u003e FittedTransformer\u003cB\u003e for FittedNormalizer\u003cB\u003e {\n    type Input = Tensor2D\u003cB\u003e;\n    type Output = Tensor2D\u003cB\u003e;\n    type Params = NormalizerParams;\n\n    fn transform(\u0026self, data: \u0026Self::Input) -\u003e Result\u003cSelf::Output, PreprocessingError\u003e {\n        let (rows, cols) = data.shape();\n\n        if cols != self.n_features {\n            return Err(PreprocessingError::FeatureMismatch {\n                expected_features: self.n_features,\n                got_features: cols,\n            });\n        }\n\n        if rows == 0 {\n            return Ok(data.clone());\n        }\n\n        let flat_data = B::to_vec_1d(\u0026B::ravel_2d(\u0026data.data));\n        let mut result = vec![0.0; rows * cols];\n\n        for row in 0..rows {\n            let row_start = row * cols;\n            let row_data: Vec\u003cf64\u003e = flat_data[row_start..row_start + cols].to_vec();\n\n            let norm = match self.norm {\n                NormType::L1 =\u003e row_data.iter().map(|x| x.abs()).sum::\u003cf64\u003e(),\n                NormType::L2 =\u003e (row_data.iter().map(|x| x * x).sum::\u003cf64\u003e()).sqrt(),\n                NormType::Max =\u003e row_data.iter().map(|x| x.abs()).fold(0.0_f64, f64::max),\n            };\n\n            let scale = if norm == 0.0 { 1.0 } else { 1.0 / norm };\n            for col in 0..cols {\n                result[row_start + col] = flat_data[row_start + col] * scale;\n            }\n        }\n\n        Ok(Tensor2D {\n            data: B::from_vec_2d(result.iter().map(|\u0026x| x as f32).collect(), rows, cols),\n            backend: PhantomData::\u003cB\u003e,\n        })\n    }\n\n    fn inverse_transform(\u0026self, _data: \u0026Self::Output) -\u003e Result\u003cSelf::Input, PreprocessingError\u003e {\n        Err(PreprocessingError::InvalidParameter(\n            \"Normalizer does not support inverse_transform (norm information is lost)\".to_string(),\n        ))\n    }\n\n    fn extract_params(\u0026self) -\u003e Self::Params {\n        NormalizerParams {\n            norm: self.norm,\n            n_features: self.n_features,\n        }\n    }\n\n    fn from_params(params: Self::Params) -\u003e Result\u003cSelf, PreprocessingError\u003e {\n        Ok(Self {\n            norm: params.norm,\n            n_features: params.n_features,\n            _backend: PhantomData,\n        })\n    }\n\n    fn n_features_in(\u0026self) -\u003e usize {\n        self.n_features\n    }\n\n    fn save_to_file\u003cP: AsRef\u003cstd::path::Path\u003e\u003e(\u0026self, path: P) -\u003e std::io::Result\u003c()\u003e {\n        let params = self.extract_params();\n        let bytes = bincode::serialize(\u0026params).map_err(std::io::Error::other)?;\n        std::fs::write(path, bytes)\n    }\n\n    fn load_from_file\u003cP: AsRef\u003cstd::path::Path\u003e\u003e(path: P) -\u003e Result\u003cSelf, PreprocessingError\u003e\n    where\n        Self: Sized,\n    {\n        let bytes = std::fs::read(path)?;\n        let params: NormalizerParams = bincode::deserialize(\u0026bytes)\n            .map_err(|e| PreprocessingError::SerializationError(e.to_string()))?;\n        Self::from_params(params)\n    }\n}\n\n#[cfg(test)]\nmod tests {\n    use super::*;\n    use crate::backend::CpuBackend;\n\n    fn create_test_data() -\u003e Tensor2D\u003cCpuBackend\u003e {\n        // [[3, 4], [1, 0], [0, 0]]\n        // Row 0: L2 norm = 5, so normalized = [0.6, 0.8]\n        // Row 1: L2 norm = 1, so normalized = [1, 0]\n        // Row 2: L2 norm = 0, so stays [0, 0]\n        Tensor2D::new(vec![3.0f32, 4.0, 1.0, 0.0, 0.0, 0.0], 3, 2)\n    }\n\n    #[test]\n    fn test_normalizer_l2() {\n        let data = create_test_data();\n        let normalizer = Normalizer::\u003cCpuBackend\u003e::new(NormType::L2);\n        let normalized = normalizer.fit_transform(\u0026data).unwrap();\n        let values = normalized.ravel().to_vec();\n\n        // Row 0: [3, 4] / 5 = [0.6, 0.8]\n        assert!((values[0] - 0.6).abs() \u003c 1e-6);\n        assert!((values[1] - 0.8).abs() \u003c 1e-6);\n\n        // Row 1: [1, 0] / 1 = [1, 0]\n        assert!((values[2] - 1.0).abs() \u003c 1e-6);\n        assert!((values[3] - 0.0).abs() \u003c 1e-6);\n\n        // Row 2: [0, 0] / 1 (zero case) = [0, 0]\n        assert!((values[4] - 0.0).abs() \u003c 1e-6);\n        assert!((values[5] - 0.0).abs() \u003c 1e-6);\n    }\n\n    #[test]\n    fn test_normalizer_l1() {\n        let data = create_test_data();\n        let normalizer = Normalizer::\u003cCpuBackend\u003e::new(NormType::L1);\n        let normalized = normalizer.fit_transform(\u0026data).unwrap();\n        let values = normalized.ravel().to_vec();\n\n        // Row 0: [3, 4] / 7 = [0.428..., 0.571...]\n        assert!((values[0] - 3.0 / 7.0).abs() \u003c 1e-6);\n        assert!((values[1] - 4.0 / 7.0).abs() \u003c 1e-6);\n\n        // Row 1: [1, 0] / 1 = [1, 0]\n        assert!((values[2] - 1.0).abs() \u003c 1e-6);\n        assert!((values[3] - 0.0).abs() \u003c 1e-6);\n    }\n\n    #[test]\n    fn test_normalizer_max() {\n        let data = create_test_data();\n        let normalizer = Normalizer::\u003cCpuBackend\u003e::new(NormType::Max);\n        let normalized = normalizer.fit_transform(\u0026data).unwrap();\n        let values = normalized.ravel().to_vec();\n\n        // Row 0: [3, 4] / 4 = [0.75, 1]\n        assert!((values[0] - 0.75).abs() \u003c 1e-6);\n        assert!((values[1] - 1.0).abs() \u003c 1e-6);\n\n        // Row 1: [1, 0] / 1 = [1, 0]\n        assert!((values[2] - 1.0).abs() \u003c 1e-6);\n        assert!((values[3] - 0.0).abs() \u003c 1e-6);\n    }\n\n    #[test]\n    fn test_normalizer_transform() {\n        let data = create_test_data();\n        let normalizer = Normalizer::\u003cCpuBackend\u003e::new(NormType::L2);\n        let fitted = normalizer.fit(\u0026data).unwrap();\n\n        let normalized = fitted.transform(\u0026data).unwrap();\n        let values = normalized.ravel().to_vec();\n\n        // Same as fit_transform\n        assert!((values[0] - 0.6).abs() \u003c 1e-6);\n        assert!((values[1] - 0.8).abs() \u003c 1e-6);\n    }\n\n    #[test]\n    fn test_normalizer_inverse_not_supported() {\n        let data = create_test_data();\n        let normalizer = Normalizer::\u003cCpuBackend\u003e::new(NormType::L2);\n        let fitted = normalizer.fit(\u0026data).unwrap();\n\n        let normalized = fitted.transform(\u0026data).unwrap();\n        let result = fitted.inverse_transform(\u0026normalized);\n\n        assert!(matches!(\n            result,\n            Err(PreprocessingError::InvalidParameter(_))\n        ));\n    }\n\n    #[test]\n    fn test_normalizer_serialization() {\n        let data = create_test_data();\n        let normalizer = Normalizer::\u003cCpuBackend\u003e::new(NormType::L2);\n        let fitted = normalizer.fit(\u0026data).unwrap();\n\n        let params = fitted.extract_params();\n        let restored = FittedNormalizer::\u003cCpuBackend\u003e::from_params(params).unwrap();\n\n        let normalized1 = fitted.transform(\u0026data).unwrap();\n        let normalized2 = restored.transform(\u0026data).unwrap();\n\n        let n1 = normalized1.ravel().to_vec();\n        let n2 = normalized2.ravel().to_vec();\n\n        for (a, b) in n1.iter().zip(n2.iter()) {\n            assert!((a - b).abs() \u003c 1e-6);\n        }\n    }\n\n    #[test]\n    fn test_normalizer_feature_mismatch() {\n        let data = create_test_data(); // 2 features\n        let normalizer = Normalizer::\u003cCpuBackend\u003e::new(NormType::L2);\n        let fitted = normalizer.fit(\u0026data).unwrap();\n\n        let wrong_data = Tensor2D::\u003cCpuBackend\u003e::new(vec![1.0f32, 2.0, 3.0], 1, 3); // 3 features\n        let result = fitted.transform(\u0026wrong_data);\n\n        assert!(matches!(\n            result,\n            Err(PreprocessingError::FeatureMismatch {\n                expected_features: 2,\n                got_features: 3\n            })\n        ));\n    }\n}\n","traces":[{"line":57,"address":[],"length":0,"stats":{"Line":0}},{"line":58,"address":[],"length":0,"stats":{"Line":0}},{"line":64,"address":[1763280],"length":1,"stats":{"Line":1}},{"line":78,"address":[1760352],"length":1,"stats":{"Line":1}},{"line":79,"address":[1760400],"length":1,"stats":{"Line":1}},{"line":81,"address":[1760426],"length":1,"stats":{"Line":1}},{"line":82,"address":[1760460],"length":1,"stats":{"Line":0}},{"line":83,"address":[1760432],"length":1,"stats":{"Line":0}},{"line":87,"address":[1760539],"length":1,"stats":{"Line":1}},{"line":88,"address":[1760537],"length":1,"stats":{"Line":1}},{"line":89,"address":[],"length":0,"stats":{"Line":0}},{"line":90,"address":[],"length":0,"stats":{"Line":0}},{"line":94,"address":[1757920,1760176,1760182],"length":1,"stats":{"Line":1}},{"line":96,"address":[1757986],"length":1,"stats":{"Line":1}},{"line":98,"address":[1758023],"length":1,"stats":{"Line":1}},{"line":99,"address":[1758063],"length":1,"stats":{"Line":0}},{"line":100,"address":[1758029],"length":1,"stats":{"Line":0}},{"line":104,"address":[1758173,1758225],"length":1,"stats":{"Line":2}},{"line":105,"address":[1758328],"length":1,"stats":{"Line":3}},{"line":107,"address":[1758492,1758405],"length":1,"stats":{"Line":6}},{"line":109,"address":[1759000,1758613,1758971],"length":1,"stats":{"Line":6}},{"line":110,"address":[1759037,1758979],"length":1,"stats":{"Line":6}},{"line":113,"address":[1759135],"length":1,"stats":{"Line":3}},{"line":114,"address":[1759328,1759181,1760240,1760254],"length":1,"stats":{"Line":4}},{"line":115,"address":[1759212,1759489,1760286,1760272],"length":1,"stats":{"Line":4}},{"line":116,"address":[1759246,1759647,1760208,1760222],"length":1,"stats":{"Line":4}},{"line":120,"address":[1759440,1759743],"length":1,"stats":{"Line":6}},{"line":121,"address":[1760171,1759796],"length":1,"stats":{"Line":6}},{"line":122,"address":[1759945,1759986],"length":1,"stats":{"Line":6}},{"line":126,"address":[1758825],"length":1,"stats":{"Line":3}},{"line":127,"address":[1758646,1760320,1760330],"length":1,"stats":{"Line":9}},{"line":128,"address":[],"length":0,"stats":{"Line":0}},{"line":143,"address":[],"length":0,"stats":{"Line":0}},{"line":144,"address":[],"length":0,"stats":{"Line":0}},{"line":153,"address":[1760784,1763108,1763102],"length":1,"stats":{"Line":1}},{"line":154,"address":[1760850],"length":1,"stats":{"Line":1}},{"line":156,"address":[1760895],"length":1,"stats":{"Line":1}},{"line":157,"address":[1760946],"length":1,"stats":{"Line":1}},{"line":158,"address":[1760943],"length":1,"stats":{"Line":1}},{"line":159,"address":[],"length":0,"stats":{"Line":0}},{"line":163,"address":[1760908],"length":1,"stats":{"Line":1}},{"line":164,"address":[1761008],"length":1,"stats":{"Line":0}},{"line":167,"address":[1761098,1761150],"length":1,"stats":{"Line":1}},{"line":168,"address":[1761253],"length":1,"stats":{"Line":1}},{"line":170,"address":[1761330,1761417],"length":1,"stats":{"Line":2}},{"line":171,"address":[1761925,1761538,1761896],"length":1,"stats":{"Line":2}},{"line":172,"address":[1761904,1761962],"length":1,"stats":{"Line":2}},{"line":174,"address":[1762060],"length":1,"stats":{"Line":1}},{"line":175,"address":[1763182,1762107,1763168,1762254],"length":1,"stats":{"Line":0}},{"line":176,"address":[1763200,1763214,1762138,1762415],"length":1,"stats":{"Line":4}},{"line":177,"address":[1763136,1762172,1763150,1762573],"length":1,"stats":{"Line":0}},{"line":180,"address":[1762366,1762669],"length":1,"stats":{"Line":2}},{"line":181,"address":[1762722,1763097],"length":1,"stats":{"Line":3}},{"line":182,"address":[1762871,1762912],"length":1,"stats":{"Line":2}},{"line":186,"address":[1761750],"length":1,"stats":{"Line":2}},{"line":187,"address":[1763248,1763258,1761571],"length":1,"stats":{"Line":4}},{"line":188,"address":[],"length":0,"stats":{"Line":0}},{"line":192,"address":[1760656],"length":1,"stats":{"Line":1}},{"line":193,"address":[1760706],"length":1,"stats":{"Line":1}},{"line":194,"address":[1760679],"length":1,"stats":{"Line":1}},{"line":198,"address":[1760640],"length":1,"stats":{"Line":1}},{"line":200,"address":[1760645],"length":1,"stats":{"Line":1}},{"line":201,"address":[1760648],"length":1,"stats":{"Line":1}},{"line":205,"address":[1760576],"length":1,"stats":{"Line":1}},{"line":206,"address":[1760590],"length":1,"stats":{"Line":1}},{"line":207,"address":[],"length":0,"stats":{"Line":0}},{"line":208,"address":[],"length":0,"stats":{"Line":0}},{"line":209,"address":[],"length":0,"stats":{"Line":0}},{"line":213,"address":[1760624],"length":1,"stats":{"Line":0}},{"line":214,"address":[1760629],"length":1,"stats":{"Line":0}},{"line":217,"address":[],"length":0,"stats":{"Line":0}},{"line":218,"address":[],"length":0,"stats":{"Line":0}},{"line":219,"address":[],"length":0,"stats":{"Line":0}},{"line":220,"address":[],"length":0,"stats":{"Line":0}},{"line":227,"address":[],"length":0,"stats":{"Line":0}},{"line":228,"address":[],"length":0,"stats":{"Line":0}},{"line":229,"address":[],"length":0,"stats":{"Line":0}},{"line":230,"address":[],"length":0,"stats":{"Line":0}}],"covered":49,"coverable":78},{"path":["/","home","victor","ML","machinelearne-rs","lib","src","preprocessing","scaling","robust.rs"],"content":"//! Robust Scaler.\n//!\n//! Scales features using statistics that are robust to outliers.\n//! Uses median and interquartile range (IQR) instead of mean and std.\n//!\n//! The transformation is:\n//! ```text\n//! X_scaled = (X - median) / IQR\n//! ```\n//!\n//! where IQR is the range between the 1st quartile (25%) and 3rd quartile (75%).\n//!\n//! # Example\n//! ```ignore\n//! use machinelearne_rs::preprocessing::{Transformer, RobustScaler};\n//! use machinelearne_rs::backend::CpuBackend;\n//!\n//! let scaler = RobustScaler::\u003cCpuBackend\u003e::new()\n//!     .with_centering(true);\n//!\n//! let fitted = scaler.fit(\u0026data)?;\n//! let scaled = fitted.transform(\u0026data)?;\n//! ```\n\nuse crate::backend::{Backend, Tensor1D, Tensor2D};\nuse crate::preprocessing::error::PreprocessingError;\nuse crate::preprocessing::traits::{FittedTransformer, Transformer};\nuse serde::{Deserialize, Serialize};\nuse std::marker::PhantomData;\n\n/// Configuration for RobustScaler.\n#[derive(Clone, Serialize, Deserialize)]\npub struct RobustScalerConfig {\n    /// If True, center the data by median before scaling.\n    pub with_centering: bool,\n    /// If True, scale the data by IQR.\n    pub with_scaling: bool,\n    /// Quantile range for IQR (default: (25.0, 75.0)).\n    pub quantile_range: (f64, f64),\n}\n\nimpl Default for RobustScalerConfig {\n    fn default() -\u003e Self {\n        Self {\n            with_centering: true,\n            with_scaling: true,\n            quantile_range: (25.0, 75.0),\n        }\n    }\n}\n\n/// Serializable parameters for a fitted RobustScaler.\n#[derive(Clone, Serialize, Deserialize)]\npub struct RobustScalerParams {\n    /// Configuration options.\n    pub config: RobustScalerConfig,\n    /// Center (median) for each feature.\n    pub center_: Vec\u003cf64\u003e,\n    /// Scale (IQR) for each feature.\n    pub scale_: Vec\u003cf64\u003e,\n    /// Number of features seen during fit.\n    pub n_features: usize,\n}\n\n/// RobustScaler transformer (unfitted).\n///\n/// Scales features using statistics that are robust to outliers.\n#[derive(Clone)]\npub struct RobustScaler\u003cB: Backend\u003e {\n    config: RobustScalerConfig,\n    _backend: PhantomData\u003cB\u003e,\n}\n\nimpl\u003cB: Backend\u003e Default for RobustScaler\u003cB\u003e {\n    fn default() -\u003e Self {\n        Self::new()\n    }\n}\n\nimpl\u003cB: Backend\u003e RobustScaler\u003cB\u003e {\n    /// Create a new RobustScaler with default configuration.\n    pub fn new() -\u003e Self {\n        Self {\n            config: RobustScalerConfig::default(),\n            _backend: PhantomData,\n        }\n    }\n\n    /// Set whether to center data by median.\n    pub fn with_centering(mut self, with_centering: bool) -\u003e Self {\n        self.config.with_centering = with_centering;\n        self\n    }\n\n    /// Set whether to scale data by IQR.\n    pub fn with_scaling(mut self, with_scaling: bool) -\u003e Self {\n        self.config.with_scaling = with_scaling;\n        self\n    }\n\n    /// Set the quantile range for IQR calculation.\n    pub fn with_quantile_range(mut self, min: f64, max: f64) -\u003e Self {\n        assert!(\n            (0.0..=100.0).contains(\u0026min) \u0026\u0026 (0.0..=100.0).contains(\u0026max) \u0026\u0026 min \u003c max,\n            \"Invalid quantile range: must be 0 \u003c= min \u003c max \u003c= 100\"\n        );\n        self.config.quantile_range = (min, max);\n        self\n    }\n}\n\n/// Compute quantiles for a column of data.\nfn compute_quantiles(data: \u0026[f64], q_low: f64, q_high: f64) -\u003e (f64, f64) {\n    let mut sorted: Vec\u003cf64\u003e = data.to_vec();\n    sorted.sort_by(|a, b| a.partial_cmp(b).unwrap_or(std::cmp::Ordering::Equal));\n\n    let n = sorted.len();\n    if n == 0 {\n        return (0.0, 1.0);\n    }\n\n    // Linear interpolation for quantiles\n    let low_idx = (q_low / 100.0 * (n - 1) as f64).min((n - 1) as f64);\n    let high_idx = (q_high / 100.0 * (n - 1) as f64).min((n - 1) as f64);\n\n    let low_val = interpolate(\u0026sorted, low_idx);\n    let high_val = interpolate(\u0026sorted, high_idx);\n\n    (low_val, high_val)\n}\n\n/// Linear interpolation at a fractional index.\nfn interpolate(sorted: \u0026[f64], idx: f64) -\u003e f64 {\n    let lower = idx.floor() as usize;\n    let upper = (lower + 1).min(sorted.len() - 1);\n    let frac = idx - lower as f64;\n\n    sorted[lower] * (1.0 - frac) + sorted[upper] * frac\n}\n\nimpl\u003cB: Backend\u003e Transformer\u003cB\u003e for RobustScaler\u003cB\u003e {\n    type Input = Tensor2D\u003cB\u003e;\n    type Output = Tensor2D\u003cB\u003e;\n    type Params = RobustScalerParams;\n    type Fitted = FittedRobustScaler\u003cB\u003e;\n\n    fn fit(\u0026self, data: \u0026Self::Input) -\u003e Result\u003cSelf::Fitted, PreprocessingError\u003e {\n        let (rows, cols) = data.shape();\n\n        if rows == 0 {\n            return Err(PreprocessingError::EmptyData(\n                \"Cannot fit RobustScaler on empty data\".to_string(),\n            ));\n        }\n\n        // Get the raw data\n        let flat_data = B::to_vec_1d(\u0026B::ravel_2d(\u0026data.data));\n\n        // Compute center (median) and scale (IQR) for each column\n        let mut center_vals = vec![0.0; cols];\n        let mut scale_vals = vec![1.0; cols];\n\n        let (q_low, q_high) = self.config.quantile_range;\n\n        for col in 0..cols {\n            // Extract column data\n            let column_data: Vec\u003cf64\u003e = (0..rows).map(|row| flat_data[row * cols + col]).collect();\n\n            // Compute quantiles\n            let (low, high) = compute_quantiles(\u0026column_data, q_low, q_high);\n\n            if self.config.with_centering {\n                // Median is the midpoint of the IQR\n                center_vals[col] = (low + high) / 2.0;\n            }\n\n            if self.config.with_scaling {\n                let iqr = high - low;\n                scale_vals[col] = if iqr == 0.0 { 1.0 } else { iqr };\n            }\n        }\n\n        let center_ = Tensor1D {\n            data: B::from_vec_1d(center_vals.iter().map(|\u0026x| x as f32).collect()),\n            backend: PhantomData::\u003cB\u003e,\n        };\n\n        let scale_ = Tensor1D {\n            data: B::from_vec_1d(scale_vals.iter().map(|\u0026x| x as f32).collect()),\n            backend: PhantomData::\u003cB\u003e,\n        };\n\n        Ok(FittedRobustScaler {\n            config: self.config.clone(),\n            center_,\n            scale_,\n            n_features: cols,\n            _backend: PhantomData,\n        })\n    }\n\n    fn fit_transform(\u0026self, data: \u0026Self::Input) -\u003e Result\u003cSelf::Output, PreprocessingError\u003e {\n        let fitted = self.fit(data)?;\n        fitted.transform(data)\n    }\n}\n\n/// Fitted RobustScaler ready for inference.\n#[derive(Clone)]\npub struct FittedRobustScaler\u003cB: Backend\u003e {\n    config: RobustScalerConfig,\n    center_: Tensor1D\u003cB\u003e,\n    scale_: Tensor1D\u003cB\u003e,\n    n_features: usize,\n    _backend: PhantomData\u003cB\u003e,\n}\n\nimpl\u003cB: Backend\u003e FittedRobustScaler\u003cB\u003e {\n    /// Get the center (median) values for each feature.\n    pub fn center(\u0026self) -\u003e \u0026Tensor1D\u003cB\u003e {\n        \u0026self.center_\n    }\n\n    /// Get the scale (IQR) values for each feature.\n    pub fn scale(\u0026self) -\u003e \u0026Tensor1D\u003cB\u003e {\n        \u0026self.scale_\n    }\n}\n\nimpl\u003cB: Backend\u003e FittedTransformer\u003cB\u003e for FittedRobustScaler\u003cB\u003e {\n    type Input = Tensor2D\u003cB\u003e;\n    type Output = Tensor2D\u003cB\u003e;\n    type Params = RobustScalerParams;\n\n    fn transform(\u0026self, data: \u0026Self::Input) -\u003e Result\u003cSelf::Output, PreprocessingError\u003e {\n        let (_, cols) = data.shape();\n\n        if cols != self.n_features {\n            return Err(PreprocessingError::FeatureMismatch {\n                expected_features: self.n_features,\n                got_features: cols,\n            });\n        }\n\n        let mut result_data = data.data.clone();\n\n        if self.config.with_centering {\n            result_data = B::broadcast_sub_1d_to_2d_rows(\u0026result_data, \u0026self.center_.data);\n        }\n\n        if self.config.with_scaling {\n            result_data = B::broadcast_div_1d_to_2d_rows(\u0026result_data, \u0026self.scale_.data);\n        }\n\n        Ok(Tensor2D {\n            data: result_data,\n            backend: PhantomData::\u003cB\u003e,\n        })\n    }\n\n    fn inverse_transform(\u0026self, data: \u0026Self::Output) -\u003e Result\u003cSelf::Input, PreprocessingError\u003e {\n        let (_, cols) = data.shape();\n\n        if cols != self.n_features {\n            return Err(PreprocessingError::FeatureMismatch {\n                expected_features: self.n_features,\n                got_features: cols,\n            });\n        }\n\n        let mut result_data = data.data.clone();\n\n        if self.config.with_scaling {\n            result_data = B::broadcast_mul_1d_to_2d_rows(\u0026result_data, \u0026self.scale_.data);\n        }\n\n        if self.config.with_centering {\n            result_data = B::broadcast_add_1d_to_2d_rows(\u0026result_data, \u0026self.center_.data);\n        }\n\n        Ok(Tensor2D {\n            data: result_data,\n            backend: PhantomData::\u003cB\u003e,\n        })\n    }\n\n    fn extract_params(\u0026self) -\u003e Self::Params {\n        RobustScalerParams {\n            config: self.config.clone(),\n            center_: self.center_.to_vec(),\n            scale_: self.scale_.to_vec(),\n            n_features: self.n_features,\n        }\n    }\n\n    fn from_params(params: Self::Params) -\u003e Result\u003cSelf, PreprocessingError\u003e {\n        let center_ = Tensor1D {\n            data: B::from_vec_1d(params.center_.iter().map(|\u0026x| x as f32).collect()),\n            backend: PhantomData::\u003cB\u003e,\n        };\n        let scale_ = Tensor1D {\n            data: B::from_vec_1d(params.scale_.iter().map(|\u0026x| x as f32).collect()),\n            backend: PhantomData::\u003cB\u003e,\n        };\n\n        Ok(Self {\n            config: params.config,\n            center_,\n            scale_,\n            n_features: params.n_features,\n            _backend: PhantomData,\n        })\n    }\n\n    fn n_features_in(\u0026self) -\u003e usize {\n        self.n_features\n    }\n\n    fn save_to_file\u003cP: AsRef\u003cstd::path::Path\u003e\u003e(\u0026self, path: P) -\u003e std::io::Result\u003c()\u003e {\n        let params = self.extract_params();\n        let bytes = bincode::serialize(\u0026params).map_err(std::io::Error::other)?;\n        std::fs::write(path, bytes)\n    }\n\n    fn load_from_file\u003cP: AsRef\u003cstd::path::Path\u003e\u003e(path: P) -\u003e Result\u003cSelf, PreprocessingError\u003e\n    where\n        Self: Sized,\n    {\n        let bytes = std::fs::read(path)?;\n        let params: RobustScalerParams = bincode::deserialize(\u0026bytes)\n            .map_err(|e| PreprocessingError::SerializationError(e.to_string()))?;\n        Self::from_params(params)\n    }\n}\n\n#[cfg(test)]\nmod tests {\n    use super::*;\n    use crate::backend::CpuBackend;\n\n    fn create_test_data() -\u003e Tensor2D\u003cCpuBackend\u003e {\n        // [[1, 2], [2, 4], [3, 6], [4, 8], [5, 100]]  -- second column has outlier\n        Tensor2D::new(\n            vec![1.0f32, 2.0, 2.0, 4.0, 3.0, 6.0, 4.0, 8.0, 5.0, 100.0],\n            5,\n            2,\n        )\n    }\n\n    #[test]\n    fn test_robust_scaler_fit() {\n        let data = create_test_data();\n        let scaler = RobustScaler::\u003cCpuBackend\u003e::new();\n        let fitted = scaler.fit(\u0026data).unwrap();\n\n        let center = fitted.center().to_vec();\n        let scale = fitted.scale().to_vec();\n\n        // First column: [1, 2, 3, 4, 5], median = 3, Q1 = 2, Q3 = 4, IQR = 2\n        assert!((center[0] - 3.0).abs() \u003c 0.1);\n        assert!((scale[0] - 2.0).abs() \u003c 0.1);\n\n        // Second column: [2, 4, 6, 8, 100], median = 6, Q1 = 4, Q3 = 8, IQR = 4\n        // (outlier 100 doesn't significantly affect median/IQR)\n        assert!((center[1] - 6.0).abs() \u003c 0.1);\n        assert!((scale[1] - 4.0).abs() \u003c 0.1);\n    }\n\n    #[test]\n    fn test_robust_scaler_transform() {\n        let data = create_test_data();\n        let scaler = RobustScaler::\u003cCpuBackend\u003e::new();\n        let fitted = scaler.fit(\u0026data).unwrap();\n\n        let transformed = fitted.transform(\u0026data).unwrap();\n        let values = transformed.ravel().to_vec();\n\n        // First column center (median) should be ~3, scale ~2\n        // Values: (1-3)/2 = -1, (2-3)/2 = -0.5, (3-3)/2 = 0, (4-3)/2 = 0.5, (5-3)/2 = 1\n        assert!((values[0] - (-1.0)).abs() \u003c 0.1);\n        assert!((values[2] - (-0.5)).abs() \u003c 0.1);\n        assert!((values[4] - 0.0).abs() \u003c 0.1);\n        assert!((values[6] - 0.5).abs() \u003c 0.1);\n        assert!((values[8] - 1.0).abs() \u003c 0.1);\n    }\n\n    #[test]\n    fn test_robust_scaler_inverse_transform() {\n        let data = create_test_data();\n        let scaler = RobustScaler::\u003cCpuBackend\u003e::new();\n        let fitted = scaler.fit(\u0026data).unwrap();\n\n        let transformed = fitted.transform(\u0026data).unwrap();\n        let recovered = fitted.inverse_transform(\u0026transformed).unwrap();\n\n        let original = data.ravel().to_vec();\n        let recovered_vec = recovered.ravel().to_vec();\n\n        for (o, r) in original.iter().zip(recovered_vec.iter()) {\n            assert!((o - r).abs() \u003c 1e-6, \"Expected {}, got {}\", o, r);\n        }\n    }\n\n    #[test]\n    fn test_robust_scaler_without_centering() {\n        let data = create_test_data();\n        let scaler = RobustScaler::\u003cCpuBackend\u003e::new().with_centering(false);\n        let fitted = scaler.fit(\u0026data).unwrap();\n\n        let center = fitted.center().to_vec();\n        assert!(center.iter().all(|\u0026c| c == 0.0));\n    }\n\n    #[test]\n    fn test_robust_scaler_without_scaling() {\n        let data = create_test_data();\n        let scaler = RobustScaler::\u003cCpuBackend\u003e::new().with_scaling(false);\n        let fitted = scaler.fit(\u0026data).unwrap();\n\n        let scale = fitted.scale().to_vec();\n        assert!(scale.iter().all(|\u0026s| s == 1.0));\n    }\n\n    #[test]\n    fn test_robust_scaler_serialization() {\n        let data = create_test_data();\n        let scaler = RobustScaler::\u003cCpuBackend\u003e::new();\n        let fitted = scaler.fit(\u0026data).unwrap();\n\n        let params = fitted.extract_params();\n        let restored = FittedRobustScaler::\u003cCpuBackend\u003e::from_params(params).unwrap();\n\n        let transformed1 = fitted.transform(\u0026data).unwrap();\n        let transformed2 = restored.transform(\u0026data).unwrap();\n\n        let t1 = transformed1.ravel().to_vec();\n        let t2 = transformed2.ravel().to_vec();\n\n        for (i, (a, b)) in t1.iter().zip(t2.iter()).enumerate() {\n            assert!(\n                (a - b).abs() \u003c 1e-6,\n                \"Mismatch at index {}: {} vs {}\",\n                i,\n                a,\n                b\n            );\n        }\n    }\n\n    #[test]\n    fn test_robust_scaler_feature_mismatch() {\n        let data = create_test_data(); // 2 features\n        let scaler = RobustScaler::\u003cCpuBackend\u003e::new();\n        let fitted = scaler.fit(\u0026data).unwrap();\n\n        let wrong_data = Tensor2D::\u003cCpuBackend\u003e::new(vec![1.0f32, 2.0, 3.0], 1, 3); // 3 features\n        let result = fitted.transform(\u0026wrong_data);\n\n        assert!(matches!(\n            result,\n            Err(PreprocessingError::FeatureMismatch {\n                expected_features: 2,\n                got_features: 3\n            })\n        ));\n    }\n}\n","traces":[{"line":43,"address":[2361792],"length":1,"stats":{"Line":3}},{"line":75,"address":[],"length":0,"stats":{"Line":0}},{"line":76,"address":[],"length":0,"stats":{"Line":0}},{"line":82,"address":[2966672],"length":1,"stats":{"Line":2}},{"line":84,"address":[2966685],"length":1,"stats":{"Line":2}},{"line":90,"address":[2966624],"length":1,"stats":{"Line":1}},{"line":91,"address":[2966638],"length":1,"stats":{"Line":1}},{"line":92,"address":[2966644],"length":1,"stats":{"Line":1}},{"line":96,"address":[2966576],"length":1,"stats":{"Line":1}},{"line":97,"address":[2966590],"length":1,"stats":{"Line":1}},{"line":98,"address":[2966596],"length":1,"stats":{"Line":1}},{"line":102,"address":[],"length":0,"stats":{"Line":0}},{"line":103,"address":[],"length":0,"stats":{"Line":0}},{"line":104,"address":[],"length":0,"stats":{"Line":0}},{"line":105,"address":[],"length":0,"stats":{"Line":0}},{"line":107,"address":[],"length":0,"stats":{"Line":0}},{"line":108,"address":[],"length":0,"stats":{"Line":0}},{"line":113,"address":[2363311,2362304,2363305],"length":1,"stats":{"Line":1}},{"line":114,"address":[2362385],"length":1,"stats":{"Line":3}},{"line":115,"address":[2362398,2362477],"length":1,"stats":{"Line":9}},{"line":117,"address":[2362492],"length":1,"stats":{"Line":2}},{"line":118,"address":[2362523],"length":1,"stats":{"Line":3}},{"line":119,"address":[2362529],"length":1,"stats":{"Line":0}},{"line":123,"address":[2362590,2362679],"length":1,"stats":{"Line":5}},{"line":124,"address":[2362881],"length":1,"stats":{"Line":2}},{"line":126,"address":[2363133],"length":1,"stats":{"Line":3}},{"line":127,"address":[2363202],"length":1,"stats":{"Line":2}},{"line":129,"address":[2363269],"length":1,"stats":{"Line":3}},{"line":133,"address":[2361840],"length":1,"stats":{"Line":2}},{"line":134,"address":[2361876],"length":1,"stats":{"Line":3}},{"line":135,"address":[2362140,2361968],"length":1,"stats":{"Line":2}},{"line":136,"address":[2362078],"length":1,"stats":{"Line":3}},{"line":138,"address":[2362133,2362179,2362280],"length":1,"stats":{"Line":5}},{"line":147,"address":[2957802,2958243,2956192],"length":1,"stats":{"Line":3}},{"line":148,"address":[2956258],"length":1,"stats":{"Line":3}},{"line":150,"address":[2956287],"length":1,"stats":{"Line":3}},{"line":151,"address":[2956331],"length":1,"stats":{"Line":0}},{"line":152,"address":[2956297],"length":1,"stats":{"Line":0}},{"line":157,"address":[2956518,2956466],"length":1,"stats":{"Line":1}},{"line":160,"address":[2956605],"length":1,"stats":{"Line":3}},{"line":161,"address":[2956638],"length":1,"stats":{"Line":1}},{"line":163,"address":[2956721],"length":1,"stats":{"Line":3}},{"line":165,"address":[2956766,2956861],"length":1,"stats":{"Line":4}},{"line":167,"address":[2956990,2958342,2958320,2957824],"length":1,"stats":{"Line":8}},{"line":170,"address":[2957924,2957839],"length":1,"stats":{"Line":4}},{"line":172,"address":[2957981,2958087],"length":1,"stats":{"Line":6}},{"line":174,"address":[2958015],"length":1,"stats":{"Line":3}},{"line":177,"address":[2958238,2957995],"length":1,"stats":{"Line":4}},{"line":178,"address":[2958119],"length":1,"stats":{"Line":3}},{"line":179,"address":[2958138],"length":1,"stats":{"Line":3}},{"line":184,"address":[2957068,2958256,2958266],"length":1,"stats":{"Line":8}},{"line":189,"address":[2958288,2957257,2957324,2958298],"length":1,"stats":{"Line":14}},{"line":193,"address":[2957583],"length":1,"stats":{"Line":3}},{"line":194,"address":[2957464],"length":1,"stats":{"Line":4}},{"line":195,"address":[2957519],"length":1,"stats":{"Line":3}},{"line":196,"address":[2957551],"length":1,"stats":{"Line":4}},{"line":197,"address":[],"length":0,"stats":{"Line":0}},{"line":198,"address":[],"length":0,"stats":{"Line":0}},{"line":202,"address":[],"length":0,"stats":{"Line":0}},{"line":203,"address":[],"length":0,"stats":{"Line":0}},{"line":204,"address":[],"length":0,"stats":{"Line":0}},{"line":220,"address":[2966752],"length":1,"stats":{"Line":1}},{"line":221,"address":[],"length":0,"stats":{"Line":0}},{"line":225,"address":[2966736],"length":1,"stats":{"Line":1}},{"line":226,"address":[2966744],"length":1,"stats":{"Line":1}},{"line":235,"address":[2960128,2960700,2960694],"length":1,"stats":{"Line":1}},{"line":236,"address":[2960177],"length":1,"stats":{"Line":2}},{"line":238,"address":[2960200],"length":1,"stats":{"Line":1}},{"line":239,"address":[2960252],"length":1,"stats":{"Line":1}},{"line":240,"address":[2960248],"length":1,"stats":{"Line":1}},{"line":241,"address":[],"length":0,"stats":{"Line":0}},{"line":245,"address":[2960210],"length":1,"stats":{"Line":1}},{"line":247,"address":[2960461,2960225],"length":1,"stats":{"Line":3}},{"line":248,"address":[2960384,2960335,2960391],"length":1,"stats":{"Line":4}},{"line":251,"address":[2960302,2960676],"length":1,"stats":{"Line":4}},{"line":252,"address":[2960567,2960603],"length":1,"stats":{"Line":2}},{"line":255,"address":[2960489],"length":1,"stats":{"Line":2}},{"line":256,"address":[2960466],"length":1,"stats":{"Line":2}},{"line":257,"address":[],"length":0,"stats":{"Line":0}},{"line":261,"address":[2960108,2959536,2960102],"length":1,"stats":{"Line":1}},{"line":262,"address":[2959585],"length":1,"stats":{"Line":1}},{"line":264,"address":[2959608],"length":1,"stats":{"Line":1}},{"line":265,"address":[2959660],"length":1,"stats":{"Line":0}},{"line":266,"address":[2959656],"length":1,"stats":{"Line":0}},{"line":267,"address":[],"length":0,"stats":{"Line":0}},{"line":271,"address":[2959618],"length":1,"stats":{"Line":1}},{"line":273,"address":[2959633,2959873],"length":1,"stats":{"Line":2}},{"line":274,"address":[2959796,2959803,2959730],"length":1,"stats":{"Line":2}},{"line":277,"address":[2960084,2959710],"length":1,"stats":{"Line":2}},{"line":278,"address":[2959992,2960011],"length":1,"stats":{"Line":1}},{"line":281,"address":[2959901],"length":1,"stats":{"Line":1}},{"line":282,"address":[2959878],"length":1,"stats":{"Line":1}},{"line":283,"address":[],"length":0,"stats":{"Line":0}},{"line":287,"address":[2959296,2959513,2959507],"length":1,"stats":{"Line":1}},{"line":289,"address":[2959319],"length":1,"stats":{"Line":1}},{"line":290,"address":[2959343],"length":1,"stats":{"Line":1}},{"line":291,"address":[2959353],"length":1,"stats":{"Line":1}},{"line":292,"address":[2959414],"length":1,"stats":{"Line":1}},{"line":296,"address":[2959189,2958464],"length":1,"stats":{"Line":1}},{"line":298,"address":[2959226,2958558,2958494,2959216],"length":1,"stats":{"Line":4}},{"line":302,"address":[2959258,2958758,2959248,2958687],"length":1,"stats":{"Line":4}},{"line":306,"address":[2958996],"length":1,"stats":{"Line":1}},{"line":307,"address":[2958908],"length":1,"stats":{"Line":1}},{"line":308,"address":[2958944],"length":1,"stats":{"Line":1}},{"line":309,"address":[],"length":0,"stats":{"Line":0}},{"line":310,"address":[2958992],"length":1,"stats":{"Line":1}},{"line":311,"address":[],"length":0,"stats":{"Line":0}},{"line":315,"address":[2959280],"length":1,"stats":{"Line":0}},{"line":316,"address":[2959285],"length":1,"stats":{"Line":0}},{"line":319,"address":[],"length":0,"stats":{"Line":0}},{"line":320,"address":[],"length":0,"stats":{"Line":0}},{"line":321,"address":[],"length":0,"stats":{"Line":0}},{"line":322,"address":[],"length":0,"stats":{"Line":0}},{"line":329,"address":[],"length":0,"stats":{"Line":0}},{"line":330,"address":[],"length":0,"stats":{"Line":0}},{"line":331,"address":[],"length":0,"stats":{"Line":0}},{"line":332,"address":[],"length":0,"stats":{"Line":0}}],"covered":82,"coverable":117},{"path":["/","home","victor","ML","machinelearne-rs","lib","src","preprocessing","scaling","standard.rs"],"content":"//! Standard Scaler (Z-score normalization).\n//!\n//! Transforms features by removing the mean and scaling to unit variance.\n//!\n//! The standard score of a sample `x` is calculated as:\n//! ```text\n//! z = (x - u) / s\n//! ```\n//! where `u` is the mean of the training samples, and `s` is the standard deviation.\n//!\n//! # Example\n//! ```ignore\n//! use machinelearne_rs::preprocessing::{Transformer, StandardScaler};\n//! use machinelearne_rs::backend::CpuBackend;\n//!\n//! let scaler = StandardScaler::\u003cCpuBackend\u003e::new()\n//!     .with_mean(true)\n//!     .with_std(true);\n//!\n//! let fitted = scaler.fit(\u0026data)?;\n//! let scaled = fitted.transform(\u0026data)?;\n//!\n//! // Later, for inference:\n//! let loaded = StandardScaler::load_from_file(\"scaler.bin\")?;\n//! let new_scaled = loaded.transform(\u0026new_data)?;\n//! ```\n\nuse crate::backend::{Backend, Tensor1D, Tensor2D};\nuse crate::preprocessing::error::PreprocessingError;\nuse crate::preprocessing::traits::{FittedTransformer, Transformer};\nuse serde::{Deserialize, Serialize};\nuse std::marker::PhantomData;\n\n/// Configuration for StandardScaler.\n#[derive(Clone, Debug, Serialize, Deserialize)]\npub struct StandardScalerConfig {\n    /// If True, center the data before scaling.\n    pub with_mean: bool,\n    /// If True, scale the data to unit variance.\n    pub with_std: bool,\n}\n\nimpl Default for StandardScalerConfig {\n    fn default() -\u003e Self {\n        Self {\n            with_mean: true,\n            with_std: true,\n        }\n    }\n}\n\n/// Serializable parameters for a fitted StandardScaler.\n#[derive(Clone, Debug, Serialize, Deserialize)]\npub struct StandardScalerParams {\n    /// Configuration options.\n    pub config: StandardScalerConfig,\n    /// Mean of each feature (None if with_mean=False).\n    pub mean: Vec\u003cf64\u003e,\n    /// Standard deviation of each feature (None if with_std=False).\n    pub std: Vec\u003cf64\u003e,\n    /// Number of features seen during fit.\n    pub n_features: usize,\n}\n\n// Note: SerializableParams is automatically implemented via blanket impl in serialization.rs\n// for types that implement Serialize + Deserialize\n\n/// StandardScaler transformer (unfitted).\n///\n/// Transforms features by removing the mean and scaling to unit variance.\n#[derive(Clone)]\npub struct StandardScaler\u003cB: Backend\u003e {\n    config: StandardScalerConfig,\n    _backend: PhantomData\u003cB\u003e,\n}\n\nimpl\u003cB: Backend\u003e Default for StandardScaler\u003cB\u003e {\n    fn default() -\u003e Self {\n        Self::new()\n    }\n}\n\nimpl\u003cB: Backend\u003e StandardScaler\u003cB\u003e {\n    /// Create a new StandardScaler with default configuration.\n    pub fn new() -\u003e Self {\n        Self {\n            config: StandardScalerConfig::default(),\n            _backend: PhantomData,\n        }\n    }\n\n    /// Set whether to center data by mean.\n    pub fn with_mean(mut self, with_mean: bool) -\u003e Self {\n        self.config.with_mean = with_mean;\n        self\n    }\n\n    /// Set whether to scale data to unit variance.\n    pub fn with_std(mut self, with_std: bool) -\u003e Self {\n        self.config.with_std = with_std;\n        self\n    }\n}\n\nimpl\u003cB: Backend\u003e Transformer\u003cB\u003e for StandardScaler\u003cB\u003e {\n    type Input = Tensor2D\u003cB\u003e;\n    type Output = Tensor2D\u003cB\u003e;\n    type Params = StandardScalerParams;\n    type Fitted = FittedStandardScaler\u003cB\u003e;\n\n    fn fit(\u0026self, data: \u0026Self::Input) -\u003e Result\u003cSelf::Fitted, PreprocessingError\u003e {\n        let (rows, cols) = data.shape();\n\n        if rows == 0 {\n            return Err(PreprocessingError::EmptyData(\n                \"Cannot fit StandardScaler on empty data\".to_string(),\n            ));\n        }\n\n        let mean = if self.config.with_mean {\n            Tensor1D {\n                data: B::col_mean_2d(\u0026data.data),\n                backend: PhantomData::\u003cB\u003e,\n            }\n        } else {\n            Tensor1D {\n                data: B::zeros_1d(cols),\n                backend: PhantomData::\u003cB\u003e,\n            }\n        };\n\n        let std = if self.config.with_std {\n            Tensor1D {\n                data: B::col_std_2d(\u0026data.data, 0), // population std (ddof=0)\n                backend: PhantomData::\u003cB\u003e,\n            }\n        } else {\n            let ones: Vec\u003cf64\u003e = vec![1.0; cols];\n            Tensor1D {\n                data: B::from_vec_1d(ones.iter().map(|\u0026x| x as f32).collect()),\n                backend: PhantomData::\u003cB\u003e,\n            }\n        };\n\n        // Handle zero std (constant features)\n        let std_vec = std.to_vec();\n        let std_adjusted: Vec\u003cf64\u003e = std_vec\n            .iter()\n            .map(|\u0026s| if s == 0.0 { 1.0 } else { s })\n            .collect();\n        let std_final = Tensor1D {\n            data: B::from_vec_1d(std_adjusted.iter().map(|\u0026x| x as f32).collect()),\n            backend: PhantomData::\u003cB\u003e,\n        };\n\n        Ok(FittedStandardScaler {\n            config: self.config.clone(),\n            mean,\n            std: std_final,\n            n_features: cols,\n            _backend: PhantomData,\n        })\n    }\n\n    fn fit_transform(\u0026self, data: \u0026Self::Input) -\u003e Result\u003cSelf::Output, PreprocessingError\u003e {\n        let fitted = self.fit(data)?;\n        fitted.transform(data)\n    }\n}\n\n/// Fitted StandardScaler ready for inference.\n#[derive(Clone)]\npub struct FittedStandardScaler\u003cB: Backend\u003e {\n    config: StandardScalerConfig,\n    mean: Tensor1D\u003cB\u003e,\n    std: Tensor1D\u003cB\u003e,\n    n_features: usize,\n    _backend: PhantomData\u003cB\u003e,\n}\n\nimpl\u003cB: Backend\u003e FittedStandardScaler\u003cB\u003e {\n    /// Get the mean values for each feature.\n    pub fn mean(\u0026self) -\u003e \u0026Tensor1D\u003cB\u003e {\n        \u0026self.mean\n    }\n\n    /// Get the standard deviation values for each feature.\n    pub fn std(\u0026self) -\u003e \u0026Tensor1D\u003cB\u003e {\n        \u0026self.std\n    }\n}\n\nimpl\u003cB: Backend\u003e FittedTransformer\u003cB\u003e for FittedStandardScaler\u003cB\u003e {\n    type Input = Tensor2D\u003cB\u003e;\n    type Output = Tensor2D\u003cB\u003e;\n    type Params = StandardScalerParams;\n\n    fn transform(\u0026self, data: \u0026Self::Input) -\u003e Result\u003cSelf::Output, PreprocessingError\u003e {\n        let (_, cols) = data.shape();\n\n        if cols != self.n_features {\n            return Err(PreprocessingError::FeatureMismatch {\n                expected_features: self.n_features,\n                got_features: cols,\n            });\n        }\n\n        let mut result_data = data.data.clone();\n\n        if self.config.with_mean {\n            result_data = B::broadcast_sub_1d_to_2d_rows(\u0026result_data, \u0026self.mean.data);\n        }\n\n        if self.config.with_std {\n            result_data = B::broadcast_div_1d_to_2d_rows(\u0026result_data, \u0026self.std.data);\n        }\n\n        Ok(Tensor2D {\n            data: result_data,\n            backend: PhantomData,\n        })\n    }\n\n    fn inverse_transform(\u0026self, data: \u0026Self::Output) -\u003e Result\u003cSelf::Input, PreprocessingError\u003e {\n        let (_, cols) = data.shape();\n\n        if cols != self.n_features {\n            return Err(PreprocessingError::FeatureMismatch {\n                expected_features: self.n_features,\n                got_features: cols,\n            });\n        }\n\n        let mut result_data = data.data.clone();\n\n        if self.config.with_std {\n            result_data = B::broadcast_mul_1d_to_2d_rows(\u0026result_data, \u0026self.std.data);\n        }\n\n        if self.config.with_mean {\n            result_data = B::broadcast_add_1d_to_2d_rows(\u0026result_data, \u0026self.mean.data);\n        }\n\n        Ok(Tensor2D {\n            data: result_data,\n            backend: PhantomData,\n        })\n    }\n\n    fn extract_params(\u0026self) -\u003e Self::Params {\n        StandardScalerParams {\n            config: self.config.clone(),\n            mean: self.mean.to_vec(),\n            std: self.std.to_vec(),\n            n_features: self.n_features,\n        }\n    }\n\n    fn from_params(params: Self::Params) -\u003e Result\u003cSelf, PreprocessingError\u003e {\n        let mean = Tensor1D {\n            data: B::from_vec_1d(params.mean.iter().map(|\u0026x| x as f32).collect()),\n            backend: PhantomData,\n        };\n        let std = Tensor1D {\n            data: B::from_vec_1d(params.std.iter().map(|\u0026x| x as f32).collect()),\n            backend: PhantomData,\n        };\n\n        Ok(Self {\n            config: params.config,\n            mean,\n            std,\n            n_features: params.n_features,\n            _backend: PhantomData,\n        })\n    }\n\n    fn n_features_in(\u0026self) -\u003e usize {\n        self.n_features\n    }\n\n    fn save_to_file\u003cP: AsRef\u003cstd::path::Path\u003e\u003e(\u0026self, path: P) -\u003e std::io::Result\u003c()\u003e {\n        let params = self.extract_params();\n        let bytes = bincode::serialize(\u0026params).map_err(std::io::Error::other)?;\n        std::fs::write(path, bytes)\n    }\n\n    fn load_from_file\u003cP: AsRef\u003cstd::path::Path\u003e\u003e(path: P) -\u003e Result\u003cSelf, PreprocessingError\u003e\n    where\n        Self: Sized,\n    {\n        let bytes = std::fs::read(path)?;\n        let params: StandardScalerParams = bincode::deserialize(\u0026bytes)\n            .map_err(|e| PreprocessingError::SerializationError(e.to_string()))?;\n        Self::from_params(params)\n    }\n}\n\n#[cfg(test)]\nmod tests {\n    use super::*;\n    use crate::backend::CpuBackend;\n\n    fn create_test_data() -\u003e Tensor2D\u003cCpuBackend\u003e {\n        // [[0, 1], [0, 1], [1, 3]]\n        Tensor2D::new(vec![0.0f32, 1.0, 0.0, 1.0, 1.0, 3.0], 3, 2)\n    }\n\n    #[test]\n    fn test_standard_scaler_fit() {\n        let data = create_test_data();\n        let scaler = StandardScaler::\u003cCpuBackend\u003e::new();\n        let fitted = scaler.fit(\u0026data).unwrap();\n\n        // Mean: [1/3, 5/3]\n        let mean = fitted.mean().to_vec();\n        assert!((mean[0] - 1.0 / 3.0).abs() \u003c 1e-10);\n        assert!((mean[1] - 5.0 / 3.0).abs() \u003c 1e-10);\n    }\n\n    #[test]\n    fn test_standard_scaler_transform() {\n        let data = create_test_data();\n        let scaler = StandardScaler::\u003cCpuBackend\u003e::new();\n        let fitted = scaler.fit(\u0026data).unwrap();\n\n        let transformed = fitted.transform(\u0026data).unwrap();\n\n        // After standardization, each column should have meanâ0 and stdâ1\n        let mean_vals: Vec\u003cf64\u003e =\n            CpuBackend::to_vec_1d(\u0026CpuBackend::col_mean_2d(\u0026transformed.data));\n        let std_vals: Vec\u003cf64\u003e =\n            CpuBackend::to_vec_1d(\u0026CpuBackend::col_std_2d(\u0026transformed.data, 0));\n\n        assert!(mean_vals[0].abs() \u003c 1e-10, \"mean[0] = {}\", mean_vals[0]);\n        assert!(mean_vals[1].abs() \u003c 1e-10, \"mean[1] = {}\", mean_vals[1]);\n        // Using a slightly larger tolerance for numerical stability\n        assert!((std_vals[0] - 1.0).abs() \u003c 1e-8, \"std[0] = {}\", std_vals[0]);\n        assert!((std_vals[1] - 1.0).abs() \u003c 1e-8, \"std[1] = {}\", std_vals[1]);\n    }\n\n    #[test]\n    fn test_standard_scaler_inverse_transform() {\n        let data = create_test_data();\n        let scaler = StandardScaler::\u003cCpuBackend\u003e::new();\n        let fitted = scaler.fit(\u0026data).unwrap();\n\n        let transformed = fitted.transform(\u0026data).unwrap();\n        let recovered = fitted.inverse_transform(\u0026transformed).unwrap();\n\n        let original = data.ravel().to_vec();\n        let recovered_vec = recovered.ravel().to_vec();\n\n        for (o, r) in original.iter().zip(recovered_vec.iter()) {\n            assert!((o - r).abs() \u003c 1e-10);\n        }\n    }\n\n    #[test]\n    fn test_standard_scaler_without_mean() {\n        let data = create_test_data();\n        let scaler = StandardScaler::\u003cCpuBackend\u003e::new().with_mean(false);\n        let fitted = scaler.fit(\u0026data).unwrap();\n\n        // Mean should be zeros\n        let mean = fitted.mean().to_vec();\n        assert!(mean.iter().all(|\u0026m| m == 0.0));\n    }\n\n    #[test]\n    fn test_standard_scaler_without_std() {\n        let data = create_test_data();\n        let scaler = StandardScaler::\u003cCpuBackend\u003e::new().with_std(false);\n        let fitted = scaler.fit(\u0026data).unwrap();\n\n        // Std should be ones\n        let std = fitted.std().to_vec();\n        assert!(std.iter().all(|\u0026s| s == 1.0));\n    }\n\n    #[test]\n    fn test_standard_scaler_serialization() {\n        let data = create_test_data();\n        let scaler = StandardScaler::\u003cCpuBackend\u003e::new();\n        let fitted = scaler.fit(\u0026data).unwrap();\n\n        let params = fitted.extract_params();\n        let restored = FittedStandardScaler::\u003cCpuBackend\u003e::from_params(params).unwrap();\n\n        let transformed1 = fitted.transform(\u0026data).unwrap();\n        let transformed2 = restored.transform(\u0026data).unwrap();\n\n        let t1 = transformed1.ravel().to_vec();\n        let t2 = transformed2.ravel().to_vec();\n\n        // Use 1e-6 tolerance due to f32-\u003ef64 conversion precision\n        for (i, (a, b)) in t1.iter().zip(t2.iter()).enumerate() {\n            assert!(\n                (a - b).abs() \u003c 1e-6,\n                \"Mismatch at index {}: {} vs {}\",\n                i,\n                a,\n                b\n            );\n        }\n    }\n\n    #[test]\n    fn test_standard_scaler_feature_mismatch() {\n        let data = create_test_data(); // 2 features\n        let scaler = StandardScaler::\u003cCpuBackend\u003e::new();\n        let fitted = scaler.fit(\u0026data).unwrap();\n\n        // Try to transform with wrong number of features\n        let wrong_data = Tensor2D::\u003cCpuBackend\u003e::new(vec![1.0f32, 2.0, 3.0], 1, 3); // 3 features\n        let result = fitted.transform(\u0026wrong_data);\n\n        assert!(matches!(\n            result,\n            Err(PreprocessingError::FeatureMismatch {\n                expected_features: 2,\n                got_features: 3\n            })\n        ));\n    }\n\n    #[test]\n    fn test_standard_scaler_fit_transform() {\n        let data = create_test_data();\n        let scaler = StandardScaler::\u003cCpuBackend\u003e::new();\n\n        let transformed = scaler.fit_transform(\u0026data).unwrap();\n\n        // After standardization, each column should have meanâ0 and stdâ1\n        let mean_vals: Vec\u003cf64\u003e =\n            CpuBackend::to_vec_1d(\u0026CpuBackend::col_mean_2d(\u0026transformed.data));\n        let std_vals: Vec\u003cf64\u003e =\n            CpuBackend::to_vec_1d(\u0026CpuBackend::col_std_2d(\u0026transformed.data, 0));\n\n        assert!(mean_vals[0].abs() \u003c 1e-10);\n        assert!(mean_vals[1].abs() \u003c 1e-10);\n    }\n}\n","traces":[{"line":44,"address":[3426928],"length":1,"stats":{"Line":5}},{"line":78,"address":[],"length":0,"stats":{"Line":0}},{"line":79,"address":[],"length":0,"stats":{"Line":0}},{"line":85,"address":[3438064],"length":1,"stats":{"Line":3}},{"line":87,"address":[3438065],"length":1,"stats":{"Line":4}},{"line":93,"address":[3438128],"length":1,"stats":{"Line":1}},{"line":94,"address":[3438156],"length":1,"stats":{"Line":1}},{"line":95,"address":[3438160],"length":1,"stats":{"Line":1}},{"line":99,"address":[3438080],"length":1,"stats":{"Line":1}},{"line":100,"address":[3438108],"length":1,"stats":{"Line":1}},{"line":101,"address":[3438112],"length":1,"stats":{"Line":1}},{"line":111,"address":[3428864,3430479,3429684],"length":1,"stats":{"Line":4}},{"line":112,"address":[3428930],"length":1,"stats":{"Line":4}},{"line":114,"address":[3428967],"length":1,"stats":{"Line":4}},{"line":115,"address":[3429011],"length":1,"stats":{"Line":0}},{"line":116,"address":[3428977],"length":1,"stats":{"Line":0}},{"line":120,"address":[3429230,3429130],"length":1,"stats":{"Line":5}},{"line":122,"address":[3429240],"length":1,"stats":{"Line":4}},{"line":127,"address":[3429161],"length":1,"stats":{"Line":1}},{"line":132,"address":[3429317,3429738],"length":1,"stats":{"Line":10}},{"line":134,"address":[3429381],"length":1,"stats":{"Line":5}},{"line":138,"address":[3429331],"length":1,"stats":{"Line":1}},{"line":140,"address":[3430554,3429516,3430544,3429440],"length":1,"stats":{"Line":4}},{"line":146,"address":[3429677],"length":1,"stats":{"Line":2}},{"line":147,"address":[3429788],"length":1,"stats":{"Line":5}},{"line":149,"address":[3429879,3430586,3430576],"length":1,"stats":{"Line":15}},{"line":152,"address":[3429996,3430512,3429929,3430522],"length":1,"stats":{"Line":20}},{"line":156,"address":[3430280],"length":1,"stats":{"Line":5}},{"line":157,"address":[3430128],"length":1,"stats":{"Line":5}},{"line":158,"address":[3430208],"length":1,"stats":{"Line":5}},{"line":159,"address":[3430248],"length":1,"stats":{"Line":5}},{"line":160,"address":[],"length":0,"stats":{"Line":0}},{"line":161,"address":[],"length":0,"stats":{"Line":0}},{"line":165,"address":[3428496,3428851,3428845],"length":1,"stats":{"Line":1}},{"line":166,"address":[3428534],"length":1,"stats":{"Line":1}},{"line":167,"address":[3428781],"length":1,"stats":{"Line":1}},{"line":183,"address":[3438192],"length":1,"stats":{"Line":1}},{"line":184,"address":[],"length":0,"stats":{"Line":0}},{"line":188,"address":[3438176],"length":1,"stats":{"Line":1}},{"line":189,"address":[3438184],"length":1,"stats":{"Line":1}},{"line":198,"address":[3432822,3432828,3432256],"length":1,"stats":{"Line":1}},{"line":199,"address":[3432305],"length":1,"stats":{"Line":2}},{"line":201,"address":[3432328],"length":1,"stats":{"Line":1}},{"line":202,"address":[3432380],"length":1,"stats":{"Line":1}},{"line":203,"address":[3432376],"length":1,"stats":{"Line":1}},{"line":204,"address":[],"length":0,"stats":{"Line":0}},{"line":208,"address":[3432338],"length":1,"stats":{"Line":1}},{"line":210,"address":[3432353,3432589],"length":1,"stats":{"Line":3}},{"line":211,"address":[3432463,3432512,3432519],"length":1,"stats":{"Line":3}},{"line":214,"address":[3432804,3432430],"length":1,"stats":{"Line":4}},{"line":215,"address":[3432695,3432731],"length":1,"stats":{"Line":2}},{"line":218,"address":[3432617],"length":1,"stats":{"Line":3}},{"line":219,"address":[3432594],"length":1,"stats":{"Line":3}},{"line":220,"address":[],"length":0,"stats":{"Line":0}},{"line":224,"address":[3432236,3432230,3431664],"length":1,"stats":{"Line":1}},{"line":225,"address":[3431713],"length":1,"stats":{"Line":1}},{"line":227,"address":[3431736],"length":1,"stats":{"Line":1}},{"line":228,"address":[3431788],"length":1,"stats":{"Line":0}},{"line":229,"address":[3431784],"length":1,"stats":{"Line":0}},{"line":230,"address":[],"length":0,"stats":{"Line":0}},{"line":234,"address":[3431746],"length":1,"stats":{"Line":1}},{"line":236,"address":[3431761,3432001],"length":1,"stats":{"Line":2}},{"line":237,"address":[3431931,3431858,3431924],"length":1,"stats":{"Line":2}},{"line":240,"address":[3432212,3431838],"length":1,"stats":{"Line":2}},{"line":241,"address":[3432139,3432120],"length":1,"stats":{"Line":1}},{"line":244,"address":[3432029],"length":1,"stats":{"Line":1}},{"line":245,"address":[3432006],"length":1,"stats":{"Line":1}},{"line":246,"address":[],"length":0,"stats":{"Line":0}},{"line":250,"address":[3431424,3431651,3431645],"length":1,"stats":{"Line":1}},{"line":252,"address":[3431455],"length":1,"stats":{"Line":1}},{"line":253,"address":[3431482],"length":1,"stats":{"Line":1}},{"line":254,"address":[3431492],"length":1,"stats":{"Line":1}},{"line":255,"address":[3431563],"length":1,"stats":{"Line":1}},{"line":259,"address":[3430656,3431323],"length":1,"stats":{"Line":1}},{"line":261,"address":[3430686,3430750,3431354,3431344],"length":1,"stats":{"Line":4}},{"line":265,"address":[3430879,3430950,3431376,3431386],"length":1,"stats":{"Line":4}},{"line":269,"address":[3431158],"length":1,"stats":{"Line":1}},{"line":270,"address":[3431100],"length":1,"stats":{"Line":1}},{"line":271,"address":[3431106],"length":1,"stats":{"Line":1}},{"line":272,"address":[],"length":0,"stats":{"Line":0}},{"line":273,"address":[3431154],"length":1,"stats":{"Line":1}},{"line":274,"address":[],"length":0,"stats":{"Line":0}},{"line":278,"address":[3431408],"length":1,"stats":{"Line":4}},{"line":279,"address":[3431413],"length":1,"stats":{"Line":4}},{"line":282,"address":[],"length":0,"stats":{"Line":0}},{"line":283,"address":[],"length":0,"stats":{"Line":0}},{"line":284,"address":[],"length":0,"stats":{"Line":0}},{"line":285,"address":[],"length":0,"stats":{"Line":0}},{"line":292,"address":[],"length":0,"stats":{"Line":0}},{"line":293,"address":[],"length":0,"stats":{"Line":0}},{"line":294,"address":[],"length":0,"stats":{"Line":0}},{"line":295,"address":[],"length":0,"stats":{"Line":0}}],"covered":69,"coverable":92},{"path":["/","home","victor","ML","machinelearne-rs","lib","src","preprocessing","traits.rs"],"content":"//! Core traits for preprocessing transformers.\n//!\n//! This module defines the two central traits:\n//! - [`Transformer`]: Used during fitting; has hyperparameters and can learn from data.\n//! - [`FittedTransformer`]: After fitting; ready for inference and serialization.\n\nuse crate::backend::Backend;\nuse crate::preprocessing::error::PreprocessingError;\nuse crate::serialization::SerializableParams;\n\n/// Trait for unfitted transformers with hyperparameters.\n///\n/// A transformer learns parameters from training data and can then transform\n/// new data using those learned parameters. This trait represents the\n/// configurable, unfitted state.\n///\n/// # Type Parameters\n/// - `B`: The backend (e.g., `CpuBackend`) used for computation.\n/// - `Input`: Input data type (typically `Tensor2D\u003cB\u003e`).\n/// - `Output`: Output data type (typically `Tensor2D\u003cB\u003e`).\n/// - `Params`: Serializable representation of learned parameters.\n/// - `Fitted`: The corresponding fitted transformer type.\n///\n/// # Example\n/// ```ignore\n/// use machinelearne_rs::preprocessing::{Transformer, StandardScaler};\n/// use machinelearne_rs::backend::CpuBackend;\n///\n/// let scaler = StandardScaler::\u003cCpuBackend\u003e::new();\n/// let fitted = scaler.fit(\u0026data)?;\n/// let transformed = fitted.transform(\u0026new_data)?;\n/// ```\npub trait Transformer\u003cB: Backend\u003e: Clone {\n    /// Input data type for transformation.\n    type Input;\n    /// Output data type after transformation.\n    type Output;\n    /// Serializable representation of learned parameters.\n    type Params: SerializableParams;\n    /// The fitted transformer type ready for inference.\n    type Fitted: FittedTransformer\u003c\n        B,\n        Params = Self::Params,\n        Input = Self::Input,\n        Output = Self::Output,\n    \u003e;\n\n    /// Fit the transformer to the training data.\n    ///\n    /// Learns parameters (e.g., mean and std for StandardScaler) from the data.\n    ///\n    /// # Errors\n    /// Returns [`PreprocessingError`] if:\n    /// - Data is empty\n    /// - Data contains invalid values (NaN, Inf)\n    /// - Shape is incompatible with the transformer\n    fn fit(\u0026self, data: \u0026Self::Input) -\u003e Result\u003cSelf::Fitted, PreprocessingError\u003e;\n\n    /// Fit the transformer and transform the data in one step.\n    ///\n    /// This is often more efficient than calling `fit` followed by `transform`,\n    /// especially for transformers that can compute the transform during fitting.\n    fn fit_transform(\u0026self, data: \u0026Self::Input) -\u003e Result\u003cSelf::Output, PreprocessingError\u003e;\n}\n\n/// Trait for fitted transformers ready for inference.\n///\n/// After fitting, a transformer contains learned parameters (e.g., mean_, std_\n/// for StandardScaler) and can transform new data. It can also be serialized\n/// and deserialized for deployment.\n///\n/// # Type Parameters\n/// - `B`: The backend used for computation.\n/// - `Params`: Serializable representation of learned parameters.\n///\n/// # Guarantees\n/// - `extract_params()` + `from_params()` is a round-trip.\n/// - `save_to_file` / `load_from_file` are cross-platform compatible.\npub trait FittedTransformer\u003cB: Backend\u003e: Clone {\n    /// Input data type for transformation.\n    type Input;\n    /// Output data type after transformation.\n    type Output;\n    /// Serializable representation of learned parameters.\n    type Params: SerializableParams;\n\n    /// Transform data using learned parameters.\n    ///\n    /// # Errors\n    /// Returns [`PreprocessingError`] if:\n    /// - Input shape doesn't match expected number of features\n    /// - Input contains invalid values\n    fn transform(\u0026self, data: \u0026Self::Input) -\u003e Result\u003cSelf::Output, PreprocessingError\u003e;\n\n    /// Reverse the transformation (if supported).\n    ///\n    /// Not all transformers support inverse transformation.\n    /// For example, OneHotEncoder cannot be inverted.\n    ///\n    /// # Errors\n    /// Returns [`PreprocessingError`] if inverse transform is not supported\n    /// or if the data cannot be inverse transformed.\n    fn inverse_transform(\u0026self, data: \u0026Self::Output) -\u003e Result\u003cSelf::Input, PreprocessingError\u003e;\n\n    /// Extract learned parameters as a serializable representation.\n    fn extract_params(\u0026self) -\u003e Self::Params;\n\n    /// Reconstruct a fitted transformer from parameters.\n    fn from_params(params: Self::Params) -\u003e Result\u003cSelf, PreprocessingError\u003e\n    where\n        Self: Sized;\n\n    /// Save the fitted transformer to a file.\n    fn save_to_file\u003cP: AsRef\u003cstd::path::Path\u003e\u003e(\u0026self, path: P) -\u003e std::io::Result\u003c()\u003e {\n        let params = self.extract_params();\n        let bytes = params.to_bytes().map_err(std::io::Error::other)?;\n        std::fs::write(path, bytes)\n    }\n\n    /// Load a fitted transformer from a file.\n    fn load_from_file\u003cP: AsRef\u003cstd::path::Path\u003e\u003e(path: P) -\u003e Result\u003cSelf, PreprocessingError\u003e\n    where\n        Self: Sized,\n    {\n        let bytes = std::fs::read(path)?;\n        let params = Self::Params::from_bytes(\u0026bytes)\n            .map_err(|e| PreprocessingError::SerializationError(e.to_string()))?;\n        Self::from_params(params)\n    }\n\n    /// Returns the number of features seen during fit.\n    fn n_features_in(\u0026self) -\u003e usize;\n}\n\n/// Marker trait for transformers that don't require fitting.\n///\n/// Stateless transformers (like Normalizer) can transform data without\n/// learning any parameters. They implement both `Transformer` and this trait.\npub trait StatelessTransformer\u003cB: Backend\u003e: Transformer\u003cB\u003e {\n    /// Transform data without fitting.\n    ///\n    /// For stateless transformers, this is equivalent to `fit_transform`\n    /// but communicates that no learning occurs.\n    fn transform_direct(data: \u0026Self::Input) -\u003e Result\u003cSelf::Output, PreprocessingError\u003e;\n}\n","traces":[{"line":114,"address":[3469878,3469312,3469840],"length":1,"stats":{"Line":4}},{"line":115,"address":[1999586],"length":1,"stats":{"Line":4}},{"line":116,"address":[3374885,3374828,3375222],"length":1,"stats":{"Line":8}},{"line":117,"address":[2817440,2817544],"length":1,"stats":{"Line":8}},{"line":121,"address":[3375264,3375955,3375982],"length":1,"stats":{"Line":4}},{"line":125,"address":[2000153],"length":1,"stats":{"Line":4}},{"line":126,"address":[2817837,2817920,2818063,2817966],"length":1,"stats":{"Line":12}},{"line":127,"address":[2000479,2000407],"length":1,"stats":{"Line":4}},{"line":128,"address":[2000693],"length":1,"stats":{"Line":4}}],"covered":9,"coverable":9},{"path":["/","home","victor","ML","machinelearne-rs","lib","src","regularizers","mod.rs"],"content":"use crate::backend::scalar::Scalar;\nuse crate::backend::Backend;\nuse crate::loss::{LinearParams, Tensor1D};\nuse crate::model::linear::LinearRegression;\nuse crate::model::TrainableModel;\n/// Computes the regularization penalty and its gradient w.r.t. model parameters.\n///\n/// This trait enables pluggable regularization strategies (e.g., L2, L1) without\n/// modifying the model or trainer logic.\n///\n/// # Returns\n/// A tuple `(penalty, grad)` where:\n/// - `penalty` is a scalar value added to the total loss (for logging/metrics),\n/// - `grad` is the gradient of the penalty w.r.t. model parameters, to be combined\n///   with the data-driven gradient during backpropagation.\npub trait Regularizer\u003cB: Backend, M: TrainableModel\u003cB\u003e\u003e {\n    fn regularizer_penalty_grad(\u0026self, model: \u0026M) -\u003e (Scalar\u003cB\u003e, M::Gradients);\n}\n\n/// L2 (ridge) regularization: penalty = Î» * ||w||Â².\n///\n/// Only applies to weights; bias is not regularized (standard practice).\n///\n/// Gradient w.r.t. weights: â/âw (Î» * wáµw) = 2Î»w.\npub struct L2\u003cB: Backend\u003e {\n    lambda: Scalar\u003cB\u003e,\n}\n\nimpl\u003cB: Backend\u003e L2\u003cB\u003e {\n    /// Creates an L2 regularizer with strength `lambda`.\n    ///\n    /// # Arguments\n    /// * `lambda` â non-negative regularization coefficient (Î» â¥ 0).\n    pub fn new(lambda: f64) -\u003e Self {\n        Self {\n            lambda: Scalar::\u003cB\u003e::new(lambda),\n        }\n    }\n}\n\nimpl\u003cB\u003e Regularizer\u003cB, LinearRegression\u003cB\u003e\u003e for L2\u003cB\u003e\nwhere\n    B: Backend,\n{\n    fn regularizer_penalty_grad(\n        \u0026self,\n        model: \u0026LinearRegression\u003cB\u003e,\n    ) -\u003e (\n        Scalar\u003cB\u003e,\n        \u003cLinearRegression\u003cB\u003e as TrainableModel\u003cB\u003e\u003e::Gradients,\n    ) {\n        let params = model.params();\n        let weight_grad = params.weights.scale(\u0026(self.lambda * Scalar::\u003cB\u003e::new(2.)));\n\n        let loss = params.weights.dot(\u0026params.weights);\n\n        let loss = self.lambda * loss;\n\n        (\n            loss,\n            LinearParams::\u003cB\u003e {\n                weights: weight_grad,\n                bias: Scalar::\u003cB\u003e::new(0.),\n            },\n        )\n    }\n}\n/// A no-op regularizer that adds zero penalty and zero gradient.\n///\n/// Useful as a default when no regularization is desired.\npub struct NoRegularizer;\n\nimpl\u003cB\u003e Regularizer\u003cB, LinearRegression\u003cB\u003e\u003e for NoRegularizer\nwhere\n    B: Backend,\n{\n    fn regularizer_penalty_grad(\n        \u0026self,\n        model: \u0026LinearRegression\u003cB\u003e,\n    ) -\u003e (\n        Scalar\u003cB\u003e,\n        \u003cLinearRegression\u003cB\u003e as TrainableModel\u003cB\u003e\u003e::Gradients,\n    ) {\n        let params = model.params();\n        let weight_grad = Tensor1D::\u003cB\u003e::zeros(params.weights.len());\n\n        (\n            Scalar::\u003cB\u003e::new(0.),\n            LinearParams::\u003cB\u003e {\n                weights: weight_grad,\n                bias: Scalar::\u003cB\u003e::new(0.),\n            },\n        )\n    }\n}\n\n#[cfg(test)]\nmod tests {\n    use super::*;\n    use crate::backend::CpuBackend;\n    use crate::model::linear::{LinearParams, LinearRegression};\n\n    #[test]\n    fn test_l2_regularizer() {\n        // Ð¡Ð¾Ð·Ð´Ð°ÑÐ¼ Ð¿Ð°ÑÐ°Ð¼ÐµÑÑÑ Ð½Ð°Ð¿ÑÑÐ¼ÑÑ\n        let weights = Tensor1D::\u003cCpuBackend\u003e::new(vec![3.0f32, 4.0]);\n        let bias = Scalar::\u003cCpuBackend\u003e::new(1.0);\n        let params = LinearParams { weights, bias };\n\n        let model = LinearRegression::\u003cCpuBackend\u003e::from_params(params);\n\n        let lambda = 0.5;\n        let l2 = L2::\u003cCpuBackend\u003e::new(lambda);\n\n        let (penalty, grad) = l2.regularizer_penalty_grad(\u0026model);\n\n        // ||w||Â² = 3Â² + 4Â² = 25\n        // penalty = Î» * ||w||Â² = 0.5 * 25 = 12.5\n        assert!((penalty.data - 12.5).abs() \u003c 1e-12);\n\n        // grad_w = 2 * Î» * w = 2 * 0.5 * [3, 4] = [3, 4]\n        assert_eq!(grad.weights.to_vec(), vec![3.0, 4.0]);\n        // grad_b = 0\n        assert_eq!(grad.bias.data, 0.0);\n    }\n\n    #[test]\n    fn test_no_regularizer() {\n        let weights = Tensor1D::\u003cCpuBackend\u003e::new(vec![1.0f32, 2.0, 3.0]);\n        let bias = Scalar::\u003cCpuBackend\u003e::new(5.0);\n        let params = LinearParams { weights, bias };\n        let model = LinearRegression::\u003cCpuBackend\u003e::from_params(params);\n\n        let no_reg = NoRegularizer;\n        let (penalty, grad) = no_reg.regularizer_penalty_grad(\u0026model);\n\n        assert_eq!(penalty.data, 0.0);\n        assert_eq!(grad.weights.to_vec(), vec![0.0, 0.0, 0.0]);\n        assert_eq!(grad.bias.data, 0.0);\n    }\n\n    #[test]\n    fn test_l2_zero_weights() {\n        let weights = Tensor1D::\u003cCpuBackend\u003e::zeros(2);\n        let bias = Scalar::\u003cCpuBackend\u003e::new(0.0);\n        let params = LinearParams { weights, bias };\n        let model = LinearRegression::\u003cCpuBackend\u003e::from_params(params);\n\n        let l2 = L2::\u003cCpuBackend\u003e::new(1.0);\n        let (penalty, grad) = l2.regularizer_penalty_grad(\u0026model);\n\n        assert_eq!(penalty.data, 0.0);\n        assert_eq!(grad.weights.to_vec(), vec![0.0, 0.0]);\n        assert_eq!(grad.bias.data, 0.0);\n    }\n}\n","traces":[{"line":34,"address":[2581392],"length":1,"stats":{"Line":1}},{"line":36,"address":[2581398],"length":1,"stats":{"Line":1}},{"line":45,"address":[2581408,2581906,2581900],"length":1,"stats":{"Line":1}},{"line":52,"address":[2581459],"length":1,"stats":{"Line":1}},{"line":53,"address":[2581490],"length":1,"stats":{"Line":1}},{"line":55,"address":[2581566,2581634],"length":1,"stats":{"Line":2}},{"line":57,"address":[2581643],"length":1,"stats":{"Line":1}},{"line":60,"address":[],"length":0,"stats":{"Line":0}},{"line":61,"address":[2581787],"length":1,"stats":{"Line":1}},{"line":62,"address":[2581675],"length":1,"stats":{"Line":1}},{"line":63,"address":[2581712],"length":1,"stats":{"Line":1}},{"line":77,"address":[2582264,2582270,2581936],"length":1,"stats":{"Line":1}},{"line":84,"address":[2581982],"length":1,"stats":{"Line":1}},{"line":85,"address":[2582003],"length":1,"stats":{"Line":1}},{"line":88,"address":[2582029],"length":1,"stats":{"Line":1}},{"line":89,"address":[2582175],"length":1,"stats":{"Line":1}},{"line":90,"address":[2582080],"length":1,"stats":{"Line":1}},{"line":91,"address":[2582108],"length":1,"stats":{"Line":1}}],"covered":17,"coverable":18},{"path":["/","home","victor","ML","machinelearne-rs","lib","src","serialization.rs"],"content":"//! Serialization of fitted model parameters.\n//!\n//! This module provides a backend-agnostic way to serialize and deserialize\n//! the numerical parameters of a fitted model, without coupling to specific\n//! serialization formats or backend resources (e.g., GPU buffers).\n\nuse std::error::Error;\n\n/// A trait for parameter representations that can be serialized to and from bytes.\n///\n/// Implementors should contain only plain numerical data (e.g., `Vec\u003cf32\u003e`, scalars),\n/// not backend-specific tensors or handles.\npub trait SerializableParams: Sized {\n    /// The error type returned during (de)serialization.\n    type Error: Error + Send + Sync + 'static;\n\n    /// Serialize the parameters into a byte buffer.\n    fn to_bytes(\u0026self) -\u003e Result\u003cVec\u003cu8\u003e, Self::Error\u003e;\n\n    /// Deserialize the parameters from a byte buffer.\n    fn from_bytes(bytes: \u0026[u8]) -\u003e Result\u003cSelf, Self::Error\u003e;\n}\n\n// Optional serde integration\n#[cfg(feature = \"serde\")]\nimpl\u003cT\u003e SerializableParams for T\nwhere\n    T: serde::Serialize + for\u003c'de\u003e serde::Deserialize\u003c'de\u003e,\n{\n    type Error = bincode::Error;\n\n    fn to_bytes(\u0026self) -\u003e Result\u003cVec\u003cu8\u003e, Self::Error\u003e {\n        bincode::serialize(self)\n    }\n\n    fn from_bytes(bytes: \u0026[u8]) -\u003e Result\u003cSelf, Self::Error\u003e {\n        bincode::deserialize(bytes)\n    }\n}\n","traces":[{"line":32,"address":[3407552],"length":1,"stats":{"Line":8}},{"line":33,"address":[3440513],"length":1,"stats":{"Line":8}},{"line":36,"address":[2823056],"length":1,"stats":{"Line":7}},{"line":37,"address":[2005861],"length":1,"stats":{"Line":7}}],"covered":4,"coverable":4},{"path":["/","home","victor","ML","machinelearne-rs","lib","src","trainer","mod.rs"],"content":"// trainer/mod.rs\nuse crate::{\n    backend::{Backend, Scalar, ScalarOps, Tensor1D, Tensor2D},\n    dataset::Dataset,\n    loss::Loss,\n    model::{ParamOps, TrainableModel},\n    optimizer::Optimizer,\n    regularizers::Regularizer,\n};\nuse std::fmt::{Debug, Display};\nuse std::marker::PhantomData;\n\n/// Orchestrates the training loop for a `TrainableModel`.\n///\n/// Combines a loss function, optimizer, and regularizer to fit a model on a dataset.\n/// Once built via `TrainerBuilder`, it is immutable and can be reused across multiple models\n/// (as long as types match).\n///\n/// The `fit` method returns a `FittedModel` (via `IntoFitted`), which contains only inference logic.\npub struct Trainer\u003cB, L, O, M, P, R\u003e\nwhere\n    B: Backend,\n    L: Loss\u003cB\u003e,\n    M: TrainableModel\u003cB, Params = P, Gradients = P\u003e,\n    O: Optimizer\u003cB, P\u003e,\n    R: Regularizer\u003cB, M\u003e,\n{\n    pub(crate) batch_size: usize,\n    pub(crate) max_epochs: usize,\n    pub(crate) verbose: bool,\n    pub(crate) loss_fn: L,\n    pub(crate) optimizer: O,\n    pub(crate) regularizer: R,\n    _phantom_backend: PhantomData\u003cB\u003e,\n    _phantom_model: PhantomData\u003cM\u003e,\n}\n\n/// Fluent builder for constructing a `Trainer` with custom hyperparameters.\n///\n/// Defaults:\n/// - `batch_size`: 32\n/// - `max_epochs`: 1000\n/// - `verbose`: true\npub struct TrainerBuilder\u003cB, L, O, M, P, R\u003e\nwhere\n    B: Backend,\n    L: Loss\u003cB\u003e,\n    M: TrainableModel\u003cB, Params = P, Gradients = P\u003e,\n    O: Optimizer\u003cB, P\u003e,\n    R: Regularizer\u003cB, M\u003e,\n{\n    batch_size: usize,\n    max_epochs: usize,\n    verbose: bool,\n    loss_fn: L,\n    optimizer: O,\n    regularizer: R,\n    _phantom_backend: PhantomData\u003cB\u003e,\n    _phantom_model: PhantomData\u003cM\u003e,\n}\n\nimpl\u003cB, L, O, M, P, R\u003e TrainerBuilder\u003cB, L, O, M, P, R\u003e\nwhere\n    B: Backend,\n    L: Loss\u003cB\u003e,\n    M: TrainableModel\u003cB, Params = P, Gradients = P\u003e,\n    O: Optimizer\u003cB, P\u003e,\n    R: Regularizer\u003cB, M\u003e,\n{\n    /// Creates a new `TrainerBuilder` with the given components.\n    ///\n    /// # Arguments\n    /// * `loss_fn` â differentiable loss (e.g., `MSELoss`)\n    /// * `optimizer` â parameter updater (e.g., `SGD`)\n    /// * `regularizer` â optional penalty term (e.g., `L2` or `NoRegularizer`)\n    pub fn new(loss_fn: L, optimizer: O, regularizer: R) -\u003e Self {\n        Self {\n            batch_size: 32,\n            max_epochs: 1000,\n            verbose: true,\n            loss_fn,\n            optimizer,\n            regularizer,\n            _phantom_backend: PhantomData,\n            _phantom_model: PhantomData,\n        }\n    }\n\n    pub fn batch_size(mut self, size: usize) -\u003e Self {\n        self.batch_size = size;\n        self\n    }\n\n    pub fn max_epochs(mut self, epochs: usize) -\u003e Self {\n        self.max_epochs = epochs;\n        self\n    }\n\n    /// Sets verbosity for training output.\n    ///\n    /// When `false`, suppresses epoch-by-epoch loss output.\n    /// Useful for benchmarking or production training.\n    pub fn verbose(mut self, verbose: bool) -\u003e Self {\n        self.verbose = verbose;\n        self\n    }\n\n    pub fn build(self) -\u003e Trainer\u003cB, L, O, M, P, R\u003e {\n        Trainer {\n            batch_size: self.batch_size,\n            max_epochs: self.max_epochs,\n            verbose: self.verbose,\n            loss_fn: self.loss_fn,\n            optimizer: self.optimizer,\n            regularizer: self.regularizer,\n            _phantom_backend: PhantomData,\n            _phantom_model: PhantomData,\n        }\n    }\n}\n\n// --- Ð ÐµÐ°Ð»Ð¸Ð·Ð°ÑÐ¸Ñ fit Ð¿ÐµÑÐµÐ½Ð¾ÑÐ¸ÑÑÑ Ð² Trainer ---\nimpl\u003cB, L, O, M, P, R\u003e Trainer\u003cB, L, O, M, P, R\u003e\nwhere\n    B: Backend,\n    B::Scalar: Debug + Display,\n    L: Loss\u003cB, Target = Tensor1D\u003cB\u003e, Prediction = Tensor1D\u003cB\u003e\u003e,\n    M: TrainableModel\u003c\n        B,\n        Input = Tensor2D\u003cB\u003e,\n        Prediction = L::Prediction,\n        Params = P,\n        Gradients = P,\n    \u003e,\n    O: Optimizer\u003cB, P\u003e,\n    R: Regularizer\u003cB, M\u003e,\n    P: ParamOps\u003cB\u003e,\n{\n    /// Trains the model on the provided dataset for up to `max_epochs`.\n    ///\n    /// # Returns\n    /// A fitted model ready for inference (`M::Output`), or an error if:\n    /// - The dataset is empty\n    /// - The dataset length is unknown (required for loss averaging)\n    /// - A batch fails to load\n    ///\n    /// # Notes\n    /// - Loss is averaged over the entire dataset per epoch.\n    /// - Gradients are averaged per batch before applying regularization.\n    pub fn fit\u003cD\u003e(\u0026self, mut model: M, dataset: \u0026D) -\u003e Result\u003cM::Output, String\u003e\n    where\n        D: Dataset,\n    {\n        let n_total = dataset.len().ok_or(\"Dataset length unknown\")?;\n        if n_total == 0 {\n            return Err(\"Dataset is empty\".into());\n        }\n\n        for epoch in 0..self.max_epochs {\n            let mut total_loss = Scalar::\u003cB\u003e::new(0.);\n            //let mut total_samples = 0;\n            for batch_result in dataset.batches::\u003cB\u003e(self.batch_size) {\n                let (batch_x, batch_y) =\n                    batch_result.map_err(|e| format!(\"Data error: {:?}\", e))?;\n                //let mut total_loss = Scalar::\u003cB\u003e::new(0.);\n                let preds = model.forward(\u0026batch_x);\n                total_loss = total_loss + self.loss_fn.loss(\u0026preds, \u0026batch_y);\n                let (reg_penalty, reg_grad) = self.regularizer.regularizer_penalty_grad(\u0026model);\n                total_loss = total_loss + reg_penalty;\n                let grad_preds = self.loss_fn.grad_wrt_prediction(\u0026preds, \u0026batch_y);\n                let grads = model.backward(\u0026batch_x, \u0026grad_preds);\n\n                let total_grads = grads.add(\u0026reg_grad);\n                let new_params = self.optimizer.step(model.params(), \u0026total_grads);\n                model.update_params(\u0026new_params);\n            }\n\n            let avg_loss = total_loss / Scalar::\u003cB\u003e::new(n_total as f64);\n            if self.verbose {\n                println!(\"Epoch {}: loss = {}\", epoch, avg_loss.data.to_f64());\n            }\n        }\n\n        Ok(model.into_fitted())\n    }\n}\n\n// --- Ð­ÐºÑÐ¿Ð¾ÑÑ ÑÐ´Ð¾Ð±Ð½Ð¾Ð³Ð¾ ÐºÐ¾Ð½ÑÑÑÑÐºÑÐ¾ÑÐ° ---\nimpl\u003cB, L, O, M, P, R\u003e Trainer\u003cB, L, O, M, P, R\u003e\nwhere\n    B: Backend,\n    L: Loss\u003cB\u003e,\n    M: TrainableModel\u003cB, Params = P, Gradients = P\u003e,\n    O: Optimizer\u003cB, P\u003e,\n    R: Regularizer\u003cB, M\u003e,\n{\n    /// Convenience constructor that starts the builder pattern.\n    ///\n    /// Equivalent to `TrainerBuilder::new(...)`.\n    pub fn builder(loss_fn: L, optimizer: O, regularizer: R) -\u003e TrainerBuilder\u003cB, L, O, M, P, R\u003e {\n        TrainerBuilder::new(loss_fn, optimizer, regularizer)\n    }\n}\n\n#[cfg(test)]\nmod tests {\n    use super::*;\n    use crate::{\n        backend::CpuBackend,\n        dataset::memory::InMemoryDataset,\n        loss::MSELoss,\n        model::linear::InferenceModel,\n        model::linear::LinearRegression,\n        optimizer::SGD,\n        regularizers::{NoRegularizer, L2},\n    };\n\n    // === TrainerBuilder Tests ===\n\n    #[test]\n    fn test_trainer_builder_default_values() {\n        let builder = TrainerBuilder::new(MSELoss, SGD::\u003cCpuBackend\u003e::new(0.01), NoRegularizer);\n\n        assert_eq!(builder.batch_size, 32);\n        assert_eq!(builder.max_epochs, 1000);\n        assert_eq!(builder.verbose, true);\n    }\n\n    #[test]\n    fn test_trainer_builder_custom_batch_size() {\n        let builder = TrainerBuilder::new(MSELoss, SGD::\u003cCpuBackend\u003e::new(0.01), NoRegularizer)\n            .batch_size(64);\n\n        assert_eq!(builder.batch_size, 64);\n        assert_eq!(builder.max_epochs, 1000);\n    }\n\n    #[test]\n    fn test_trainer_builder_custom_max_epochs() {\n        let builder = TrainerBuilder::new(MSELoss, SGD::\u003cCpuBackend\u003e::new(0.01), NoRegularizer)\n            .max_epochs(500);\n\n        assert_eq!(builder.batch_size, 32);\n        assert_eq!(builder.max_epochs, 500);\n    }\n\n    #[test]\n    fn test_trainer_builder_verbose() {\n        let builder = TrainerBuilder::new(MSELoss, SGD::\u003cCpuBackend\u003e::new(0.01), NoRegularizer)\n            .verbose(false);\n\n        assert_eq!(builder.verbose, false);\n\n        let builder_verbose =\n            TrainerBuilder::new(MSELoss, SGD::\u003cCpuBackend\u003e::new(0.01), NoRegularizer).verbose(true);\n\n        assert_eq!(builder_verbose.verbose, true);\n    }\n\n    #[test]\n    fn test_trainer_builder_chaining() {\n        let builder = TrainerBuilder::new(MSELoss, SGD::\u003cCpuBackend\u003e::new(0.01), NoRegularizer)\n            .batch_size(128)\n            .max_epochs(250)\n            .verbose(false);\n\n        assert_eq!(builder.batch_size, 128);\n        assert_eq!(builder.max_epochs, 250);\n        assert_eq!(builder.verbose, false);\n    }\n\n    #[test]\n    fn test_trainer_builder_chaining_order_independent() {\n        let builder1 = TrainerBuilder::new(MSELoss, SGD::\u003cCpuBackend\u003e::new(0.01), NoRegularizer)\n            .batch_size(16)\n            .max_epochs(100);\n\n        let builder2 = TrainerBuilder::new(MSELoss, SGD::\u003cCpuBackend\u003e::new(0.01), NoRegularizer)\n            .max_epochs(100)\n            .batch_size(16);\n\n        assert_eq!(builder1.batch_size, builder2.batch_size);\n        assert_eq!(builder1.max_epochs, builder2.max_epochs);\n    }\n\n    #[test]\n    fn test_trainer_builder_small_batch_size() {\n        let builder =\n            TrainerBuilder::new(MSELoss, SGD::\u003cCpuBackend\u003e::new(0.01), NoRegularizer).batch_size(1);\n\n        assert_eq!(builder.batch_size, 1);\n    }\n\n    #[test]\n    fn test_trainer_builder_large_batch_size() {\n        let builder = TrainerBuilder::new(MSELoss, SGD::\u003cCpuBackend\u003e::new(0.01), NoRegularizer)\n            .batch_size(10000);\n\n        assert_eq!(builder.batch_size, 10000);\n    }\n\n    #[test]\n    fn test_trainer_builder_zero_epochs() {\n        let builder =\n            TrainerBuilder::new(MSELoss, SGD::\u003cCpuBackend\u003e::new(0.01), NoRegularizer).max_epochs(0);\n\n        assert_eq!(builder.max_epochs, 0);\n    }\n\n    #[test]\n    fn test_trainer_builder_large_epochs() {\n        let builder = TrainerBuilder::new(MSELoss, SGD::\u003cCpuBackend\u003e::new(0.01), NoRegularizer)\n            .max_epochs(100000);\n\n        assert_eq!(builder.max_epochs, 100000);\n    }\n\n    #[test]\n    fn test_trainer_builder_creates_valid_trainer() {\n        let builder = TrainerBuilder::new(MSELoss, SGD::\u003cCpuBackend\u003e::new(0.01), NoRegularizer)\n            .batch_size(64)\n            .max_epochs(200)\n            .verbose(false);\n\n        let trainer = builder.build();\n\n        assert_eq!(trainer.batch_size, 64);\n        assert_eq!(trainer.max_epochs, 200);\n        assert_eq!(trainer.verbose, false);\n    }\n\n    #[test]\n    fn test_trainer_builder_does_not_consume_loss_fn() {\n        let _builder = TrainerBuilder::new(MSELoss, SGD::\u003cCpuBackend\u003e::new(0.01), NoRegularizer)\n            .batch_size(16)\n            .max_epochs(50);\n\n        // Components are reused via Clone for SGD, creating fresh instances for other builders\n        let _builder2 = TrainerBuilder::new(MSELoss, SGD::\u003cCpuBackend\u003e::new(0.01), NoRegularizer);\n    }\n\n    #[test]\n    fn test_trainer_builder_clone_components() {\n        let builder1 = TrainerBuilder::new(MSELoss, SGD::\u003cCpuBackend\u003e::new(0.01), NoRegularizer)\n            .batch_size(32)\n            .max_epochs(100);\n\n        // SGD implements Clone, so we can use it multiple times\n        let _builder2 = TrainerBuilder::new(MSELoss, builder1.optimizer.clone(), NoRegularizer);\n    }\n\n    #[test]\n    fn test_trainer_builder_zero_batch_size() {\n        // Builder allows 0 batch size - this is up to the user to validate\n        let builder =\n            TrainerBuilder::new(MSELoss, SGD::\u003cCpuBackend\u003e::new(0.01), NoRegularizer).batch_size(0);\n\n        assert_eq!(builder.batch_size, 0);\n\n        // Building should still work, but fit() will fail on batch iteration\n        let trainer = builder.build();\n        assert_eq!(trainer.batch_size, 0);\n    }\n\n    // === Trainer Tests (existing tests preserved) ===\n\n    #[test]\n    fn test_trainer_fit_linear_regression() {\n        // Ð¡Ð¾Ð·Ð´Ð°ÑÐ¼ ÑÐ¸Ð½ÑÐµÑÐ¸ÑÐµÑÐºÐ¸Ð¹ Ð´Ð°ÑÐ°ÑÐµÑ: y = 2*x1 + 3*x2 + 1\n        let x = vec![\n            vec![1.0, 0.0],\n            vec![0.0, 1.0],\n            vec![1.0, 1.0],\n            vec![2.0, 3.0],\n        ];\n        let y = vec![3.0, 4.0, 6.0, 14.0]; // 2*2 + 3*3 + 1 = 4+9+1=14\n        let dataset = InMemoryDataset::new(x, y).unwrap();\n\n        let model = LinearRegression::\u003cCpuBackend\u003e::new(2);\n        let loss = MSELoss;\n        let optimizer = SGD::new(0.1); // learning rate\n        let regularizer = NoRegularizer;\n\n        let trainer = Trainer::builder(loss, optimizer, regularizer)\n            .batch_size(4)\n            .max_epochs(100)\n            .verbose(false) // Suppress output in tests\n            .build();\n\n        let fitted_model = trainer.fit(model, \u0026dataset).unwrap();\n\n        // ÐÑÐ¾Ð²ÐµÑÐ¸Ð¼ Ð¿ÑÐµÐ´ÑÐºÐ°Ð·Ð°Ð½Ð¸Ñ\n        let test_input = Tensor2D::\u003cCpuBackend\u003e::new(vec![1.0, 0.0, 0.0, 1.0], 2, 2);\n        let preds = fitted_model.predict_batch(\u0026test_input);\n        let pred_vec = preds.to_vec();\n\n        // ÐÐ¶Ð¸Ð´Ð°ÐµÐ¼ Ð¿ÑÐ¸Ð±Ð»Ð¸Ð¶ÐµÐ½Ð¸Ðµ Ðº [3.0, 4.0]\n        assert!((pred_vec[0] - 3.0).abs() \u003c 0.5);\n        assert!((pred_vec[1] - 4.0).abs() \u003c 0.5);\n    }\n\n    #[test]\n    fn test_trainer_with_l2_regularization() {\n        let x = vec![vec![1.0], vec![2.0], vec![3.0]];\n        let y = vec![2.0, 4.0, 6.0]; // y = 2*x\n        let dataset = InMemoryDataset::new(x, y).unwrap();\n\n        let model = LinearRegression::\u003cCpuBackend\u003e::new(1);\n        let loss = MSELoss;\n        let optimizer = SGD::new(0.01);\n        let regularizer = L2::\u003cCpuBackend\u003e::new(1.0); // ÑÐ¸Ð»ÑÐ½Ð°Ñ ÑÐµÐ³ÑÐ»ÑÑÐ¸Ð·Ð°ÑÐ¸Ñ\n\n        let trainer = Trainer::builder(loss, optimizer, regularizer)\n            .batch_size(3)\n            .max_epochs(500)\n            .verbose(false) // Suppress output in tests\n            .build();\n\n        let fitted_model = trainer.fit(model, \u0026dataset).unwrap();\n        let weights = fitted_model.extract_params().weights;\n\n        // ÐÐµÐ· ÑÐµÐ³ÑÐ»ÑÑÐ¸Ð·Ð°ÑÐ¸Ð¸ Ð²ÐµÑ Ð±ÑÐ» Ð±Ñ ~2.0, Ñ L2 â Ð¼ÐµÐ½ÑÑÐµ\n        assert!(weights[0] \u003c 2.0);\n        assert!(weights[0] \u003e 0.0);\n    }\n\n    #[test]\n    fn test_trainer_empty_dataset() {\n        let x = vec![];\n        let y = vec![];\n        // Empty datasets should error at creation\n        let _dataset = InMemoryDataset::new(x, y).unwrap_err();\n    }\n\n    #[test]\n    fn test_trainer_unknown_dataset_length() {\n        // Ð¡Ð¾Ð·Ð´Ð°Ð´Ð¸Ð¼ mock-Ð´Ð°ÑÐ°ÑÐµÑ Ð±ÐµÐ· len()\n        struct MockDatasetWithoutLen {\n            x: Vec\u003cVec\u003cf32\u003e\u003e,\n            y: Vec\u003cf32\u003e,\n        }\n\n        impl Dataset for MockDatasetWithoutLen {\n            type Error = String; // â Ð¼ÐµÐ½ÑÐµÐ¼ Ð½Ð° String\n            type Item = ();\n\n            fn len(\u0026self) -\u003e Option\u003cusize\u003e {\n                None\n            }\n\n            fn get_batch\u003cB: Backend\u003e(\n                \u0026self,\n                range: std::ops::Range\u003cusize\u003e,\n            ) -\u003e Result\u003c(Tensor2D\u003cB\u003e, Tensor1D\u003cB\u003e), Self::Error\u003e {\n                let batch_x = \u0026self.x[range.clone()];\n                let batch_y = \u0026self.y[range];\n                let n = batch_x.len();\n                if n == 0 {\n                    return Err(\"Empty batch\".into());\n                }\n                let cols = batch_x[0].len();\n                let data = batch_x.iter().flat_map(|r| r.iter()).copied().collect();\n                Ok((\n                    Tensor2D::new(data, n, cols),\n                    Tensor1D::new(batch_y.to_vec()),\n                ))\n            }\n        }\n\n        let dataset = MockDatasetWithoutLen {\n            x: vec![vec![1.0]],\n            y: vec![1.0],\n        };\n\n        let model = LinearRegression::\u003cCpuBackend\u003e::new(1);\n        let loss = MSELoss;\n        let optimizer = SGD::\u003cCpuBackend\u003e::new(0.1);\n        let regularizer = NoRegularizer;\n\n        let trainer = Trainer::builder(loss, optimizer, regularizer)\n            .batch_size(1)\n            .max_epochs(1)\n            .build();\n\n        let result = trainer.fit(model, \u0026dataset);\n        assert!(result.is_err());\n    }\n}\n","traces":[{"line":76,"address":[2955872,2955920],"length":1,"stats":{"Line":2}},{"line":89,"address":[2955728,2955680],"length":1,"stats":{"Line":2}},{"line":90,"address":[2955688,2955745],"length":1,"stats":{"Line":2}},{"line":91,"address":[2955691,2955748],"length":1,"stats":{"Line":2}},{"line":94,"address":[2955824,2955776],"length":1,"stats":{"Line":3}},{"line":95,"address":[2955793,2955832],"length":1,"stats":{"Line":2}},{"line":96,"address":[2955797,2955836],"length":1,"stats":{"Line":2}},{"line":103,"address":[2956080,2956128],"length":1,"stats":{"Line":2}},{"line":104,"address":[2956103,2956142],"length":1,"stats":{"Line":3}},{"line":105,"address":[2956108,2956148],"length":1,"stats":{"Line":2}},{"line":108,"address":[2955968,2956032],"length":1,"stats":{"Line":2}},{"line":110,"address":[2955974,2956038],"length":1,"stats":{"Line":2}},{"line":111,"address":[2955977,2956041],"length":1,"stats":{"Line":2}},{"line":112,"address":[2956045,2955981],"length":1,"stats":{"Line":3}},{"line":113,"address":[],"length":0,"stats":{"Line":0}},{"line":114,"address":[2955985,2956049],"length":1,"stats":{"Line":2}},{"line":115,"address":[2955991],"length":1,"stats":{"Line":1}},{"line":150,"address":[2952512,2955020,2949900,2949916,2952460,2949952,2947392,2952476,2955036],"length":1,"stats":{"Line":3}},{"line":154,"address":[2947466,2952586,2949911,2952682,2947562,2952471,2950026,2950122,2955031],"length":1,"stats":{"Line":7}},{"line":155,"address":[2950285,2952845,2947725],"length":1,"stats":{"Line":2}},{"line":156,"address":[2952851,2950291,2947731,2947810,2950370,2952930],"length":1,"stats":{"Line":0}},{"line":159,"address":[2950447,2947887,2953007,2950333,2952893,2947773],"length":1,"stats":{"Line":4}},{"line":160,"address":[2950722,2947991,2948162,2950551,2953282,2953111],"length":1,"stats":{"Line":4}},{"line":162,"address":[2950731,2948171,2953291],"length":1,"stats":{"Line":2}},{"line":163,"address":[2954062,2951502,2948942],"length":1,"stats":{"Line":2}},{"line":164,"address":[],"length":0,"stats":{"Line":0}},{"line":166,"address":[2949038,2954158,2951598],"length":1,"stats":{"Line":2}},{"line":167,"address":[2954310,2954213,2949190,2949093,2951653,2951750],"length":1,"stats":{"Line":4}},{"line":168,"address":[2954354,2949234,2951794],"length":1,"stats":{"Line":2}},{"line":169,"address":[2954499,2949303,2949379,2954423,2951863,2951939],"length":1,"stats":{"Line":4}},{"line":170,"address":[2949388,2954508,2951948],"length":1,"stats":{"Line":2}},{"line":171,"address":[2952015,2949455,2954575],"length":1,"stats":{"Line":2}},{"line":173,"address":[2954646,2952086,2949526],"length":1,"stats":{"Line":2}},{"line":174,"address":[2954790,2949586,2952146,2952230,2949670,2954706],"length":1,"stats":{"Line":4}},{"line":175,"address":[2952253,2954813,2949693],"length":1,"stats":{"Line":2}},{"line":178,"address":[2953538,2948418,2950978],"length":1,"stats":{"Line":2}},{"line":179,"address":[2951093,2953653,2948533],"length":1,"stats":{"Line":2}},{"line":180,"address":[2951109,2953669,2948549],"length":1,"stats":{"Line":0}},{"line":184,"address":[2950575,2948015,2953135],"length":1,"stats":{"Line":2}},{"line":200,"address":[2955584,2955632],"length":1,"stats":{"Line":2}},{"line":201,"address":[2955602,2955656],"length":1,"stats":{"Line":3}}],"covered":37,"coverable":41}]};
        var previousData = {"files":[{"path":["/","home","victor","ML","machinelearne-rs","benchmarks","benches","linear_regression","predict.rs"],"content":"use benchmarks::data::CaliforniaHousingDataset;\nuse criterion::{black_box, criterion_group, criterion_main, BenchmarkId, Criterion};\nuse machinelearne_rs::{\n    backend::Tensor2D,\n    dataset::InMemoryDataset,\n    loss::MSELoss,\n    model::{linear::LinearRegression, InferenceModel},\n    optimizer::SGD,\n    regularizers::NoRegularizer,\n    trainer::Trainer,\n    CpuBackend,\n};\n\n/// Helper function to convert Vec\u003cVec\u003cf32\u003e\u003e to Tensor2D\u003cCpuBackend\u003e\nfn vec_to_tensor2d(data: \u0026[Vec\u003cf32\u003e]) -\u003e Tensor2D\u003cCpuBackend\u003e {\n    let flat: Vec\u003cf32\u003e = data.iter().flatten().copied().collect();\n    Tensor2D::new(flat, data.len(), data.first().map(|v| v.len()).unwrap_or(0))\n}\n\n/// Train a model once for prediction benchmarks\nfn train_model_for_prediction(\n) -\u003e machinelearne_rs::model::linear::LinearModel\u003cCpuBackend, machinelearne_rs::model::state::Fitted\u003e\n{\n    let dataset = CaliforniaHousingDataset::load(\"datasets/california_housing.csv\")\n        .expect(\"Failed to load dataset\");\n    let (train_dataset, _test_dataset) = dataset.split(0.8);\n    let in_memory = train_dataset\n        .to_in_memory_dataset()\n        .expect(\"Failed to create dataset\");\n\n    let model = LinearRegression::\u003cCpuBackend\u003e::new(8);\n    let optimizer = SGD::new(0.001);\n    let trainer = Trainer::builder(MSELoss, optimizer, NoRegularizer)\n        .batch_size(32)\n        .max_epochs(50)\n        .build();\n\n    trainer.fit(model, \u0026in_memory).expect(\"Failed to fit model\")\n}\n\nfn bench_predict_single(c: \u0026mut Criterion) {\n    let model = train_model_for_prediction();\n\n    c.bench_function(\"predict_single\", |b| {\n        let input: Vec\u003cf32\u003e = vec![5.0, 30.0, 6.0, 1.0, 500.0, 3.0, 37.8, -122.2];\n        let input_tensor = machinelearne_rs::backend::Tensor1D::new(input);\n        b.iter(|| {\n            let pred = model.predict(black_box(\u0026input_tensor));\n            black_box(pred);\n        });\n    });\n}\n\nfn bench_predict_batch(c: \u0026mut Criterion) {\n    let model = train_model_for_prediction();\n\n    // Test different batch sizes\n    for batch_size in [10, 100, 1000, 10000].iter() {\n        c.bench_with_input(\n            BenchmarkId::new(\"predict_batch\", batch_size),\n            batch_size,\n            |b, \u0026bs| {\n                // Create test data\n                let test_x: Vec\u003cVec\u003cf32\u003e\u003e = (0..bs)\n                    .map(|_| vec![5.0, 30.0, 6.0, 1.0, 500.0, 3.0, 37.8, -122.2])\n                    .collect();\n\n                b.iter(|| {\n                    let predictions = model.predict_batch(black_box(\u0026vec_to_tensor2d(\u0026test_x)));\n                    black_box(predictions);\n                });\n            },\n        );\n    }\n}\n\nfn bench_predict_throughput(c: \u0026mut Criterion) {\n    let model = train_model_for_prediction();\n\n    // Test different batch sizes for throughput\n    for batch_size in [100, 1000, 10000].iter() {\n        c.bench_with_input(\n            BenchmarkId::new(\"predict_throughput\", batch_size),\n            batch_size,\n            |b, \u0026bs| {\n                // Create test data\n                let test_x: Vec\u003cVec\u003cf32\u003e\u003e = (0..bs)\n                    .map(|_| vec![5.0, 30.0, 6.0, 1.0, 500.0, 3.0, 37.8, -122.2])\n                    .collect();\n\n                b.iter(|| {\n                    let predictions = model.predict_batch(black_box(\u0026vec_to_tensor2d(\u0026test_x)));\n                    // Throughput is calculated as batch_size / time\n                    black_box((predictions.len(), predictions));\n                });\n            },\n        );\n    }\n}\n\nfn bench_predict_warmup(c: \u0026mut Criterion) {\n    let model = train_model_for_prediction();\n\n    c.bench_function(\"predict_warmup\", |b| {\n        let input: Vec\u003cf32\u003e = vec![5.0, 30.0, 6.0, 1.0, 500.0, 3.0, 37.8, -122.2];\n        let input_tensor = machinelearne_rs::backend::Tensor1D::new(input);\n\n        // Warmup\n        for _ in 0..5 {\n            let _ = model.predict(\u0026input_tensor);\n        }\n\n        b.iter(|| {\n            let pred = model.predict(black_box(\u0026input_tensor));\n            black_box(pred);\n        });\n    });\n}\n\nfn bench_predict_latencies(c: \u0026mut Criterion) {\n    let model = train_model_for_prediction();\n\n    // Measure latency percentiles for single prediction\n    let mut group = c.benchmark_group(\"predict_latency\");\n    group.sample_size(10000);\n\n    group.bench_function(\"p50\", |b| {\n        let input: Vec\u003cf32\u003e = vec![5.0, 30.0, 6.0, 1.0, 500.0, 3.0, 37.8, -122.2];\n        let input_tensor = machinelearne_rs::backend::Tensor1D::new(input);\n        b.iter(|| {\n            let pred = model.predict(black_box(\u0026input_tensor));\n            black_box(pred);\n        });\n    });\n\n    group.finish();\n}\n\nfn bench_predict_batch_warmup(c: \u0026mut Criterion) {\n    let model = train_model_for_prediction();\n\n    for batch_size in [100, 1000].iter() {\n        c.bench_with_input(\n            BenchmarkId::new(\"predict_batch_warmup\", batch_size),\n            batch_size,\n            |b, \u0026bs| {\n                let test_x: Vec\u003cVec\u003cf32\u003e\u003e = (0..bs)\n                    .map(|_| vec![5.0, 30.0, 6.0, 1.0, 500.0, 3.0, 37.8, -122.2])\n                    .collect();\n\n                // Warmup\n                for _ in 0..5 {\n                    let _ = model.predict_batch(\u0026vec_to_tensor2d(\u0026test_x));\n                }\n\n                b.iter(|| {\n                    let predictions = model.predict_batch(black_box(\u0026vec_to_tensor2d(\u0026test_x)));\n                    black_box(predictions);\n                });\n            },\n        );\n    }\n}\n\nfn bench_predict_different_features(c: \u0026mut Criterion) {\n    // Train models with different feature counts\n    let dataset = CaliforniaHousingDataset::load(\"datasets/california_housing.csv\")\n        .expect(\"Failed to load dataset\");\n    let (train_dataset, _) = dataset.split(0.8);\n    let train_memory = train_dataset\n        .to_in_memory_dataset()\n        .expect(\"Failed to create dataset\");\n\n    // Train models with 1, 2, 4, and 8 features\n    let models = vec![\n        (1, {\n            let subset = dataset.select_features(\u0026[0]);\n            let (train, _) = subset.split(0.8);\n            let in_memory = train.to_in_memory_dataset().unwrap();\n            let model = LinearRegression::\u003cCpuBackend\u003e::new(1);\n            let optimizer = SGD::new(0.01);\n            let trainer = Trainer::builder(MSELoss, optimizer, NoRegularizer)\n                .batch_size(32)\n                .max_epochs(50)\n                .build();\n            trainer.fit(model, \u0026in_memory).expect(\"Failed to fit model\")\n        }),\n        (2, {\n            let subset = dataset.select_features(\u0026[0, 1]);\n            let (train, _) = subset.split(0.8);\n            let in_memory = train.to_in_memory_dataset().unwrap();\n            let model = LinearRegression::\u003cCpuBackend\u003e::new(2);\n            let optimizer = SGD::new(0.001);\n            let trainer = Trainer::builder(MSELoss, optimizer, NoRegularizer)\n                .batch_size(32)\n                .max_epochs(50)\n                .build();\n            trainer.fit(model, \u0026in_memory).expect(\"Failed to fit model\")\n        }),\n        (4, {\n            let subset = dataset.select_features(\u0026[0, 1, 2, 3]);\n            let (train, _) = subset.split(0.8);\n            let in_memory = train.to_in_memory_dataset().unwrap();\n            let model = LinearRegression::\u003cCpuBackend\u003e::new(4);\n            let optimizer = SGD::new(0.001);\n            let trainer = Trainer::builder(MSELoss, optimizer, NoRegularizer)\n                .batch_size(32)\n                .max_epochs(50)\n                .build();\n            trainer.fit(model, \u0026in_memory).expect(\"Failed to fit model\")\n        }),\n        (8, {\n            let model = LinearRegression::\u003cCpuBackend\u003e::new(8);\n            let optimizer = SGD::new(0.001);\n            let trainer = Trainer::builder(MSELoss, optimizer, NoRegularizer)\n                .batch_size(32)\n                .max_epochs(50)\n                .build();\n            trainer\n                .fit(model, \u0026train_memory)\n                .expect(\"Failed to fit model\")\n        }),\n    ];\n\n    for (n_features, model) in models {\n        c.bench_with_input(\n            BenchmarkId::new(\"predict_by_features\", n_features),\n            \u0026n_features,\n            |b, n_feat| {\n                let input: Vec\u003cf32\u003e = (0..*n_feat).map(|i| i as f32 * 1.0).collect();\n                let input_tensor = machinelearne_rs::backend::Tensor1D::new(input);\n                b.iter(|| {\n                    let pred = model.predict(black_box(\u0026input_tensor));\n                    black_box(pred);\n                });\n            },\n        );\n    }\n}\n\ncriterion_group!(\n    benches,\n    bench_predict_single,\n    bench_predict_batch,\n    bench_predict_throughput,\n    bench_predict_warmup,\n    bench_predict_latencies,\n    bench_predict_batch_warmup,\n    bench_predict_different_features\n);\ncriterion_main!(benches);\n","traces":[],"covered":0,"coverable":0},{"path":["/","home","victor","ML","machinelearne-rs","benchmarks","benches","linear_regression","train_1_feature.rs"],"content":"use benchmarks::data::CaliforniaHousingDataset;\nuse benchmarks::metrics::Metrics;\nuse criterion::{black_box, criterion_group, criterion_main, BenchmarkId, Criterion};\nuse machinelearne_rs::{\n    backend::Tensor2D,\n    dataset::InMemoryDataset,\n    loss::MSELoss,\n    model::{linear::LinearRegression, InferenceModel},\n    optimizer::SGD,\n    regularizers::NoRegularizer,\n    trainer::Trainer,\n    CpuBackend,\n};\n\n/// Helper function to convert Vec\u003cVec\u003cf32\u003e\u003e to Tensor2D\u003cCpuBackend\u003e\nfn vec_to_tensor2d(data: \u0026[Vec\u003cf32\u003e]) -\u003e Tensor2D\u003cCpuBackend\u003e {\n    let flat: Vec\u003cf32\u003e = data.iter().flatten().copied().collect();\n    Tensor2D::new(flat, data.len(), data.first().map(|v| v.len()).unwrap_or(0))\n}\n\nfn bench_train_1_feature(c: \u0026mut Criterion) {\n    // Load dataset and select only first feature\n    let dataset = CaliforniaHousingDataset::load(\"datasets/california_housing.csv\")\n        .expect(\"Failed to load dataset\");\n    let subset = dataset.select_features(\u0026[0]); // Only MedInc\n    let (train_dataset, _test_dataset) = subset.split(0.8);\n    let in_memory = train_dataset\n        .to_in_memory_dataset()\n        .expect(\"Failed to create dataset\");\n\n    // Test different batch sizes\n    for batch_size in [16, 32, 64, 128].iter() {\n        c.bench_with_input(\n            BenchmarkId::new(\"train_1_feature\", batch_size),\n            batch_size,\n            |b, \u0026bs| {\n                let trainer = Trainer::builder(MSELoss, SGD::new(0.01), NoRegularizer)\n                    .batch_size(bs)\n                    .max_epochs(10)\n                    .build();\n\n                b.iter(|| {\n                    let model_copy = LinearRegression::\u003cCpuBackend\u003e::new(1);\n                    let fitted = trainer\n                        .fit(model_copy, \u0026in_memory)\n                        .expect(\"Failed to fit model\");\n\n                    // Verify the model works\n                    let test_x: Vec\u003cf32\u003e = vec![5.0];\n                    let test_tensor = machinelearne_rs::backend::Tensor1D::new(test_x);\n                    let _pred = fitted.predict(black_box(\u0026test_tensor));\n\n                    fitted\n                });\n            },\n        );\n    }\n}\n\nfn bench_train_1_feature_learning_rate(c: \u0026mut Criterion) {\n    // Load dataset and select only first feature\n    let dataset = CaliforniaHousingDataset::load(\"datasets/california_housing.csv\")\n        .expect(\"Failed to load dataset\");\n    let subset = dataset.select_features(\u0026[0]); // Only MedInc\n    let (train_dataset, _test_dataset) = subset.split(0.8);\n    let in_memory = train_dataset\n        .to_in_memory_dataset()\n        .expect(\"Failed to create dataset\");\n\n    // Test different learning rates\n    for lr in [0.001, 0.01, 0.1].iter() {\n        c.bench_with_input(\n            BenchmarkId::new(\"train_1_feature_lr\", lr),\n            lr,\n            |b, \u0026learning_rate| {\n                let optimizer = SGD::new(learning_rate);\n                let trainer = Trainer::builder(MSELoss, optimizer, NoRegularizer)\n                    .batch_size(32)\n                    .max_epochs(10)\n                    .build();\n\n                b.iter(|| {\n                    let model_copy = LinearRegression::\u003cCpuBackend\u003e::new(1);\n                    let _ = trainer.fit(model_copy, \u0026in_memory);\n                });\n            },\n        );\n    }\n}\n\nfn bench_train_1_feature_epochs(c: \u0026mut Criterion) {\n    // Load dataset and select only first feature\n    let dataset = CaliforniaHousingDataset::load(\"datasets/california_housing.csv\")\n        .expect(\"Failed to load dataset\");\n    let subset = dataset.select_features(\u0026[0]); // Only MedInc\n    let (train_dataset, _test_dataset) = subset.split(0.8);\n    let in_memory = train_dataset\n        .to_in_memory_dataset()\n        .expect(\"Failed to create dataset\");\n\n    // Test different epoch counts\n    for epochs in [10, 50, 100].iter() {\n        c.bench_with_input(\n            BenchmarkId::new(\"train_1_feature_epochs\", epochs),\n            epochs,\n            |b, \u0026max_epochs| {\n                let optimizer = SGD::new(0.01);\n                let trainer = Trainer::builder(MSELoss, optimizer, NoRegularizer)\n                    .batch_size(32)\n                    .max_epochs(max_epochs)\n                    .build();\n\n                b.iter(|| {\n                    let model_copy = LinearRegression::\u003cCpuBackend\u003e::new(1);\n                    let _ = trainer.fit(model_copy, \u0026in_memory);\n                });\n            },\n        );\n    }\n}\n\nfn bench_train_1_feature_with_metrics(c: \u0026mut Criterion) {\n    // Load dataset and select only first feature\n    let dataset = CaliforniaHousingDataset::load(\"datasets/california_housing.csv\")\n        .expect(\"Failed to load dataset\");\n    let subset = dataset.select_features(\u0026[0]); // Only MedInc\n    let (train_dataset, test_dataset) = subset.split(0.8);\n    let train_memory = train_dataset\n        .to_in_memory_dataset()\n        .expect(\"Failed to create dataset\");\n\n    // Test the full pipeline and measure metrics\n    c.bench_function(\"train_1_feature_with_metrics\", |b| {\n        b.iter(|| {\n            let model = LinearRegression::\u003cCpuBackend\u003e::new(1);\n            let optimizer = SGD::new(0.01);\n            let trainer = Trainer::builder(MSELoss, optimizer, NoRegularizer)\n                .batch_size(32)\n                .max_epochs(50)\n                .build();\n\n            let fitted = trainer\n                .fit(model, \u0026train_memory)\n                .expect(\"Failed to fit model\");\n\n            // Evaluate on test set\n            let test_x = test_dataset.features().to_vec();\n            let test_y = test_dataset.target().to_vec();\n            let pred_tensor = fitted.predict_batch(black_box(\u0026vec_to_tensor2d(\u0026test_x)));\n\n            let predictions: Vec\u003cf32\u003e =\n                pred_tensor.to_vec().into_iter().map(|v| v as f32).collect();\n\n            let mse = Metrics::mse(\u0026test_y, \u0026predictions);\n            let mae = Metrics::mae(\u0026test_y, \u0026predictions);\n            let r2 = Metrics::r_squared(\u0026test_y, \u0026predictions);\n\n            // Black box the metrics to ensure they're computed\n            black_box((mse, mae, r2));\n        });\n    });\n}\n\ncriterion_group!(\n    benches,\n    bench_train_1_feature,\n    bench_train_1_feature_learning_rate,\n    bench_train_1_feature_epochs,\n    bench_train_1_feature_with_metrics\n);\ncriterion_main!(benches);\n","traces":[],"covered":0,"coverable":0},{"path":["/","home","victor","ML","machinelearne-rs","benchmarks","benches","linear_regression","train_2_features.rs"],"content":"use benchmarks::data::CaliforniaHousingDataset;\nuse benchmarks::metrics::Metrics;\nuse criterion::{black_box, criterion_group, criterion_main, BenchmarkId, Criterion};\nuse machinelearne_rs::{\n    backend::Tensor2D,\n    dataset::InMemoryDataset,\n    loss::MSELoss,\n    model::{linear::LinearRegression, InferenceModel},\n    optimizer::SGD,\n    regularizers::NoRegularizer,\n    trainer::Trainer,\n    CpuBackend,\n};\n\n/// Helper function to convert Vec\u003cVec\u003cf32\u003e\u003e to Tensor2D\u003cCpuBackend\u003e\nfn vec_to_tensor2d(data: \u0026[Vec\u003cf32\u003e]) -\u003e Tensor2D\u003cCpuBackend\u003e {\n    let flat: Vec\u003cf32\u003e = data.iter().flatten().copied().collect();\n    Tensor2D::new(flat, data.len(), data.first().map(|v| v.len()).unwrap_or(0))\n}\n\nfn bench_train_2_features(c: \u0026mut Criterion) {\n    // Load dataset and select first 2 features\n    let dataset = CaliforniaHousingDataset::load(\"datasets/california_housing.csv\")\n        .expect(\"Failed to load dataset\");\n    let subset = dataset.select_features(\u0026[0, 1]); // MedInc and HouseAge\n    let (train_dataset, _test_dataset) = subset.split(0.8);\n    let in_memory = train_dataset\n        .to_in_memory_dataset()\n        .expect(\"Failed to create dataset\");\n\n    // Test different batch sizes\n    for batch_size in [16, 32, 64, 128].iter() {\n        c.bench_with_input(\n            BenchmarkId::new(\"train_2_features\", batch_size),\n            batch_size,\n            |b, \u0026bs| {\n                let optimizer = SGD::new(0.001);\n                let trainer = Trainer::builder(MSELoss, optimizer, NoRegularizer)\n                    .batch_size(bs)\n                    .max_epochs(10)\n                    .build();\n\n                b.iter(|| {\n                    let model_copy = LinearRegression::\u003cCpuBackend\u003e::new(2);\n                    let fitted = trainer\n                        .fit(model_copy, \u0026in_memory)\n                        .expect(\"Failed to fit model\");\n\n                    // Verify the model works\n                    let test_x: Vec\u003cf32\u003e = vec![5.0, 30.0];\n                    let test_tensor = machinelearne_rs::backend::Tensor1D::new(test_x);\n                    let _pred = fitted.predict(black_box(\u0026test_tensor));\n\n                    fitted\n                });\n            },\n        );\n    }\n}\n\nfn bench_train_2_features_learning_rate(c: \u0026mut Criterion) {\n    // Load dataset and select first 2 features\n    let dataset = CaliforniaHousingDataset::load(\"datasets/california_housing.csv\")\n        .expect(\"Failed to load dataset\");\n    let subset = dataset.select_features(\u0026[0, 1]); // MedInc and HouseAge\n    let (train_dataset, _test_dataset) = subset.split(0.8);\n    let in_memory = train_dataset\n        .to_in_memory_dataset()\n        .expect(\"Failed to create dataset\");\n\n    // Test different learning rates\n    for lr in [0.001, 0.01, 0.1].iter() {\n        c.bench_with_input(\n            BenchmarkId::new(\"train_2_features_lr\", lr),\n            lr,\n            |b, \u0026learning_rate| {\n                let optimizer = SGD::new(learning_rate);\n                let trainer = Trainer::builder(MSELoss, optimizer, NoRegularizer)\n                    .batch_size(32)\n                    .max_epochs(10)\n                    .build();\n\n                b.iter(|| {\n                    let model_copy = LinearRegression::\u003cCpuBackend\u003e::new(2);\n                    let _ = trainer\n                        .fit(model_copy, \u0026in_memory)\n                        .expect(\"Failed to fit model\");\n                });\n            },\n        );\n    }\n}\n\nfn bench_train_2_features_epochs(c: \u0026mut Criterion) {\n    // Load dataset and select first 2 features\n    let dataset = CaliforniaHousingDataset::load(\"datasets/california_housing.csv\")\n        .expect(\"Failed to load dataset\");\n    let subset = dataset.select_features(\u0026[0, 1]); // MedInc and HouseAge\n    let (train_dataset, _test_dataset) = subset.split(0.8);\n    let in_memory = train_dataset\n        .to_in_memory_dataset()\n        .expect(\"Failed to create dataset\");\n\n    // Test different epoch counts\n    for epochs in [10, 50, 100].iter() {\n        c.bench_with_input(\n            BenchmarkId::new(\"train_2_features_epochs\", epochs),\n            epochs,\n            |b, \u0026max_epochs| {\n                let optimizer = SGD::new(0.01);\n                let trainer = Trainer::builder(MSELoss, optimizer, NoRegularizer)\n                    .batch_size(32)\n                    .max_epochs(max_epochs)\n                    .build();\n\n                b.iter(|| {\n                    let model_copy = LinearRegression::\u003cCpuBackend\u003e::new(2);\n                    let _ = trainer\n                        .fit(model_copy, \u0026in_memory)\n                        .expect(\"Failed to fit model\");\n                });\n            },\n        );\n    }\n}\n\nfn bench_train_2_features_with_metrics(c: \u0026mut Criterion) {\n    // Load dataset and select first 2 features\n    let dataset = CaliforniaHousingDataset::load(\"datasets/california_housing.csv\")\n        .expect(\"Failed to load dataset\");\n    let subset = dataset.select_features(\u0026[0, 1]); // MedInc and HouseAge\n    let (train_dataset, test_dataset) = subset.split(0.8);\n    let train_memory = train_dataset\n        .to_in_memory_dataset()\n        .expect(\"Failed to create dataset\");\n\n    // Test the full pipeline and measure metrics\n    c.bench_function(\"train_2_features_with_metrics\", |b| {\n        b.iter(|| {\n            let model = LinearRegression::\u003cCpuBackend\u003e::new(2);\n            let optimizer = SGD::new(0.001);\n            let trainer = Trainer::builder(MSELoss, optimizer, NoRegularizer)\n                .batch_size(32)\n                .max_epochs(50)\n                .build();\n\n            let fitted = trainer\n                .fit(model, \u0026train_memory)\n                .expect(\"Failed to fit model\");\n\n            // Evaluate on test set\n            let test_x = test_dataset.features().to_vec();\n            let test_y = test_dataset.target().to_vec();\n            let pred_tensor = fitted.predict_batch(black_box(\u0026vec_to_tensor2d(\u0026test_x)));\n\n            let predictions: Vec\u003cf32\u003e =\n                pred_tensor.to_vec().into_iter().map(|v| v as f32).collect();\n\n            let mse = Metrics::mse(\u0026test_y, \u0026predictions);\n            let mae = Metrics::mae(\u0026test_y, \u0026predictions);\n            let r2 = Metrics::r_squared(\u0026test_y, \u0026predictions);\n\n            black_box((mse, mae, r2));\n        });\n    });\n}\n\ncriterion_group!(\n    benches,\n    bench_train_2_features,\n    bench_train_2_features_learning_rate,\n    bench_train_2_features_epochs,\n    bench_train_2_features_with_metrics\n);\ncriterion_main!(benches);\n","traces":[],"covered":0,"coverable":0},{"path":["/","home","victor","ML","machinelearne-rs","benchmarks","benches","linear_regression","train_4_features.rs"],"content":"use benchmarks::data::CaliforniaHousingDataset;\nuse benchmarks::metrics::Metrics;\nuse criterion::{black_box, criterion_group, criterion_main, BenchmarkId, Criterion};\nuse machinelearne_rs::{\n    backend::Tensor2D,\n    dataset::InMemoryDataset,\n    loss::MSELoss,\n    model::{linear::LinearRegression, InferenceModel},\n    optimizer::SGD,\n    regularizers::NoRegularizer,\n    trainer::Trainer,\n    CpuBackend,\n};\n\n/// Helper function to convert Vec\u003cVec\u003cf32\u003e\u003e to Tensor2D\u003cCpuBackend\u003e\nfn vec_to_tensor2d(data: \u0026[Vec\u003cf32\u003e]) -\u003e Tensor2D\u003cCpuBackend\u003e {\n    let flat: Vec\u003cf32\u003e = data.iter().flatten().copied().collect();\n    Tensor2D::new(flat, data.len(), data.first().map(|v| v.len()).unwrap_or(0))\n}\n\nfn bench_train_4_features(c: \u0026mut Criterion) {\n    // Load dataset and select first 4 features\n    let dataset = CaliforniaHousingDataset::load(\"datasets/california_housing.csv\")\n        .expect(\"Failed to load dataset\");\n    let subset = dataset.select_features(\u0026[0, 1, 2, 3]); // MedInc, HouseAge, AveRooms, AveBedrms\n    let (train_dataset, _test_dataset) = subset.split(0.8);\n    let in_memory = train_dataset\n        .to_in_memory_dataset()\n        .expect(\"Failed to create dataset\");\n\n    // Test different batch sizes\n    for batch_size in [16, 32, 64, 128].iter() {\n        c.bench_with_input(\n            BenchmarkId::new(\"train_4_features\", batch_size),\n            batch_size,\n            |b, \u0026bs| {\n                let optimizer = SGD::new(0.001);\n                let trainer = Trainer::builder(MSELoss, optimizer, NoRegularizer)\n                    .batch_size(bs)\n                    .max_epochs(10)\n                    .build();\n\n                b.iter(|| {\n                    let model_copy = LinearRegression::\u003cCpuBackend\u003e::new(4);\n                    let fitted = trainer\n                        .fit(model_copy, \u0026in_memory)\n                        .expect(\"Failed to fit model\");\n\n                    // Verify the model works\n                    let test_x: Vec\u003cf32\u003e = vec![5.0, 30.0, 6.0, 1.0];\n                    let test_tensor = machinelearne_rs::backend::Tensor1D::new(test_x);\n                    let _pred = fitted.predict(black_box(\u0026test_tensor));\n\n                    fitted\n                });\n            },\n        );\n    }\n}\n\nfn bench_train_4_features_learning_rate(c: \u0026mut Criterion) {\n    // Load dataset and select first 4 features\n    let dataset = CaliforniaHousingDataset::load(\"datasets/california_housing.csv\")\n        .expect(\"Failed to load dataset\");\n    let subset = dataset.select_features(\u0026[0, 1, 2, 3]); // MedInc, HouseAge, AveRooms, AveBedrms\n    let (train_dataset, _test_dataset) = subset.split(0.8);\n    let in_memory = train_dataset\n        .to_in_memory_dataset()\n        .expect(\"Failed to create dataset\");\n\n    // Test different learning rates\n    for lr in [0.001, 0.01, 0.1].iter() {\n        c.bench_with_input(\n            BenchmarkId::new(\"train_4_features_lr\", lr),\n            lr,\n            |b, \u0026learning_rate| {\n                let optimizer = SGD::new(learning_rate);\n                let trainer = Trainer::builder(MSELoss, optimizer, NoRegularizer)\n                    .batch_size(32)\n                    .max_epochs(10)\n                    .build();\n\n                b.iter(|| {\n                    let model_copy = LinearRegression::\u003cCpuBackend\u003e::new(4);\n                    let _ = trainer\n                        .fit(model_copy, \u0026in_memory)\n                        .expect(\"Failed to fit model\");\n                });\n            },\n        );\n    }\n}\n\nfn bench_train_4_features_epochs(c: \u0026mut Criterion) {\n    // Load dataset and select first 4 features\n    let dataset = CaliforniaHousingDataset::load(\"datasets/california_housing.csv\")\n        .expect(\"Failed to load dataset\");\n    let subset = dataset.select_features(\u0026[0, 1, 2, 3]); // MedInc, HouseAge, AveRooms, AveBedrms\n    let (train_dataset, _test_dataset) = subset.split(0.8);\n    let in_memory = train_dataset\n        .to_in_memory_dataset()\n        .expect(\"Failed to create dataset\");\n\n    // Test different epoch counts\n    for epochs in [10, 50, 100].iter() {\n        c.bench_with_input(\n            BenchmarkId::new(\"train_4_features_epochs\", epochs),\n            epochs,\n            |b, \u0026max_epochs| {\n                let optimizer = SGD::new(0.01);\n                let trainer = Trainer::builder(MSELoss, optimizer, NoRegularizer)\n                    .batch_size(32)\n                    .max_epochs(max_epochs)\n                    .build();\n\n                b.iter(|| {\n                    let model_copy = LinearRegression::\u003cCpuBackend\u003e::new(4);\n                    let _ = trainer\n                        .fit(model_copy, \u0026in_memory)\n                        .expect(\"Failed to fit model\");\n                });\n            },\n        );\n    }\n}\n\nfn bench_train_4_features_with_metrics(c: \u0026mut Criterion) {\n    // Load dataset and select first 4 features\n    let dataset = CaliforniaHousingDataset::load(\"datasets/california_housing.csv\")\n        .expect(\"Failed to load dataset\");\n    let subset = dataset.select_features(\u0026[0, 1, 2, 3]); // MedInc, HouseAge, AveRooms, AveBedrms\n    let (train_dataset, test_dataset) = subset.split(0.8);\n    let train_memory = train_dataset\n        .to_in_memory_dataset()\n        .expect(\"Failed to create dataset\");\n\n    // Test the full pipeline and measure metrics\n    c.bench_function(\"train_4_features_with_metrics\", |b| {\n        b.iter(|| {\n            let model = LinearRegression::\u003cCpuBackend\u003e::new(4);\n            let optimizer = SGD::new(0.001);\n            let trainer = Trainer::builder(MSELoss, optimizer, NoRegularizer)\n                .batch_size(32)\n                .max_epochs(50)\n                .build();\n\n            let fitted = trainer\n                .fit(model, \u0026train_memory)\n                .expect(\"Failed to fit model\");\n\n            // Evaluate on test set\n            let test_x = test_dataset.features().to_vec();\n            let test_y = test_dataset.target().to_vec();\n            let pred_tensor = fitted.predict_batch(black_box(\u0026vec_to_tensor2d(\u0026test_x)));\n\n            let predictions: Vec\u003cf32\u003e =\n                pred_tensor.to_vec().into_iter().map(|v| v as f32).collect();\n\n            let mse = Metrics::mse(\u0026test_y, \u0026predictions);\n            let mae = Metrics::mae(\u0026test_y, \u0026predictions);\n            let r2 = Metrics::r_squared(\u0026test_y, \u0026predictions);\n\n            black_box((mse, mae, r2));\n        });\n    });\n}\n\ncriterion_group!(\n    benches,\n    bench_train_4_features,\n    bench_train_4_features_learning_rate,\n    bench_train_4_features_epochs,\n    bench_train_4_features_with_metrics\n);\ncriterion_main!(benches);\n","traces":[],"covered":0,"coverable":0},{"path":["/","home","victor","ML","machinelearne-rs","benchmarks","benches","linear_regression","train_8_features.rs"],"content":"use benchmarks::data::CaliforniaHousingDataset;\nuse benchmarks::metrics::Metrics;\nuse criterion::{black_box, criterion_group, criterion_main, BenchmarkId, Criterion};\nuse machinelearne_rs::{\n    backend::Tensor2D,\n    dataset::InMemoryDataset,\n    loss::MSELoss,\n    model::{linear::LinearRegression, InferenceModel},\n    optimizer::SGD,\n    regularizers::NoRegularizer,\n    trainer::Trainer,\n    CpuBackend,\n};\n\n/// Helper function to convert Vec\u003cVec\u003cf32\u003e\u003e to Tensor2D\u003cCpuBackend\u003e\nfn vec_to_tensor2d(data: \u0026[Vec\u003cf32\u003e]) -\u003e Tensor2D\u003cCpuBackend\u003e {\n    let flat: Vec\u003cf32\u003e = data.iter().flatten().copied().collect();\n    Tensor2D::new(flat, data.len(), data.first().map(|v| v.len()).unwrap_or(0))\n}\n\nfn bench_train_8_features(c: \u0026mut Criterion) {\n    // Load dataset with all 8 features\n    let dataset = CaliforniaHousingDataset::load(\"datasets/california_housing.csv\")\n        .expect(\"Failed to load dataset\");\n    // Use all features (0-7)\n    let (train_dataset, _test_dataset) = dataset.split(0.8);\n    let in_memory = train_dataset\n        .to_in_memory_dataset()\n        .expect(\"Failed to create dataset\");\n\n    // Test different batch sizes\n    for batch_size in [16, 32, 64, 128].iter() {\n        c.bench_with_input(\n            BenchmarkId::new(\"train_8_features\", batch_size),\n            batch_size,\n            |b, \u0026bs| {\n                let optimizer = SGD::new(0.001);\n                let trainer = Trainer::builder(MSELoss, optimizer, NoRegularizer)\n                    .batch_size(bs)\n                    .max_epochs(10)\n                    .build();\n\n                b.iter(|| {\n                    let model_copy = LinearRegression::\u003cCpuBackend\u003e::new(8);\n                    let fitted = trainer\n                        .fit(model_copy, \u0026in_memory)\n                        .expect(\"Failed to fit model\");\n\n                    // Verify the model works\n                    let test_x: Vec\u003cf32\u003e = vec![5.0, 30.0, 6.0, 1.0, 500.0, 3.0, 37.8, -122.2];\n                    let test_tensor = machinelearne_rs::backend::Tensor1D::new(test_x);\n                    let _pred = fitted.predict(black_box(\u0026test_tensor));\n\n                    fitted\n                });\n            },\n        );\n    }\n}\n\nfn bench_train_8_features_learning_rate(c: \u0026mut Criterion) {\n    // Load dataset with all 8 features\n    let dataset = CaliforniaHousingDataset::load(\"datasets/california_housing.csv\")\n        .expect(\"Failed to load dataset\");\n    // Use all features (0-7)\n    let (train_dataset, _test_dataset) = dataset.split(0.8);\n    let in_memory = train_dataset\n        .to_in_memory_dataset()\n        .expect(\"Failed to create dataset\");\n\n    // Test different learning rates\n    for lr in [0.001, 0.01, 0.1].iter() {\n        c.bench_with_input(\n            BenchmarkId::new(\"train_8_features_lr\", lr),\n            lr,\n            |b, \u0026learning_rate| {\n                let optimizer = SGD::new(learning_rate);\n                let trainer = Trainer::builder(MSELoss, optimizer, NoRegularizer)\n                    .batch_size(32)\n                    .max_epochs(10)\n                    .build();\n\n                b.iter(|| {\n                    let model_copy = LinearRegression::\u003cCpuBackend\u003e::new(8);\n                    let _ = trainer\n                        .fit(model_copy, \u0026in_memory)\n                        .expect(\"Failed to fit model\");\n                });\n            },\n        );\n    }\n}\n\nfn bench_train_8_features_epochs(c: \u0026mut Criterion) {\n    // Load dataset with all 8 features\n    let dataset = CaliforniaHousingDataset::load(\"datasets/california_housing.csv\")\n        .expect(\"Failed to load dataset\");\n    // Use all features (0-7)\n    let (train_dataset, _test_dataset) = dataset.split(0.8);\n    let in_memory = train_dataset\n        .to_in_memory_dataset()\n        .expect(\"Failed to create dataset\");\n\n    // Test different epoch counts\n    for epochs in [10, 50, 100].iter() {\n        c.bench_with_input(\n            BenchmarkId::new(\"train_8_features_epochs\", epochs),\n            epochs,\n            |b, \u0026max_epochs| {\n                let optimizer = SGD::new(0.01);\n                let trainer = Trainer::builder(MSELoss, optimizer, NoRegularizer)\n                    .batch_size(32)\n                    .max_epochs(max_epochs)\n                    .build();\n\n                b.iter(|| {\n                    let model_copy = LinearRegression::\u003cCpuBackend\u003e::new(8);\n                    let _ = trainer\n                        .fit(model_copy, \u0026in_memory)\n                        .expect(\"Failed to fit model\");\n                });\n            },\n        );\n    }\n}\n\nfn bench_train_8_features_with_metrics(c: \u0026mut Criterion) {\n    // Load dataset with all 8 features\n    let dataset = CaliforniaHousingDataset::load(\"datasets/california_housing.csv\")\n        .expect(\"Failed to load dataset\");\n    // Use all features (0-7)\n    let (train_dataset, test_dataset) = dataset.split(0.8);\n    let train_memory = train_dataset\n        .to_in_memory_dataset()\n        .expect(\"Failed to create dataset\");\n\n    // Test the full pipeline and measure metrics\n    c.bench_function(\"train_8_features_with_metrics\", |b| {\n        b.iter(|| {\n            let model = LinearRegression::\u003cCpuBackend\u003e::new(8);\n            let optimizer = SGD::new(0.001);\n            let trainer = Trainer::builder(MSELoss, optimizer, NoRegularizer)\n                .batch_size(32)\n                .max_epochs(50)\n                .build();\n\n            let fitted = trainer\n                .fit(model, \u0026train_memory)\n                .expect(\"Failed to fit model\");\n\n            // Evaluate on test set\n            let test_x = test_dataset.features().to_vec();\n            let test_y = test_dataset.target().to_vec();\n            let pred_tensor = fitted.predict_batch(black_box(\u0026vec_to_tensor2d(\u0026test_x)));\n\n            let predictions: Vec\u003cf32\u003e =\n                pred_tensor.to_vec().into_iter().map(|v| v as f32).collect();\n\n            let mse = Metrics::mse(\u0026test_y, \u0026predictions);\n            let mae = Metrics::mae(\u0026test_y, \u0026predictions);\n            let r2 = Metrics::r_squared(\u0026test_y, \u0026predictions);\n\n            black_box((mse, mae, r2));\n        });\n    });\n}\n\ncriterion_group!(\n    benches,\n    bench_train_8_features,\n    bench_train_8_features_learning_rate,\n    bench_train_8_features_epochs,\n    bench_train_8_features_with_metrics\n);\ncriterion_main!(benches);\n","traces":[],"covered":0,"coverable":0},{"path":["/","home","victor","ML","machinelearne-rs","benchmarks","benches","metrics.rs"],"content":"use benchmarks::metrics::Metrics;\nuse criterion::{black_box, criterion_group, criterion_main, BenchmarkId, Criterion};\n\nfn bench_mse(c: \u0026mut Criterion) {\n    // Test different array sizes\n    for size in [100, 1000, 10000, 100000].iter() {\n        c.bench_with_input(BenchmarkId::new(\"mse\", size), size, |b, \u0026n| {\n            let y_true: Vec\u003cf32\u003e = (0..n).map(|i| i as f32 * 0.1).collect();\n            let y_pred: Vec\u003cf32\u003e = (0..n).map(|i| i as f32 * 0.1 + 0.5).collect();\n\n            b.iter(|| {\n                let mse = Metrics::mse(black_box(\u0026y_true), black_box(\u0026y_pred));\n                black_box(mse);\n            });\n        });\n    }\n}\n\nfn bench_mae(c: \u0026mut Criterion) {\n    // Test different array sizes\n    for size in [100, 1000, 10000, 100000].iter() {\n        c.bench_with_input(BenchmarkId::new(\"mae\", size), size, |b, \u0026n| {\n            let y_true: Vec\u003cf32\u003e = (0..n).map(|i| i as f32 * 0.1).collect();\n            let y_pred: Vec\u003cf32\u003e = (0..n).map(|i| i as f32 * 0.1 + 0.5).collect();\n\n            b.iter(|| {\n                let mae = Metrics::mae(black_box(\u0026y_true), black_box(\u0026y_pred));\n                black_box(mae);\n            });\n        });\n    }\n}\n\nfn bench_r_squared(c: \u0026mut Criterion) {\n    // Test different array sizes\n    for size in [100, 1000, 10000, 100000].iter() {\n        c.bench_with_input(BenchmarkId::new(\"r_squared\", size), size, |b, \u0026n| {\n            let y_true: Vec\u003cf32\u003e = (0..n).map(|i| i as f32 * 0.1).collect();\n            let y_pred: Vec\u003cf32\u003e = (0..n).map(|i| i as f32 * 0.1 + 0.5).collect();\n\n            b.iter(|| {\n                let r2 = Metrics::r_squared(black_box(\u0026y_true), black_box(\u0026y_pred));\n                black_box(r2);\n            });\n        });\n    }\n}\n\nfn bench_rmse(c: \u0026mut Criterion) {\n    // Test different array sizes\n    for size in [100, 1000, 10000, 100000].iter() {\n        c.bench_with_input(BenchmarkId::new(\"rmse\", size), size, |b, \u0026n| {\n            let y_true: Vec\u003cf32\u003e = (0..n).map(|i| i as f32 * 0.1).collect();\n            let y_pred: Vec\u003cf32\u003e = (0..n).map(|i| i as f32 * 0.1 + 0.5).collect();\n\n            b.iter(|| {\n                let rmse = Metrics::rmse(black_box(\u0026y_true), black_box(\u0026y_pred));\n                black_box(rmse);\n            });\n        });\n    }\n}\n\nfn bench_calculate_all(c: \u0026mut Criterion) {\n    // Test different array sizes\n    for size in [100, 1000, 10000, 100000].iter() {\n        c.bench_with_input(BenchmarkId::new(\"calculate_all\", size), size, |b, \u0026n| {\n            let y_true: Vec\u003cf32\u003e = (0..n).map(|i| i as f32 * 0.1).collect();\n            let y_pred: Vec\u003cf32\u003e = (0..n).map(|i| i as f32 * 0.1 + 0.5).collect();\n\n            b.iter(|| {\n                let metrics = Metrics::calculate_all(black_box(\u0026y_true), black_box(\u0026y_pred));\n                black_box(metrics);\n            });\n        });\n    }\n}\n\nfn bench_mse_perfect_prediction(c: \u0026mut Criterion) {\n    // Benchmark MSE with perfect predictions (should be fast)\n    for size in [100, 1000, 10000].iter() {\n        c.bench_with_input(BenchmarkId::new(\"mse_perfect\", size), size, |b, \u0026n| {\n            let y_true: Vec\u003cf32\u003e = (0..n).map(|i| i as f32 * 0.1).collect();\n            let y_pred = y_true.clone(); // Perfect prediction\n\n            b.iter(|| {\n                let mse = Metrics::mse(black_box(\u0026y_true), black_box(\u0026y_pred));\n                black_box(mse);\n            });\n        });\n    }\n}\n\nfn bench_r_squared_perfect_prediction(c: \u0026mut Criterion) {\n    // Benchmark RÂ² with perfect predictions (should be ~1.0)\n    for size in [100, 1000, 10000].iter() {\n        c.bench_with_input(\n            BenchmarkId::new(\"r_squared_perfect\", size),\n            size,\n            |b, \u0026n| {\n                let y_true: Vec\u003cf32\u003e = (0..n).map(|i| i as f32 * 0.1).collect();\n                let y_pred = y_true.clone(); // Perfect prediction\n\n                b.iter(|| {\n                    let r2 = Metrics::r_squared(black_box(\u0026y_true), black_box(\u0026y_pred));\n                    black_box(r2);\n                });\n            },\n        );\n    }\n}\n\nfn bench_metrics_comparison(c: \u0026mut Criterion) {\n    // Compare all metrics on the same data\n    let sizes = [1000, 10000, 100000];\n\n    for size in sizes.iter() {\n        let y_true: Vec\u003cf32\u003e = (0..*size).map(|i| i as f32 * 0.1).collect();\n        let y_pred: Vec\u003cf32\u003e = (0..*size).map(|i| i as f32 * 0.1 + 0.5).collect();\n\n        let mut group = c.benchmark_group(format!(\"metrics_comparison_{}\", size));\n\n        group.bench_function(\"mse\", |b| {\n            b.iter(|| {\n                let mse = Metrics::mse(black_box(\u0026y_true), black_box(\u0026y_pred));\n                black_box(mse);\n            });\n        });\n\n        group.bench_function(\"mae\", |b| {\n            b.iter(|| {\n                let mae = Metrics::mae(black_box(\u0026y_true), black_box(\u0026y_pred));\n                black_box(mae);\n            });\n        });\n\n        group.bench_function(\"r_squared\", |b| {\n            b.iter(|| {\n                let r2 = Metrics::r_squared(black_box(\u0026y_true), black_box(\u0026y_pred));\n                black_box(r2);\n            });\n        });\n\n        group.bench_function(\"rmse\", |b| {\n            b.iter(|| {\n                let rmse = Metrics::rmse(black_box(\u0026y_true), black_box(\u0026y_pred));\n                black_box(rmse);\n            });\n        });\n\n        group.finish();\n    }\n}\n\nfn bench_metrics_large_arrays(c: \u0026mut Criterion) {\n    // Benchmark with very large arrays\n    c.bench_function(\"mse_large_1M\", |b| {\n        let n = 1_000_000;\n        let y_true: Vec\u003cf32\u003e = (0..n).map(|i| i as f32 * 0.1).collect();\n        let y_pred: Vec\u003cf32\u003e = (0..n).map(|i| i as f32 * 0.1 + 0.5).collect();\n\n        b.iter(|| {\n            let mse = Metrics::mse(black_box(\u0026y_true), black_box(\u0026y_pred));\n            black_box(mse);\n        });\n    });\n\n    c.bench_function(\"mae_large_1M\", |b| {\n        let n = 1_000_000;\n        let y_true: Vec\u003cf32\u003e = (0..n).map(|i| i as f32 * 0.1).collect();\n        let y_pred: Vec\u003cf32\u003e = (0..n).map(|i| i as f32 * 0.1 + 0.5).collect();\n\n        b.iter(|| {\n            let mae = Metrics::mae(black_box(\u0026y_true), black_box(\u0026y_pred));\n            black_box(mae);\n        });\n    });\n\n    c.bench_function(\"r_squared_large_1M\", |b| {\n        let n = 1_000_000;\n        let y_true: Vec\u003cf32\u003e = (0..n).map(|i| i as f32 * 0.1).collect();\n        let y_pred: Vec\u003cf32\u003e = (0..n).map(|i| i as f32 * 0.1 + 0.5).collect();\n\n        b.iter(|| {\n            let r2 = Metrics::r_squared(black_box(\u0026y_true), black_box(\u0026y_pred));\n            black_box(r2);\n        });\n    });\n}\n\ncriterion_group!(\n    benches,\n    bench_mse,\n    bench_mae,\n    bench_r_squared,\n    bench_rmse,\n    bench_calculate_all,\n    bench_mse_perfect_prediction,\n    bench_r_squared_perfect_prediction,\n    bench_metrics_comparison,\n    bench_metrics_large_arrays\n);\ncriterion_main!(benches);\n","traces":[],"covered":0,"coverable":0},{"path":["/","home","victor","ML","machinelearne-rs","benchmarks","src","bin","backend_comparison.rs"],"content":"#!/usr/bin/env rust-script\n//!\n//! Compare CPU backend vs ndarray backend performance\n\nuse benchmarks::data::CaliforniaHousingDataset;\nuse benchmarks::metrics::Metrics;\nuse machinelearne_rs::{\n    backend::{CpuBackend, Tensor2D},\n    dataset::InMemoryDataset,\n    loss::MSELoss,\n    model::{linear::LinearRegression, InferenceModel},\n    optimizer::SGD,\n    regularizers::NoRegularizer,\n    trainer::Trainer,\n};\n\nuse serde_json::json;\nuse std::fs::File;\nuse std::io::Write;\nuse std::time::Instant;\n\n/// Helper function to convert Vec\u003cVec\u003cf32\u003e\u003e to Tensor2D\u003cCpuBackend\u003e\nfn vec_to_tensor2d_cpu(data: \u0026[Vec\u003cf32\u003e]) -\u003e Tensor2D\u003cCpuBackend\u003e {\n    let flat: Vec\u003cf32\u003e = data.iter().flatten().copied().collect();\n    Tensor2D::new(flat, data.len(), data.first().map(|v| v.len()).unwrap_or(0))\n}\n\n#[cfg(feature = \"ndarray\")]\nuse machinelearne_rs::backend::NdarrayBackend;\n\n/// Helper function to convert Vec\u003cVec\u003cf32\u003e\u003e to Tensor2D for ndarray backend\n#[cfg(feature = \"ndarray\")]\nfn vec_to_tensor2d_ndarray(data: \u0026[Vec\u003cf32\u003e]) -\u003e Tensor2D\u003cNdarrayBackend\u003e {\n    let flat: Vec\u003cf32\u003e = data.iter().flatten().copied().collect();\n    Tensor2D::new(flat, data.len(), data.first().map(|v| v.len()).unwrap_or(0))\n}\n\n/// Apply z-score standardization to features\nfn standardize_features(\n    train_features: \u0026[Vec\u003cf32\u003e],\n    test_features: \u0026[Vec\u003cf32\u003e],\n) -\u003e (Vec\u003cVec\u003cf32\u003e\u003e, Vec\u003cVec\u003cf32\u003e\u003e) {\n    let n_samples = train_features.len();\n    let n_features = if n_samples \u003e 0 {\n        train_features[0].len()\n    } else {\n        return (train_features.to_vec(), test_features.to_vec());\n    };\n\n    let mut means = vec![0.0_f32; n_features];\n    let mut stds = vec![0.0_f32; n_features];\n\n    for feature_idx in 0..n_features {\n        let sum: f32 = (0..n_samples).map(|i| train_features[i][feature_idx]).sum();\n        means[feature_idx] = sum / n_samples as f32;\n\n        let variance: f32 = (0..n_samples)\n            .map(|i| {\n                let diff = train_features[i][feature_idx] - means[feature_idx];\n                diff * diff\n            })\n            .sum();\n        stds[feature_idx] = f32::sqrt(variance / n_samples as f32);\n\n        if stds[feature_idx] \u003c 1e-6 {\n            stds[feature_idx] = 1.0;\n        }\n    }\n\n    let train_scaled: Vec\u003cVec\u003cf32\u003e\u003e = (0..n_samples)\n        .map(|i| {\n            (0..n_features)\n                .map(|j| (train_features[i][j] - means[j]) / stds[j])\n                .collect()\n        })\n        .collect();\n\n    let test_scaled: Vec\u003cVec\u003cf32\u003e\u003e = (0..test_features.len())\n        .map(|i| {\n            (0..n_features)\n                .map(|j| (test_features[i][j] - means[j]) / stds[j])\n                .collect()\n        })\n        .collect();\n\n    (train_scaled, test_scaled)\n}\n\n/// Benchmark CPU backend\nfn benchmark_cpu_backend(n_features: usize) -\u003e serde_json::Value {\n    let feature_indices: Vec\u003cusize\u003e = (0..n_features).collect();\n\n    let dataset = CaliforniaHousingDataset::load(\"benchmarks/datasets/california_housing.csv\")\n        .expect(\"Failed to load dataset\");\n    let subset = dataset.select_features(\u0026feature_indices);\n    let (train_dataset, _, test_dataset) = subset.split_train_val_test(0.8, 0.0);\n\n    let train_features = train_dataset.features();\n    let train_target = train_dataset.target();\n    let test_features = test_dataset.features();\n    let test_target = test_dataset.target();\n\n    let (train_features_scaled, test_features_scaled) =\n        standardize_features(train_features, test_features);\n\n    let train_memory = InMemoryDataset::new(train_features_scaled, train_target.to_vec())\n        .expect(\"Failed to create training dataset\");\n\n    let lr = 0.01;\n    let model = LinearRegression::\u003cCpuBackend\u003e::new(n_features);\n    let optimizer = SGD::\u003cCpuBackend\u003e::new(lr);\n\n    let trainer = Trainer::builder(MSELoss, optimizer, NoRegularizer)\n        .batch_size(32)\n        .max_epochs(50)\n        .build();\n\n    let start = Instant::now();\n    let fitted = trainer\n        .fit(model, \u0026train_memory)\n        .expect(\"Failed to fit model\");\n    let train_time_ms = start.elapsed().as_millis();\n\n    let test_tensor = vec_to_tensor2d_cpu(\u0026test_features_scaled);\n    let pred_tensor = fitted.predict_batch(\u0026test_tensor);\n\n    let predictions: Vec\u003cf32\u003e = pred_tensor.to_vec().into_iter().map(|v| v as f32).collect();\n\n    let mse = Metrics::mse(test_target, \u0026predictions);\n    let mae = Metrics::mae(test_target, \u0026predictions);\n    let r2 = Metrics::r_squared(test_target, \u0026predictions);\n\n    json!({\n        \"backend\": \"CpuBackend\",\n        \"n_features\": n_features,\n        \"train_time_ms\": train_time_ms,\n        \"mse\": mse,\n        \"mae\": mae,\n        \"r2\": r2,\n    })\n}\n\n/// Benchmark ndarray backend (only available with ndarray feature)\n#[cfg(feature = \"ndarray\")]\nfn benchmark_ndarray_backend(n_features: usize) -\u003e serde_json::Value {\n    let feature_indices: Vec\u003cusize\u003e = (0..n_features).collect();\n\n    let dataset = CaliforniaHousingDataset::load(\"benchmarks/datasets/california_housing.csv\")\n        .expect(\"Failed to load dataset\");\n    let subset = dataset.select_features(\u0026feature_indices);\n    let (train_dataset, _, test_dataset) = subset.split_train_val_test(0.8, 0.0);\n\n    let train_features = train_dataset.features();\n    let train_target = train_dataset.target();\n    let test_features = test_dataset.features();\n    let test_target = test_dataset.target();\n\n    let (train_features_scaled, test_features_scaled) =\n        standardize_features(train_features, test_features);\n\n    let train_memory = InMemoryDataset::new(train_features_scaled, train_target.to_vec())\n        .expect(\"Failed to create training dataset\");\n\n    let lr = 0.01;\n    let model = LinearRegression::\u003cNdarrayBackend\u003e::new(n_features);\n    let optimizer = SGD::\u003cNdarrayBackend\u003e::new(lr);\n\n    let trainer = Trainer::builder(MSELoss, optimizer, NoRegularizer)\n        .batch_size(32)\n        .max_epochs(50)\n        .build();\n\n    let start = Instant::now();\n    let fitted = trainer\n        .fit(model, \u0026train_memory)\n        .expect(\"Failed to fit model\");\n    let train_time_ms = start.elapsed().as_millis();\n\n    let test_tensor = vec_to_tensor2d_ndarray(\u0026test_features_scaled);\n    let pred_tensor = fitted.predict_batch(\u0026test_tensor);\n\n    let predictions: Vec\u003cf32\u003e = pred_tensor.to_vec().into_iter().map(|v| v as f32).collect();\n\n    let mse = Metrics::mse(test_target, \u0026predictions);\n    let mae = Metrics::mae(test_target, \u0026predictions);\n    let r2 = Metrics::r_squared(test_target, \u0026predictions);\n\n    json!({\n        \"backend\": \"NdarrayBackend\",\n        \"n_features\": n_features,\n        \"train_time_ms\": train_time_ms,\n        \"mse\": mse,\n        \"mae\": mae,\n        \"r2\": r2,\n    })\n}\n\n/// Placeholder for when ndarray feature is not enabled\n#[cfg(not(feature = \"ndarray\"))]\nfn benchmark_ndarray_backend(n_features: usize) -\u003e serde_json::Value {\n    json!({\n        \"backend\": \"NdarrayBackend\",\n        \"n_features\": n_features,\n        \"train_time_ms\": null,\n        \"mse\": null,\n        \"mae\": null,\n        \"r2\": null,\n        \"disabled_reason\": \"ndarray feature not enabled\"\n    })\n}\n\nfn main() {\n    println!(\"Running backend comparison benchmarks...\\n\");\n\n    let mut results = Vec::new();\n\n    // Test with different feature counts\n    for n_features in [1, 2, 4, 8] {\n        println!(\"Testing {} features...\", n_features);\n\n        // CPU Backend\n        println!(\"  CpuBackend...\");\n        results.push(benchmark_cpu_backend(n_features));\n\n        // Ndarray Backend (if available)\n        println!(\"  NdarrayBackend...\");\n        results.push(benchmark_ndarray_backend(n_features));\n    }\n\n    let output = json!({\n        \"results\": results\n    });\n\n    // Write to file\n    let mut file = File::create(\"benchmarks/results/backend_comparison.json\")\n        .expect(\"Failed to create output file\");\n    file.write_all(serde_json::to_string_pretty(\u0026output).unwrap().as_bytes())\n        .expect(\"Failed to write to file\");\n\n    println!(\"\\nResults saved to benchmarks/results/backend_comparison.json\");\n\n    // Print comparison table\n    println!(\"\\nBackend Comparison (50 epochs, lr=0.01, bs=32):\");\n    println!(\n        \"{:\u003c15} {:\u003c12} {:\u003e12} {:\u003e10} {:\u003e10} {:\u003e10}\",\n        \"Backend\", \"Features\", \"Time (ms)\", \"MSE\", \"MAE\", \"RÂ²\"\n    );\n    println!(\"{}\", \"-\".repeat(70));\n\n    #[cfg(feature = \"ndarray\")]\n    for n_features in [1, 2, 4, 8] {\n        let cpu_result = results\n            .iter()\n            .find(|r| r[\"backend\"] == \"CpuBackend\" \u0026\u0026 r[\"n_features\"] == n_features)\n            .unwrap();\n\n        let ndarray_result = results\n            .iter()\n            .find(|r| r[\"backend\"] == \"NdarrayBackend\" \u0026\u0026 r[\"n_features\"] == n_features)\n            .unwrap();\n\n        let cpu_time = cpu_result[\"train_time_ms\"].as_f64().unwrap();\n        let ndarray_time = ndarray_result[\"train_time_ms\"].as_f64().unwrap();\n        let speedup = if ndarray_time \u003e 0.0 {\n            cpu_time / ndarray_time\n        } else {\n            0.0\n        };\n\n        println!(\n            \"{:\u003c15} {:\u003c12} {:\u003e12.2} {:\u003e10.4} {:\u003e10.4} {:\u003e10.4}\",\n            \"CpuBackend\",\n            n_features,\n            cpu_time,\n            cpu_result[\"mse\"].as_f64().unwrap(),\n            cpu_result[\"mae\"].as_f64().unwrap(),\n            cpu_result[\"r2\"].as_f64().unwrap(),\n        );\n\n        if ndarray_time \u003e 0.0 {\n            println!(\n                \"{:\u003c15} {:\u003c12} {:\u003e12.2} {:\u003e10.4} {:\u003e10.4} {:\u003e10.4} [{:\u003e6.1}x]\",\n                \"NdarrayBackend\",\n                n_features,\n                ndarray_time,\n                ndarray_result[\"mse\"].as_f64().unwrap(),\n                ndarray_result[\"mae\"].as_f64().unwrap(),\n                ndarray_result[\"r2\"].as_f64().unwrap(),\n                speedup,\n            );\n        } else {\n            println!(\n                \"{:\u003c15} {:\u003c12} {:\u003e12.2} {:\u003e10.4} {:\u003e10.4} {:\u003e10.4} [N/A]\",\n                \"NdarrayBackend\",\n                n_features,\n                ndarray_time,\n                ndarray_result[\"mse\"].as_f64().unwrap(),\n                ndarray_result[\"mae\"].as_f64().unwrap(),\n                ndarray_result[\"r2\"].as_f64().unwrap(),\n            );\n        }\n\n        println!();\n    }\n\n    #[cfg(not(feature = \"ndarray\"))]\n    {\n        println!(\"\\nNote: Run with --features ndarray to benchmark ndarray backend\");\n    }\n\n    // Print sklearn comparison\n    println!(\"Sklearn SGDRegressor (for reference, 1000 iterations):\");\n    println!(\n        \"{:\u003c30} {:\u003e12} {:\u003e10} {:\u003e10} {:\u003e10}\",\n        \"Config\", \"Features\", \"Time (ms)\", \"MSE\", \"RÂ²\"\n    );\n    println!(\"{}\", \"-\".repeat(75));\n    println!(\n        \"{:\u003c30} {:\u003c12} {:\u003e12.2} {:\u003e10.4} {:\u003e10.4}\",\n        \"lr_constant\", 2, 5.09, 0.6697, 0.4889\n    );\n    println!(\n        \"{:\u003c30} {:\u003c12} {:\u003e12.2} {:\u003e10.4} {:\u003e10.4}\",\n        \"lr_adaptive\", 2, 25.38, 0.6630, 0.4941\n    );\n}\n","traces":[],"covered":0,"coverable":0},{"path":["/","home","victor","ML","machinelearne-rs","benchmarks","src","bin","collect_metrics.rs"],"content":"#!/usr/bin/env rust-script\n//!\n//! Collect metrics from machinelearne-rs benchmarks\n//!\n//! This runs each training configuration once and records:\n//! - Training time\n//! - MSE\n//! - MAE\n//! - RÂ²\n\nuse benchmarks::data::CaliforniaHousingDataset;\nuse benchmarks::metrics::Metrics;\nuse machinelearne_rs::{\n    backend::Tensor2D,\n    dataset::InMemoryDataset,\n    loss::MSELoss,\n    model::{linear::LinearRegression, InferenceModel},\n    optimizer::SGD,\n    regularizers::NoRegularizer,\n    trainer::Trainer,\n    CpuBackend,\n};\nuse serde_json::json;\nuse std::fs::File;\nuse std::io::Write;\nuse std::time::Instant;\n\n/// Simple black_box function to prevent optimization\n#[inline(never)]\nfn black_box\u003cT\u003e(dummy: T) -\u003e T {\n    std::hint::black_box(dummy)\n}\n\n/// Helper function to convert Vec\u003cVec\u003cf32\u003e\u003e to Tensor2D\u003cCpuBackend\u003e\nfn vec_to_tensor2d(data: \u0026[Vec\u003cf32\u003e]) -\u003e Tensor2D\u003cCpuBackend\u003e {\n    let flat: Vec\u003cf32\u003e = data.iter().flatten().copied().collect();\n    Tensor2D::new(flat, data.len(), data.first().map(|v| v.len()).unwrap_or(0))\n}\n\n/// Apply z-score standardization to features\n///\n/// This computes mean and std from training data, then applies:\n/// z-score = (x - mean) / std\n/// to both train and test data using the same statistics.\nfn standardize_features(\n    train_features: \u0026[Vec\u003cf32\u003e],\n    test_features: \u0026[Vec\u003cf32\u003e],\n) -\u003e (Vec\u003cVec\u003cf32\u003e\u003e, Vec\u003cVec\u003cf32\u003e\u003e) {\n    let n_samples = train_features.len();\n    let n_features = if n_samples \u003e 0 {\n        train_features[0].len()\n    } else {\n        return (train_features.to_vec(), test_features.to_vec());\n    };\n\n    // Compute mean and std for each feature from training data\n    let mut means = vec![0.0_f32; n_features];\n    let mut stds = vec![0.0_f32; n_features];\n\n    for feature_idx in 0..n_features {\n        // Compute mean\n        let sum: f32 = (0..n_samples).map(|i| train_features[i][feature_idx]).sum();\n        means[feature_idx] = sum / n_samples as f32;\n\n        // Compute variance\n        let variance: f32 = (0..n_samples)\n            .map(|i| {\n                let diff = train_features[i][feature_idx] - means[feature_idx];\n                diff * diff\n            })\n            .sum();\n        stds[feature_idx] = f32::sqrt(variance / n_samples as f32);\n\n        // Prevent division by zero\n        if stds[feature_idx] \u003c 1e-6 {\n            stds[feature_idx] = 1.0;\n        }\n    }\n\n    // Apply z-score to training data\n    let train_scaled: Vec\u003cVec\u003cf32\u003e\u003e = (0..n_samples)\n        .map(|i| {\n            (0..n_features)\n                .map(|j| (train_features[i][j] - means[j]) / stds[j])\n                .collect()\n        })\n        .collect();\n\n    // Apply z-score to test data (using train's mean/std)\n    let test_scaled: Vec\u003cVec\u003cf32\u003e\u003e = (0..test_features.len())\n        .map(|i| {\n            (0..n_features)\n                .map(|j| (test_features[i][j] - means[j]) / stds[j])\n                .collect()\n        })\n        .collect();\n\n    (train_scaled, test_scaled)\n}\n\n/// Collect metrics for a given feature count\nfn collect_metrics(n_features: usize) -\u003e serde_json::Value {\n    let feature_indices: Vec\u003cusize\u003e = (0..n_features).collect();\n\n    let dataset = CaliforniaHousingDataset::load(\"benchmarks/datasets/california_housing.csv\")\n        .expect(\"Failed to load dataset\");\n    let subset = dataset.select_features(\u0026feature_indices);\n\n    // Split into train and test (function returns train, val, test)\n    let (train_dataset, _, test_dataset) = subset.split_train_val_test(0.8, 0.0);\n\n    // Get raw features and targets\n    let train_features = train_dataset.features();\n    let train_target = train_dataset.target();\n    let test_features = test_dataset.features();\n    let test_target = test_dataset.target();\n\n    // Apply z-score standardization\n    let (train_features_scaled, test_features_scaled) =\n        standardize_features(train_features, test_features);\n\n    // Create in-memory datasets with scaled features\n    let train_memory = InMemoryDataset::new(train_features_scaled, train_target.to_vec())\n        .expect(\"Failed to create training dataset\");\n\n    // Use learning rate appropriate for standardized features\n    let lr = 0.01;\n    let model = LinearRegression::\u003cCpuBackend\u003e::new(n_features);\n    let optimizer = SGD::\u003cCpuBackend\u003e::new(lr);\n\n    let trainer = Trainer::builder(MSELoss, optimizer, NoRegularizer)\n        .batch_size(32)\n        .max_epochs(50)\n        .build();\n\n    // Time the training\n    let start = Instant::now();\n    let fitted = trainer\n        .fit(model, \u0026train_memory)\n        .expect(\"Failed to fit model\");\n    let train_time_ms = start.elapsed().as_millis();\n\n    // Predict on test set\n    let test_tensor = vec_to_tensor2d(\u0026test_features_scaled);\n    let pred_tensor = fitted.predict_batch(black_box(\u0026test_tensor));\n\n    let predictions: Vec\u003cf32\u003e = pred_tensor.to_vec().into_iter().map(|v| v as f32).collect();\n\n    let mse = Metrics::mse(test_target, \u0026predictions);\n    let mae = Metrics::mae(test_target, \u0026predictions);\n    let r2 = Metrics::r_squared(test_target, \u0026predictions);\n\n    json!({\n        \"n_features\": n_features,\n        \"model\": \"LinearRegression\",\n        \"train_time_ms\": train_time_ms,\n        \"mse\": mse,\n        \"mae\": mae,\n        \"r2\": r2,\n    })\n}\n\nfn main() {\n    let mut results = Vec::new();\n\n    // Collect metrics for different feature counts\n    for n_features in [1, 2, 4, 8] {\n        println!(\"Collecting metrics for {} features...\", n_features);\n        results.push(collect_metrics(n_features));\n    }\n\n    let output = json!({\n        \"results\": results\n    });\n\n    // Write to file\n    let mut file =\n        File::create(\"benchmarks/results/rust_metrics.json\").expect(\"Failed to create output file\");\n    file.write_all(serde_json::to_string_pretty(\u0026output).unwrap().as_bytes())\n        .expect(\"Failed to write to file\");\n\n    println!(\"\\nMetrics collected and saved to benchmarks/results/rust_metrics.json\");\n}\n","traces":[{"line":30,"address":[],"length":0,"stats":{"Line":0}},{"line":31,"address":[],"length":0,"stats":{"Line":0}}],"covered":0,"coverable":2},{"path":["/","home","victor","ML","machinelearne-rs","benchmarks","src","bin","fair_comparison.rs"],"content":"#!/usr/bin/env rust-script\n//!\n//! Fair comparison benchmark for machinelearne-rs vs sklearn.\n//!\n//! This benchmark runs with EXACT configurations to match sklearn:\n//! - Same number of epochs (5, 50, etc.)\n//! - Same batch sizes (full batch = all training samples)\n//! - Same learning rate (0.01)\n//! - Same data preprocessing (z-score standardization)\n//!\n//! Key principle: Compare identical workloads, not different defaults.\n\nuse machinelearne_rs::{\n    backend::{CpuBackend, Tensor1D},\n    dataset::InMemoryDataset,\n    loss::MSELoss,\n    model::{linear::LinearRegression, InferenceModel},\n    optimizer::SGD,\n    regularizers::NoRegularizer,\n    trainer::Trainer,\n};\n\nuse serde_json::json;\nuse std::fs::File;\nuse std::io::Write;\nuse std::time::{Duration, Instant};\n\ntype Features = Vec\u003cVec\u003cf32\u003e\u003e;\ntype Targets = Vec\u003cf32\u003e;\n\n// ============================================================================\n// Data Loading and Preprocessing (matches sklearn exactly)\n// ============================================================================\n\nfn load_california_housing() -\u003e (Vec\u003cVec\u003cf32\u003e\u003e, Vec\u003cf32\u003e) {\n    let data = std::fs::read_to_string(\"benchmarks/datasets/california_housing.csv\")\n        .expect(\"Failed to read California Housing dataset\");\n\n    let mut features = Vec::new();\n    let mut targets = Vec::new();\n\n    for (i, line) in data.lines().enumerate() {\n        if i == 0 {\n            continue; // Skip header\n        }\n\n        let values: Vec\u003cf32\u003e = line\n            .split(',')\n            .map(|s| s.trim().parse().expect(\"Failed to parse number\"))\n            .collect();\n\n        if values.len() == 9 {\n            features.push(values[..8].to_vec());\n            targets.push(values[8]);\n        }\n    }\n\n    (features, targets)\n}\n\nfn split_train_test(\n    features: \u0026[Vec\u003cf32\u003e],\n    targets: \u0026[f32],\n) -\u003e (Features, Features, Targets, Targets) {\n    let n = features.len();\n    let train_size = (n as f64 * 0.8) as usize;\n\n    let train_features = features[..train_size].to_vec();\n    let train_targets = targets[..train_size].to_vec();\n    let test_features = features[train_size..].to_vec();\n    let test_targets = targets[train_size..].to_vec();\n\n    (train_features, test_features, train_targets, test_targets)\n}\n\nfn standardize_features(\n    train_features: \u0026[Vec\u003cf32\u003e],\n    test_features: \u0026[Vec\u003cf32\u003e],\n) -\u003e (Vec\u003cVec\u003cf32\u003e\u003e, Vec\u003cVec\u003cf32\u003e\u003e) {\n    let n_samples = train_features.len();\n    let n_features = train_features[0].len();\n\n    // Compute mean and std from training data (same as sklearn StandardScaler)\n    let mut means = vec![0.0_f32; n_features];\n    let mut stds = vec![0.0_f32; n_features];\n\n    for feature_idx in 0..n_features {\n        let sum: f32 = (0..n_samples).map(|i| train_features[i][feature_idx]).sum();\n        means[feature_idx] = sum / n_samples as f32;\n\n        // sklearn uses biased variance (divide by n, not n-1)\n        let variance: f32 = (0..n_samples)\n            .map(|i| {\n                let diff = train_features[i][feature_idx] - means[feature_idx];\n                diff * diff\n            })\n            .sum();\n        let std = (variance / n_samples as f32).sqrt();\n        // sklearn uses with_std=True which prevents division by zero\n        stds[feature_idx] = if std \u003e 1e-8 { std } else { 1.0 };\n    }\n\n    // Apply z-score to train and test data using same statistics\n    let train_scaled: Vec\u003cVec\u003cf32\u003e\u003e = (0..n_samples)\n        .map(|i| {\n            (0..n_features)\n                .map(|j| (train_features[i][j] - means[j]) / stds[j])\n                .collect()\n        })\n        .collect();\n\n    let test_scaled: Vec\u003cVec\u003cf32\u003e\u003e = test_features\n        .iter()\n        .map(|row| {\n            (0..n_features)\n                .map(|j| (row[j] - means[j]) / stds[j])\n                .collect()\n        })\n        .collect();\n\n    (train_scaled, test_scaled)\n}\n\n// ============================================================================\n// Metrics (matches sklearn exactly)\n// ============================================================================\n\nfn compute_metrics(predictions: \u0026[f32], targets: \u0026[f32]) -\u003e (f64, f64, f64) {\n    let n = targets.len();\n\n    // MSE\n    let mse: f64 = predictions\n        .iter()\n        .zip(targets.iter())\n        .map(|(p, t)| (*p as f64 - *t as f64).powi(2))\n        .sum::\u003cf64\u003e()\n        / n as f64;\n\n    // MAE\n    let mae: f64 = predictions\n        .iter()\n        .zip(targets.iter())\n        .map(|(p, t)| (*p as f64 - *t as f64).abs())\n        .sum::\u003cf64\u003e()\n        / n as f64;\n\n    // RÂ²\n    let target_mean: f64 = targets.iter().map(|t| *t as f64).sum::\u003cf64\u003e() / n as f64;\n    let ss_res: f64 = predictions\n        .iter()\n        .zip(targets.iter())\n        .map(|(p, t)| (*p as f64 - *t as f64).powi(2))\n        .sum();\n    let ss_tot: f64 = targets\n        .iter()\n        .map(|t| (*t as f64 - target_mean).powi(2))\n        .sum();\n    let r_squared = 1.0 - ss_res / ss_tot;\n\n    (mse, mae, r_squared)\n}\n\n// ============================================================================\n// Benchmark Functions\n// ============================================================================\n\nstruct BenchmarkConfig {\n    name: String,\n    n_features: usize,\n    n_epochs: usize,\n    batch_size: usize,\n    learning_rate: f64,\n    n_runs: usize,\n}\n\nstruct BenchmarkResult {\n    config: BenchmarkConfig,\n    train_time_mean_ms: f64,\n    train_time_std_ms: f64,\n    train_time_min_ms: f64,\n    time_per_epoch_ms: f64,\n    time_per_sample_us: f64,\n    mse: f64,\n    mae: f64,\n    r2: f64,\n    weights: Vec\u003cf32\u003e,\n    bias: f32,\n    samples_processed: usize,\n}\n\nfn run_single_benchmark(\n    config: \u0026BenchmarkConfig,\n    train_features: \u0026[Vec\u003cf32\u003e],\n    train_targets: \u0026[f32],\n    test_features: \u0026[Vec\u003cf32\u003e],\n    test_targets: \u0026[f32],\n) -\u003e BenchmarkResult {\n    let n_train = train_features.len();\n    let samples_per_epoch = if config.batch_size \u003e= n_train {\n        n_train\n    } else {\n        (n_train / config.batch_size + 1) * config.batch_size\n    };\n    let samples_processed = config.n_epochs * samples_per_epoch;\n\n    let mut times: Vec\u003cDuration\u003e = Vec::with_capacity(config.n_runs);\n    let mut final_weights: Vec\u003cf32\u003e = Vec::new();\n    let mut final_bias: f32 = 0.0;\n    let mut final_mse = 0.0;\n    let mut final_mae = 0.0;\n    let mut final_r2 = 0.0;\n\n    for _ in 0..config.n_runs {\n        // Create dataset\n        let train_dataset = InMemoryDataset::new(train_features.to_vec(), train_targets.to_vec())\n            .expect(\"Failed to create dataset\");\n\n        // Create model and trainer\n        let model = LinearRegression::\u003cCpuBackend\u003e::new(config.n_features);\n        let optimizer = SGD::new(config.learning_rate);\n\n        let trainer = Trainer::builder(MSELoss, optimizer, NoRegularizer)\n            .batch_size(config.batch_size)\n            .max_epochs(config.n_epochs)\n            .verbose(false) // Quiet mode for clean benchmark output\n            .build();\n\n        // Train\n        let start = Instant::now();\n        let fitted = trainer.fit(model, \u0026train_dataset).expect(\"Training failed\");\n        let elapsed = start.elapsed();\n        times.push(elapsed);\n\n        // Get predictions\n        let predictions: Vec\u003cf32\u003e = test_features\n            .iter()\n            .map(|sample| {\n                let tensor_input = Tensor1D::\u003cCpuBackend\u003e::new(sample.clone());\n                fitted.predict(\u0026tensor_input).to_f64() as f32\n            })\n            .collect();\n\n        // Compute metrics\n        let (mse, mae, r2) = compute_metrics(\u0026predictions, test_targets);\n\n        // Store final results\n        let params = fitted.extract_params();\n        final_weights = params.weights.clone();\n        final_bias = params.bias;\n        final_mse = mse;\n        final_mae = mae;\n        final_r2 = r2;\n    }\n\n    // Calculate statistics\n    let times_ms: Vec\u003cf64\u003e = times.iter().map(|t| t.as_secs_f64() * 1000.0).collect();\n    let mean_time = times_ms.iter().sum::\u003cf64\u003e() / times_ms.len() as f64;\n    let std_time = if times_ms.len() \u003e 1 {\n        let variance = times_ms\n            .iter()\n            .map(|t| (t - mean_time).powi(2))\n            .sum::\u003cf64\u003e()\n            / (times_ms.len() - 1) as f64;\n        variance.sqrt()\n    } else {\n        0.0\n    };\n    let min_time = times_ms.iter().cloned().fold(f64::INFINITY, f64::min);\n\n    BenchmarkResult {\n        config: BenchmarkConfig {\n            name: config.name.clone(),\n            ..config.clone()\n        },\n        train_time_mean_ms: mean_time,\n        train_time_std_ms: std_time,\n        train_time_min_ms: min_time,\n        time_per_epoch_ms: mean_time / config.n_epochs as f64,\n        time_per_sample_us: mean_time * 1e6 / samples_processed as f64,\n        mse: final_mse,\n        mae: final_mae,\n        r2: final_r2,\n        weights: final_weights,\n        bias: final_bias,\n        samples_processed,\n    }\n}\n\nimpl Clone for BenchmarkConfig {\n    fn clone(\u0026self) -\u003e Self {\n        Self {\n            name: self.name.clone(),\n            n_features: self.n_features,\n            n_epochs: self.n_epochs,\n            batch_size: self.batch_size,\n            learning_rate: self.learning_rate,\n            n_runs: self.n_runs,\n        }\n    }\n}\n\nfn run_all_benchmarks() -\u003e Vec\u003cserde_json::Value\u003e {\n    println!(\"{}\", \"=\".repeat(70));\n    println!(\"MACHINELEARN-RS FAIR COMPARISON BENCHMARKS\");\n    println!(\"{}\", \"=\".repeat(70));\n\n    // Load and preprocess data\n    let (features, targets) = load_california_housing();\n    println!(\n        \"\\nLoaded {} samples with {} features\",\n        features.len(),\n        features[0].len()\n    );\n\n    let (train_features_all, test_features_all, train_targets, test_targets) =\n        split_train_test(\u0026features, \u0026targets);\n\n    let (train_scaled_all, test_scaled_all) =\n        standardize_features(\u0026train_features_all, \u0026test_features_all);\n\n    println!(\n        \"Train: {} samples, Test: {} samples\",\n        train_scaled_all.len(),\n        test_scaled_all.len()\n    );\n\n    let n_train = train_scaled_all.len();\n    let mut results = Vec::new();\n\n    // Define benchmark configurations\n    // Test different combinations of batch size and learning rate\n    let configs = vec![\n        // 2 features: Full batch with optimal LR=0.5\n        BenchmarkConfig {\n            name: \"2feat_full_lr0.5_e5\".to_string(),\n            n_features: 2,\n            n_epochs: 5,\n            batch_size: n_train,\n            learning_rate: 0.5,\n            n_runs: 10,\n        },\n        BenchmarkConfig {\n            name: \"2feat_full_lr0.5_e50\".to_string(),\n            n_features: 2,\n            n_epochs: 50,\n            batch_size: n_train,\n            learning_rate: 0.5,\n            n_runs: 5,\n        },\n        // 2 features: Full batch with sklearn-compatible LR=0.1\n        BenchmarkConfig {\n            name: \"2feat_full_lr0.1_e5\".to_string(),\n            n_features: 2,\n            n_epochs: 5,\n            batch_size: n_train,\n            learning_rate: 0.1,\n            n_runs: 10,\n        },\n        BenchmarkConfig {\n            name: \"2feat_full_lr0.1_e50\".to_string(),\n            n_features: 2,\n            n_epochs: 50,\n            batch_size: n_train,\n            learning_rate: 0.1,\n            n_runs: 5,\n        },\n        // 2 features: Mini-batch with default LR=0.01\n        BenchmarkConfig {\n            name: \"2feat_minibatch_lr0.01_e5\".to_string(),\n            n_features: 2,\n            n_epochs: 5,\n            batch_size: 32,\n            learning_rate: 0.01,\n            n_runs: 10,\n        },\n        BenchmarkConfig {\n            name: \"2feat_minibatch_lr0.01_e50\".to_string(),\n            n_features: 2,\n            n_epochs: 50,\n            batch_size: 32,\n            learning_rate: 0.01,\n            n_runs: 5,\n        },\n        // 8 features: Full batch\n        BenchmarkConfig {\n            name: \"8feat_full_lr0.1_e5\".to_string(),\n            n_features: 8,\n            n_epochs: 5,\n            batch_size: n_train,\n            learning_rate: 0.1,\n            n_runs: 10,\n        },\n        BenchmarkConfig {\n            name: \"8feat_full_lr0.1_e50\".to_string(),\n            n_features: 8,\n            n_epochs: 50,\n            batch_size: n_train,\n            learning_rate: 0.1,\n            n_runs: 5,\n        },\n    ];\n\n    for config in configs {\n        println!(\"\\n{}\", \"-\".repeat(50));\n        println!(\n            \"Test: {} ({} features, {} epochs, batch={}, lr={})\",\n            config.name,\n            config.n_features,\n            config.n_epochs,\n            config.batch_size,\n            config.learning_rate\n        );\n\n        // Select features\n        let n_feat = config.n_features;\n        let train_features: Vec\u003cVec\u003cf32\u003e\u003e = train_scaled_all\n            .iter()\n            .map(|row| row[..n_feat].to_vec())\n            .collect();\n        let test_features: Vec\u003cVec\u003cf32\u003e\u003e = test_scaled_all\n            .iter()\n            .map(|row| row[..n_feat].to_vec())\n            .collect();\n\n        let result = run_single_benchmark(\n            \u0026config,\n            \u0026train_features,\n            \u0026train_targets,\n            \u0026test_features,\n            \u0026test_targets,\n        );\n\n        println!(\n            \"  Time: {:.3} +/- {:.3} ms\",\n            result.train_time_mean_ms, result.train_time_std_ms\n        );\n        println!(\"  Time per epoch: {:.3} ms\", result.time_per_epoch_ms);\n        println!(\"  Time per sample: {:.4} us\", result.time_per_sample_us);\n        println!(\"  MSE: {:.6}, R2: {:.6}\", result.mse, result.r2);\n        println!(\"  Weights: {:?}, Bias: {:.6}\", result.weights, result.bias);\n\n        results.push(json!({\n            \"test\": result.config.name,\n            \"implementation\": \"rust\",\n            \"n_features\": result.config.n_features,\n            \"n_epochs\": result.config.n_epochs,\n            \"batch_size\": result.config.batch_size,\n            \"learning_rate\": result.config.learning_rate,\n            \"n_train_samples\": n_train,\n            \"n_test_samples\": test_features.len(),\n            \"samples_processed\": result.samples_processed,\n            \"train_time_mean_ms\": result.train_time_mean_ms,\n            \"train_time_std_ms\": result.train_time_std_ms,\n            \"train_time_min_ms\": result.train_time_min_ms,\n            \"time_per_epoch_ms\": result.time_per_epoch_ms,\n            \"time_per_sample_us\": result.time_per_sample_us,\n            \"mse\": result.mse,\n            \"mae\": result.mae,\n            \"r2\": result.r2,\n            \"weights\": result.weights,\n            \"bias\": result.bias,\n            \"n_runs\": result.config.n_runs,\n        }));\n    }\n\n    results\n}\n\nfn main() {\n    let results = run_all_benchmarks();\n\n    // Save results\n    let output = json!({\n        \"results\": results\n    });\n\n    let mut file = File::create(\"benchmarks/results/rust_fair_comparison.json\")\n        .expect(\"Failed to create output file\");\n    file.write_all(serde_json::to_string_pretty(\u0026output).unwrap().as_bytes())\n        .expect(\"Failed to write to file\");\n\n    println!(\"\\nResults saved to benchmarks/results/rust_fair_comparison.json\");\n\n    // Print summary\n    println!(\"\\n{}\", \"=\".repeat(70));\n    println!(\"BENCHMARK COMPLETE\");\n    println!(\"{}\", \"=\".repeat(70));\n\n    println!(\"\\nLegend:\");\n    println!(\"  full = full batch (all training samples)\");\n    println!(\"  minibatch = batch size 32\");\n    println!(\"  lr = learning rate\");\n    println!(\"  e = epochs\");\n\n    println!(\"\\nResults:\");\n    println!(\n        \"{:\u003c30} {:\u003e10} {:\u003e10} {:\u003e10} {:\u003e8}\",\n        \"Test\", \"Time (ms)\", \"ms/epoch\", \"MSE\", \"R2\"\n    );\n    println!(\"{}\", \"-\".repeat(70));\n\n    for r in \u0026results {\n        let test = r[\"test\"].as_str().unwrap();\n        let time_ms = r[\"train_time_mean_ms\"].as_f64().unwrap();\n        let time_per_epoch = r[\"time_per_epoch_ms\"].as_f64().unwrap();\n        let mse = r[\"mse\"].as_f64().unwrap();\n        let r2 = r[\"r2\"].as_f64().unwrap();\n\n        println!(\n            \"{:\u003c30} {:\u003e10.2} {:\u003e10.3} {:\u003e10.4} {:\u003e8.4}\",\n            test, time_ms, time_per_epoch, mse, r2\n        );\n    }\n}\n","traces":[],"covered":0,"coverable":0},{"path":["/","home","victor","ML","machinelearne-rs","benchmarks","src","bin","full_batch_comparison.rs"],"content":"use machinelearne_rs::{\n    dataset::InMemoryDataset, loss::MSELoss, model::linear::LinearRegression,\n    model::InferenceModel, optimizer::SGD, regularizers::NoRegularizer, trainer::TrainerBuilder,\n    Tensor1D,\n};\n\nuse std::time::Instant;\n\nuse machinelearne_rs::backend::CpuBackend;\n\ntype Features = Vec\u003cVec\u003cf32\u003e\u003e;\ntype Targets = Vec\u003cf32\u003e;\n\nfn load_california_housing() -\u003e (Vec\u003cVec\u003cf32\u003e\u003e, Vec\u003cf32\u003e) {\n    // Load CSV and parse\n    let data = std::fs::read_to_string(\"benchmarks/datasets/california_housing.csv\")\n        .expect(\"Failed to read California Housing dataset\");\n\n    let mut features = Vec::new();\n    let mut targets = Vec::new();\n\n    for (i, line) in data.lines().enumerate() {\n        if i == 0 {\n            continue; // Skip header\n        }\n\n        let values: Vec\u003cf32\u003e = line\n            .split(',')\n            .map(|s| s.trim().parse().expect(\"Failed to parse number\"))\n            .collect();\n\n        if values.len() == 9 {\n            features.push(values[..8].to_vec());\n            targets.push(values[8]);\n        }\n    }\n\n    (features, targets)\n}\n\nfn split_train_test(\n    features: \u0026[Vec\u003cf32\u003e],\n    targets: \u0026[f32],\n) -\u003e (Features, Features, Targets, Targets) {\n    let n = features.len();\n    let train_size = (n as f64 * 0.8) as usize;\n\n    let train_features = features[..train_size].to_vec();\n    let train_targets = targets[..train_size].to_vec();\n    let test_features = features[train_size..].to_vec();\n    let test_targets = targets[train_size..].to_vec();\n\n    (train_features, test_features, train_targets, test_targets)\n}\n\nfn standardize_features(\n    train_features: \u0026[Vec\u003cf32\u003e],\n    test_features: \u0026[Vec\u003cf32\u003e],\n) -\u003e (Vec\u003cVec\u003cf32\u003e\u003e, Vec\u003cVec\u003cf32\u003e\u003e) {\n    let n_samples = train_features.len();\n    let n_features = train_features[0].len();\n\n    // Compute mean and std from training data\n    let mut means = vec![0.0_f32; n_features];\n    let mut stds = vec![0.0_f32; n_features];\n\n    for feature_idx in 0..n_features {\n        let sum: f32 = (0..n_samples).map(|i| train_features[i][feature_idx]).sum();\n        means[feature_idx] = sum / n_samples as f32;\n\n        let variance: f32 = (0..n_samples)\n            .map(|i| {\n                let diff = train_features[i][feature_idx] - means[feature_idx];\n                diff * diff\n            })\n            .sum();\n        let std = (variance / n_samples as f32).sqrt();\n        stds[feature_idx] = if std \u003e 1e-8 { std } else { 1.0 };\n    }\n\n    // Apply z-score to train and test data using same statistics\n    let train_scaled: Vec\u003cVec\u003cf32\u003e\u003e = (0..n_samples)\n        .map(|i| {\n            (0..n_features)\n                .map(|j| (train_features[i][j] - means[j]) / stds[j])\n                .collect()\n        })\n        .collect();\n\n    let test_scaled: Vec\u003cVec\u003cf32\u003e\u003e = test_features\n        .iter()\n        .map(|row| {\n            (0..n_features)\n                .map(|j| (row[j] - means[j]) / stds[j])\n                .collect()\n        })\n        .collect();\n\n    (train_scaled, test_scaled)\n}\n\nfn compute_metrics(predictions: \u0026[f32], targets: \u0026[f32]) -\u003e (f64, f64, f64) {\n    let n = targets.len();\n    let mse: f64 = predictions\n        .iter()\n        .zip(targets.iter())\n        .map(|(p, t)| (p - t) as f64 * (p - t) as f64)\n        .sum::\u003cf64\u003e()\n        / n as f64;\n\n    let mae: f64 = predictions\n        .iter()\n        .zip(targets.iter())\n        .map(|(p, t)| (p - t).abs() as f64)\n        .sum::\u003cf64\u003e()\n        / n as f64;\n\n    let target_mean: f64 = targets.iter().map(|t| *t as f64).sum::\u003cf64\u003e() / n as f64;\n    let ss_res: f64 = predictions\n        .iter()\n        .zip(targets.iter())\n        .map(|(p, t)| (p - t) as f64 * (p - t) as f64)\n        .sum();\n    let ss_tot: f64 = targets\n        .iter()\n        .map(|t| (*t as f64 - target_mean) * (*t as f64 - target_mean))\n        .sum();\n    let r_squared = 1.0 - ss_res / ss_tot;\n\n    (mse, mae, r_squared)\n}\n\nfn main() {\n    println!(\"Full-Batch vs Mini-Batch Comparison for Rust\");\n    println!(\"==============================================\\n\");\n\n    let (features, targets) = load_california_housing();\n    println!(\n        \"Loaded {} samples with {} features\\n\",\n        features.len(),\n        features[0].len()\n    );\n\n    let (train_features, test_features, train_targets, test_targets) =\n        split_train_test(\u0026features, \u0026targets);\n    let (train_features_scaled, test_features_scaled) =\n        standardize_features(\u0026train_features, \u0026test_features);\n\n    // Test configurations\n    let test_cases = vec![\n        (2, 5, 16512, \"Full-batch, 5 epochs (like sklearn)\"),\n        (2, 50, 16512, \"Full-batch, 50 epochs\"),\n        (2, 50, 32, \"Mini-batch (bs=32), 50 epochs (current default)\"),\n        (2, 1000, 32, \"Mini-batch (bs=32), 1000 epochs\"),\n    ];\n\n    for (n_features, n_epochs, batch_size, desc) in test_cases {\n        println!(\"Test: {}\", desc);\n        println!(\n            \"Features: {}, Epochs: {}, Batch size: {}\",\n            n_features, n_epochs, batch_size\n        );\n\n        // Select features\n        let train_selected: Vec\u003cVec\u003cf32\u003e\u003e = train_features_scaled\n            .iter()\n            .map(|row| row[..n_features].to_vec())\n            .collect();\n        let test_selected: Vec\u003cVec\u003cf32\u003e\u003e = test_features_scaled\n            .iter()\n            .map(|row| row[..n_features].to_vec())\n            .collect();\n\n        let train_dataset = InMemoryDataset::new(train_selected.clone(), train_targets.clone())\n            .expect(\"Failed to create dataset\");\n\n        // Train with CPU backend\n        let start = Instant::now();\n        let model_cpu = LinearRegression::\u003cCpuBackend\u003e::new(n_features);\n\n        let loss_fn = MSELoss;\n        let optimizer = SGD::new(0.01);\n        let regularizer = NoRegularizer;\n\n        let trainer_cpu = TrainerBuilder::new(loss_fn, optimizer, regularizer)\n            .batch_size(batch_size)\n            .max_epochs(n_epochs)\n            .build();\n\n        let fitted_cpu = trainer_cpu\n            .fit(model_cpu, \u0026train_dataset)\n            .expect(\"Training failed\");\n        let training_time_cpu = start.elapsed();\n\n        // Predictions\n        let predictions_cpu: Vec\u003cf32\u003e = test_selected\n            .iter()\n            .map(|sample| {\n                let tensor_input = Tensor1D::\u003cCpuBackend\u003e::new(sample.clone());\n                let result = fitted_cpu.predict(\u0026tensor_input);\n                result.to_f64() as f32\n            })\n            .collect();\n\n        // Metrics\n        let (mse, mae, r_squared) = compute_metrics(\u0026predictions_cpu, \u0026test_targets);\n\n        println!(\"CPU Backend:\");\n        println!(\n            \"  Training time: {:.2} ms\",\n            training_time_cpu.as_secs_f64() * 1000.0\n        );\n        println!(\"  MSE: {:.4}\", mse);\n        println!(\"  MAE: {:.4}\", mae);\n        println!(\"  RÂ²: {:.4}\", r_squared);\n        println!();\n    }\n\n    // Compare with sklearn numbers\n    println!(\"Sklearn SGDRegressor (for reference):\");\n    println!(\"  lr_constant: 5.09 ms (2 features, full-batch, converges in 5 epochs)\");\n    println!(\"  lr_adaptive: 25.38 ms (2 features, full-batch, converges in 5 epochs)\");\n    println!(\"  Training details (verbose output):\");\n    println!(\"    -- Epoch 1, T: 100, Avg. loss: 0.584\");\n    println!(\"    -- Epoch 2, T: 200, Avg. loss: 0.574\");\n    println!(\"    -- Epoch 3, T: 300, Avg. loss: 0.571\");\n    println!(\"    -- Epoch 4, T: 400, Avg. loss: 0.570\");\n    println!(\"    -- Epoch 5, T: 500, Avg. loss: 0.566\");\n    println!(\"  Converges in 5 epochs (not full 1000)\");\n}\n","traces":[],"covered":0,"coverable":0},{"path":["/","home","victor","ML","machinelearne-rs","benchmarks","src","bin","learning_rate_search.rs"],"content":"use machinelearne_rs::{\n    dataset::InMemoryDataset, loss::MSELoss, model::linear::LinearRegression,\n    model::InferenceModel, optimizer::SGD, regularizers::NoRegularizer, trainer::TrainerBuilder,\n    Tensor1D,\n};\n\nuse std::time::Instant;\n\nuse machinelearne_rs::backend::CpuBackend;\n\ntype Features = Vec\u003cVec\u003cf32\u003e\u003e;\ntype Targets = Vec\u003cf32\u003e;\n\nfn load_california_housing() -\u003e (Vec\u003cVec\u003cf32\u003e\u003e, Vec\u003cf32\u003e) {\n    // Load CSV and parse\n    let data = std::fs::read_to_string(\"benchmarks/datasets/california_housing.csv\")\n        .expect(\"Failed to read California Housing dataset\");\n\n    let mut features = Vec::new();\n    let mut targets = Vec::new();\n\n    for (i, line) in data.lines().enumerate() {\n        if i == 0 {\n            continue; // Skip header\n        }\n\n        let values: Vec\u003cf32\u003e = line\n            .split(',')\n            .map(|s| s.trim().parse().expect(\"Failed to parse number\"))\n            .collect();\n\n        if values.len() == 9 {\n            features.push(values[..8].to_vec());\n            targets.push(values[8]);\n        }\n    }\n\n    (features, targets)\n}\n\nfn split_train_test(\n    features: \u0026[Vec\u003cf32\u003e],\n    targets: \u0026[f32],\n) -\u003e (Features, Features, Targets, Targets) {\n    let n = features.len();\n    let train_size = (n as f64 * 0.8) as usize;\n\n    let train_features = features[..train_size].to_vec();\n    let train_targets = targets[..train_size].to_vec();\n    let test_features = features[train_size..].to_vec();\n    let test_targets = targets[train_size..].to_vec();\n\n    (train_features, test_features, train_targets, test_targets)\n}\n\nfn standardize_features(\n    train_features: \u0026[Vec\u003cf32\u003e],\n    test_features: \u0026[Vec\u003cf32\u003e],\n) -\u003e (Vec\u003cVec\u003cf32\u003e\u003e, Vec\u003cVec\u003cf32\u003e\u003e) {\n    let n_samples = train_features.len();\n    let n_features = train_features[0].len();\n\n    // Compute mean and std from training data\n    let mut means = vec![0.0_f32; n_features];\n    let mut stds = vec![0.0_f32; n_features];\n\n    for feature_idx in 0..n_features {\n        let sum: f32 = (0..n_samples).map(|i| train_features[i][feature_idx]).sum();\n        means[feature_idx] = sum / n_samples as f32;\n\n        let variance: f32 = (0..n_samples)\n            .map(|i| {\n                let diff = train_features[i][feature_idx] - means[feature_idx];\n                diff * diff\n            })\n            .sum();\n        let std = (variance / n_samples as f32).sqrt();\n        stds[feature_idx] = if std \u003e 1e-8 { std } else { 1.0 };\n    }\n\n    // Apply z-score to train and test data using same statistics\n    let train_scaled: Vec\u003cVec\u003cf32\u003e\u003e = (0..n_samples)\n        .map(|i| {\n            (0..n_features)\n                .map(|j| (train_features[i][j] - means[j]) / stds[j])\n                .collect()\n        })\n        .collect();\n\n    let test_scaled: Vec\u003cVec\u003cf32\u003e\u003e = test_features\n        .iter()\n        .map(|row| {\n            (0..n_features)\n                .map(|j| (row[j] - means[j]) / stds[j])\n                .collect()\n        })\n        .collect();\n\n    (train_scaled, test_scaled)\n}\n\nfn compute_metrics(predictions: \u0026[f32], targets: \u0026[f32]) -\u003e (f64, f64, f64) {\n    let n = targets.len();\n    let mse: f64 = predictions\n        .iter()\n        .zip(targets.iter())\n        .map(|(p, t)| (p - t) as f64 * (p - t) as f64)\n        .sum::\u003cf64\u003e()\n        / n as f64;\n\n    let mae: f64 = predictions\n        .iter()\n        .zip(targets.iter())\n        .map(|(p, t)| (p - t).abs() as f64)\n        .sum::\u003cf64\u003e()\n        / n as f64;\n\n    let target_mean: f64 = targets.iter().map(|t| *t as f64).sum::\u003cf64\u003e() / n as f64;\n    let ss_res: f64 = predictions\n        .iter()\n        .zip(targets.iter())\n        .map(|(p, t)| (p - t) as f64 * (p - t) as f64)\n        .sum();\n    let ss_tot: f64 = targets\n        .iter()\n        .map(|t| (*t as f64 - target_mean) * (*t as f64 - target_mean))\n        .sum();\n    let r_squared = 1.0 - ss_res / ss_tot;\n\n    (mse, mae, r_squared)\n}\n\nfn main() {\n    println!(\"Learning Rate Search for Full-Batch Training\");\n    println!(\"===========================================\\n\");\n\n    let (features, targets) = load_california_housing();\n    println!(\n        \"Loaded {} samples with {} features\\n\",\n        features.len(),\n        features[0].len()\n    );\n\n    let (train_features, test_features, train_targets, test_targets) =\n        split_train_test(\u0026features, \u0026targets);\n    let (train_features_scaled, test_features_scaled) =\n        standardize_features(\u0026train_features, \u0026test_features);\n\n    let n_features = 2;\n    let n_epochs = 5;\n    let batch_size = 16512; // Full batch (all training samples)\n\n    // Learning rates to test (logarithmic scale)\n    let learning_rates = vec![\n        1e-4, 2e-4, 5e-4, 1e-3, 2e-3, 5e-3, 1e-2, 2e-2, 5e-2, 1e-1, 2e-1, 5e-1,\n    ];\n\n    println!(\n        \"Testing {} learning rates ({} epochs, batch size = {})\\n\",\n        learning_rates.len(),\n        n_epochs,\n        batch_size\n    );\n\n    let mut best_lr = 0.0;\n    let mut best_mse = f64::MAX;\n    let mut best_r2 = f64::MIN;\n\n    for lr in learning_rates {\n        // Select features\n        let train_selected: Vec\u003cVec\u003cf32\u003e\u003e = train_features_scaled\n            .iter()\n            .map(|row| row[..n_features].to_vec())\n            .collect();\n        let test_selected: Vec\u003cVec\u003cf32\u003e\u003e = test_features_scaled\n            .iter()\n            .map(|row| row[..n_features].to_vec())\n            .collect();\n\n        let train_dataset = InMemoryDataset::new(train_selected.clone(), train_targets.clone())\n            .expect(\"Failed to create dataset\");\n\n        // Train with CPU backend\n        let start = Instant::now();\n        let model_cpu = LinearRegression::\u003cCpuBackend\u003e::new(n_features);\n\n        let loss_fn = MSELoss;\n        let optimizer = SGD::new(lr);\n        let regularizer = NoRegularizer;\n\n        let trainer_cpu = TrainerBuilder::new(loss_fn, optimizer, regularizer)\n            .batch_size(batch_size)\n            .max_epochs(n_epochs)\n            .build();\n\n        let fitted_cpu = trainer_cpu\n            .fit(model_cpu, \u0026train_dataset)\n            .expect(\"Training failed\");\n        let training_time = start.elapsed();\n\n        // Predictions\n        let predictions: Vec\u003cf32\u003e = test_selected\n            .iter()\n            .map(|sample| {\n                let tensor_input = Tensor1D::\u003cCpuBackend\u003e::new(sample.clone());\n                let result = fitted_cpu.predict(\u0026tensor_input);\n                result.to_f64() as f32\n            })\n            .collect();\n\n        // Metrics\n        let (mse, mae, r_squared) = compute_metrics(\u0026predictions, \u0026test_targets);\n\n        println!(\n            \"LR = {:.6}: Time = {:.2} ms, MSE = {:.4}, MAE = {:.4}, RÂ² = {:.4}\",\n            lr,\n            training_time.as_secs_f64() * 1000.0,\n            mse,\n            mae,\n            r_squared\n        );\n\n        // Track best\n        if mse \u003c best_mse {\n            best_mse = mse;\n            best_r2 = r_squared;\n            best_lr = lr;\n        }\n    }\n\n    println!(\"\\n=== Best Learning Rate ===\");\n    println!(\"LR = {:.6}\", best_lr);\n    println!(\"MSE = {:.4}\", best_mse);\n    println!(\"RÂ² = {:.4}\", best_r2);\n\n    println!(\"\\n=== Sklearn Comparison ===\");\n    println!(\"Sklearn lr_constant: 5.09 ms, MSE = 0.6697, RÂ² = 0.4889\");\n    println!(\"Sklearn lr_adaptive: 25.38 ms, MSE = 0.6630, RÂ² = 0.4941\");\n\n    if best_lr \u003e 0.0 {\n        println!(\"Rust is competitive at LR = {:.6}\", best_lr);\n    }\n}\n","traces":[],"covered":0,"coverable":0},{"path":["/","home","victor","ML","machinelearne-rs","benchmarks","src","bin","sgd_comparison.rs"],"content":"#!/usr/bin/env rust-script\n//!\n//! Compare Rust SGD performance against sklearn SGDRegressor\n//!\n//! Matches sklearn's SGDRegressor configuration:\n//! - max_iter=1000\n//! - tol=1e-3 (early stopping)\n//! - learning_rate=0.01\n//! - StandardScaler for feature scaling\n\nuse benchmarks::data::CaliforniaHousingDataset;\nuse benchmarks::metrics::Metrics;\nuse machinelearne_rs::{\n    backend::Tensor2D,\n    dataset::InMemoryDataset,\n    loss::MSELoss,\n    model::{linear::LinearRegression, InferenceModel},\n    optimizer::SGD,\n    regularizers::NoRegularizer,\n    trainer::Trainer,\n    CpuBackend,\n};\nuse serde_json::json;\nuse std::fs::File;\nuse std::io::Write;\nuse std::time::Instant;\n\n/// Helper function to convert Vec\u003cVec\u003cf32\u003e\u003e to Tensor2D\u003cCpuBackend\u003e\nfn vec_to_tensor2d(data: \u0026[Vec\u003cf32\u003e]) -\u003e Tensor2D\u003cCpuBackend\u003e {\n    let flat: Vec\u003cf32\u003e = data.iter().flatten().copied().collect();\n    Tensor2D::new(flat, data.len(), data.first().map(|v| v.len()).unwrap_or(0))\n}\n\n/// Apply z-score standardization to features\nfn standardize_features(\n    train_features: \u0026[Vec\u003cf32\u003e],\n    test_features: \u0026[Vec\u003cf32\u003e],\n) -\u003e (Vec\u003cVec\u003cf32\u003e\u003e, Vec\u003cVec\u003cf32\u003e\u003e) {\n    let n_samples = train_features.len();\n    let n_features = if n_samples \u003e 0 {\n        train_features[0].len()\n    } else {\n        return (train_features.to_vec(), test_features.to_vec());\n    };\n\n    // Compute mean and std for each feature from training data\n    let mut means = vec![0.0_f32; n_features];\n    let mut stds = vec![0.0_f32; n_features];\n\n    for feature_idx in 0..n_features {\n        // Compute mean\n        let sum: f32 = (0..n_samples).map(|i| train_features[i][feature_idx]).sum();\n        means[feature_idx] = sum / n_samples as f32;\n\n        // Compute variance\n        let variance: f32 = (0..n_samples)\n            .map(|i| {\n                let diff = train_features[i][feature_idx] - means[feature_idx];\n                diff * diff\n            })\n            .sum();\n        stds[feature_idx] = f32::sqrt(variance / n_samples as f32);\n\n        // Prevent division by zero\n        if stds[feature_idx] \u003c 1e-6 {\n            stds[feature_idx] = 1.0;\n        }\n    }\n\n    // Apply z-score to training data\n    let train_scaled: Vec\u003cVec\u003cf32\u003e\u003e = (0..n_samples)\n        .map(|i| {\n            (0..n_features)\n                .map(|j| (train_features[i][j] - means[j]) / stds[j])\n                .collect()\n        })\n        .collect();\n\n    // Apply z-score to test data (using train's mean/std)\n    let test_scaled: Vec\u003cVec\u003cf32\u003e\u003e = (0..test_features.len())\n        .map(|i| {\n            (0..n_features)\n                .map(|j| (test_features[i][j] - means[j]) / stds[j])\n                .collect()\n        })\n        .collect();\n\n    (train_scaled, test_scaled)\n}\n\n/// Collect metrics with different configurations\nfn run_comparison() -\u003e serde_json::Value {\n    let dataset = CaliforniaHousingDataset::load(\"benchmarks/datasets/california_housing.csv\")\n        .expect(\"Failed to load dataset\");\n\n    // Test with 2 features (same as sklearn benchmark)\n    let feature_indices: Vec\u003cusize\u003e = vec![0, 1];\n    let subset = dataset.select_features(\u0026feature_indices);\n    let (train_dataset, _, test_dataset) = subset.split_train_val_test(0.8, 0.0);\n\n    let train_features = train_dataset.features();\n    let train_target = train_dataset.target();\n    let test_features = test_dataset.features();\n    let test_target = test_dataset.target();\n\n    // Apply z-score standardization\n    let (train_features_scaled, test_features_scaled) =\n        standardize_features(train_features, test_features);\n\n    let train_memory = InMemoryDataset::new(train_features_scaled, train_target.to_vec())\n        .expect(\"Failed to create training dataset\");\n\n    let mut results = Vec::new();\n\n    // Configuration 1: 50 epochs (current config)\n    let lr = 0.01;\n    let model1 = LinearRegression::\u003cCpuBackend\u003e::new(2);\n    let optimizer = SGD::\u003cCpuBackend\u003e::new(lr);\n\n    let trainer = Trainer::builder(MSELoss, optimizer, NoRegularizer)\n        .batch_size(32)\n        .max_epochs(50)\n        .build();\n\n    let start = Instant::now();\n    let fitted1 = trainer\n        .fit(model1, \u0026train_memory)\n        .expect(\"Failed to fit model\");\n    let train_time_50 = start.elapsed().as_millis();\n\n    let test_tensor = vec_to_tensor2d(\u0026test_features_scaled);\n    let pred_tensor = fitted1.predict_batch(\u0026test_tensor);\n    let predictions: Vec\u003cf32\u003e = pred_tensor.to_vec().into_iter().map(|v| v as f32).collect();\n\n    results.push(json!({\n        \"config\": \"50 epochs\",\n        \"train_time_ms\": train_time_50,\n        \"mse\": Metrics::mse(test_target, \u0026predictions),\n        \"mae\": Metrics::mae(test_target, \u0026predictions),\n        \"r2\": Metrics::r_squared(test_target, \u0026predictions),\n    }));\n\n    // Configuration 2: 1000 epochs (matching sklearn)\n    let model2 = LinearRegression::\u003cCpuBackend\u003e::new(2);\n    let optimizer2 = SGD::\u003cCpuBackend\u003e::new(lr);\n\n    let trainer2 = Trainer::builder(MSELoss, optimizer2, NoRegularizer)\n        .batch_size(32)\n        .max_epochs(1000)\n        .build();\n\n    let start = Instant::now();\n    let fitted2 = trainer2\n        .fit(model2, \u0026train_memory)\n        .expect(\"Failed to fit model\");\n    let train_time_1000 = start.elapsed().as_millis();\n\n    let pred_tensor2 = fitted2.predict_batch(\u0026test_tensor);\n    let predictions2: Vec\u003cf32\u003e = pred_tensor2\n        .to_vec()\n        .into_iter()\n        .map(|v| v as f32)\n        .collect();\n\n    results.push(json!({\n        \"config\": \"1000 epochs (sklearn match)\",\n        \"train_time_ms\": train_time_1000,\n        \"mse\": Metrics::mse(test_target, \u0026predictions2),\n        \"mae\": Metrics::mae(test_target, \u0026predictions2),\n        \"r2\": Metrics::r_squared(test_target, \u0026predictions2),\n    }));\n\n    // Configuration 3: 1000 epochs with larger batch size\n    let model3 = LinearRegression::\u003cCpuBackend\u003e::new(2);\n    let optimizer3 = SGD::\u003cCpuBackend\u003e::new(lr);\n\n    let trainer3 = Trainer::builder(MSELoss, optimizer3, NoRegularizer)\n        .batch_size(128)\n        .max_epochs(1000)\n        .build();\n\n    let start = Instant::now();\n    let fitted3 = trainer3\n        .fit(model3, \u0026train_memory)\n        .expect(\"Failed to fit model\");\n    let train_time_1000_bs128 = start.elapsed().as_millis();\n\n    let pred_tensor3 = fitted3.predict_batch(\u0026test_tensor);\n    let predictions3: Vec\u003cf32\u003e = pred_tensor3\n        .to_vec()\n        .into_iter()\n        .map(|v| v as f32)\n        .collect();\n\n    results.push(json!({\n        \"config\": \"1000 epochs, batch_size=128\",\n        \"train_time_ms\": train_time_1000_bs128,\n        \"mse\": Metrics::mse(test_target, \u0026predictions3),\n        \"mae\": Metrics::mae(test_target, \u0026predictions3),\n        \"r2\": Metrics::r_squared(test_target, \u0026predictions3),\n    }));\n\n    json!({\n        \"n_features\": 2,\n        \"model\": \"LinearRegression\",\n        \"results\": results\n    })\n}\n\nfn main() {\n    println!(\"Running SGD comparison benchmarks...\");\n    let result = run_comparison();\n\n    // Write to file\n    let mut file = File::create(\"benchmarks/results/rust_sgd_comparison.json\")\n        .expect(\"Failed to create output file\");\n    file.write_all(serde_json::to_string_pretty(\u0026result).unwrap().as_bytes())\n        .expect(\"Failed to write to file\");\n\n    println!(\"\\nResults saved to benchmarks/results/rust_sgd_comparison.json\");\n\n    // Print summary\n    let results = result[\"results\"].as_array().unwrap();\n    println!(\"\\nRust SGD Performance:\");\n    println!(\n        \"{:\u003c30} {:\u003e12} {:\u003e10} {:\u003e10} {:\u003e10}\",\n        \"Config\", \"Time (ms)\", \"MSE\", \"MAE\", \"RÂ²\"\n    );\n    for r in results {\n        let config = r[\"config\"].as_str().unwrap();\n        let time = r[\"train_time_ms\"].as_f64().unwrap();\n        let mse = r[\"mse\"].as_f64().unwrap();\n        let mae = r[\"mae\"].as_f64().unwrap();\n        let r2 = r[\"r2\"].as_f64().unwrap();\n        println!(\n            \"{:\u003c30} {:\u003e12.2} {:\u003e10.4} {:\u003e10.4} {:\u003e10.4}\",\n            config, time, mse, mae, r2\n        );\n    }\n\n    println!(\"\\nSklearn SGDRegressor (for comparison):\");\n    println!(\n        \"{:\u003c30} {:\u003e12} {:\u003e10} {:\u003e10} {:\u003e10}\",\n        \"Config\", \"Time (ms)\", \"MSE\", \"MAE\", \"RÂ²\"\n    );\n    println!(\n        \"{:\u003c30} {:\u003e12.2} {:\u003e10.4} {:\u003e10.4} {:\u003e10.4}\",\n        \"lr_constant (1000 iter)\", 5.09, 0.6697, 0.5978, 0.4889\n    );\n    println!(\n        \"{:\u003c30} {:\u003e12.2} {:\u003e10.4} {:\u003e10.4} {:\u003e10.4}\",\n        \"lr_adaptive (1000 iter)\", 25.38, 0.6630, 0.6061, 0.4941\n    );\n}\n","traces":[],"covered":0,"coverable":0},{"path":["/","home","victor","ML","machinelearne-rs","benchmarks","src","data","california_housing.rs"],"content":"use csv::ReaderBuilder;\nuse machinelearne_rs::dataset::InMemoryDataset;\nuse std::fs::File;\nuse std::io::{self, BufReader};\n\n/// California Housing dataset loader.\n///\n/// Loads the California Housing dataset from CSV and provides methods\n/// to select subsets of features and split into train/validation/test sets.\n///\n/// The dataset contains 20640 samples with 8 features:\n/// - MedInc: Median income in block group\n/// - HouseAge: Median house age in block group\n/// - AveRooms: Average number of rooms per household\n/// - AveBedrms: Average number of bedrooms per household\n/// - Population: Block group population\n/// - AveOccup: Average number of household members\n/// - Latitude: Block group latitude\n/// - Longitude: Block group longitude\n///\n/// Target variable: MedHouseVal (Median house value for California districts)\n#[derive(Debug, Clone)]\npub struct CaliforniaHousingDataset {\n    features: Vec\u003cVec\u003cf32\u003e\u003e,\n    target: Vec\u003cf32\u003e,\n    feature_names: Vec\u003c\u0026'static str\u003e,\n}\n\nimpl CaliforniaHousingDataset {\n    /// Load the California Housing dataset from CSV.\n    ///\n    /// # Arguments\n    ///\n    /// * `path` - Path to the CSV file (default: \"benchmarks/datasets/california_housing.csv\")\n    ///\n    /// # Example\n    ///\n    /// ```no_run\n    /// use benchmarks::data::CaliforniaHousingDataset;\n    ///\n    /// let dataset = CaliforniaHousingDataset::load(\n    ///     \"benchmarks/datasets/california_housing.csv\"\n    /// ).unwrap();\n    /// ```\n    pub fn load(path: \u0026str) -\u003e io::Result\u003cSelf\u003e {\n        let file = File::open(path)?;\n        let reader = BufReader::new(file);\n        let mut rdr = ReaderBuilder::new().from_reader(reader);\n\n        let mut features = Vec::new();\n        let mut target = Vec::new();\n\n        for result in rdr.records() {\n            let record = result?;\n            // Columns: MedInc, HouseAge, AveRooms, AveBedrms, Population, AveOccup, Latitude, Longitude, MedHouseVal\n            let med_inc: f32 = record[0].parse().unwrap_or(0.0);\n            let house_age: f32 = record[1].parse().unwrap_or(0.0);\n            let ave_rooms: f32 = record[2].parse().unwrap_or(0.0);\n            let ave_bedrms: f32 = record[3].parse().unwrap_or(0.0);\n            let population: f32 = record[4].parse().unwrap_or(0.0);\n            let ave_occup: f32 = record[5].parse().unwrap_or(0.0);\n            let latitude: f32 = record[6].parse().unwrap_or(0.0);\n            let longitude: f32 = record[7].parse().unwrap_or(0.0);\n            let med_house_val: f32 = record[8].parse().unwrap_or(0.0);\n\n            features.push(vec![\n                med_inc, house_age, ave_rooms, ave_bedrms, population, ave_occup, latitude,\n                longitude,\n            ]);\n            target.push(med_house_val);\n        }\n\n        Ok(Self {\n            features,\n            target,\n            feature_names: vec![\n                \"MedInc\",\n                \"HouseAge\",\n                \"AveRooms\",\n                \"AveBedrms\",\n                \"Population\",\n                \"AveOccup\",\n                \"Latitude\",\n                \"Longitude\",\n            ],\n        })\n    }\n\n    /// Get the number of samples in the dataset.\n    pub fn len(\u0026self) -\u003e usize {\n        self.features.len()\n    }\n\n    /// Check if the dataset is empty.\n    pub fn is_empty(\u0026self) -\u003e bool {\n        self.features.is_empty()\n    }\n\n    /// Get the number of features.\n    pub fn n_features(\u0026self) -\u003e usize {\n        self.features.first().map(|f| f.len()).unwrap_or(0)\n    }\n\n    /// Get feature names.\n    pub fn feature_names(\u0026self) -\u003e \u0026[\u0026'static str] {\n        \u0026self.feature_names\n    }\n\n    /// Select a subset of features.\n    ///\n    /// # Arguments\n    ///\n    /// * `feature_indices` - Indices of features to select (0-indexed)\n    ///\n    /// # Example\n    ///\n    /// ```no_run\n    /// let dataset = CaliforniaHousingDataset::load(...).unwrap();\n    /// // Select only MedInc and HouseAge (first 2 features)\n    /// let subset = dataset.select_features(\u0026[0, 1]);\n    /// ```\n    pub fn select_features(\u0026self, feature_indices: \u0026[usize]) -\u003e Self {\n        let features: Vec\u003cVec\u003cf32\u003e\u003e = self\n            .features\n            .iter()\n            .map(|row| {\n                feature_indices\n                    .iter()\n                    .map(|\u0026idx| row.get(idx).copied().unwrap_or(0.0))\n                    .collect()\n            })\n            .collect();\n        let feature_names: Vec\u003c\u0026'static str\u003e = feature_indices\n            .iter()\n            .map(|\u0026idx| self.feature_names.get(idx).copied().unwrap_or(\"Unknown\"))\n            .collect();\n\n        Self {\n            features,\n            target: self.target.clone(),\n            feature_names,\n        }\n    }\n\n    /// Split the dataset into train and test sets.\n    ///\n    /// # Arguments\n    ///\n    /// * `train_ratio` - Fraction of data to use for training (0.0 to 1.0)\n    ///\n    /// # Returns\n    ///\n    /// (train_dataset, test_dataset)\n    ///\n    /// # Example\n    ///\n    /// ```no_run\n    /// let dataset = CaliforniaHousingDataset::load(...).unwrap();\n    /// let (train, test) = dataset.split(0.8); // 80% train, 20% test\n    /// ```\n    pub fn split(\u0026self, train_ratio: f32) -\u003e (Self, Self) {\n        let n_samples = self.len();\n        let n_train = (n_samples as f32 * train_ratio) as usize;\n\n        let train_features = self.features[..n_train].to_vec();\n        let train_target = self.target[..n_train].to_vec();\n\n        let test_features = self.features[n_train..].to_vec();\n        let test_target = self.target[n_train..].to_vec();\n\n        (\n            Self {\n                features: train_features,\n                target: train_target,\n                feature_names: self.feature_names.clone(),\n            },\n            Self {\n                features: test_features,\n                target: test_target,\n                feature_names: self.feature_names.clone(),\n            },\n        )\n    }\n\n    /// Split the dataset into train, validation, and test sets.\n    ///\n    /// # Arguments\n    ///\n    /// * `train_ratio` - Fraction of data to use for training\n    /// * `val_ratio` - Fraction of data to use for validation\n    ///\n    /// # Returns\n    ///\n    /// (train_dataset, val_dataset, test_dataset)\n    pub fn split_train_val_test(\u0026self, train_ratio: f32, val_ratio: f32) -\u003e (Self, Self, Self) {\n        let n_samples = self.len();\n        let n_train = (n_samples as f32 * train_ratio) as usize;\n        let n_val = (n_samples as f32 * val_ratio) as usize;\n\n        let train_features = self.features[..n_train].to_vec();\n        let train_target = self.target[..n_train].to_vec();\n\n        let val_features = self.features[n_train..n_train + n_val].to_vec();\n        let val_target = self.target[n_train..n_train + n_val].to_vec();\n\n        let test_features = self.features[n_train + n_val..].to_vec();\n        let test_target = self.target[n_train + n_val..].to_vec();\n\n        (\n            Self {\n                features: train_features,\n                target: train_target,\n                feature_names: self.feature_names.clone(),\n            },\n            Self {\n                features: val_features,\n                target: val_target,\n                feature_names: self.feature_names.clone(),\n            },\n            Self {\n                features: test_features,\n                target: test_target,\n                feature_names: self.feature_names.clone(),\n            },\n        )\n    }\n\n    /// Convert to an InMemoryDataset for use with machinelearne-rs.\n    pub fn to_in_memory_dataset(\u0026self) -\u003e Result\u003cInMemoryDataset, String\u003e {\n        InMemoryDataset::new(self.features.clone(), self.target.clone())\n    }\n\n    /// Get a reference to the features.\n    pub fn features(\u0026self) -\u003e \u0026[Vec\u003cf32\u003e] {\n        \u0026self.features\n    }\n\n    /// Get a reference to the target values.\n    pub fn target(\u0026self) -\u003e \u0026[f32] {\n        \u0026self.target\n    }\n}\n\n#[cfg(test)]\nmod tests {\n    use super::*;\n    use machinelearne_rs::dataset::Dataset;\n\n    #[test]\n    fn test_load_dataset() {\n        let dataset = CaliforniaHousingDataset::load(\"benchmarks/datasets/california_housing.csv\")\n            .expect(\"Failed to load dataset\");\n        assert_eq!(dataset.len(), 20640);\n        assert_eq!(dataset.n_features(), 8);\n        assert_eq!(dataset.feature_names().len(), 8);\n    }\n\n    #[test]\n    fn test_select_features() {\n        let dataset = CaliforniaHousingDataset::load(\"benchmarks/datasets/california_housing.csv\")\n            .expect(\"Failed to load dataset\");\n        let subset = dataset.select_features(\u0026[0, 1]); // MedInc and HouseAge\n        assert_eq!(subset.n_features(), 2);\n        assert_eq!(subset.feature_names(), \u0026[\"MedInc\", \"HouseAge\"]);\n    }\n\n    #[test]\n    fn test_split() {\n        let dataset = CaliforniaHousingDataset::load(\"benchmarks/datasets/california_housing.csv\")\n            .expect(\"Failed to load dataset\");\n        let (train, test) = dataset.split(0.8);\n        assert_eq!(train.len(), 16512);\n        assert_eq!(test.len(), 4128);\n    }\n\n    #[test]\n    fn test_to_in_memory_dataset() {\n        let dataset = CaliforniaHousingDataset::load(\"benchmarks/datasets/california_housing.csv\")\n            .expect(\"Failed to load dataset\");\n        let im_dataset = dataset.to_in_memory_dataset().expect(\"Failed to convert\");\n        assert_eq!(im_dataset.len(), Some(20640));\n    }\n}\n","traces":[],"covered":0,"coverable":0},{"path":["/","home","victor","ML","machinelearne-rs","benchmarks","src","data","mod.rs"],"content":"pub mod california_housing;\n\npub use california_housing::CaliforniaHousingDataset;\n","traces":[],"covered":0,"coverable":0},{"path":["/","home","victor","ML","machinelearne-rs","benchmarks","src","lib.rs"],"content":"//! Benchmark utilities and common modules for machinelearne-rs benchmarks.\n//!\n//! This library provides common functionality for benchmarking the machinelearne-rs\n//! library against sklearn, including:\n//!\n//! - Data loading utilities\n//! - Metrics calculation (MSE, MAE, RÂ²)\n//! - Timing and benchmarking utilities\n\npub mod data;\npub mod metrics;\npub mod utils;\n\npub use data::CaliforniaHousingDataset;\npub use metrics::{Metrics, RegressionMetrics};\npub use utils::{benchmark_fn, benchmark_with_warmup, time_fn, BenchmarkStats, Timer};\n","traces":[],"covered":0,"coverable":0},{"path":["/","home","victor","ML","machinelearne-rs","benchmarks","src","main.rs"],"content":"// Benchmark utilities and common modules\n// This is the main entry point for running benchmarks\n\nfn main() {\n    println!(\"Machinelearne-rs Benchmark Suite\");\n    println!();\n    println!(\"Usage:\");\n    println!(\"  cargo bench --package benchmarks --all-benchmarks\");\n    println!(\"  cargo bench --package benchmarks --bench \u003cbenchmark_name\u003e\");\n    println!();\n    println!(\"Available benchmarks:\");\n    println!(\"  - train_1_feature: Training benchmarks with 1 feature\");\n    println!(\"  - train_2_features: Training benchmarks with 2 features\");\n    println!(\"  - train_4_features: Training benchmarks with 4 features\");\n    println!(\"  - train_8_features: Training benchmarks with 8 features\");\n    println!(\"  - predict: Prediction latency and throughput benchmarks\");\n    println!(\"  - metrics: Metrics computation benchmarks\");\n    println!();\n    println!(\"To run all benchmarks:\");\n    println!(\"  bash benchmarks/scripts/run_all.sh\");\n}\n","traces":[],"covered":0,"coverable":0},{"path":["/","home","victor","ML","machinelearne-rs","benchmarks","src","metrics.rs"],"content":"/// Metrics for evaluating regression models.\npub struct Metrics;\n\nimpl Metrics {\n    /// Calculate Mean Squared Error (MSE).\n    ///\n    /// MSE = mean((y_true - y_pred)^2)\n    ///\n    /// # Arguments\n    ///\n    /// * `y_true` - Ground truth values\n    /// * `y_pred` - Predicted values\n    ///\n    /// # Returns\n    ///\n    /// The MSE value (lower is better)\n    pub fn mse(y_true: \u0026[f32], y_pred: \u0026[f32]) -\u003e f32 {\n        assert_eq!(\n            y_true.len(),\n            y_pred.len(),\n            \"Arrays must have the same length\"\n        );\n\n        if y_true.is_empty() {\n            return 0.0;\n        }\n\n        let sum_sq: f32 = y_true\n            .iter()\n            .zip(y_pred.iter())\n            .map(|(\u0026t, \u0026p)| (t - p).powi(2))\n            .sum();\n\n        sum_sq / y_true.len() as f32\n    }\n\n    /// Calculate Root Mean Squared Error (RMSE).\n    ///\n    /// RMSE = sqrt(MSE)\n    ///\n    /// # Arguments\n    ///\n    /// * `y_true` - Ground truth values\n    /// * `y_pred` - Predicted values\n    ///\n    /// # Returns\n    ///\n    /// The RMSE value (lower is better, in same units as the target)\n    pub fn rmse(y_true: \u0026[f32], y_pred: \u0026[f32]) -\u003e f32 {\n        Self::mse(y_true, y_pred).sqrt()\n    }\n\n    /// Calculate Mean Absolute Error (MAE).\n    ///\n    /// MAE = mean(|y_true - y_pred|)\n    ///\n    /// # Arguments\n    ///\n    /// * `y_true` - Ground truth values\n    /// * `y_pred` - Predicted values\n    ///\n    /// # Returns\n    ///\n    /// The MAE value (lower is better)\n    pub fn mae(y_true: \u0026[f32], y_pred: \u0026[f32]) -\u003e f32 {\n        assert_eq!(\n            y_true.len(),\n            y_pred.len(),\n            \"Arrays must have the same length\"\n        );\n\n        if y_true.is_empty() {\n            return 0.0;\n        }\n\n        let sum_abs: f32 = y_true\n            .iter()\n            .zip(y_pred.iter())\n            .map(|(\u0026t, \u0026p)| (t - p).abs())\n            .sum();\n\n        sum_abs / y_true.len() as f32\n    }\n\n    /// Calculate RÂ² (coefficient of determination).\n    ///\n    /// RÂ² = 1 - (SS_res / SS_tot)\n    ///\n    /// where:\n    /// - SS_res = sum((y_true - y_pred)^2)  (residual sum of squares)\n    /// - SS_tot = sum((y_true - mean(y_true))^2)  (total sum of squares)\n    ///\n    /// RÂ² ranges from 0 to 1, where 1 indicates perfect prediction.\n    /// Values can be negative if the model is arbitrarily worse than the mean.\n    ///\n    /// # Arguments\n    ///\n    /// * `y_true` - Ground truth values\n    /// * `y_pred` - Predicted values\n    ///\n    /// # Returns\n    ///\n    /// The RÂ² value (higher is better)\n    pub fn r_squared(y_true: \u0026[f32], y_pred: \u0026[f32]) -\u003e f32 {\n        assert_eq!(\n            y_true.len(),\n            y_pred.len(),\n            \"Arrays must have the same length\"\n        );\n\n        if y_true.is_empty() {\n            return 0.0;\n        }\n\n        let mean_true: f32 = y_true.iter().copied().sum::\u003cf32\u003e() / y_true.len() as f32;\n\n        let ss_res: f32 = y_true\n            .iter()\n            .zip(y_pred.iter())\n            .map(|(\u0026t, \u0026p)| (t - p).powi(2))\n            .sum();\n\n        let ss_tot: f32 = y_true.iter().map(|\u0026t| (t - mean_true).powi(2)).sum();\n\n        if ss_tot == 0.0 {\n            // All values are the same, perfect prediction if predictions are also the same\n            return if ss_res == 0.0 { 1.0 } else { 0.0 };\n        }\n\n        1.0 - (ss_res / ss_tot)\n    }\n\n    /// Calculate all metrics at once.\n    ///\n    /// # Arguments\n    ///\n    /// * `y_true` - Ground truth values\n    /// * `y_pred` - Predicted values\n    ///\n    /// # Returns\n    ///\n    /// A struct containing MSE, RMSE, MAE, and RÂ²\n    pub fn calculate_all(y_true: \u0026[f32], y_pred: \u0026[f32]) -\u003e RegressionMetrics {\n        RegressionMetrics {\n            mse: Self::mse(y_true, y_pred),\n            rmse: Self::rmse(y_true, y_pred),\n            mae: Self::mae(y_true, y_pred),\n            r_squared: Self::r_squared(y_true, y_pred),\n        }\n    }\n}\n\n/// Struct to hold all regression metrics.\n#[derive(Debug, Clone, Copy)]\npub struct RegressionMetrics {\n    pub mse: f32,\n    pub rmse: f32,\n    pub mae: f32,\n    pub r_squared: f32,\n}\n\nimpl RegressionMetrics {\n    /// Create a new RegressionMetrics instance.\n    pub fn new(mse: f32, mae: f32, r_squared: f32) -\u003e Self {\n        Self {\n            mse,\n            rmse: mse.sqrt(),\n            mae,\n            r_squared,\n        }\n    }\n}\n\n#[cfg(test)]\nmod tests {\n    use super::*;\n\n    #[test]\n    fn test_mse_perfect() {\n        let y_true = vec![1.0, 2.0, 3.0, 4.0];\n        let y_pred = vec![1.0, 2.0, 3.0, 4.0];\n        assert!((Metrics::mse(\u0026y_true, \u0026y_pred) - 0.0).abs() \u003c 1e-6);\n    }\n\n    #[test]\n    fn test_mse_error() {\n        let y_true = vec![1.0, 2.0, 3.0, 4.0];\n        let y_pred = vec![2.0, 3.0, 4.0, 5.0];\n        // Errors: [-1, -1, -1, -1], squared: [1, 1, 1, 1], mean: 1.0\n        assert!((Metrics::mse(\u0026y_true, \u0026y_pred) - 1.0).abs() \u003c 1e-6);\n    }\n\n    #[test]\n    fn test_mae() {\n        let y_true = vec![1.0, 2.0, 3.0, 4.0];\n        let y_pred = vec![2.0, 3.0, 4.0, 5.0];\n        // Errors: [-1, -1, -1, -1], abs: [1, 1, 1, 1], mean: 1.0\n        assert!((Metrics::mae(\u0026y_true, \u0026y_pred) - 1.0).abs() \u003c 1e-6);\n    }\n\n    #[test]\n    fn test_r_squared_perfect() {\n        let y_true = vec![1.0, 2.0, 3.0, 4.0];\n        let y_pred = vec![1.0, 2.0, 3.0, 4.0];\n        assert!((Metrics::r_squared(\u0026y_true, \u0026y_pred) - 1.0).abs() \u003c 1e-6);\n    }\n\n    #[test]\n    fn test_r_squared_mean() {\n        let y_true = vec![2.0, 2.0, 2.0, 2.0];\n        let y_pred = vec![2.0, 2.0, 2.0, 2.0];\n        // All predictions equal the mean (which is 2.0), so RÂ² should be 1.0\n        assert!((Metrics::r_squared(\u0026y_true, \u0026y_pred) - 1.0).abs() \u003c 1e-6);\n    }\n\n    #[test]\n    fn test_calculate_all() {\n        let y_true = vec![1.0, 2.0, 3.0, 4.0];\n        let y_pred = vec![1.0, 2.0, 3.0, 4.0];\n        let metrics = Metrics::calculate_all(\u0026y_true, \u0026y_pred);\n        assert!((metrics.mse - 0.0).abs() \u003c 1e-6);\n        assert!((metrics.mae - 0.0).abs() \u003c 1e-6);\n        assert!((metrics.r_squared - 1.0).abs() \u003c 1e-6);\n    }\n}\n","traces":[],"covered":0,"coverable":0},{"path":["/","home","victor","ML","machinelearne-rs","benchmarks","src","utils.rs"],"content":"use std::time::{Duration, Instant};\n\n/// Timer for measuring elapsed time.\n#[derive(Debug)]\npub struct Timer {\n    start: Option\u003cInstant\u003e,\n    total: Duration,\n}\n\nimpl Timer {\n    /// Create a new timer.\n    pub fn new() -\u003e Self {\n        Self {\n            start: None,\n            total: Duration::ZERO,\n        }\n    }\n\n    /// Start the timer.\n    pub fn start(\u0026mut self) {\n        self.start = Some(Instant::now());\n    }\n\n    /// Stop the timer and add the elapsed time to the total.\n    pub fn stop(\u0026mut self) -\u003e Duration {\n        if let Some(start) = self.start.take() {\n            let elapsed = start.elapsed();\n            self.total += elapsed;\n            elapsed\n        } else {\n            Duration::ZERO\n        }\n    }\n\n    /// Get the total elapsed time.\n    pub fn total(\u0026self) -\u003e Duration {\n        self.total\n    }\n\n    /// Get the total elapsed time in milliseconds.\n    pub fn total_ms(\u0026self) -\u003e f64 {\n        self.total.as_secs_f64() * 1000.0\n    }\n\n    /// Get the total elapsed time in seconds.\n    pub fn total_secs(\u0026self) -\u003e f64 {\n        self.total.as_secs_f64()\n    }\n\n    /// Reset the timer.\n    pub fn reset(\u0026mut self) {\n        self.start = None;\n        self.total = Duration::ZERO;\n    }\n}\n\nimpl Default for Timer {\n    fn default() -\u003e Self {\n        Self::new()\n    }\n}\n\n/// Run a function and measure its execution time.\n///\n/// # Arguments\n///\n/// * `f` - Function to execute\n///\n/// # Returns\n///\n/// A tuple of (result, elapsed_time)\npub fn time_fn\u003cF, R\u003e(f: F) -\u003e (R, Duration)\nwhere\n    F: FnOnce() -\u003e R,\n{\n    let start = Instant::now();\n    let result = f();\n    let elapsed = start.elapsed();\n    (result, elapsed)\n}\n\n/// Run a function multiple times and return the mean and std dev of execution times.\n///\n/// # Arguments\n///\n/// * `iterations` - Number of iterations to run\n/// * `f` - Function to execute\n///\n/// # Returns\n///\n/// A tuple of (results, mean_time_ms, std_dev_ms)\npub fn benchmark_fn\u003cF, R\u003e(iterations: usize, mut f: F) -\u003e (Vec\u003cR\u003e, f64, f64)\nwhere\n    F: FnMut() -\u003e R,\n{\n    let mut results = Vec::with_capacity(iterations);\n    let mut times = Vec::with_capacity(iterations);\n\n    for _ in 0..iterations {\n        let (result, elapsed) = time_fn(\u0026mut f);\n        results.push(result);\n        times.push(elapsed.as_secs_f64() * 1000.0);\n    }\n\n    let mean = times.iter().sum::\u003cf64\u003e() / times.len() as f64;\n    let variance = times.iter().map(|\u0026t| (t - mean).powi(2)).sum::\u003cf64\u003e() / times.len() as f64;\n    let std_dev = variance.sqrt();\n\n    (results, mean, std_dev)\n}\n\n/// Run a function multiple times with warmup and return statistics.\n///\n/// # Arguments\n///\n/// * `warmup` - Number of warmup iterations (not counted in results)\n/// * `iterations` - Number of measurement iterations\n/// * `f` - Function to execute\n///\n/// # Returns\n///\n/// A tuple of (results, mean_time_ms, std_dev_ms, min_ms, max_ms)\npub fn benchmark_with_warmup\u003cF, R\u003e(\n    warmup: usize,\n    iterations: usize,\n    mut f: F,\n) -\u003e (Vec\u003cR\u003e, f64, f64, f64, f64)\nwhere\n    F: FnMut() -\u003e R,\n{\n    // Warmup\n    for _ in 0..warmup {\n        let _ = f();\n    }\n\n    // Actual benchmarking\n    let mut results = Vec::with_capacity(iterations);\n    let mut times = Vec::with_capacity(iterations);\n\n    for _ in 0..iterations {\n        let start = Instant::now();\n        let result = f();\n        let elapsed = start.elapsed().as_secs_f64() * 1000.0;\n        results.push(result);\n        times.push(elapsed);\n    }\n\n    let mean = times.iter().sum::\u003cf64\u003e() / times.len() as f64;\n    let variance = times.iter().map(|\u0026t| (t - mean).powi(2)).sum::\u003cf64\u003e() / times.len() as f64;\n    let std_dev = variance.sqrt();\n    let min = times.iter().copied().fold(f64::INFINITY, f64::min);\n    let max = times.iter().copied().fold(f64::NEG_INFINITY, f64::max);\n\n    (results, mean, std_dev, min, max)\n}\n\n/// Statistics for benchmarking results.\n#[derive(Debug, Clone)]\npub struct BenchmarkStats {\n    pub mean_ms: f64,\n    pub std_dev_ms: f64,\n    pub min_ms: f64,\n    pub max_ms: f64,\n    pub median_ms: f64,\n    pub p95_ms: f64,\n    pub p99_ms: f64,\n}\n\nimpl BenchmarkStats {\n    /// Calculate statistics from a list of times in milliseconds.\n    pub fn from_times(mut times: Vec\u003cf64\u003e) -\u003e Self {\n        times.sort_by(|a, b| a.partial_cmp(b).unwrap());\n\n        let mean = times.iter().sum::\u003cf64\u003e() / times.len() as f64;\n        let variance = times.iter().map(|\u0026t| (t - mean).powi(2)).sum::\u003cf64\u003e() / times.len() as f64;\n        let std_dev = variance.sqrt();\n        let min = times.first().copied().unwrap_or(0.0);\n        let max = times.last().copied().unwrap_or(0.0);\n\n        let n = times.len();\n        let median = if n.is_multiple_of(2) {\n            (times[n / 2 - 1] + times[n / 2]) / 2.0\n        } else {\n            times[n / 2]\n        };\n\n        let p95_idx = ((n as f64 * 0.95) as usize).min(n - 1);\n        let p95 = times[p95_idx];\n\n        let p99_idx = ((n as f64 * 0.99) as usize).min(n - 1);\n        let p99 = times[p99_idx];\n\n        Self {\n            mean_ms: mean,\n            std_dev_ms: std_dev,\n            min_ms: min,\n            max_ms: max,\n            median_ms: median,\n            p95_ms: p95,\n            p99_ms: p99,\n        }\n    }\n}\n\n#[cfg(test)]\nmod tests {\n    use super::*;\n\n    #[test]\n    fn test_timer() {\n        let mut timer = Timer::new();\n        timer.start();\n        std::thread::sleep(Duration::from_millis(10));\n        let elapsed = timer.stop();\n        assert!(elapsed.as_millis() \u003e= 10);\n        assert!(timer.total_ms() \u003e= 10.0);\n    }\n\n    #[test]\n    fn test_benchmark_fn() {\n        let fn_to_test = || {\n            let mut sum = 0i32;\n            for i in 0..1000 {\n                sum += i;\n            }\n            sum\n        };\n\n        let (results, mean, std_dev) = benchmark_fn(10, fn_to_test);\n        assert_eq!(results.len(), 10);\n        assert!(mean \u003e 0.0);\n        assert!(std_dev \u003e= 0.0);\n    }\n\n    #[test]\n    fn test_benchmark_with_warmup() {\n        let fn_to_test = || 42;\n\n        let (results, mean, std_dev, min, max) = benchmark_with_warmup(5, 10, fn_to_test);\n        assert_eq!(results.len(), 10);\n        assert_eq!(results[0], 42);\n        assert!(mean \u003e= 0.0);\n        assert!(std_dev \u003e= 0.0);\n        assert!(max \u003e= min);\n    }\n\n    #[test]\n    fn test_benchmark_stats() {\n        let times = vec![1.0, 2.0, 3.0, 4.0, 5.0];\n        let stats = BenchmarkStats::from_times(times);\n\n        assert!((stats.mean_ms - 3.0).abs() \u003c 1e-6);\n        assert!((stats.median_ms - 3.0).abs() \u003c 1e-6);\n        assert!((stats.min_ms - 1.0).abs() \u003c 1e-6);\n        assert!((stats.max_ms - 5.0).abs() \u003c 1e-6);\n    }\n}\n","traces":[{"line":76,"address":[],"length":0,"stats":{"Line":0}},{"line":77,"address":[],"length":0,"stats":{"Line":0}},{"line":78,"address":[],"length":0,"stats":{"Line":0}},{"line":79,"address":[],"length":0,"stats":{"Line":0}},{"line":96,"address":[],"length":0,"stats":{"Line":0}},{"line":97,"address":[],"length":0,"stats":{"Line":0}},{"line":99,"address":[],"length":0,"stats":{"Line":0}},{"line":100,"address":[],"length":0,"stats":{"Line":0}},{"line":101,"address":[],"length":0,"stats":{"Line":0}},{"line":102,"address":[],"length":0,"stats":{"Line":0}},{"line":105,"address":[],"length":0,"stats":{"Line":0}},{"line":106,"address":[],"length":0,"stats":{"Line":0}},{"line":107,"address":[],"length":0,"stats":{"Line":0}},{"line":109,"address":[],"length":0,"stats":{"Line":0}},{"line":132,"address":[],"length":0,"stats":{"Line":0}},{"line":133,"address":[],"length":0,"stats":{"Line":0}},{"line":137,"address":[],"length":0,"stats":{"Line":0}},{"line":138,"address":[],"length":0,"stats":{"Line":0}},{"line":140,"address":[],"length":0,"stats":{"Line":0}},{"line":141,"address":[],"length":0,"stats":{"Line":0}},{"line":142,"address":[],"length":0,"stats":{"Line":0}},{"line":143,"address":[],"length":0,"stats":{"Line":0}},{"line":144,"address":[],"length":0,"stats":{"Line":0}},{"line":145,"address":[],"length":0,"stats":{"Line":0}},{"line":148,"address":[],"length":0,"stats":{"Line":0}},{"line":149,"address":[],"length":0,"stats":{"Line":0}},{"line":150,"address":[],"length":0,"stats":{"Line":0}},{"line":151,"address":[],"length":0,"stats":{"Line":0}},{"line":152,"address":[],"length":0,"stats":{"Line":0}},{"line":154,"address":[],"length":0,"stats":{"Line":0}}],"covered":0,"coverable":30},{"path":["/","home","victor","ML","machinelearne-rs","lib","examples","house_price_pipeline.rs"],"content":"//! Comprehensive ML Pipeline Example: House Price Prediction\n//!\n//! This example demonstrates a complete end-to-end ML workflow including:\n//! - Mixed-type dataset (numerical + categorical features)\n//! - Different preprocessing per column type using ColumnTransformer\n//! - Missing value imputation\n//! - Feature engineering with polynomial features\n//! - Model training\n//! - Pipeline serialization and loading\n//! - Inference on new data\n//!\n//! # Features:\n//! - Column 0: sqft (numerical, needs scaling)\n//! - Column 1: bedrooms (numerical, may have missing values)\n//! - Column 2: bathrooms (numerical, needs scaling)\n//! - Column 3: neighborhood (categorical, one-hot encoded)\n//! - Column 4: condition (ordinal, 0-3 scale, passed through)\n//!\n//! Run with: cargo run --example house_price_pipeline\n\nuse machinelearne_rs::{\n    backend::CpuBackend,\n    dataset::memory::InMemoryDataset,\n    loss::MSELoss,\n    model::linear::{LinearModel, LinearParams, SerializableLinearParams},\n    model::state::Fitted,\n    optimizer::SGD,\n    preprocessing::{\n        ColumnSpec, ColumnTransformer, FittedTransformer, ImputeStrategy, OneHotEncoder, Pipeline,\n        PolynomialFeatures, PredictivePipeline, SimpleImputer, StandardScaler, Transformer,\n    },\n    regularizers::NoRegularizer,\n    trainer::Trainer,\n    Tensor2D,\n};\nuse std::error::Error;\n\n// Feature indices\nconst SQFT: usize = 0;\nconst BEDROOMS: usize = 1;\nconst BATHROOMS: usize = 2;\nconst NEIGHBORHOOD: usize = 3;\nconst CONDITION: usize = 4;\n\nfn main() -\u003e Result\u003c(), Box\u003cdyn Error\u003e\u003e {\n    println!(\"=== House Price Prediction Pipeline ===\\n\");\n\n    // 1. Create synthetic training data\n    // Features: [sqft, bedrooms, bathrooms, neighborhood, condition]\n    // neighborhood: 0=downtown, 1=suburban, 2=rural\n    // condition: 0=poor, 1=fair, 2=good, 3=excellent\n    let x_train_raw: Vec\u003cVec\u003cf32\u003e\u003e = vec![\n        vec![1500.0, 3.0, 2.0, 0.0, 2.0], // downtown, good\n        vec![2000.0, 4.0, 3.0, 1.0, 3.0], // suburban, excellent\n        vec![1200.0, 2.0, 1.0, 2.0, 1.0], // rural, fair\n        vec![1800.0, 3.0, 2.0, 0.0, 3.0], // downtown, excellent\n        vec![2200.0, 4.0, 3.0, 1.0, 2.0], // suburban, good\n        vec![1100.0, 2.0, 1.0, 2.0, 0.0], // rural, poor\n        vec![2500.0, 5.0, 4.0, 0.0, 3.0], // downtown, excellent\n        vec![1400.0, 3.0, 2.0, 1.0, 1.0], // suburban, fair\n        // Some with missing values (NaN for bedrooms)\n        vec![1600.0, f32::NAN, 2.0, 0.0, 2.0], // missing bedrooms\n        vec![1900.0, f32::NAN, 3.0, 1.0, 3.0], // missing bedrooms\n    ];\n\n    // Target: house prices (in thousands)\n    let y_train: Vec\u003cf32\u003e = vec![\n        350.0, 450.0, 180.0, 420.0, 480.0, 150.0, 550.0, 280.0, 360.0, 410.0,\n    ];\n\n    println!(\n        \"Training data: {} samples with {} features each\",\n        x_train_raw.len(),\n        x_train_raw[0].len()\n    );\n\n    // 2. Convert to tensor\n    let flat_x: Vec\u003cf32\u003e = x_train_raw.iter().flatten().copied().collect();\n    let x_train = Tensor2D::\u003cCpuBackend\u003e::new(flat_x, x_train_raw.len(), 5);\n\n    // 3. Build ColumnTransformer for heterogeneous preprocessing\n    println!(\"\\nBuilding ColumnTransformer...\");\n\n    // Numerical columns (sqft, bedrooms, bathrooms): impute + scale\n    let numerical_pipeline = Pipeline::\u003cCpuBackend\u003e::new()\n        .add_simple_imputer(SimpleImputer::new(ImputeStrategy::Mean))\n        .add_standard_scaler(StandardScaler::new());\n\n    // Categorical column (neighborhood): one-hot encoding\n    let categorical_encoder = OneHotEncoder::\u003cCpuBackend\u003e::new();\n\n    // Ordinal column (condition): scale (for ordinal features, scaling is acceptable)\n    let ordinal_scaler = StandardScaler::\u003cCpuBackend\u003e::new();\n\n    // Build column transformer\n    let column_transformer = ColumnTransformer::\u003cCpuBackend\u003e::new()\n        // Numerical: sqft, bedrooms, bathrooms (columns 0, 1, 2)\n        .add_pipeline(\n            numerical_pipeline,\n            ColumnSpec::Indices(vec![SQFT, BEDROOMS, BATHROOMS]),\n        )\n        // Categorical: neighborhood (column 3) - one-hot encoding\n        .add_one_hot_encoder(categorical_encoder, ColumnSpec::Indices(vec![NEIGHBORHOOD]))\n        // Ordinal: condition (column 4) - scale\n        .add_standard_scaler(ordinal_scaler, ColumnSpec::Indices(vec![CONDITION]));\n\n    // 4. Fit the column transformer\n    println!(\"Fitting column transformer...\");\n    let fitted_ct = column_transformer.fit(\u0026x_train)?;\n    println!(\"  Input features: {}\", fitted_ct.n_features_in());\n    println!(\"  Output features: {}\", fitted_ct.n_features_out());\n\n    // 5. Transform training data\n    let x_preprocessed = fitted_ct.transform(\u0026x_train)?;\n\n    // 6. Add polynomial features (degree 2, interaction only to avoid too many features)\n    println!(\"\\nGenerating polynomial features...\");\n    let poly = PolynomialFeatures::\u003cCpuBackend\u003e::new()\n        .with_degree(2)\n        .with_include_bias(false)\n        .with_interaction_only(true);\n\n    let fitted_poly = poly.fit(\u0026x_preprocessed)?;\n    println!(\n        \"  Polynomial features: {} -\u003e {}\",\n        fitted_poly.n_features_in(),\n        fitted_poly.n_features_out()\n    );\n\n    let x_final = fitted_poly.transform(\u0026x_preprocessed)?;\n\n    // 7. Train the model\n    println!(\"\\nTraining linear regression model...\");\n    let n_features = fitted_poly.n_features_out();\n\n    // Create dataset from preprocessed data\n    let x_final_vec: Vec\u003cVec\u003cf32\u003e\u003e = {\n        let (rows, cols) = x_final.shape();\n        let flat = x_final.ravel().to_vec();\n        (0..rows)\n            .map(|r| (0..cols).map(|c| flat[r * cols + c] as f32).collect())\n            .collect()\n    };\n    let dataset = InMemoryDataset::new(x_final_vec, y_train.clone())?;\n\n    // Build and train\n    let model = machinelearne_rs::model::linear::LinearRegression::\u003cCpuBackend\u003e::new(n_features);\n    let loss = MSELoss;\n    let optimizer = SGD::new(0.001);\n    let regularizer = NoRegularizer;\n\n    let trainer = Trainer::builder(loss, optimizer, regularizer)\n        .batch_size(5)\n        .max_epochs(2000)\n        .build();\n\n    let fitted_model = trainer.fit(model, \u0026dataset)?;\n    println!(\"  Model trained successfully!\");\n\n    // 8. Create and save the complete pipeline\n    println!(\"\\nSaving complete pipeline...\");\n    let pipeline = PredictivePipeline::new(fitted_ct, Some(fitted_poly), fitted_model);\n\n    let temp_file = std::env::temp_dir().join(\"house_price_pipeline.bin\");\n    pipeline.save_to_file(\u0026temp_file)?;\n    println!(\"  Pipeline saved to: {:?}\", temp_file);\n\n    // 9. Load the pipeline (simulating deployment)\n    println!(\"\\nLoading pipeline for inference...\");\n    let loaded_pipeline =\n        PredictivePipeline::\u003cCpuBackend, LinearModel\u003cCpuBackend, Fitted\u003e\u003e::load_from_file(\n            \u0026temp_file,\n            |bytes| {\n                let serial_params: SerializableLinearParams =\n                    bincode::deserialize(bytes).map_err(|e| {\n                        machinelearne_rs::preprocessing::PreprocessingError::SerializationError(\n                            e.to_string(),\n                        )\n                    })?;\n                let params = LinearParams::try_from(serial_params).map_err(|e| {\n                    machinelearne_rs::preprocessing::PreprocessingError::SerializationError(\n                        e.to_string(),\n                    )\n                })?;\n                Ok(\u003cLinearModel\u003cCpuBackend, Fitted\u003e\u003e::new(params))\n            },\n        )?;\n    println!(\"  Pipeline loaded successfully!\");\n    println!(\n        \"  Loaded pipeline expects {} input features\",\n        loaded_pipeline.n_features_in()\n    );\n\n    // 10. Make predictions on new data\n    println!(\"\\n=== Predictions on New Houses ===\\n\");\n\n    // New house 1: 1700 sqft, 3 bed, 2 bath, suburban, excellent condition\n    let house1 = Tensor2D::\u003cCpuBackend\u003e::new(vec![1700.0, 3.0, 2.0, 1.0, 3.0], 1, 5);\n    let price1 = loaded_pipeline.predict(\u0026house1)?;\n    println!(\n        \"House 1 (suburban, 3bed/2bath, 1700sqft, excellent): ${:.0}k\",\n        price1.to_vec()[0]\n    );\n\n    // New house 2: 1300 sqft, 2 bed, 1 bath, downtown, good condition\n    let house2 = Tensor2D::\u003cCpuBackend\u003e::new(vec![1300.0, 2.0, 1.0, 0.0, 2.0], 1, 5);\n    let price2 = loaded_pipeline.predict(\u0026house2)?;\n    println!(\n        \"House 2 (downtown, 2bed/1bath, 1300sqft, good): ${:.0}k\",\n        price2.to_vec()[0]\n    );\n\n    // New house 3: 2400 sqft, with missing bedrooms, rural, fair condition\n    let house3 = Tensor2D::\u003cCpuBackend\u003e::new(vec![2400.0, f32::NAN, 3.0, 2.0, 1.0], 1, 5);\n    let price3 = loaded_pipeline.predict(\u0026house3)?;\n    println!(\n        \"House 3 (rural, ?bed/3bath, 2400sqft, fair, missing data): ${:.0}k\",\n        price3.to_vec()[0]\n    );\n\n    // Batch prediction: multiple houses at once\n    let houses_batch = Tensor2D::\u003cCpuBackend\u003e::new(\n        vec![\n            1500.0, 3.0, 2.0, 0.0, 2.0, // house A\n            2000.0, 4.0, 3.0, 1.0, 3.0, // house B\n            1200.0, 2.0, 1.0, 2.0, 1.0, // house C\n        ],\n        3,\n        5,\n    );\n    let prices_batch = loaded_pipeline.predict(\u0026houses_batch)?;\n    println!(\"\\nBatch predictions:\");\n    let prices = prices_batch.to_vec();\n    for (i, price) in prices.iter().enumerate() {\n        println!(\"  House {}: ${:.0}k\", i + 1, price);\n    }\n\n    // Cleanup\n    std::fs::remove_file(\u0026temp_file).ok();\n\n    println!(\"\\n=== Pipeline Complete ===\");\n    Ok(())\n}\n","traces":[],"covered":0,"coverable":0},{"path":["/","home","victor","ML","machinelearne-rs","lib","examples","titanic_pipeline.rs"],"content":"//! Titanic Survival Prediction Pipeline\n//!\n//! This example demonstrates a complete ML workflow on the classic Titanic dataset,\n//! showcasing all preprocessing capabilities:\n//! - Mixed feature types (numerical, categorical, ordinal)\n//! - Missing value imputation\n//! - Feature scaling and encoding\n//! - Binary classification with accuracy metrics\n//! - Pipeline serialization\n//!\n//! Run with: cargo run --example titanic_pipeline\n\nuse machinelearne_rs::{\n    backend::CpuBackend,\n    dataset::memory::InMemoryDataset,\n    loss::BCEWithLogitsLoss,\n    model::linear::{LinearModel, LinearParams, SerializableLinearParams},\n    model::state::Fitted,\n    model::InferenceModel,\n    optimizer::SGD,\n    preprocessing::{\n        ColumnSpec, ColumnTransformer, FittedTransformer, ImputeStrategy, OneHotEncoder, Pipeline,\n        PredictivePipeline, SimpleImputer, StandardScaler, Transformer,\n    },\n    regularizers::NoRegularizer,\n    trainer::Trainer,\n    Tensor2D,\n};\nuse std::error::Error;\n\n// Feature indices for the raw data\n// [Pclass, Sex, Age, SibSp, Parch, Fare, Embarked]\nconst PCLASS: usize = 0;\nconst SEX: usize = 1;\nconst AGE: usize = 2;\nconst SIBSP: usize = 3;\nconst PARCH: usize = 4;\nconst FARE: usize = 5;\nconst EMBARKED: usize = 6;\n\n/// Titanic passenger data (subset of the classic dataset).\n/// Features: [Pclass, Sex, Age, SibSp, Parch, Fare, Embarked]\n/// - Pclass: 1=1st, 2=2nd, 3=3rd (ordinal)\n/// - Sex: 0=male, 1=female (will be one-hot encoded)\n/// - Age: in years (has missing values as f32::NAN)\n/// - SibSp: number of siblings/spouses aboard\n/// - Parch: number of parents/children aboard\n/// - Fare: ticket fare\n/// - Embarked: 0=C, 1=Q, 2=S (Cherbourg, Queenstown, Southampton - will be one-hot encoded)\n/// Target: Survived (0=no, 1=yes)\nfn get_titanic_data() -\u003e (Vec\u003cVec\u003cf32\u003e\u003e, Vec\u003cf32\u003e) {\n    let features: Vec\u003cVec\u003cf32\u003e\u003e = vec![\n        // First class passengers\n        vec![1.0, 1.0, 29.0, 0.0, 0.0, 211.3375, 0.0], // Female, survived\n        vec![1.0, 0.0, 0.9167, 1.0, 2.0, 151.55, 0.0], // Male infant, survived\n        vec![1.0, 1.0, 2.0, 1.0, 2.0, 151.55, 0.0],    // Female child, survived\n        vec![1.0, 0.0, 30.0, 1.0, 0.0, 164.8667, 0.0], // Male, survived\n        vec![1.0, 1.0, 25.0, 1.0, 0.0, 151.55, 0.0],   // Female, survived\n        vec![1.0, 0.0, 48.0, 0.0, 0.0, 26.55, 1.0],    // Male, didn't survive\n        vec![1.0, 0.0, 36.0, 1.0, 0.0, 135.6333, 1.0], // Male, survived\n        vec![1.0, 1.0, 27.0, 1.0, 0.0, 153.4625, 1.0], // Female, survived\n        vec![1.0, 0.0, 22.0, 0.0, 0.0, 135.6333, 1.0], // Male, didn't survive\n        vec![1.0, 1.0, 38.0, 0.0, 0.0, 80.0, 2.0],     // Female, survived\n        // Second class passengers\n        vec![2.0, 1.0, 29.0, 0.0, 2.0, 23.0, 2.0], // Female, survived\n        vec![2.0, 0.0, 32.0, 0.0, 0.0, 10.5, 2.0], // Male, didn't survive\n        vec![2.0, 1.0, 24.0, 0.0, 0.0, 13.0, 2.0], // Female, survived\n        vec![2.0, 0.0, 36.0, 0.0, 0.0, 13.0, 2.0], // Male, didn't survive\n        vec![2.0, 1.0, 28.0, 0.0, 0.0, 12.65, 2.0], // Female, survived\n        vec![2.0, 0.0, 25.0, 0.0, 0.0, 13.0, 2.0], // Male, didn't survive\n        vec![2.0, 1.0, 18.0, 0.0, 1.0, 33.0, 2.0], // Female, survived\n        vec![2.0, 0.0, 19.0, 1.0, 0.0, 26.0, 2.0], // Male, didn't survive\n        vec![2.0, 1.0, 23.0, 0.0, 0.0, 10.5, 2.0], // Female, survived\n        vec![2.0, 0.0, 34.0, 0.0, 0.0, 13.0, 2.0], // Male, didn't survive\n        // Third class passengers\n        vec![3.0, 0.0, 22.0, 0.0, 0.0, 7.25, 2.0], // Male, didn't survive\n        vec![3.0, 1.0, 26.0, 0.0, 0.0, 7.925, 2.0], // Female, survived\n        vec![3.0, 0.0, 24.0, 0.0, 0.0, 8.4583, 0.0], // Male, didn't survive\n        vec![3.0, 1.0, 21.0, 0.0, 0.0, 7.75, 1.0], // Female, survived\n        vec![3.0, 0.0, 22.0, 0.0, 0.0, 7.8958, 2.0], // Male, didn't survive\n        vec![3.0, 1.0, 27.0, 0.0, 2.0, 21.075, 2.0], // Female, survived\n        vec![3.0, 0.0, 30.0, 0.0, 0.0, 7.8958, 2.0], // Male, didn't survive\n        vec![3.0, 1.0, 18.0, 0.0, 0.0, 7.775, 2.0], // Female, survived\n        vec![3.0, 0.0, 19.0, 0.0, 0.0, 7.8958, 2.0], // Male, didn't survive\n        vec![3.0, 1.0, 15.0, 0.0, 0.0, 8.0292, 1.0], // Female, survived\n        // More diverse examples with missing ages\n        vec![1.0, 1.0, f32::NAN, 1.0, 0.0, 78.85, 2.0], // Female, missing age, survived\n        vec![3.0, 0.0, f32::NAN, 0.0, 0.0, 7.75, 1.0],  // Male, missing age, didn't survive\n        vec![2.0, 1.0, f32::NAN, 0.0, 0.0, 10.5, 2.0],  // Female, missing age, survived\n        vec![3.0, 0.0, f32::NAN, 1.0, 0.0, 15.5, 2.0],  // Male, missing age, didn't survive\n        vec![1.0, 0.0, 45.0, 0.0, 0.0, 28.7125, 2.0],   // Male, didn't survive\n        vec![3.0, 1.0, 31.0, 1.0, 0.0, 18.0, 2.0],      // Female, survived\n        vec![1.0, 0.0, 54.0, 0.0, 0.0, 51.8625, 2.0],   // Male, didn't survive\n        vec![3.0, 1.0, 4.0, 3.0, 1.0, 31.275, 2.0],     // Female child, survived\n        vec![2.0, 0.0, 29.0, 0.0, 0.0, 13.0, 2.0],      // Male, didn't survive\n        vec![3.0, 0.0, 25.0, 1.0, 0.0, 7.775, 2.0],     // Male, didn't survive\n        // Additional samples for better training\n        vec![1.0, 1.0, 35.0, 0.0, 0.0, 128.0, 2.0], // Female, survived\n        vec![3.0, 0.0, 28.0, 0.0, 0.0, 7.05, 2.0],  // Male, didn't survive\n        vec![2.0, 1.0, 30.0, 0.0, 0.0, 26.0, 2.0],  // Female, survived\n        vec![3.0, 0.0, 20.0, 0.0, 0.0, 8.05, 2.0],  // Male, didn't survive\n        vec![1.0, 1.0, 49.0, 0.0, 0.0, 110.8833, 0.0], // Female, survived\n        vec![3.0, 0.0, 21.0, 2.0, 0.0, 11.5, 2.0],  // Male, didn't survive\n        vec![2.0, 0.0, 39.0, 0.0, 0.0, 26.0, 2.0],  // Male, didn't survive\n        vec![3.0, 1.0, 16.0, 1.0, 1.0, 20.2125, 2.0], // Female, survived\n        vec![1.0, 0.0, 80.0, 0.0, 0.0, 30.0, 2.0],  // Male, survived (oldest passenger)\n        vec![3.0, 0.0, 33.0, 0.0, 0.0, 7.8958, 2.0], // Male, didn't survive\n    ];\n\n    let targets: Vec\u003cf32\u003e = vec![\n        // First class\n        1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, // Second class\n        1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, // Third class\n        0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, // Missing ages\n        1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, // Additional\n        1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0,\n    ];\n\n    (features, targets)\n}\n\n/// Calculate sigmoid for binary classification probability.\nfn sigmoid(x: f64) -\u003e f64 {\n    1.0 / (1.0 + (-x).exp())\n}\n\n/// Calculate classification metrics.\nfn calculate_metrics(predictions: \u0026[bool], targets: \u0026[bool]) -\u003e (f64, f64, f64, f64) {\n    let mut tp = 0usize; // True positives\n    let mut tn = 0usize; // True negatives\n    let mut fp = 0usize; // False positives\n    let mut fn_ = 0usize; // False negatives\n\n    for (pred, target) in predictions.iter().zip(targets.iter()) {\n        match (pred, target) {\n            (true, true) =\u003e tp += 1,\n            (false, false) =\u003e tn += 1,\n            (true, false) =\u003e fp += 1,\n            (false, true) =\u003e fn_ += 1,\n        }\n    }\n\n    let accuracy = (tp + tn) as f64 / predictions.len() as f64;\n    let precision = if tp + fp == 0 {\n        0.0\n    } else {\n        tp as f64 / (tp + fp) as f64\n    };\n    let recall = if tp + fn_ == 0 {\n        0.0\n    } else {\n        tp as f64 / (tp + fn_) as f64\n    };\n    let f1 = if precision + recall == 0.0 {\n        0.0\n    } else {\n        2.0 * precision * recall / (precision + recall)\n    };\n\n    (accuracy, precision, recall, f1)\n}\n\nfn main() -\u003e Result\u003c(), Box\u003cdyn Error\u003e\u003e {\n    println!(\"=== Titanic Survival Prediction Pipeline ===\\n\");\n\n    // 1. Load data\n    println!(\"Loading Titanic dataset...\");\n    let (features, targets) = get_titanic_data();\n    let n_samples = features.len();\n    println!(\"  {} passengers loaded\", n_samples);\n    println!(\n        \"  {} survivors ({}%)\",\n        targets.iter().filter(|\u0026\u0026t| t == 1.0).count(),\n        100.0 * targets.iter().filter(|\u0026\u0026t| t == 1.0).count() as f64 / n_samples as f64\n    );\n\n    // 2. Train/test split (80/20)\n    let split_idx = (n_samples as f64 * 0.8) as usize;\n    let (train_features, test_features) = features.split_at(split_idx);\n    let (train_targets, test_targets) = targets.split_at(split_idx);\n\n    println!(\n        \"\\nTrain/test split: {}/{}\",\n        train_features.len(),\n        test_features.len()\n    );\n\n    // 3. Convert to tensors\n    let flat_train: Vec\u003cf32\u003e = train_features.iter().flatten().copied().collect();\n    let x_train = Tensor2D::\u003cCpuBackend\u003e::new(flat_train, train_features.len(), 7);\n    let y_train = train_targets.to_vec();\n\n    let flat_test: Vec\u003cf32\u003e = test_features.iter().flatten().copied().collect();\n    let x_test = Tensor2D::\u003cCpuBackend\u003e::new(flat_test, test_features.len(), 7);\n\n    // 4. Build preprocessing pipeline\n    println!(\"\\nBuilding preprocessing pipeline...\");\n\n    // Numerical features (Age, SibSp, Parch, Fare): impute + scale\n    let numerical_pipeline = Pipeline::\u003cCpuBackend\u003e::new()\n        .add_simple_imputer(SimpleImputer::new(ImputeStrategy::Mean))\n        .add_standard_scaler(StandardScaler::new());\n\n    // Categorical features (Sex, Embarked): one-hot encoding\n    // Ordinal feature (Pclass): scale (treat as continuous for simplicity)\n\n    let column_transformer = ColumnTransformer::\u003cCpuBackend\u003e::new()\n        // Pclass (ordinal, column 0) - scale\n        .add_standard_scaler(StandardScaler::new(), ColumnSpec::Indices(vec![PCLASS]))\n        // Sex (categorical, column 1) - one-hot encode\n        .add_one_hot_encoder(OneHotEncoder::new(), ColumnSpec::Indices(vec![SEX]))\n        // Age, SibSp, Parch, Fare (numerical, columns 2,3,4,5) - impute + scale\n        .add_pipeline(\n            numerical_pipeline,\n            ColumnSpec::Indices(vec![AGE, SIBSP, PARCH, FARE]),\n        )\n        // Embarked (categorical, column 6) - one-hot encode\n        .add_one_hot_encoder(OneHotEncoder::new(), ColumnSpec::Indices(vec![EMBARKED]));\n\n    // 5. Fit preprocessor\n    println!(\"Fitting preprocessor...\");\n    let fitted_ct = column_transformer.fit(\u0026x_train)?;\n    println!(\"  Input features: {}\", fitted_ct.n_features_in());\n    println!(\"  Output features: {}\", fitted_ct.n_features_out());\n\n    // 6. Transform data\n    let x_train_processed = fitted_ct.transform(\u0026x_train)?;\n    let x_test_processed = fitted_ct.transform(\u0026x_test)?;\n\n    // 7. Optionally add polynomial features (comment out for simpler model)\n    // Uncomment the following block to use polynomial features:\n    /*\n    println!(\"\\nAdding polynomial features...\");\n    let poly = PolynomialFeatures::\u003cCpuBackend\u003e::new()\n        .with_degree(2)\n        .with_include_bias(false)\n        .with_interaction_only(true);\n    let fitted_poly = poly.fit(\u0026x_train_processed)?;\n    println!(\"  Features after polynomial: {}\", fitted_poly.n_features_out());\n    let x_train_final = fitted_poly.transform(\u0026x_train_processed)?;\n    let x_test_final = fitted_poly.transform(\u0026x_test_processed)?;\n    */\n    // For now, use linear features\n    let fitted_poly = None;\n    let x_train_final = x_train_processed;\n    let x_test_final = x_test_processed;\n\n    // 8. Create dataset for training\n    let (n_rows, n_features) = x_train_final.shape();\n    let x_train_vec: Vec\u003cVec\u003cf32\u003e\u003e = {\n        let flat = x_train_final.ravel().to_vec();\n        (0..n_rows)\n            .map(|r| {\n                (0..n_features)\n                    .map(|c| flat[r * n_features + c] as f32)\n                    .collect()\n            })\n            .collect()\n    };\n\n    let dataset = InMemoryDataset::new(x_train_vec, y_train.clone())?;\n\n    // 9. Train model (Logistic Regression using BCEWithLogitsLoss)\n    println!(\"\\nTraining logistic regression model...\");\n    let model = machinelearne_rs::model::linear::LinearRegression::\u003cCpuBackend\u003e::new(n_features);\n    let loss = BCEWithLogitsLoss;\n    let optimizer = SGD::new(0.5);\n    let regularizer = NoRegularizer;\n\n    let trainer = Trainer::builder(loss, optimizer, regularizer)\n        .batch_size(10)\n        .max_epochs(1000)\n        .build();\n\n    let fitted_model = trainer.fit(model, \u0026dataset)?;\n\n    // 10. Evaluate on test set\n    println!(\"\\n=== Evaluation on Test Set ===\\n\");\n\n    // Get predictions\n    let test_logits = fitted_model.predict_batch(\u0026x_test_final);\n    let logits_vec = test_logits.to_vec();\n\n    // Convert to binary predictions (threshold at 0.5 probability)\n    let predictions: Vec\u003cbool\u003e = logits_vec\n        .iter()\n        .map(|\u0026logit| sigmoid(logit as f64) \u003e= 0.5)\n        .collect();\n    let actual: Vec\u003cbool\u003e = test_targets.iter().map(|\u0026t| t == 1.0).collect();\n\n    // Calculate metrics\n    let (accuracy, precision, recall, f1) = calculate_metrics(\u0026predictions, \u0026actual);\n\n    println!(\"Predictions vs Actual:\");\n    println!(\n        \"{:\u003c5} {:\u003c10} {:\u003c10} {:\u003c10}\",\n        \"Idx\", \"Prob\", \"Pred\", \"Actual\"\n    );\n    println!(\"{}\", \"-\".repeat(40));\n    for (i, (logit, \u0026actual_val)) in logits_vec.iter().zip(test_targets.iter()).enumerate() {\n        let prob = sigmoid(*logit as f64);\n        let pred = if prob \u003e= 0.5 { \"Survived\" } else { \"Died\" };\n        let act = if actual_val == 1.0 {\n            \"Survived\"\n        } else {\n            \"Died\"\n        };\n        println!(\"{:\u003c5} {:\u003c10.3} {:\u003c10} {:\u003c10}\", i, prob, pred, act);\n    }\n\n    println!(\"\\n=== Classification Metrics ===\");\n    println!(\"Accuracy:  {:.1}%\", accuracy * 100.0);\n    println!(\"Precision: {:.1}%\", precision * 100.0);\n    println!(\"Recall:    {:.1}%\", recall * 100.0);\n    println!(\"F1 Score:  {:.1}%\", f1 * 100.0);\n\n    // 11. Save complete pipeline\n    println!(\"\\n=== Saving Pipeline ===\");\n    let pipeline = PredictivePipeline::new(fitted_ct, fitted_poly, fitted_model);\n\n    let temp_file = std::env::temp_dir().join(\"titanic_pipeline.bin\");\n    pipeline.save_to_file(\u0026temp_file)?;\n    println!(\"Pipeline saved to: {:?}\", temp_file);\n\n    // 12. Load and verify\n    println!(\"\\nLoading pipeline...\");\n    let loaded_pipeline =\n        PredictivePipeline::\u003cCpuBackend, LinearModel\u003cCpuBackend, Fitted\u003e\u003e::load_from_file(\n            \u0026temp_file,\n            |bytes| {\n                let serial_params: SerializableLinearParams =\n                    bincode::deserialize(bytes).map_err(|e| {\n                        machinelearne_rs::preprocessing::PreprocessingError::SerializationError(\n                            e.to_string(),\n                        )\n                    })?;\n                let params = LinearParams::try_from(serial_params).map_err(|e| {\n                    machinelearne_rs::preprocessing::PreprocessingError::SerializationError(\n                        e.to_string(),\n                    )\n                })?;\n                Ok(\u003cLinearModel\u003cCpuBackend, Fitted\u003e\u003e::new(params))\n            },\n        )?;\n    println!(\"Pipeline loaded successfully!\");\n    println!(\n        \"Expected input features: {}\",\n        loaded_pipeline.n_features_in()\n    );\n\n    // 13. Demo prediction for a new passenger\n    println!(\"\\n=== Demo: Predict for New Passenger ===\");\n    // New passenger: 1st class, female, 25 years old, no family, $100 fare, embarked at Southampton\n    let new_passenger =\n        Tensor2D::\u003cCpuBackend\u003e::new(vec![1.0, 1.0, 25.0, 0.0, 0.0, 100.0, 2.0], 1, 7);\n    let logit = loaded_pipeline.predict(\u0026new_passenger)?;\n    let prob = sigmoid(logit.to_vec()[0] as f64);\n    println!(\"New passenger: 1st class, female, 25yo, no family, $100 fare, Southampton\");\n    println!(\"Survival probability: {:.1}%\", prob * 100.0);\n\n    // Another passenger: 3rd class, male, 30 years old, no family, $10 fare, Southampton\n    let another_passenger =\n        Tensor2D::\u003cCpuBackend\u003e::new(vec![3.0, 0.0, 30.0, 0.0, 0.0, 10.0, 2.0], 1, 7);\n    let logit2 = loaded_pipeline.predict(\u0026another_passenger)?;\n    let prob2 = sigmoid(logit2.to_vec()[0] as f64);\n    println!(\"\\nAnother passenger: 3rd class, male, 30yo, no family, $10 fare, Southampton\");\n    println!(\"Survival probability: {:.1}%\", prob2 * 100.0);\n\n    // Cleanup\n    std::fs::remove_file(\u0026temp_file).ok();\n\n    println!(\"\\n=== Pipeline Complete ===\");\n    Ok(())\n}\n","traces":[],"covered":0,"coverable":0},{"path":["/","home","victor","ML","machinelearne-rs","lib","examples","train_linear.rs"],"content":"// examples/train_linear.rs Ð¸Ð»Ð¸ Ð² ÑÐµÑÑÐ°Ñ\nuse machinelearne_rs::{\n    dataset::memory::InMemoryDataset,\n    loss::MSELoss,\n    model::{linear::LinearRegressor, InferenceModel},\n    optimizer::SGD,\n    regularizers::NoRegularizer,\n    trainer::Trainer,\n    CpuBackend, Tensor1D,\n};\n\nfn main() {\n    let model = LinearRegressor::new(2); // 2 ÑÐ¸ÑÐ¸\n    let loss = MSELoss;\n    let opt = SGD::new(0.1);\n    let reg = NoRegularizer;\n    let trainer = Trainer::builder(loss, opt, reg)\n        .batch_size(20)\n        .max_epochs(500)\n        .build();\n    let x = vec![\n        vec![1.0, 1.0], // y â 1*1 + 2*1 = 3\n        vec![2.0, 1.0], // y â 2 + 2 = 4\n        vec![1.0, 2.0], // y â 1 + 4 = 5\n        vec![2.0, 2.0], // y â 2 + 4 = 6\n        vec![3.0, 3.0], // â Ð²ÑÐ±ÑÐ¾Ñ Ð¿Ð¾ y!\n    ];\n    let y = vec![3.0, 4.0, 5.0, 6.0, 30.0];\n    let dataset = InMemoryDataset::new(x, y).unwrap();\n\n    let fitted_model = trainer.fit(model, \u0026dataset).unwrap();\n    let inp = Tensor1D::\u003cCpuBackend\u003e::new((\u0026[4.0, 5.0]).to_vec());\n    let pred = fitted_model.predict(\u0026inp);\n    println!(\"Prediction: {:?}\", pred);\n}\n","traces":[],"covered":0,"coverable":0},{"path":["/","home","victor","ML","machinelearne-rs","lib","examples","train_linear_l2.rs"],"content":"// examples/train_linear.rs Ð¸Ð»Ð¸ Ð² ÑÐµÑÑÐ°Ñ\nuse machinelearne_rs::{\n    dataset::memory::InMemoryDataset, loss::MSELoss, model::linear::LinearRegressor,\n    model::InferenceModel, optimizer::SGD, regularizers::L2, trainer::Trainer, CpuBackend,\n    Tensor1D,\n};\n\nfn main() {\n    let model = LinearRegressor::new(2); // 2 ÑÐ¸ÑÐ¸\n    let loss = MSELoss;\n    let opt = SGD::new(0.01);\n    let reg = L2::new(0.1);\n    let trainer = Trainer::builder(loss, opt, reg)\n        .batch_size(20)\n        .max_epochs(500)\n        .build();\n    let x = vec![vec![1.0, 2.0], vec![2.0, 3.0], vec![3.0, 4.0]];\n    let y = vec![3.0, 5.0, 7.0];\n\n    let dataset = InMemoryDataset::new(x, y).unwrap();\n\n    let fitted_model = trainer.fit(model, \u0026dataset).unwrap();\n    println!(\n        \"Prediction: {:?}\",\n        fitted_model.predict(\u0026Tensor1D::\u003cCpuBackend\u003e::new((\u0026[4.0, 5.0]).to_vec()))\n    );\n}\n","traces":[],"covered":0,"coverable":0},{"path":["/","home","victor","ML","machinelearne-rs","lib","examples","train_linear_mae.rs"],"content":"// examples/train_linear.rs Ð¸Ð»Ð¸ Ð² ÑÐµÑÑÐ°Ñ\nuse machinelearne_rs::{\n    dataset::memory::InMemoryDataset, loss::MAELoss, model::linear::LinearRegressor,\n    model::InferenceModel, optimizer::SGD, regularizers::NoRegularizer, trainer::Trainer,\n    CpuBackend, Tensor1D,\n};\n\nfn main() {\n    let model = LinearRegressor::new(2); // 2 ÑÐ¸ÑÐ¸\n    let loss = MAELoss;\n    let opt = SGD::new(0.01);\n    let reg = NoRegularizer;\n    let trainer = Trainer::builder(loss, opt, reg)\n        .batch_size(20)\n        .max_epochs(500)\n        .build();\n    let x = vec![\n        vec![1.0, 1.0], // y â 1*1 + 2*1 = 3\n        vec![2.0, 1.0], // y â 2 + 2 = 4\n        vec![1.0, 2.0], // y â 1 + 4 = 5\n        vec![2.0, 2.0], // y â 2 + 4 = 6\n        vec![3.0, 3.0], // â Ð²ÑÐ±ÑÐ¾Ñ Ð¿Ð¾ y!\n    ];\n    let y = vec![3.0, 4.0, 5.0, 6.0, 30.0];\n\n    let dataset = InMemoryDataset::new(x, y).unwrap();\n\n    let fitted_model = trainer.fit(model, \u0026dataset).unwrap();\n    println!(\n        \"Prediction: {:?}\",\n        fitted_model.predict(\u0026Tensor1D::\u003cCpuBackend\u003e::new((\u0026[4.0, 5.0]).to_vec()))\n    );\n}\n","traces":[],"covered":0,"coverable":0},{"path":["/","home","victor","ML","machinelearne-rs","lib","examples","train_linear_ndarray.rs"],"content":"// examples/train_linear.rs Ð¸Ð»Ð¸ Ð² ÑÐµÑÑÐ°Ñ\n\nuse machinelearne_rs::{\n    dataset::memory::InMemoryDataset,\n    loss::MSELoss,\n    model::{linear::LinearRegression, InferenceModel},\n    optimizer::SGD,\n    regularizers::NoRegularizer,\n    trainer::Trainer,\n    Tensor1D,\n};\n#[cfg(feature = \"ndarray\")]\nfn main() {\n    use machinelearne_rs::backend::NdarrayBackend;\n    let model = LinearRegression::\u003cNdarrayBackend\u003e::new(2); // 2 ÑÐ¸ÑÐ¸\n    let loss = MSELoss;\n    let opt = SGD::new(0.1);\n    let reg = NoRegularizer;\n    let trainer = Trainer::builder(loss, opt, reg)\n        .batch_size(20)\n        .max_epochs(500)\n        .build();\n    let x = vec![\n        vec![1.0, 1.0], // y â 1*1 + 2*1 = 3\n        vec![2.0, 1.0], // y â 2 + 2 = 4\n        vec![1.0, 2.0], // y â 1 + 4 = 5\n        vec![2.0, 2.0], // y â 2 + 4 = 6\n        vec![3.0, 3.0], // â Ð²ÑÐ±ÑÐ¾Ñ Ð¿Ð¾ y!\n    ];\n    let y = vec![3.0, 4.0, 5.0, 6.0, 30.0];\n    let dataset = InMemoryDataset::new(x, y).unwrap();\n\n    let fitted_model = trainer.fit(model, \u0026dataset).unwrap();\n    let inp = Tensor1D::\u003cNdarrayBackend\u003e::new((\u0026[4.0, 5.0]).to_vec());\n    let pred = fitted_model.predict(\u0026inp);\n    println!(\"Prediction: {:?}\", pred);\n}\n\n#[cfg(not(feature = \"ndarray\"))]\nfn main() {\n    println!(\"This example requires the `ndarray` feature. Run with:\");\n    println!(\"cargo run --example train_linear_ndarray --features ndarray\");\n    std::process::exit(1);\n}\n","traces":[],"covered":0,"coverable":0},{"path":["/","home","victor","ML","machinelearne-rs","lib","examples","train_logistic.rs"],"content":"// examples/train_logistic.rs\nuse machinelearne_rs::{\n    backend::scalar::Scalar, dataset::memory::InMemoryDataset, loss::BCEWithLogitsLoss,\n    model::linear::LinearRegressor, model::InferenceModel, optimizer::SGD,\n    regularizers::NoRegularizer, trainer::Trainer, CpuBackend, Tensor1D,\n};\n\nfn main() {\n    // ÐÐ¸Ð½Ð°ÑÐ½ÑÐµ Ð´Ð°Ð½Ð½ÑÐµ: y = 1 ÐµÑÐ»Ð¸ x1 + x2 \u003e 3, Ð¸Ð½Ð°ÑÐµ 0\n    let x = vec![\n        vec![1.0, 1.0], // sum=2 â y=0\n        vec![1.0, 2.0], // sum=3 â y=0 (Ð³ÑÐ°Ð½Ð¸ÑÐ½ÑÐ¹ ÑÐ»ÑÑÐ°Ð¹)\n        vec![2.0, 2.0], // sum=4 â y=1\n        vec![3.0, 1.0], // sum=4 â y=1\n        vec![0.5, 0.5], // sum=1 â y=0\n        // ÐÐ¾Ð±Ð°Ð²Ð¸Ð¼ Ð²ÑÐ±ÑÐ¾Ñ: ÑÐ¾ÑÐºÐ° Ñ y=1, ÑÐ¾ÑÑ ÑÑÐ¼Ð¼Ð° Ð¼Ð°Ð»Ð°\n        vec![1.0, 1.0], // sum=2 â Ð½Ð¾ Ð¿Ð¾Ð¼ÐµÑÐ¸Ð¼ ÐºÐ°Ðº y=1 (ÑÑÐ¼/Ð¾ÑÐ¸Ð±ÐºÐ°)\n    ];\n    let y = vec![0.0, 0.0, 1.0, 1.0, 0.0, 1.0];\n\n    let model = LinearRegressor::new(2); // 2 Ð¿ÑÐ¸Ð·Ð½Ð°ÐºÐ° â 1 Ð»Ð¾Ð³Ð¸Ñ\n    let loss = BCEWithLogitsLoss;\n    let opt = SGD::new(0.5); // Ð¼Ð¾Ð¶ÐµÑ Ð¿Ð¾ÑÑÐµÐ±Ð¾Ð²Ð°ÑÑÑÑ Ð±Ð¾Ð»ÑÑÐ¸Ð¹ LR Ð´Ð»Ñ BCE\n    let reg = NoRegularizer;\n\n    let trainer = Trainer::builder(loss, opt, reg)\n        .batch_size(10)\n        .max_epochs(1000)\n        .build();\n\n    let dataset = InMemoryDataset::new(x, y).unwrap();\n\n    let fitted_model = trainer.fit(model, \u0026dataset).unwrap();\n    let logit = fitted_model.predict(\u0026Tensor1D::\u003cCpuBackend\u003e::new((\u0026[2.5, 2.0]).to_vec()));\n    let one = Scalar::\u003cCpuBackend\u003e::new(1.);\n    let minus_one = Scalar::\u003cCpuBackend\u003e::new(-1.);\n    let prob = one / (one + (logit * minus_one));\n    let prob = prob.exp();\n    println!(\"Logit: {:?}, Probability: {:?}\", logit, prob);\n}\n","traces":[],"covered":0,"coverable":0},{"path":["/","home","victor","ML","machinelearne-rs","lib","src","backend","cpu.rs"],"content":"//! # CPU Backend\n//!\n//! Pure-Rust CPU backend implementation with zero external dependencies.\n//! Uses `f64` precision and row-major memory layout for all tensors.\n//!\n//! ## Design Characteristics\n//!\n//! - **Minimal dependencies**: No external crates required (enabled via `cpu` feature)\n//! - **Row-major layout**: 2D tensors stored as flat `Vec\u003cf64\u003e` in row-major order\n//! - **f64 precision**: All computations use double precision for numerical stability\n//! - **Naive implementations**: Straightforward algorithms prioritizing correctness;\n//!   optimizations (SIMD, cache-aware layouts) are future work\n//!\n//! ## Tensor Representations\n//!\n//! | Type          | Rust Type     | Layout                     |\n//! |---------------|---------------|----------------------------|\n//! | 1D Tensor     | `Vec\u003cf64\u003e`    | Contiguous elements        |\n//! | 2D Tensor     | `CpuTensor2D` | `(Vec\u003cf64\u003e, rows, cols)`   |\n//!\n//! ## Performance Notes\n//!\n//! - Matrix-vector multiplication (`matvec`) uses naive O(nÂ²) implementation\n//! - Transpose creates a new allocated tensor (no view/slice optimization yet)\n//! - Element-wise operations are not SIMD-accelerated (future optimization target)\n//!\n//! ## Example\n//!\n//! ```rust\n//! use machinelearne_rs::backend::{Backend, CpuBackend, cpu::CpuTensor2D};\n//!\n//! // Create a 2x2 matrix: [[1.0, 2.0], [3.0, 4.0]]\n//! let w = CpuTensor2D::new(vec![1.0, 2.0, 3.0, 4.0], 2, 2);\n//!\n//! // Create input vector [1.0, 0.0]\n//! let x = vec![1.0, 0.0];\n//!\n//! // Compute matrix-vector product\n//! let y = CpuBackend::matvec(\u0026w, \u0026x); // Result: [1.0, 3.0]\n//! ```\n\nuse super::Backend;\n\n/// Pure-Rust CPU computation backend.\n///\n/// Provides baseline tensor operations without external dependencies.\n/// All computations use `f64` precision for numerical stability during training.\n///\n/// # Usage\n///\n/// ```rust\n/// use machinelearne_rs::backend::{Backend, CpuBackend};\n///\n/// let zeros = CpuBackend::zeros_1d(5);\n/// assert_eq!(zeros, vec![0.0; 5]);\n/// ```\n#[derive(Clone, Debug, Copy)]\npub struct CpuBackend;\n\n/// Two-dimensional tensor representation for CPU backend.\n///\n/// Stores data in **row-major order** as a flat `Vec\u003cf64\u003e` with explicit shape metadata.\n///\n/// # Memory Layout\n///\n/// For a `(rows=2, cols=3)` matrix:\n/// ```text\n/// [[a, b, c],\n///  [d, e, f]]\n/// ```\n/// Stored as: `[a, b, c, d, e, f]`\n///\n/// # Invariants\n///\n/// - `data.len() == rows * cols` (enforced in `new()`)\n/// - Row `i`, column `j` element at index `i * cols + j`\n#[derive(Debug, Clone)]\npub struct CpuTensor2D(pub Vec\u003cf64\u003e, pub usize, pub usize);\n\nimpl CpuTensor2D {\n    /// Creates a new 2D tensor with explicit shape validation.\n    ///\n    /// # Arguments\n    /// * `data` - Elements in row-major order\n    /// * `rows` - Number of rows\n    /// * `cols` - Number of columns\n    ///\n    /// # Panics\n    /// Panics if `data.len() != rows * cols`.\n    ///\n    /// # Example\n    /// ```rust\n    /// use machinelearne_rs::backend::cpu::CpuTensor2D;\n    ///\n    /// let t = CpuTensor2D::new(vec![1.0, 2.0, 3.0, 4.0], 2, 2);\n    /// assert_eq!(t.0, vec![1.0, 2.0, 3.0, 4.0]);\n    /// assert_eq!((t.1, t.2), (2, 2));\n    /// ```\n    pub fn new(data: Vec\u003cf64\u003e, rows: usize, cols: usize) -\u003e Self {\n        assert_eq!(data.len(), rows * cols, \"Inconsistent shape\");\n        Self(data, rows, cols)\n    }\n}\n\nimpl From\u003c\u0026[Vec\u003cf64\u003e]\u003e for CpuTensor2D {\n    /// Converts a nested vector representation into a row-major 2D tensor.\n    ///\n    /// # Arguments\n    /// * `x` - Slice of rows, where each row is a `Vec\u003cf64\u003e`\n    ///\n    /// # Panics\n    /// * If rows have inconsistent lengths\n    /// * If input is non-empty but contains empty rows\n    ///\n    /// # Example\n    /// ```rust\n    /// use machinelearne_rs::backend::cpu::CpuTensor2D;\n    ///\n    /// let nested = vec![vec![1.0, 2.0], vec![3.0, 4.0]];\n    /// let t = CpuTensor2D::from(\u0026nested[..]);\n    /// assert_eq!(t.0, vec![1.0, 2.0, 3.0, 4.0]);\n    /// assert_eq!((t.1, t.2), (2, 2));\n    /// ```\n    fn from(x: \u0026[Vec\u003cf64\u003e]) -\u003e Self {\n        if x.is_empty() {\n            return CpuTensor2D::new(Vec::new(), 0, 0);\n        }\n        let rows = x.len();\n        let cols = x[0].len();\n        assert!(\n            x.iter().all(|row| row.len() == cols),\n            \"All rows must have same length\"\n        );\n        let data: Vec\u003cf64\u003e = x.iter().flat_map(|row| row.iter()).copied().collect();\n        CpuTensor2D::new(data, rows, cols)\n    }\n}\n\nimpl Backend for CpuBackend {\n    type Scalar = f64;\n    type Tensor1D = Vec\u003cf64\u003e;\n    type Tensor2D = CpuTensor2D;\n    type Device = ();\n\n    /// Returns the default device identifier for CPU backend.\n    ///\n    /// Always returns unit type `()` since CPU operations don't require device selection.\n    fn default_device() -\u003e Self::Device {}\n\n    // --- Constructors ---\n\n    /// Creates a 1D tensor filled with zeros.\n    ///\n    /// # Arguments\n    /// * `len` - Number of elements\n    ///\n    /// # Example\n    /// ```rust\n    /// use machinelearne_rs::backend::{Backend, CpuBackend};\n    /// let zeros = CpuBackend::zeros_1d(3);\n    /// assert_eq!(zeros, vec![0.0, 0.0, 0.0]);\n    /// ```\n    fn zeros_1d(len: usize) -\u003e Self::Tensor1D {\n        vec![0.; len]\n    }\n\n    /// Creates a 2D tensor filled with zeros.\n    ///\n    /// # Arguments\n    /// * `rows` - Number of rows\n    /// * `cols` - Number of columns\n    ///\n    /// # Example\n    /// ```rust\n    /// use machinelearne_rs::backend::{Backend, CpuBackend};\n    /// let zeros = CpuBackend::zeros_2d(2, 3);\n    /// assert_eq!(zeros.0, vec![0.0; 6]);\n    /// assert_eq!((zeros.1, zeros.2), (2, 3));\n    /// ```\n    fn zeros_2d(rows: usize, cols: usize) -\u003e Self::Tensor2D {\n        CpuTensor2D::new(vec![0.; rows * cols], rows, cols)\n    }\n\n    /// Constructs a 1D tensor from `f32` data (converts to `f64`).\n    ///\n    /// # Arguments\n    /// * `data` - Source values as `f32`\n    ///\n    /// # Example\n    /// ```rust\n    /// use machinelearne_rs::backend::{Backend, CpuBackend};\n    /// let t = CpuBackend::from_vec_1d(vec![1.0f32, 2.5]);\n    /// assert_eq!(t, vec![1.0, 2.5]);\n    /// ```\n    fn from_vec_1d(data: Vec\u003cf32\u003e) -\u003e Self::Tensor1D {\n        data.into_iter().map(|x| x as f64).collect()\n    }\n\n    /// Constructs a 2D tensor from `f32` data (converts to `f64`).\n    ///\n    /// # Arguments\n    /// * `data` - Source values in row-major order as `f32`\n    /// * `rows` - Number of rows\n    /// * `cols` - Number of columns\n    ///\n    /// # Panics\n    /// Panics if `data.len() != rows * cols`.\n    fn from_vec_2d(data: Vec\u003cf32\u003e, rows: usize, cols: usize) -\u003e Self::Tensor2D {\n        CpuTensor2D::new(data.into_iter().map(|x| x as f64).collect(), rows, cols)\n    }\n\n    // --- Element-wise operations (1D) ---\n\n    /// Element-wise addition of two 1D tensors.\n    ///\n    /// # Panics\n    /// Panics if tensors have different lengths.\n    fn add_1d(a: \u0026Self::Tensor1D, b: \u0026Self::Tensor1D) -\u003e Self::Tensor1D {\n        a.iter().zip(b.iter()).map(|(a, b)| a + b).collect()\n    }\n\n    /// Element-wise subtraction of two 1D tensors.\n    ///\n    /// # Panics\n    /// Panics if tensors have different lengths.\n    fn sub_1d(a: \u0026Self::Tensor1D, b: \u0026Self::Tensor1D) -\u003e Self::Tensor1D {\n        a.iter().zip(b.iter()).map(|(a, b)| a - b).collect()\n    }\n\n    /// Element-wise multiplication of two 1D tensors.\n    ///\n    /// # Panics\n    /// Panics if tensors have different lengths.\n    fn mul_1d(a: \u0026Self::Tensor1D, b: \u0026Self::Tensor1D) -\u003e Self::Tensor1D {\n        a.iter().zip(b.iter()).map(|(a, b)| a * b).collect()\n    }\n\n    /// Element-wise division of two 1D tensors.\n    ///\n    /// # Panics\n    /// Panics if tensors have different lengths or divisor contains zeros.\n    fn div_1d(a: \u0026Self::Tensor1D, b: \u0026Self::Tensor1D) -\u003e Self::Tensor1D {\n        a.iter().zip(b.iter()).map(|(a, b)| a / b).collect()\n    }\n\n    /// Multiplies each element of a 1D tensor by a scalar.\n    fn mul_scalar_1d(t: \u0026Self::Tensor1D, s: \u0026Self::Scalar) -\u003e Self::Tensor1D {\n        t.iter().map(|x| *x * s).collect()\n    }\n\n    /// Adds a scalar to each element of a 1D tensor.\n    fn add_scalar_1d(t: \u0026Self::Tensor1D, s: \u0026Self::Scalar) -\u003e Self::Tensor1D {\n        t.iter().map(|x| x + s).collect()\n    }\n\n    // --- Element-wise operations (2D) ---\n\n    /// Multiplies each element of a 2D tensor by a scalar.\n    fn mul_scalar_2d(t: \u0026Self::Tensor2D, s: \u0026Self::Scalar) -\u003e Self::Tensor2D {\n        CpuTensor2D::new(t.0.iter().map(|x| *x * s).collect(), t.1, t.2)\n    }\n\n    /// Adds a scalar to each element of a 2D tensor.\n    fn add_scalar_2d(t: \u0026Self::Tensor2D, s: \u0026Self::Scalar) -\u003e Self::Tensor2D {\n        CpuTensor2D::new(t.0.iter().map(|x| *x + s).collect(), t.1, t.2)\n    }\n\n    /// Element-wise addition of two 2D tensors.\n    ///\n    /// # Panics\n    /// Panics if tensors have different shapes.\n    fn add_2d(a: \u0026Self::Tensor2D, b: \u0026Self::Tensor2D) -\u003e Self::Tensor2D {\n        CpuTensor2D::new(\n            a.0.iter().zip(b.0.iter()).map(|(a, b)| a + b).collect(),\n            a.1,\n            a.2,\n        )\n    }\n\n    /// Element-wise subtraction of two 2D tensors.\n    ///\n    /// # Panics\n    /// Panics if tensors have different shapes (rows or columns mismatch).\n    ///\n    /// # Why panic instead of silent truncation?\n    /// Silent truncation via `.zip()` would produce mathematically invalid results\n    /// (e.g., subtracting a 3Ã2 matrix from 2Ã3) without any indication of error.\n    /// This violates the principle of least surprise and makes debugging extremely hard.\n    fn sub_2d(a: \u0026Self::Tensor2D, b: \u0026Self::Tensor2D) -\u003e Self::Tensor2D {\n        if a.1 != b.1 || a.2 != b.2 {\n            panic!(\n                \"sub_2d: shape mismatch â cannot subtract ({}, {}) from ({}, {}). \\\n             Shapes must be identical for element-wise operations.\",\n                a.1, a.2, b.1, b.2\n            );\n        }\n        CpuTensor2D::new(\n            a.0.iter().zip(b.0.iter()).map(|(x, y)| x - y).collect(),\n            a.1,\n            a.2,\n        )\n    }\n\n    /// Element-wise multiplication of two 2D tensors.\n    ///\n    /// # Panics\n    /// Panics if tensors have different shapes.\n    fn mul_2d(a: \u0026Self::Tensor2D, b: \u0026Self::Tensor2D) -\u003e Self::Tensor2D {\n        CpuTensor2D::new(\n            a.0.iter().zip(b.0.iter()).map(|(a, b)| a * b).collect(),\n            a.1,\n            a.2,\n        )\n    }\n\n    /// Element-wise division of two 2D tensors.\n    ///\n    /// # Panics\n    /// Panics if tensors have different shapes or divisor contains zeros.\n    fn div_2d(a: \u0026Self::Tensor2D, b: \u0026Self::Tensor2D) -\u003e Self::Tensor2D {\n        CpuTensor2D::new(\n            a.0.iter().zip(b.0.iter()).map(|(a, b)| a / b).collect(),\n            a.1,\n            a.2,\n        )\n    }\n\n    // --- Reduction operations ---\n\n    /// Computes the arithmetic mean of all elements in a 1D tensor.\n    ///\n    /// # Returns\n    /// * Mean value as `f64`\n    /// * Returns `0.0` for empty tensors (by convention)\n    fn mean_all_1d(t: \u0026Self::Tensor1D) -\u003e Self::Scalar {\n        if t.is_empty() {\n            0.0\n        } else {\n            t.iter().sum::\u003cf64\u003e() / t.len() as f64\n        }\n    }\n\n    /// Computes the arithmetic mean of all elements in a 2D tensor.\n    ///\n    /// # Panics\n    /// Panics if the tensor is empty (0 elements). This follows the \"fail fast\" principle\n    /// common in numerical libraries â an empty tensor almost always indicates a bug in\n    /// data preprocessing or batch construction rather than a legitimate edge case.\n    ///\n    /// # Why panic instead of returning 0.0 or NaN?\n    /// - `0.0` is mathematically incorrect and masks critical bugs (e.g., empty batches)\n    /// - `NaN` propagates silently through computations, making root cause analysis harder\n    /// - Immediate panic forces developers to fix data pipeline issues early\n    ///\n    /// # Example of legitimate use\n    /// ```should_panic\n    /// # use machinelearne_rs::backend::{Tensor2D, CpuBackend};\n    /// let empty = Tensor2D::\u003cCpuBackend\u003e::zeros(0, 5);\n    /// let m = empty.mean(); // PANIC: empty tensor detected\n    /// ```\n    fn mean_all_2d(t: \u0026Self::Tensor2D) -\u003e Self::Scalar {\n        if t.0.is_empty() {\n            panic!(\n                \"mean_all_2d: cannot compute mean of empty tensor (shape: {:?}). \\\n             This likely indicates a bug in data loading or batch construction. \\\n             Check: batch size = 0, empty dataset partition, or incorrect slicing.\",\n                t.0.len()\n            );\n        }\n        t.0.iter().sum::\u003cf64\u003e() / t.0.len() as f64\n    }\n\n    /// Computes the sum of all elements in a 2D tensor.\n    fn sum_all_2d(t: \u0026Self::Tensor2D) -\u003e Self::Scalar {\n        t.0.iter().sum::\u003cf64\u003e()\n    }\n\n    /// Computes the sum of all elements in a 1D tensor.\n    fn sum_all_1d(t: \u0026Self::Tensor1D) -\u003e Self::Scalar {\n        t.iter().sum::\u003cf64\u003e()\n    }\n\n    // --- Scalar operations ---\n\n    /// Creates a backend-specific scalar from an `f64` value.\n    ///\n    /// For CPU backend, this is a trivial identity conversion.\n    fn scalar_f64(value: f64) -\u003e Self::Scalar {\n        value\n    }\n\n    // --- Data access ---\n\n    /// Converts a 1D tensor to a `Vec\u003cf64\u003e` for inspection or metric computation.\n    ///\n    /// # Note\n    /// This clones the underlying data. Avoid in performance-critical paths.\n    fn to_vec_1d(t: \u0026Self::Tensor1D) -\u003e Vec\u003cf64\u003e {\n        t.clone()\n    }\n\n    /// Returns the number of elements in a 1D tensor.\n    fn len_1d(t: \u0026Self::Tensor1D) -\u003e usize {\n        t.len()\n    }\n\n    /// Returns the number of rows.\n    fn len_2d(t: \u0026Self::Tensor2D) -\u003e usize {\n        t.1\n    }\n\n    // --- Mathematical functions (1D) ---\n\n    /// Element-wise absolute value operation.\n    fn abs_1d(t: \u0026Self::Tensor1D) -\u003e Self::Tensor1D {\n        t.iter().map(|x| x.abs()).collect()\n    }\n\n    /// Element-wise sign function.\n    ///\n    /// Returns:\n    /// * `1.0` for positive values\n    /// * `-1.0` for negative values\n    /// * `0.0` for zero (subgradient convention)\n    fn sign_1d(x: \u0026Self::Tensor1D) -\u003e Self::Tensor1D {\n        x.iter()\n            .map(|\u0026x| {\n                if x \u003e 0.0 {\n                    1.0\n                } else if x \u003c 0.0 {\n                    -1.0\n                } else {\n                    0.0\n                }\n            })\n            .collect()\n    }\n\n    /// Element-wise maximum between two tensors.\n    ///\n    /// # Panics\n    /// Panics if tensors have different lengths.\n    fn maximum_1d(x: \u0026Self::Tensor1D, other: \u0026Self::Tensor1D) -\u003e Self::Tensor1D {\n        x.iter()\n            .zip(other.iter())\n            .map(|(\u0026a, \u0026b)| a.max(b))\n            .collect()\n    }\n\n    /// Element-wise exponential function (`e^x`).\n    fn exp_1d(x: \u0026Self::Tensor1D) -\u003e Self::Tensor1D {\n        x.iter().map(|\u0026v| v.exp()).collect()\n    }\n\n    /// Computes element-wise natural logarithm: `ln(x)`.\n    ///\n    /// # Behavior for edge cases (IEEE 754 compliant)\n    /// - `x \u003e 0.0` â finite value `ln(x)`\n    /// - `x == 0.0` â `-â` (`f64::NEG_INFINITY`)\n    /// - `x \u003c 0.0` â `NaN` (`f64::NAN`)\n    ///\n    /// Does **not** panic â follows standard floating-point semantics used by\n    /// NumPy, PyTorch, and TensorFlow.\n    ///\n    /// # Numerical stability in ML\n    /// For loss functions involving logarithms (e.g., cross-entropy):\n    /// - Avoid raw `log(x)` on unnormalized probabilities â use `log_softmax` instead\n    /// - Clip inputs when necessary: `log(max(x, Îµ))` with `Îµ = 1e-12` to avoid `-inf`\n    ///\n    /// # Example\n    /// ```\n    /// # use machinelearne_rs::backend::{Tensor1D, CpuBackend};\n    /// let x = Tensor1D::\u003cCpuBackend\u003e::new(vec![1.0f32, 0.0, -1.0]);\n    /// let y = x.log();  // Returns [0.0, -inf, NaN]\n    /// assert!((y.to_vec()[0] - 0.0).abs() \u003c 1e-12); // ln(1) = 0\n    /// assert!(y.to_vec()[1].is_infinite() \u0026\u0026 y.to_vec()[1] \u003c 0.0); // ln(0) = -inf\n    /// assert!(y.to_vec()[2].is_nan()); // ln(-1) = NaN\n    /// ```\n    fn log_1d(x: \u0026Self::Tensor1D) -\u003e Self::Tensor1D {\n        x.iter().map(|\u0026v| v.ln()).collect()\n    }\n    /// Element-wise sigmoid function with numerical stability.\n    ///\n    /// Computed as:\n    /// * `1 / (1 + exp(-x))` for `x \u003e= 0`\n    /// * `exp(x) / (1 + exp(x))` for `x \u003c 0`\n    ///\n    /// This formulation avoids overflow for large positive/negative values.\n    fn sigmoid_1d(x: \u0026Self::Tensor1D) -\u003e Self::Tensor1D {\n        x.iter()\n            .map(|\u0026z| {\n                if z \u003e= 0.0 {\n                    1.0 / (1.0 + (-z).exp())\n                } else {\n                    let ez = z.exp();\n                    ez / (1.0 + ez)\n                }\n            })\n            .collect()\n    }\n\n    // --- Mathematical functions (2D) ---\n\n    /// Element-wise absolute value for 2D tensors.\n    fn abs_2d(t: \u0026Self::Tensor2D) -\u003e Self::Tensor2D {\n        CpuTensor2D::new(t.0.iter().map(|x| x.abs()).collect(), t.1, t.2)\n    }\n\n    /// Element-wise sign function for 2D tensors.\n    fn sign_2d(x: \u0026Self::Tensor2D) -\u003e Self::Tensor2D {\n        CpuTensor2D::new(\n            x.0.iter()\n                .map(|\u0026x| {\n                    if x \u003e 0.0 {\n                        1.0\n                    } else if x \u003c 0.0 {\n                        -1.0\n                    } else {\n                        0.0\n                    }\n                })\n                .collect(),\n            x.1,\n            x.2,\n        )\n    }\n\n    /// Element-wise maximum between two 2D tensors.\n    ///\n    /// # Panics\n    /// Panics if tensors have different shapes.\n    fn maximum_2d(x: \u0026Self::Tensor2D, other: \u0026Self::Tensor2D) -\u003e Self::Tensor2D {\n        CpuTensor2D::new(Self::maximum_1d(\u0026x.0, \u0026other.0), x.1, x.2)\n    }\n\n    /// Element-wise exponential function for 2D tensors.\n    fn exp_2d(x: \u0026Self::Tensor2D) -\u003e Self::Tensor2D {\n        CpuTensor2D::new(Self::exp_1d(\u0026x.0), x.1, x.2)\n    }\n\n    /// Element-wise natural logarithm for 2D tensors.\n    ///\n    /// # Panics\n    /// Panics if any element is â¤ 0.0.\n    fn log_2d(x: \u0026Self::Tensor2D) -\u003e Self::Tensor2D {\n        CpuTensor2D::new(Self::log_1d(\u0026x.0), x.1, x.2)\n    }\n\n    /// Element-wise sigmoid function for 2D tensors.\n    fn sigmoid_2d(x: \u0026Self::Tensor2D) -\u003e Self::Tensor2D {\n        CpuTensor2D::new(Self::sigmoid_1d(\u0026x.0), x.1, x.2)\n    }\n\n    // --- Linear algebra ---\n\n    /// Matrix-vector multiplication with explicit shape checking.\n    ///\n    /// Computes `y = A * x` where:\n    /// * `A` has shape `(m, n)`\n    /// * `x` has shape `(n,)`\n    /// * Returns vector of shape `(m,)`\n    ///\n    /// # Panics\n    /// Panics if `A.cols() != x.len()`.\n    ///\n    /// # Implementation\n    /// Currently delegates to `_matvec_unchecked()` after shape validation.\n    /// TODO: Implement efficient shape checking before computation.\n    fn matvec(a: \u0026Self::Tensor2D, x: \u0026Self::Tensor1D) -\u003e Self::Tensor1D {\n        assert_eq!(\n            a.2,\n            x.len(),\n            \"Shape mismatch: A.cols={} != x.len={}\",\n            a.2,\n            x.len()\n        );\n        Self::_matvec_unchecked(a, x)\n    }\n\n    /// Matrix-vector multiplication without shape checking.\n    ///\n    /// # Safety\n    /// Caller must ensure `A.cols() == x.len()`. Undefined behavior may occur\n    /// if shapes are incompatible.\n    ///\n    /// # Implementation\n    /// Naive O(mÃn) implementation. Future optimizations:\n    /// * SIMD vectorization\n    /// * Cache-aware blocking\n    /// * Strided access patterns\n    fn _matvec_unchecked(a: \u0026CpuTensor2D, x: \u0026Vec\u003cf64\u003e) -\u003e Vec\u003cf64\u003e {\n        let CpuTensor2D(data, rows, cols) = a;\n        let mut result = Vec::with_capacity(*rows);\n        for i in 0..*rows {\n            let mut sum = 0.0;\n            for j in 0..*cols {\n                sum += data[i * *cols + j] * x[j];\n            }\n            result.push(sum);\n        }\n        result\n    }\n\n    /// Transposed matrix-vector multiplication with shape checking.\n    ///\n    /// Computes `y = Aáµ * x` where:\n    /// * `A` has shape `(m, n)`\n    /// * `x` has shape `(m,)`\n    /// * Returns vector of shape `(n,)`\n    ///\n    /// # Panics\n    /// Panics if `A.rows() != x.len()`.\n    fn matvec_transposed(a: \u0026Self::Tensor2D, x: \u0026Self::Tensor1D) -\u003e Self::Tensor1D {\n        assert_eq!(\n            a.1,\n            x.len(),\n            \"Shape mismatch: A.rows={} != x.len={}\",\n            a.1,\n            x.len()\n        );\n        Self::_matvec_transposed_unchecked(a, x)\n    }\n\n    /// Transposed matrix-vector multiplication without shape checking.\n    ///\n    /// # Safety\n    /// Caller must ensure `A.rows() == x.len()`.\n    ///\n    /// # Implementation\n    /// Currently computes full transpose then multiplies. Future optimization:\n    /// direct strided access without allocation.\n    fn _matvec_transposed_unchecked(a: \u0026Self::Tensor2D, x: \u0026Self::Tensor1D) -\u003e Self::Tensor1D {\n        Self::_matvec_unchecked(\u0026Self::transpose(a), x)\n    }\n\n    /// Returns the transpose of a 2D tensor.\n    ///\n    /// Converts matrix of shape `(m, n)` to shape `(n, m)` with elements:\n    /// `output[j][i] = input[i][j]`\n    ///\n    /// # Implementation\n    /// Allocates a new tensor. Future optimization: return view/slice without allocation.\n    fn transpose(t: \u0026CpuTensor2D) -\u003e Self::Tensor2D {\n        let CpuTensor2D(inp, rows, cols) = t;\n        let mut out = Vec::with_capacity(cols * rows);\n        for col in 0..*cols {\n            for row in 0..*rows {\n                out.push(inp[row * cols + col]);\n            }\n        }\n        CpuTensor2D::new(out, *cols, *rows)\n    }\n\n    /// Returns the shape of a 2D tensor as `(rows, cols)`.\n    fn shape(t: \u0026Self::Tensor2D) -\u003e (usize, usize) {\n        (t.1, t.2)\n    }\n\n    //Returns copy of the inner 1d vector\n    fn ravel_2d(x: \u0026Self::Tensor2D) -\u003e Self::Tensor1D {\n        x.0.clone()\n    }\n\n    // --- Column-wise operations ---\n\n    fn col_mean_2d(t: \u0026Self::Tensor2D) -\u003e Self::Tensor1D {\n        let (_, cols) = Self::shape(t);\n        if cols == 0 || t.0.is_empty() {\n            return Vec::new();\n        }\n        let rows = t.1;\n\n        let mut means = vec![0.0; cols];\n        for (col, mean) in means.iter_mut().enumerate() {\n            let mut sum = 0.0;\n            for row in 0..rows {\n                sum += t.0[row * cols + col];\n            }\n            *mean = sum / rows as f64;\n        }\n        means\n    }\n\n    fn col_std_2d(t: \u0026Self::Tensor2D, ddof: usize) -\u003e Self::Tensor1D {\n        let (_, cols) = Self::shape(t);\n        if cols == 0 || t.0.is_empty() {\n            return Vec::new();\n        }\n        let rows = t.1;\n\n        let means = Self::col_mean_2d(t);\n        let mut stds = vec![0.0; cols];\n\n        for col in 0..cols {\n            let mut var_sum = 0.0;\n            for row in 0..rows {\n                let diff = t.0[row * cols + col] - means[col];\n                var_sum += diff * diff;\n            }\n            let divisor = (rows - ddof) as f64;\n            stds[col] = (var_sum / divisor).sqrt();\n        }\n        stds\n    }\n\n    fn col_min_2d(t: \u0026Self::Tensor2D) -\u003e Self::Tensor1D {\n        let (_, cols) = Self::shape(t);\n        if cols == 0 || t.0.is_empty() {\n            return Vec::new();\n        }\n        let rows = t.1;\n\n        let mut mins = vec![f64::INFINITY; cols];\n        for (col, min) in mins.iter_mut().enumerate() {\n            for row in 0..rows {\n                let val = t.0[row * cols + col];\n                if val \u003c *min {\n                    *min = val;\n                }\n            }\n        }\n        mins\n    }\n\n    fn col_max_2d(t: \u0026Self::Tensor2D) -\u003e Self::Tensor1D {\n        let (_, cols) = Self::shape(t);\n        if cols == 0 || t.0.is_empty() {\n            return Vec::new();\n        }\n        let rows = t.1;\n\n        let mut maxs = vec![f64::NEG_INFINITY; cols];\n        for (col, max) in maxs.iter_mut().enumerate() {\n            for row in 0..rows {\n                let val = t.0[row * cols + col];\n                if val \u003e *max {\n                    *max = val;\n                }\n            }\n        }\n        maxs\n    }\n\n    fn col_sum_2d(t: \u0026Self::Tensor2D) -\u003e Self::Tensor1D {\n        let (_, cols) = Self::shape(t);\n        if cols == 0 || t.0.is_empty() {\n            return Vec::new();\n        }\n        let rows = t.1;\n\n        let mut sums = vec![0.0; cols];\n        for (col, sum) in sums.iter_mut().enumerate() {\n            for row in 0..rows {\n                *sum += t.0[row * cols + col];\n            }\n        }\n        sums\n    }\n\n    // --- Row-wise operations ---\n\n    fn row_sum_2d(t: \u0026Self::Tensor2D) -\u003e Self::Tensor1D {\n        let (rows, cols) = Self::shape(t);\n        if rows == 0 || cols == 0 {\n            return Vec::new();\n        }\n\n        let mut sums = vec![0.0; rows];\n        for (row, sum) in sums.iter_mut().enumerate() {\n            for col in 0..cols {\n                *sum += t.0[row * cols + col];\n            }\n        }\n        sums\n    }\n\n    // --- Broadcasting operations ---\n\n    fn broadcast_sub_1d_to_2d_rows(t: \u0026Self::Tensor2D, v: \u0026Self::Tensor1D) -\u003e Self::Tensor2D {\n        let (rows, cols) = Self::shape(t);\n        assert_eq!(v.len(), cols, \"Vector length must match number of columns\");\n\n        let mut result = Vec::with_capacity(rows * cols);\n        for row in 0..rows {\n            for (col, \u0026val) in v.iter().enumerate() {\n                result.push(t.0[row * cols + col] - val);\n            }\n        }\n        CpuTensor2D::new(result, rows, cols)\n    }\n\n    fn broadcast_div_1d_to_2d_rows(t: \u0026Self::Tensor2D, v: \u0026Self::Tensor1D) -\u003e Self::Tensor2D {\n        let (rows, cols) = Self::shape(t);\n        assert_eq!(v.len(), cols, \"Vector length must match number of columns\");\n\n        let mut result = Vec::with_capacity(rows * cols);\n        for row in 0..rows {\n            for (col, \u0026val) in v.iter().enumerate() {\n                result.push(t.0[row * cols + col] / val);\n            }\n        }\n        CpuTensor2D::new(result, rows, cols)\n    }\n\n    fn broadcast_mul_1d_to_2d_rows(t: \u0026Self::Tensor2D, v: \u0026Self::Tensor1D) -\u003e Self::Tensor2D {\n        let (rows, cols) = Self::shape(t);\n        assert_eq!(v.len(), cols, \"Vector length must match number of columns\");\n\n        let mut result = Vec::with_capacity(rows * cols);\n        for row in 0..rows {\n            for (col, \u0026val) in v.iter().enumerate() {\n                result.push(t.0[row * cols + col] * val);\n            }\n        }\n        CpuTensor2D::new(result, rows, cols)\n    }\n\n    fn broadcast_add_1d_to_2d_rows(t: \u0026Self::Tensor2D, v: \u0026Self::Tensor1D) -\u003e Self::Tensor2D {\n        let (rows, cols) = Self::shape(t);\n        assert_eq!(v.len(), cols, \"Vector length must match number of columns\");\n\n        let mut result = Vec::with_capacity(rows * cols);\n        for row in 0..rows {\n            for (col, \u0026val) in v.iter().enumerate() {\n                result.push(t.0[row * cols + col] + val);\n            }\n        }\n        CpuTensor2D::new(result, rows, cols)\n    }\n\n    fn sqrt_1d(t: \u0026Self::Tensor1D) -\u003e Self::Tensor1D {\n        t.iter().map(|x| x.sqrt()).collect()\n    }\n\n    fn sqrt_2d(t: \u0026Self::Tensor2D) -\u003e Self::Tensor2D {\n        CpuTensor2D::new(t.0.iter().map(|x| x.sqrt()).collect(), t.1, t.2)\n    }\n\n    // --- Column manipulation operations ---\n\n    fn hcat_2d(\n        tensors: \u0026[Self::Tensor2D],\n    ) -\u003e Result\u003cSelf::Tensor2D, crate::preprocessing::PreprocessingError\u003e {\n        if tensors.is_empty() {\n            return Err(crate::preprocessing::PreprocessingError::InvalidParameter(\n                \"Cannot horizontally concatenate empty slice of tensors\".to_string(),\n            ));\n        }\n\n        let rows = tensors[0].1;\n        if rows == 0 {\n            return Ok(CpuTensor2D::new(vec![], 0, 0));\n        }\n\n        // Verify all tensors have the same number of rows\n        for t in tensors.iter() {\n            if t.1 != rows {\n                return Err(crate::preprocessing::PreprocessingError::InvalidShape {\n                    expected: format!(\"({}, ?)\", rows),\n                    got: format!(\"({}, ?)\", t.1),\n                });\n            }\n        }\n\n        // Calculate total columns\n        let total_cols: usize = tensors.iter().map(|t| t.2).sum();\n\n        // Concatenate row by row\n        let mut result = Vec::with_capacity(rows * total_cols);\n        for row in 0..rows {\n            for t in tensors {\n                let start = row * t.2;\n                let end = start + t.2;\n                result.extend_from_slice(\u0026t.0[start..end]);\n            }\n        }\n\n        Ok(CpuTensor2D::new(result, rows, total_cols))\n    }\n\n    fn select_columns_2d(t: \u0026Self::Tensor2D, columns: \u0026[usize]) -\u003e Self::Tensor2D {\n        let (rows, cols) = Self::shape(t);\n        if columns.is_empty() {\n            return CpuTensor2D::new(vec![], rows, 0);\n        }\n\n        // Validate column indices\n        for \u0026col in columns {\n            assert!(\n                col \u003c cols,\n                \"Column index {} out of bounds (max {})\",\n                col,\n                cols - 1\n            );\n        }\n\n        let mut result = Vec::with_capacity(rows * columns.len());\n        for row in 0..rows {\n            for \u0026col in columns {\n                result.push(t.0[row * cols + col]);\n            }\n        }\n\n        CpuTensor2D::new(result, rows, columns.len())\n    }\n\n    fn one_hot_from_indices(indices: \u0026Self::Tensor1D, num_classes: usize) -\u003e Self::Tensor2D {\n        let n_samples = indices.len();\n        if n_samples == 0 || num_classes == 0 {\n            return CpuTensor2D::new(vec![], n_samples, num_classes);\n        }\n\n        // Validate indices\n        for (i, \u0026idx) in indices.iter().enumerate() {\n            assert!(\n                idx \u003e= 0.0 \u0026\u0026 idx \u003c num_classes as f64 \u0026\u0026 idx.fract() == 0.0,\n                \"Index {} at position {} is not a valid integer in range [0, {})\",\n                idx,\n                i,\n                num_classes\n            );\n        }\n\n        let mut result = vec![0.0; n_samples * num_classes];\n        for (i, \u0026idx) in indices.iter().enumerate() {\n            let col = idx as usize;\n            result[i * num_classes + col] = 1.0;\n        }\n\n        CpuTensor2D::new(result, n_samples, num_classes)\n    }\n}\n\n#[cfg(test)]\nmod matvec_tests {\n    use super::*;\n\n    #[test]\n    fn test_matvec_transpose() {\n        // ÐÑÐ¸Ð¼ÐµÑ 1: X â (3, 2), v â (3,)\n        // X = [[1.0, 2.0],\n        //      [3.0, 4.0],\n        //      [5.0, 6.0]]\n        // v = [1.0, 0.0, 2.0]\n        // Xáµ @ v = [1*1 + 3*0 + 5*2, 2*1 + 4*0 + 6*2] = [11.0, 14.0]\n\n        let x_data = vec![1.0, 2.0, 3.0, 4.0, 5.0, 6.0]; // row-major\n        let x = CpuTensor2D::new(x_data, 3, 2); // (rows=3, cols=2)\n        let v = vec![1.0, 0.0, 2.0];\n\n        let result = CpuBackend::matvec_transposed(\u0026x, \u0026v);\n        let expected = vec![11.0, 14.0];\n\n        assert_eq!(result.len(), expected.len());\n        for (r, e) in result.iter().zip(expected.iter()) {\n            assert!((r - e).abs() \u003c 1e-12, \"Expected {}, got {}\", e, r);\n        }\n\n        // ÐÑÐ¸Ð¼ÐµÑ 2: (4, 1) â Ð²ÐµÐºÑÐ¾Ñ-ÑÑÐ¾Ð»Ð±ÐµÑ (ÐºÐ°Ðº Ð² ÑÐ²Ð¾Ð¸Ñ Ð»Ð¸Ð½ÐµÐ¹Ð½ÑÑ ÑÐµÐ³ÑÐµÑÑÐ¸ÑÑ)\n        // X = [[2.0],\n        //      [3.0],\n        //      [4.0],\n        //      [5.0]]\n        // v = [1.0, 1.0, 1.0, 1.0]\n        // Xáµ @ v = [2+3+4+5] = [14.0]\n\n        let x2 = CpuTensor2D::new(vec![2.0, 3.0, 4.0, 5.0], 4, 1);\n        let v2 = vec![1.0, 1.0, 1.0, 1.0];\n        let result2 = CpuBackend::matvec_transposed(\u0026x2, \u0026v2);\n        let expected2 = vec![14.0];\n\n        assert_eq!(result2, expected2);\n\n        // ÐÑÐ¸Ð¼ÐµÑ 3: (2, 3) â output len = 3\n        // X = [[1, 0, 0],\n        //      [0, 1, 0]]\n        // v = [5.0, 7.0]\n        // Xáµ @ v = [5, 7, 0]\n\n        let x3 = CpuTensor2D::new(vec![1.0, 0.0, 0.0, 0.0, 1.0, 0.0], 2, 3);\n        let v3 = vec![5.0, 7.0];\n        let result3 = CpuBackend::matvec_transposed(\u0026x3, \u0026v3);\n        let expected3 = vec![5.0, 7.0, 0.0];\n\n        assert_eq!(result3.len(), 3);\n        for (r, e) in result3.iter().zip(expected3.iter()) {\n            assert!((r - e).abs() \u003c 1e-12, \"Expected {}, got {}\", e, r);\n        }\n    }\n\n    #[test]\n    fn test_matvec_transpose_consistency_with_transpose_and_matvec() {\n        let x = CpuTensor2D::new(vec![1.0, 2.0, 3.0, 4.0, 5.0, 6.0], 3, 2); // (3,2)\n        let v = vec![1.0, 0.0, 2.0];\n\n        let result1 = CpuBackend::matvec_transposed(\u0026x, \u0026v);\n        let x_t = CpuBackend::transpose(\u0026x);\n        let result2 = CpuBackend::matvec(\u0026x_t, \u0026v);\n\n        assert_eq!(result1.len(), result2.len());\n        for (a, b) in result1.iter().zip(result2.iter()) {\n            assert!((a - b).abs() \u003c 1e-12);\n        }\n    }\n}\n\n#[cfg(test)]\nmod tests {\n    use super::*;\n\n    #[test]\n    fn test_constructors() {\n        // zeros_1d\n        let z1 = CpuBackend::zeros_1d(3);\n        assert_eq!(z1, vec![0.0, 0.0, 0.0]);\n\n        // zeros_2d\n        let z2 = CpuBackend::zeros_2d(2, 3);\n        assert_eq!(z2.0, vec![0.0; 6]);\n        assert_eq!((z2.1, z2.2), (2, 3));\n\n        // from_vec_1d\n        let v1 = CpuBackend::from_vec_1d(vec![1.0f32, 2.0, 3.0]);\n        assert_eq!(v1, vec![1.0, 2.0, 3.0]);\n\n        // from_vec_2d\n        let v2 = CpuBackend::from_vec_2d(vec![1.0f32, 2.0, 3.0, 4.0], 2, 2);\n        assert_eq!(v2.0, vec![1.0, 2.0, 3.0, 4.0]);\n        assert_eq!((v2.1, v2.2), (2, 2));\n\n        // From\u003c\u0026[Vec\u003cf64\u003e]\u003e for CpuTensor2D\n        let nested = vec![vec![1.0, 2.0], vec![3.0, 4.0]];\n        let t = CpuTensor2D::from(\u0026nested[..]);\n        assert_eq!(t.0, vec![1.0, 2.0, 3.0, 4.0]);\n        assert_eq!((t.1, t.2), (2, 2));\n    }\n\n    #[test]\n    fn test_elementwise_ops_1d() {\n        let a = vec![1.0, 2.0];\n        let b = vec![3.0, 4.0];\n\n        assert_eq!(CpuBackend::add_1d(\u0026a, \u0026b), vec![4.0, 6.0]);\n        assert_eq!(CpuBackend::sub_1d(\u0026a, \u0026b), vec![-2.0, -2.0]);\n        assert_eq!(CpuBackend::mul_1d(\u0026a, \u0026b), vec![3.0, 8.0]);\n        assert_eq!(CpuBackend::div_1d(\u0026a, \u0026b), vec![1.0 / 3.0, 0.5]);\n\n        assert_eq!(CpuBackend::add_scalar_1d(\u0026a, \u00265.0), vec![6.0, 7.0]);\n        assert_eq!(CpuBackend::mul_scalar_1d(\u0026a, \u00262.0), vec![2.0, 4.0]);\n    }\n\n    #[test]\n    fn test_elementwise_ops_2d() {\n        let a = CpuTensor2D::new(vec![1.0, 2.0], 1, 2);\n        let b = CpuTensor2D::new(vec![3.0, 4.0], 1, 2);\n\n        let add = CpuBackend::add_2d(\u0026a, \u0026b);\n        assert_eq!(add.0, vec![4.0, 6.0]);\n\n        let mul_s = CpuBackend::mul_scalar_2d(\u0026a, \u00262.0);\n        assert_eq!(mul_s.0, vec![2.0, 4.0]);\n    }\n\n    #[test]\n    fn test_reductions() {\n        let v = vec![1.0, 2.0, 3.0];\n        assert_eq!(CpuBackend::sum_all_1d(\u0026v), 6.0);\n        assert!((CpuBackend::mean_all_1d(\u0026v) - 2.0).abs() \u003c 1e-12);\n\n        let m = CpuTensor2D::new(vec![1.0, 2.0, 3.0, 4.0], 2, 2);\n        assert_eq!(CpuBackend::sum_all_2d(\u0026m), 10.0);\n        assert!((CpuBackend::mean_all_2d(\u0026m) - 2.5).abs() \u003c 1e-12);\n    }\n\n    #[test]\n    fn test_abs_and_sign() {\n        let v = vec![-2.0, 0.0, 3.0];\n        assert_eq!(CpuBackend::abs_1d(\u0026v), vec![2.0, 0.0, 3.0]);\n        assert_eq!(CpuBackend::sign_1d(\u0026v), vec![-1.0, 0.0, 1.0]);\n\n        let m = CpuTensor2D::new(vec![-1.0, 0.0, 2.0], 1, 3);\n        let sign_m = CpuBackend::sign_2d(\u0026m);\n        assert_eq!(sign_m.0, vec![-1.0, 0.0, 1.0]);\n    }\n\n    #[test]\n    fn test_math_functions() {\n        let v = vec![0.0, 1.0];\n        assert_eq!(CpuBackend::exp_1d(\u0026v), vec![1.0, std::f64::consts::E]);\n        assert_eq!(\n            CpuBackend::log_1d(\u0026vec![1.0, std::f64::consts::E]),\n            vec![0.0, 1.0]\n        );\n\n        // sigmoid(0) = 0.5\n        let sig = CpuBackend::sigmoid_1d(\u0026vec![0.0]);\n        assert!((sig[0] - 0.5).abs() \u003c 1e-12);\n\n        // maximum\n        let a = vec![1.0, 3.0];\n        let b = vec![2.0, 2.0];\n        assert_eq!(CpuBackend::maximum_1d(\u0026a, \u0026b), vec![2.0, 3.0]);\n    }\n\n    #[test]\n    fn test_transpose() {\n        let m = CpuTensor2D::new(vec![1.0, 2.0, 3.0, 4.0, 5.0, 6.0], 2, 3); // 2x3\n        let t = CpuBackend::transpose(\u0026m); // 3x2\n        assert_eq!(t.0, vec![1.0, 4.0, 2.0, 5.0, 3.0, 6.0]);\n        assert_eq!((t.1, t.2), (3, 2));\n\n        // Double transpose = original\n        let tt = CpuBackend::transpose(\u0026t);\n        assert_eq!(tt.0, m.0);\n        assert_eq!((tt.1, tt.2), (2, 3));\n    }\n\n    #[test]\n    fn test_matvec() {\n        let m = CpuTensor2D::new(vec![1.0, 2.0, 3.0, 4.0], 2, 2);\n        let v = vec![1.0, 0.0];\n        let res = CpuBackend::matvec(\u0026m, \u0026v);\n        assert_eq!(res, vec![1.0, 3.0]); // [1*1 + 2*0, 3*1 + 4*0]\n    }\n\n    #[test]\n    fn test_edge_cases() {\n        // ÐÑÑÑÐ¾Ð¹ ÑÐµÐ½Ð·Ð¾Ñ 1D\n        let empty1d = CpuBackend::zeros_1d(0);\n        assert_eq!(empty1d.len(), 0);\n        assert_eq!(CpuBackend::sum_all_1d(\u0026empty1d), 0.0);\n\n        // ÐÑÑÑÐ¾Ð¹ ÑÐµÐ½Ð·Ð¾Ñ 2D\n        let empty2d = CpuBackend::zeros_2d(0, 0);\n        assert_eq!(empty2d.0.len(), 0);\n        assert_eq!(CpuBackend::sum_all_2d(\u0026empty2d), 0.0);\n\n        // From empty nested vec\n        let t = CpuTensor2D::from(\u0026[][..]);\n        assert_eq!(t.0.len(), 0);\n        assert_eq!((t.1, t.2), (0, 0));\n\n        // ÐÐµÐ»ÐµÐ½Ð¸Ðµ Ð½Ð° Ð½Ð¾Ð»Ñ â Ð¾Ð¶Ð¸Ð´Ð°ÐµÐ¼ panic Ð¸Ð»Ð¸ NaN?\n        // Ð ÑÐµÐºÑÑÐµÐ¹ ÑÐµÐ°Ð»Ð¸Ð·Ð°ÑÐ¸Ð¸ Ð±ÑÐ´ÐµÑ panic Ð¿ÑÐ¸ Ð´ÐµÐ»ÐµÐ½Ð¸Ð¸ Ð½Ð° 0.0.\n        // ÐÑÐ»Ð¸ ÑÑÐ¾ Ð½Ðµ Ð¶ÐµÐ»Ð°ÐµÐ¼Ð¾ â ÑÑÐ¾Ð¸Ñ Ð¾Ð±ÑÑÐ´Ð¸ÑÑ, Ð½Ð¾ Ð¿Ð¾ÐºÐ° Ð¿Ð¾ÐºÑÐ¾ÐµÐ¼ ÐºÐ°Ðº ÐµÑÑÑ.\n        let a = vec![1.0];\n        let b = vec![0.0];\n        let res = CpuBackend::div_1d(\u0026a, \u0026b);\n        assert!(res[0].is_infinite()); // Ð¸Ð»Ð¸ assert_panics, ÐµÑÐ»Ð¸ ÑÐ¾ÑÐµÑÑ ÑÐ²Ð½ÑÐ¹ ÐºÐ¾Ð½ÑÑÐ¾Ð»Ñ\n    }\n\n    #[test]\n    fn test_ravel_2d_row_major_order() {\n        // Create a 2x3 matrix: [[1.0, 2.0, 3.0],\n        //                       [4.0, 5.0, 6.0]]\n        // Larger matrix to verify no data corruption\n        let data: Vec\u003cf64\u003e = (0..12).map(|i| i as f64).collect();\n        let matrix = CpuTensor2D::new(data.clone(), 3, 4);\n        let flattened = CpuBackend::ravel_2d(\u0026matrix);\n\n        assert_eq!(flattened.to_vec(), data);\n    }\n\n    // === Column manipulation tests ===\n\n    #[test]\n    fn test_hcat_2d_basic() {\n        // [[1, 2]] + [[3, 4]] -\u003e [[1, 2, 3, 4]]\n        let a = CpuTensor2D::new(vec![1.0, 2.0], 1, 2);\n        let b = CpuTensor2D::new(vec![3.0, 4.0], 1, 2);\n        let result = CpuBackend::hcat_2d(\u0026[a, b]).unwrap();\n\n        assert_eq!(result.0, vec![1.0, 2.0, 3.0, 4.0]);\n        assert_eq!((result.1, result.2), (1, 4));\n    }\n\n    #[test]\n    fn test_hcat_2d_multiple_rows() {\n        // [[1, 2],    [[5],    [[1, 2, 5],\n        //  [3, 4]]  +  [6]] -\u003e  [3, 4, 6]]\n        let a = CpuTensor2D::new(vec![1.0, 2.0, 3.0, 4.0], 2, 2);\n        let b = CpuTensor2D::new(vec![5.0, 6.0], 2, 1);\n        let result = CpuBackend::hcat_2d(\u0026[a, b]).unwrap();\n\n        assert_eq!(result.0, vec![1.0, 2.0, 5.0, 3.0, 4.0, 6.0]);\n        assert_eq!((result.1, result.2), (2, 3));\n    }\n\n    #[test]\n    fn test_hcat_2d_three_tensors() {\n        let a = CpuTensor2D::new(vec![1.0, 4.0], 2, 1);\n        let b = CpuTensor2D::new(vec![2.0, 5.0], 2, 1);\n        let c = CpuTensor2D::new(vec![3.0, 6.0], 2, 1);\n        let result = CpuBackend::hcat_2d(\u0026[a, b, c]).unwrap();\n\n        assert_eq!(result.0, vec![1.0, 2.0, 3.0, 4.0, 5.0, 6.0]);\n        assert_eq!((result.1, result.2), (2, 3));\n    }\n\n    #[test]\n    fn test_hcat_2d_empty_error() {\n        let result = CpuBackend::hcat_2d(\u0026[]);\n        assert!(result.is_err());\n    }\n\n    #[test]\n    fn test_select_columns_2d_basic() {\n        // [[1, 2, 3],    select [0, 2]   [[1, 3],\n        //  [4, 5, 6]]                  -\u003e [4, 6]]\n        let t = CpuTensor2D::new(vec![1.0, 2.0, 3.0, 4.0, 5.0, 6.0], 2, 3);\n        let result = CpuBackend::select_columns_2d(\u0026t, \u0026[0, 2]);\n\n        assert_eq!(result.0, vec![1.0, 3.0, 4.0, 6.0]);\n        assert_eq!((result.1, result.2), (2, 2));\n    }\n\n    #[test]\n    fn test_select_columns_2d_single() {\n        let t = CpuTensor2D::new(vec![1.0, 2.0, 3.0, 4.0, 5.0, 6.0], 2, 3);\n        let result = CpuBackend::select_columns_2d(\u0026t, \u0026[1]);\n\n        assert_eq!(result.0, vec![2.0, 5.0]);\n        assert_eq!((result.1, result.2), (2, 1));\n    }\n\n    #[test]\n    fn test_select_columns_2d_reorder() {\n        // Reordering columns: [2, 0, 1]\n        let t = CpuTensor2D::new(vec![1.0, 2.0, 3.0, 4.0, 5.0, 6.0], 2, 3);\n        let result = CpuBackend::select_columns_2d(\u0026t, \u0026[2, 0, 1]);\n\n        assert_eq!(result.0, vec![3.0, 1.0, 2.0, 6.0, 4.0, 5.0]);\n    }\n\n    #[test]\n    fn test_select_columns_2d_empty() {\n        let t = CpuTensor2D::new(vec![1.0, 2.0, 3.0, 4.0, 5.0, 6.0], 2, 3);\n        let result = CpuBackend::select_columns_2d(\u0026t, \u0026[]);\n\n        assert_eq!(result.0, vec![]);\n        assert_eq!((result.1, result.2), (2, 0));\n    }\n\n    #[test]\n    fn test_one_hot_from_indices_basic() {\n        // [0, 1, 2] with 3 classes -\u003e [[1,0,0], [0,1,0], [0,0,1]]\n        let indices = vec![0.0, 1.0, 2.0];\n        let result = CpuBackend::one_hot_from_indices(\u0026indices, 3);\n\n        assert_eq!(result.0, vec![1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0]);\n        assert_eq!((result.1, result.2), (3, 3));\n    }\n\n    #[test]\n    fn test_one_hot_from_indices_repeated() {\n        // [0, 1, 0] with 2 classes -\u003e [[1,0], [0,1], [1,0]]\n        let indices = vec![0.0, 1.0, 0.0];\n        let result = CpuBackend::one_hot_from_indices(\u0026indices, 2);\n\n        assert_eq!(result.0, vec![1.0, 0.0, 0.0, 1.0, 1.0, 0.0]);\n        assert_eq!((result.1, result.2), (3, 2));\n    }\n\n    #[test]\n    fn test_one_hot_from_indices_empty() {\n        let indices: Vec\u003cf64\u003e = vec![];\n        let result = CpuBackend::one_hot_from_indices(\u0026indices, 3);\n\n        assert_eq!(result.0, vec![]);\n        assert_eq!((result.1, result.2), (0, 3));\n    }\n\n    #[test]\n    fn test_one_hot_zero_classes() {\n        let indices = vec![0.0, 1.0];\n        let result = CpuBackend::one_hot_from_indices(\u0026indices, 0);\n\n        assert_eq!(result.0, vec![]);\n        assert_eq!((result.1, result.2), (2, 0));\n    }\n}\n","traces":[{"line":99,"address":[3044064,3044494],"length":1,"stats":{"Line":6}},{"line":100,"address":[3044417,3044120,3044184],"length":1,"stats":{"Line":13}},{"line":101,"address":[3044331],"length":1,"stats":{"Line":6}},{"line":124,"address":[3043696],"length":1,"stats":{"Line":1}},{"line":125,"address":[3043755],"length":1,"stats":{"Line":1}},{"line":126,"address":[3043786],"length":1,"stats":{"Line":1}},{"line":128,"address":[3043769],"length":1,"stats":{"Line":1}},{"line":129,"address":[3043828,3043777,3043889],"length":1,"stats":{"Line":2}},{"line":130,"address":[2945440,2945472],"length":1,"stats":{"Line":3}},{"line":134,"address":[2945504,2945529],"length":1,"stats":{"Line":3}},{"line":135,"address":[3044025],"length":1,"stats":{"Line":1}},{"line":148,"address":[3052032],"length":1,"stats":{"Line":0}},{"line":163,"address":[3065888],"length":1,"stats":{"Line":2}},{"line":164,"address":[3065905],"length":1,"stats":{"Line":2}},{"line":180,"address":[3065936],"length":1,"stats":{"Line":1}},{"line":181,"address":[3066046,3065973],"length":1,"stats":{"Line":1}},{"line":195,"address":[3051008],"length":1,"stats":{"Line":1}},{"line":196,"address":[2946075,2946064],"length":1,"stats":{"Line":3}},{"line":208,"address":[3051072],"length":1,"stats":{"Line":1}},{"line":209,"address":[2946091,2946080],"length":1,"stats":{"Line":3}},{"line":218,"address":[3060944],"length":1,"stats":{"Line":1}},{"line":219,"address":[2946352,2946395],"length":1,"stats":{"Line":3}},{"line":226,"address":[3062880],"length":1,"stats":{"Line":1}},{"line":227,"address":[2946971,2946928],"length":1,"stats":{"Line":3}},{"line":234,"address":[3062496],"length":1,"stats":{"Line":1}},{"line":235,"address":[3062547],"length":1,"stats":{"Line":3}},{"line":242,"address":[3061328],"length":1,"stats":{"Line":1}},{"line":243,"address":[2946555,2946512],"length":1,"stats":{"Line":4}},{"line":247,"address":[3051808],"length":1,"stats":{"Line":1}},{"line":248,"address":[2946206,2946192],"length":1,"stats":{"Line":3}},{"line":252,"address":[3051584],"length":1,"stats":{"Line":1}},{"line":253,"address":[2946121,2946096],"length":1,"stats":{"Line":3}},{"line":259,"address":[3051904],"length":1,"stats":{"Line":1}},{"line":260,"address":[2946254,2946240],"length":1,"stats":{"Line":3}},{"line":264,"address":[3051680],"length":1,"stats":{"Line":1}},{"line":265,"address":[2946158,2946144],"length":1,"stats":{"Line":3}},{"line":272,"address":[3061120],"length":1,"stats":{"Line":1}},{"line":274,"address":[3061170],"length":1,"stats":{"Line":3}},{"line":275,"address":[3061282],"length":1,"stats":{"Line":1}},{"line":276,"address":[3061286],"length":1,"stats":{"Line":1}},{"line":289,"address":[3063056],"length":1,"stats":{"Line":1}},{"line":290,"address":[3063099],"length":1,"stats":{"Line":1}},{"line":291,"address":[3063143],"length":1,"stats":{"Line":1}},{"line":298,"address":[2947008,2947051],"length":1,"stats":{"Line":3}},{"line":299,"address":[3063564],"length":1,"stats":{"Line":1}},{"line":300,"address":[3063568],"length":1,"stats":{"Line":1}},{"line":308,"address":[3062672],"length":1,"stats":{"Line":0}},{"line":310,"address":[3062722],"length":1,"stats":{"Line":0}},{"line":311,"address":[3062834],"length":1,"stats":{"Line":0}},{"line":312,"address":[3062838],"length":1,"stats":{"Line":0}},{"line":320,"address":[3061504],"length":1,"stats":{"Line":0}},{"line":322,"address":[2946592,2946635],"length":1,"stats":{"Line":0}},{"line":323,"address":[3061666],"length":1,"stats":{"Line":0}},{"line":324,"address":[3061670],"length":1,"stats":{"Line":0}},{"line":335,"address":[3051200],"length":1,"stats":{"Line":1}},{"line":336,"address":[3051326,3051214],"length":1,"stats":{"Line":2}},{"line":337,"address":[3051328],"length":1,"stats":{"Line":0}},{"line":339,"address":[3051228],"length":1,"stats":{"Line":1}},{"line":361,"address":[3051360],"length":1,"stats":{"Line":1}},{"line":362,"address":[3051374],"length":1,"stats":{"Line":1}},{"line":363,"address":[3051500],"length":1,"stats":{"Line":1}},{"line":370,"address":[3051388],"length":1,"stats":{"Line":1}},{"line":374,"address":[3050000],"length":1,"stats":{"Line":1}},{"line":375,"address":[3050005],"length":1,"stats":{"Line":1}},{"line":379,"address":[3049952],"length":1,"stats":{"Line":1}},{"line":380,"address":[3049957],"length":1,"stats":{"Line":1}},{"line":388,"address":[3049760],"length":1,"stats":{"Line":1}},{"line":398,"address":[3066064],"length":1,"stats":{"Line":3}},{"line":399,"address":[3066081],"length":1,"stats":{"Line":3}},{"line":403,"address":[3061888],"length":1,"stats":{"Line":1}},{"line":404,"address":[3061893],"length":1,"stats":{"Line":1}},{"line":408,"address":[3061904],"length":1,"stats":{"Line":1}},{"line":409,"address":[3061909],"length":1,"stats":{"Line":1}},{"line":415,"address":[3060736],"length":1,"stats":{"Line":1}},{"line":416,"address":[3060768],"length":1,"stats":{"Line":3}},{"line":425,"address":[3065424],"length":1,"stats":{"Line":1}},{"line":426,"address":[3065456],"length":1,"stats":{"Line":1}},{"line":427,"address":[3065478],"length":1,"stats":{"Line":3}},{"line":428,"address":[2947130,2947170],"length":1,"stats":{"Line":2}},{"line":429,"address":[2947156],"length":1,"stats":{"Line":1}},{"line":430,"address":[2947181,2947145],"length":1,"stats":{"Line":2}},{"line":431,"address":[2947183],"length":1,"stats":{"Line":1}},{"line":433,"address":[2947172],"length":1,"stats":{"Line":1}},{"line":443,"address":[3048624],"length":1,"stats":{"Line":1}},{"line":444,"address":[3048675],"length":1,"stats":{"Line":1}},{"line":445,"address":[3048706],"length":1,"stats":{"Line":1}},{"line":446,"address":[3048748],"length":1,"stats":{"Line":3}},{"line":451,"address":[3061712],"length":1,"stats":{"Line":1}},{"line":452,"address":[2946685,2946672],"length":1,"stats":{"Line":3}},{"line":479,"address":[3061920],"length":1,"stats":{"Line":1}},{"line":480,"address":[3061952],"length":1,"stats":{"Line":3}},{"line":489,"address":[3049776],"length":1,"stats":{"Line":1}},{"line":490,"address":[3049808],"length":1,"stats":{"Line":1}},{"line":491,"address":[2945918,2945904],"length":1,"stats":{"Line":3}},{"line":492,"address":[2945982,2945934],"length":1,"stats":{"Line":2}},{"line":493,"address":[2945990],"length":1,"stats":{"Line":1}},{"line":495,"address":[2945949],"length":1,"stats":{"Line":1}},{"line":496,"address":[2945960],"length":1,"stats":{"Line":1}},{"line":505,"address":[3060832],"length":1,"stats":{"Line":1}},{"line":506,"address":[2946320,2946334],"length":1,"stats":{"Line":3}},{"line":510,"address":[3065520],"length":1,"stats":{"Line":1}},{"line":512,"address":[3065550],"length":1,"stats":{"Line":1}},{"line":513,"address":[3065572],"length":1,"stats":{"Line":3}},{"line":514,"address":[2947282,2947242],"length":1,"stats":{"Line":2}},{"line":515,"address":[2947268],"length":1,"stats":{"Line":1}},{"line":516,"address":[2947257,2947293],"length":1,"stats":{"Line":2}},{"line":517,"address":[2947295],"length":1,"stats":{"Line":1}},{"line":519,"address":[2947284],"length":1,"stats":{"Line":1}},{"line":522,"address":[3065580],"length":1,"stats":{"Line":1}},{"line":523,"address":[3065599],"length":1,"stats":{"Line":1}},{"line":524,"address":[3065603],"length":1,"stats":{"Line":1}},{"line":532,"address":[3048800],"length":1,"stats":{"Line":1}},{"line":533,"address":[3048829],"length":1,"stats":{"Line":1}},{"line":537,"address":[3061808],"length":1,"stats":{"Line":1}},{"line":538,"address":[3061831],"length":1,"stats":{"Line":1}},{"line":545,"address":[3062016],"length":1,"stats":{"Line":5}},{"line":546,"address":[3062039],"length":1,"stats":{"Line":5}},{"line":550,"address":[3049872],"length":1,"stats":{"Line":1}},{"line":551,"address":[3049895],"length":1,"stats":{"Line":1}},{"line":569,"address":[3062096],"length":1,"stats":{"Line":1}},{"line":570,"address":[3062241],"length":1,"stats":{"Line":1}},{"line":577,"address":[3062473],"length":1,"stats":{"Line":1}},{"line":591,"address":[3052834,3052048,3052828],"length":1,"stats":{"Line":1}},{"line":592,"address":[3052094],"length":1,"stats":{"Line":1}},{"line":593,"address":[3052140],"length":1,"stats":{"Line":1}},{"line":594,"address":[3052172,3052263],"length":1,"stats":{"Line":2}},{"line":595,"address":[3052377],"length":1,"stats":{"Line":1}},{"line":596,"address":[3052480,3052823,3052389],"length":1,"stats":{"Line":3}},{"line":597,"address":[3052599,3052660],"length":1,"stats":{"Line":2}},{"line":599,"address":[3052614],"length":1,"stats":{"Line":1}},{"line":601,"address":[3052427],"length":1,"stats":{"Line":1}},{"line":613,"address":[3052848],"length":1,"stats":{"Line":2}},{"line":614,"address":[3052931,3053010,3052899],"length":1,"stats":{"Line":5}},{"line":621,"address":[3053225],"length":1,"stats":{"Line":2}},{"line":632,"address":[3060704,3060710,3060592],"length":1,"stats":{"Line":2}},{"line":633,"address":[3060630],"length":1,"stats":{"Line":2}},{"line":643,"address":[3066849,3066112,3066821],"length":1,"stats":{"Line":2}},{"line":644,"address":[3066150],"length":1,"stats":{"Line":2}},{"line":645,"address":[3066203],"length":1,"stats":{"Line":2}},{"line":646,"address":[3066337,3066251],"length":1,"stats":{"Line":4}},{"line":647,"address":[3066448,3066583],"length":1,"stats":{"Line":4}},{"line":648,"address":[3066693],"length":1,"stats":{"Line":2}},{"line":651,"address":[3066491],"length":1,"stats":{"Line":2}},{"line":655,"address":[3060720],"length":1,"stats":{"Line":2}},{"line":656,"address":[3060725],"length":1,"stats":{"Line":2}},{"line":660,"address":[3065840],"length":1,"stats":{"Line":1}},{"line":661,"address":[3065857],"length":1,"stats":{"Line":1}},{"line":666,"address":[3050048,3050984,3050978],"length":1,"stats":{"Line":4}},{"line":667,"address":[3050098],"length":1,"stats":{"Line":4}},{"line":668,"address":[3050151,3050119],"length":1,"stats":{"Line":8}},{"line":669,"address":[3050133],"length":1,"stats":{"Line":0}},{"line":671,"address":[3050176],"length":1,"stats":{"Line":4}},{"line":673,"address":[3050193],"length":1,"stats":{"Line":4}},{"line":674,"address":[3050305,3050843,3050226],"length":1,"stats":{"Line":9}},{"line":675,"address":[3050547],"length":1,"stats":{"Line":5}},{"line":676,"address":[3050559,3050653,3050973],"length":1,"stats":{"Line":11}},{"line":677,"address":[3050762,3050858],"length":1,"stats":{"Line":9}},{"line":679,"address":[3050787],"length":1,"stats":{"Line":4}},{"line":681,"address":[3050592],"length":1,"stats":{"Line":4}},{"line":684,"address":[3046656,3047740,3047734],"length":1,"stats":{"Line":1}},{"line":685,"address":[3046722],"length":1,"stats":{"Line":4}},{"line":686,"address":[3046743,3046775],"length":1,"stats":{"Line":5}},{"line":687,"address":[3046757],"length":1,"stats":{"Line":0}},{"line":689,"address":[3046792],"length":1,"stats":{"Line":1}},{"line":691,"address":[3046820],"length":1,"stats":{"Line":4}},{"line":692,"address":[3046833],"length":1,"stats":{"Line":1}},{"line":694,"address":[3047532,3046906,3046990],"length":1,"stats":{"Line":9}},{"line":695,"address":[3047104],"length":1,"stats":{"Line":4}},{"line":696,"address":[3047729,3047116,3047223],"length":1,"stats":{"Line":9}},{"line":697,"address":[3047547,3047332],"length":1,"stats":{"Line":5}},{"line":698,"address":[3047707],"length":1,"stats":{"Line":1}},{"line":700,"address":[3047465,3047370],"length":1,"stats":{"Line":1}},{"line":701,"address":[3047439,3047490],"length":1,"stats":{"Line":5}},{"line":703,"address":[3047149],"length":1,"stats":{"Line":1}},{"line":706,"address":[3046627,3046633,3045744],"length":1,"stats":{"Line":1}},{"line":707,"address":[3045794],"length":1,"stats":{"Line":1}},{"line":708,"address":[3045847,3045815],"length":1,"stats":{"Line":3}},{"line":709,"address":[3045829],"length":1,"stats":{"Line":0}},{"line":711,"address":[3045872],"length":1,"stats":{"Line":1}},{"line":713,"address":[3045889],"length":1,"stats":{"Line":1}},{"line":714,"address":[3045927,3046006],"length":1,"stats":{"Line":3}},{"line":715,"address":[3046248,3046342],"length":1,"stats":{"Line":3}},{"line":716,"address":[3046455],"length":1,"stats":{"Line":2}},{"line":717,"address":[3046594,3046622],"length":1,"stats":{"Line":2}},{"line":718,"address":[3046618],"length":1,"stats":{"Line":2}},{"line":722,"address":[3046281],"length":1,"stats":{"Line":2}},{"line":725,"address":[3045717,3045711,3044832],"length":1,"stats":{"Line":2}},{"line":726,"address":[3044882],"length":1,"stats":{"Line":1}},{"line":727,"address":[3044935,3044903],"length":1,"stats":{"Line":4}},{"line":728,"address":[3044917],"length":1,"stats":{"Line":0}},{"line":730,"address":[3044960],"length":1,"stats":{"Line":3}},{"line":732,"address":[3044977],"length":1,"stats":{"Line":1}},{"line":733,"address":[3045015,3045094],"length":1,"stats":{"Line":4}},{"line":734,"address":[3045430,3045336],"length":1,"stats":{"Line":4}},{"line":735,"address":[3045543],"length":1,"stats":{"Line":2}},{"line":736,"address":[3045682,3045706],"length":1,"stats":{"Line":3}},{"line":737,"address":[3045702],"length":1,"stats":{"Line":2}},{"line":741,"address":[3045369],"length":1,"stats":{"Line":3}},{"line":744,"address":[3048602,3047760,3048596],"length":1,"stats":{"Line":0}},{"line":745,"address":[3047810],"length":1,"stats":{"Line":0}},{"line":746,"address":[3047863,3047831],"length":1,"stats":{"Line":0}},{"line":747,"address":[3047845],"length":1,"stats":{"Line":0}},{"line":749,"address":[3047888],"length":1,"stats":{"Line":0}},{"line":751,"address":[3047905],"length":1,"stats":{"Line":0}},{"line":752,"address":[3047938,3048011],"length":1,"stats":{"Line":0}},{"line":753,"address":[3048253,3048347,3048591],"length":1,"stats":{"Line":0}},{"line":754,"address":[3048460],"length":1,"stats":{"Line":0}},{"line":757,"address":[3048286],"length":1,"stats":{"Line":0}},{"line":762,"address":[3048880,3049727,3049733],"length":1,"stats":{"Line":0}},{"line":763,"address":[3048930],"length":1,"stats":{"Line":0}},{"line":764,"address":[3048996,3048967],"length":1,"stats":{"Line":0}},{"line":765,"address":[3048981],"length":1,"stats":{"Line":0}},{"line":768,"address":[3049010],"length":1,"stats":{"Line":0}},{"line":769,"address":[3049138,3049043],"length":1,"stats":{"Line":0}},{"line":770,"address":[3049722,3049466,3049383],"length":1,"stats":{"Line":0}},{"line":771,"address":[3049589],"length":1,"stats":{"Line":0}},{"line":774,"address":[3049416],"length":1,"stats":{"Line":0}},{"line":779,"address":[3060586,3060558,3059472],"length":1,"stats":{"Line":3}},{"line":780,"address":[3059538],"length":1,"stats":{"Line":3}},{"line":781,"address":[3059583],"length":1,"stats":{"Line":3}},{"line":783,"address":[3059838,3059748],"length":1,"stats":{"Line":3}},{"line":784,"address":[3059813,3059902],"length":1,"stats":{"Line":6}},{"line":785,"address":[3060146,3060016],"length":1,"stats":{"Line":4}},{"line":786,"address":[3060398],"length":1,"stats":{"Line":3}},{"line":789,"address":[3060049],"length":1,"stats":{"Line":3}},{"line":792,"address":[3058318,3057232,3058346],"length":1,"stats":{"Line":3}},{"line":793,"address":[3057298],"length":1,"stats":{"Line":3}},{"line":794,"address":[3057343],"length":1,"stats":{"Line":3}},{"line":796,"address":[3057508,3057598],"length":1,"stats":{"Line":3}},{"line":797,"address":[3057662,3057573],"length":1,"stats":{"Line":6}},{"line":798,"address":[3057906,3057776],"length":1,"stats":{"Line":6}},{"line":799,"address":[3058158],"length":1,"stats":{"Line":3}},{"line":802,"address":[3057809],"length":1,"stats":{"Line":3}},{"line":805,"address":[3059438,3058352,3059466],"length":1,"stats":{"Line":2}},{"line":806,"address":[3058418],"length":1,"stats":{"Line":3}},{"line":807,"address":[3058463],"length":1,"stats":{"Line":2}},{"line":809,"address":[3058718,3058628],"length":1,"stats":{"Line":3}},{"line":810,"address":[3058693,3058782],"length":1,"stats":{"Line":5}},{"line":811,"address":[3059026,3058896],"length":1,"stats":{"Line":5}},{"line":812,"address":[3059278],"length":1,"stats":{"Line":2}},{"line":815,"address":[3058929],"length":1,"stats":{"Line":3}},{"line":818,"address":[3056112,3057226,3057198],"length":1,"stats":{"Line":1}},{"line":819,"address":[3056178],"length":1,"stats":{"Line":1}},{"line":820,"address":[3056223],"length":1,"stats":{"Line":1}},{"line":822,"address":[3056388,3056478],"length":1,"stats":{"Line":1}},{"line":823,"address":[3056453,3056542],"length":1,"stats":{"Line":2}},{"line":824,"address":[3056656,3056786],"length":1,"stats":{"Line":2}},{"line":825,"address":[3057038],"length":1,"stats":{"Line":1}},{"line":828,"address":[3056689],"length":1,"stats":{"Line":1}},{"line":831,"address":[3065632],"length":1,"stats":{"Line":0}},{"line":832,"address":[3065664],"length":1,"stats":{"Line":0}},{"line":835,"address":[3065728],"length":1,"stats":{"Line":0}},{"line":836,"address":[2947360,2947374],"length":1,"stats":{"Line":0}},{"line":841,"address":[3063600,3064952,3064980],"length":1,"stats":{"Line":1}},{"line":844,"address":[3063677],"length":1,"stats":{"Line":1}},{"line":845,"address":[3063752],"length":1,"stats":{"Line":1}},{"line":846,"address":[3063718],"length":1,"stats":{"Line":1}},{"line":850,"address":[3063849,3063702,3063882],"length":1,"stats":{"Line":5}},{"line":851,"address":[3063861],"length":1,"stats":{"Line":3}},{"line":852,"address":[3063899],"length":1,"stats":{"Line":0}},{"line":856,"address":[3064009,3064059],"length":1,"stats":{"Line":6}},{"line":857,"address":[3064135],"length":1,"stats":{"Line":3}},{"line":858,"address":[3065293],"length":1,"stats":{"Line":0}},{"line":859,"address":[3064986],"length":1,"stats":{"Line":0}},{"line":860,"address":[3065125,3065185],"length":1,"stats":{"Line":0}},{"line":866,"address":[2947098,2947088],"length":1,"stats":{"Line":9}},{"line":869,"address":[3064310,3064217],"length":1,"stats":{"Line":3}},{"line":870,"address":[3064374,3064277],"length":1,"stats":{"Line":6}},{"line":871,"address":[3064496,3064669],"length":1,"stats":{"Line":6}},{"line":872,"address":[3064779,3064830],"length":1,"stats":{"Line":3}},{"line":873,"address":[3064865,3064897,3064811],"length":1,"stats":{"Line":6}},{"line":874,"address":[3064873,3064925],"length":1,"stats":{"Line":6}},{"line":878,"address":[3064524],"length":1,"stats":{"Line":3}},{"line":881,"address":[3053248,3054075,3054360],"length":1,"stats":{"Line":1}},{"line":882,"address":[3053324],"length":1,"stats":{"Line":1}},{"line":883,"address":[3053393],"length":1,"stats":{"Line":2}},{"line":884,"address":[3053442],"length":1,"stats":{"Line":1}},{"line":888,"address":[3053493,3053418],"length":1,"stats":{"Line":2}},{"line":889,"address":[3054396,3054621],"length":1,"stats":{"Line":0}},{"line":897,"address":[3053683,3053604],"length":1,"stats":{"Line":1}},{"line":898,"address":[3053658,3053747],"length":1,"stats":{"Line":2}},{"line":899,"address":[3053869,3054091],"length":1,"stats":{"Line":6}},{"line":900,"address":[3054216],"length":1,"stats":{"Line":3}},{"line":904,"address":[3053917,3053988,3054065],"length":1,"stats":{"Line":3}},{"line":907,"address":[3055745,3055717,3054640],"length":1,"stats":{"Line":1}},{"line":908,"address":[3054686],"length":1,"stats":{"Line":2}},{"line":909,"address":[3054712,3054764],"length":1,"stats":{"Line":3}},{"line":910,"address":[3054718],"length":1,"stats":{"Line":1}},{"line":914,"address":[3054780,3054896],"length":1,"stats":{"Line":4}},{"line":915,"address":[3055751],"length":1,"stats":{"Line":0}},{"line":924,"address":[3055025,3055101],"length":1,"stats":{"Line":2}},{"line":925,"address":[3055084,3055712,3055165],"length":1,"stats":{"Line":6}},{"line":926,"address":[3055409],"length":1,"stats":{"Line":2}},{"line":927,"address":[3055491,3055607],"length":1,"stats":{"Line":4}},{"line":930,"address":[3055521],"length":1,"stats":{"Line":2}}],"covered":252,"coverable":295},{"path":["/","home","victor","ML","machinelearne-rs","lib","src","backend","mod.rs"],"content":"//! # Backend Abstraction\n//!\n//! This module provides a trait-based abstraction over computation backends,\n//! enabling models to run on different hardware (CPU, GPU) and tensor libraries\n//! without code changes.\n//!\n//! ## Design Philosophy\n//!\n//! - **Minimal trait surface**: Only essential operations are exposed, keeping backend\n//!   implementations simple and testable.\n//! - **Zero-cost generics**: Backend selection happens at compile time via type parameters,\n//!   avoiding runtime dispatch overhead.\n//! - **Type-safe tensor handling**: Each backend defines its own tensor types (`Tensor1D`,\n//!   `Tensor2D`) that encapsulate storage details while exposing a uniform API.\n//! - **Feature-gated implementations**: Backends are enabled via Cargo features (`cpu`,\n//!   `ndarray`, future `cuda`, etc.), allowing users to minimize dependencies.\n//!\n//! ## Available Backends\n//!\n//! | Backend      | Feature    | Use Case                          |\n//! |--------------|------------|-----------------------------------|\n//! | `CpuBackend` | `cpu`      | Default, pure-Rust implementation |\n//! | `NdarrayBackend` | `ndarray` | Interop with `ndarray` ecosystem |\n//!\n//! ## Example\n//!\n//! ```rust\n//! use machinelearne_rs::backend::{Backend, CpuBackend, Tensor1D, Tensor2D};\n//!\n//! // Backend selection via type parameter\n//! let x: Tensor1D\u003cCpuBackend\u003e = Tensor1D::new(vec![1.0, 2.0]);\n//! let w: Tensor2D\u003cCpuBackend\u003e = Tensor2D::new(vec![0.5, 0.5, 0.5, 0.5], 2, 2);\n//!\n//! // Operations work identically across backends\n//! let y = w.dot(\u0026x);\n//! ```\n//!\n//! ## Implementing a New Backend\n//!\n//! To add a backend (e.g., CUDA):\n//! 1. Create a module with feature gate (`#[cfg(feature = \"cuda\")]`)\n//! 2. Implement concrete tensor types (`CudaTensor1D`, `CudaTensor2D`)\n//! 3. Implement the `Backend` trait with GPU-accelerated operations\n//! 4. Export types via `pub use` for user access\n//!\n//! See `cpu.rs` for a reference implementation.\n\nuse crate::preprocessing::PreprocessingError;\n\n#[cfg(feature = \"cpu\")]\npub mod cpu;\n#[cfg(feature = \"cpu\")]\n/// Pure-Rust CPU backend implementation with zero external dependencies.\npub use cpu::{CpuBackend, CpuTensor2D};\n\n#[cfg(feature = \"ndarray\")]\nmod ndarray_backend;\n#[cfg(feature = \"ndarray\")]\n/// Backend backed by the `ndarray` crate for ecosystem interoperability.\npub use ndarray_backend::{NdarrayBackend, NdarrayTensor2D};\n\n/// Scalar value representation and arithmetic operations.\npub mod scalar;\n/// One-dimensional tensor abstraction.\npub mod tensor1d;\n/// Two-dimensional tensor abstraction.\npub mod tensor2d;\n/// Shared tensor-like operations trait.\npub mod tensorlike;\n\npub use scalar::{Scalar, ScalarOps};\npub use tensor1d::Tensor1D;\npub use tensor2d::Tensor2D;\n\n/// Abstraction over computation devices and tensor operations.\n///\n/// The `Backend` trait defines a minimal set of operations required for\n/// training and inference in machine learning models. Implementations\n/// provide concrete tensor types and device-specific optimizations while\n/// maintaining a uniform API surface.\n///\n/// # Type Parameters\n///\n/// - `Scalar`: Primitive numeric type with arithmetic capabilities\n/// - `Tensor1D`: One-dimensional array representation\n/// - `Tensor2D`: Two-dimensional matrix representation\n/// - `Device`: Hardware device identifier (CPU core, GPU ID, etc.)\n///\n/// # Safety Guarantees\n///\n/// - All checked operations (`matvec`, `matvec_transposed`) validate shapes\n///   and panic on mismatch\n/// - Unchecked variants (`_matvec_unchecked`) skip validation for performance;\n///   caller must ensure correctness\n/// - Tensor types are `Clone + Send + Sync` for safe concurrent usage\n///\n/// # Example Implementation Sketch\n///\n/// ```ignore\n/// use machinelearne_rs::backend::{Backend, ScalarOps};\n/// #[derive(Clone, Debug, Copy)]\n/// struct MyBackend;\n///\n///  #[derive(Clone, Debug, Copy)]\n/// struct MyTensor1D;\n///\n///  #[derive(Clone, Debug, Copy)]\n/// struct MyTensor2D;\n///\n/// impl Backend for MyBackend {\n///     type Scalar = f64;\n///     type Tensor1D = MyTensor1D;\n///     type Tensor2D = MyTensor2D;\n///     type Device = ();\n///\n///     fn default_device() -\u003e Self::Device { () }\n///     // ... implement all required methods\n/// }\n/// ```\npub trait Backend: Clone + Copy + 'static {\n    /// Scalar type supporting arithmetic operations.\n    type Scalar: ScalarOps + Clone;\n\n    /// One-dimensional tensor type.\n    type Tensor1D: Clone + Send + Sync;\n\n    /// Two-dimensional tensor type.\n    type Tensor2D: Clone + Send + Sync;\n\n    /// Device identifier type (CPU core index, GPU handle, etc.).\n    type Device: Clone + Send + Sync;\n\n    /// Returns the default device for this backend.\n    ///\n    /// For CPU backends, typically returns a unit type `()`.\n    /// For GPU backends, may return a device handle or index.\n    fn default_device() -\u003e Self::Device;\n\n    // --- Constructors ---\n\n    /// Creates a 1D tensor filled with zeros of given length.\n    fn zeros_1d(len: usize) -\u003e Self::Tensor1D;\n\n    /// Creates a 2D tensor filled with zeros of given dimensions.\n    fn zeros_2d(rows: usize, cols: usize) -\u003e Self::Tensor2D;\n\n    /// Constructs a 1D tensor from owned data.\n    ///\n    /// # Panics\n    /// Implementation-defined (typically none for valid inputs).\n    fn from_vec_1d(data: Vec\u003cf32\u003e) -\u003e Self::Tensor1D;\n\n    /// Constructs a 2D tensor from row-major ordered data.\n    ///\n    /// # Arguments\n    /// * `data` - Flattened tensor values in row-major order\n    /// * `rows` - Number of rows\n    /// * `cols` - Number of columns\n    ///\n    /// # Panics\n    /// If `data.len() != rows * cols`.\n    fn from_vec_2d(data: Vec\u003cf32\u003e, rows: usize, cols: usize) -\u003e Self::Tensor2D;\n\n    // --- Element-wise operations (1D) ---\n\n    /// Element-wise addition of two 1D tensors.\n    ///\n    /// # Panics\n    /// If tensors have different lengths.\n    fn add_1d(a: \u0026Self::Tensor1D, b: \u0026Self::Tensor1D) -\u003e Self::Tensor1D;\n\n    /// Element-wise subtraction of two 1D tensors.\n    ///\n    /// # Panics\n    /// If tensors have different lengths.\n    fn sub_1d(a: \u0026Self::Tensor1D, b: \u0026Self::Tensor1D) -\u003e Self::Tensor1D;\n\n    /// Element-wise multiplication of two 1D tensors.\n    ///\n    /// # Panics\n    /// If tensors have different lengths.\n    fn mul_1d(a: \u0026Self::Tensor1D, b: \u0026Self::Tensor1D) -\u003e Self::Tensor1D;\n\n    /// Element-wise division of two 1D tensors.\n    ///\n    /// # Panics\n    /// If tensors have different lengths or divisor contains zeros.\n    fn div_1d(a: \u0026Self::Tensor1D, b: \u0026Self::Tensor1D) -\u003e Self::Tensor1D;\n\n    /// Multiplies each element of tensor by a scalar.\n    fn mul_scalar_1d(t: \u0026Self::Tensor1D, s: \u0026Self::Scalar) -\u003e Self::Tensor1D;\n\n    /// Adds a scalar to each element of tensor.\n    fn add_scalar_1d(t: \u0026Self::Tensor1D, s: \u0026Self::Scalar) -\u003e Self::Tensor1D;\n\n    // --- Element-wise operations (2D) ---\n\n    /// Multiplies each element of 2D tensor by a scalar.\n    fn mul_scalar_2d(t: \u0026Self::Tensor2D, s: \u0026Self::Scalar) -\u003e Self::Tensor2D;\n\n    /// Adds a scalar to each element of 2D tensor.\n    fn add_scalar_2d(t: \u0026Self::Tensor2D, s: \u0026Self::Scalar) -\u003e Self::Tensor2D;\n\n    /// Element-wise addition of two 2D tensors.\n    ///\n    /// # Panics\n    /// If tensors have different shapes.\n    fn add_2d(a: \u0026Self::Tensor2D, b: \u0026Self::Tensor2D) -\u003e Self::Tensor2D;\n\n    /// Element-wise subtraction of two 2D tensors.\n    ///\n    /// # Panics\n    /// If tensors have different shapes.\n    fn sub_2d(a: \u0026Self::Tensor2D, b: \u0026Self::Tensor2D) -\u003e Self::Tensor2D;\n\n    /// Element-wise multiplication of two 2D tensors.\n    ///\n    /// # Panics\n    /// If tensors have different shapes.\n    fn mul_2d(a: \u0026Self::Tensor2D, b: \u0026Self::Tensor2D) -\u003e Self::Tensor2D;\n\n    /// Element-wise division of two 2D tensors.\n    ///\n    /// # Panics\n    /// If tensors have different shapes or divisor contains zeros.\n    fn div_2d(a: \u0026Self::Tensor2D, b: \u0026Self::Tensor2D) -\u003e Self::Tensor2D;\n\n    // --- Reduction operations ---\n\n    /// Computes the arithmetic mean of all elements in a 1D tensor.\n    fn mean_all_1d(t: \u0026Self::Tensor1D) -\u003e Self::Scalar;\n\n    /// Computes the arithmetic mean of all elements in a 2D tensor.\n    fn mean_all_2d(t: \u0026Self::Tensor2D) -\u003e Self::Scalar;\n\n    /// Computes the sum of all elements in a 2D tensor.\n    fn sum_all_2d(t: \u0026Self::Tensor2D) -\u003e Self::Scalar;\n\n    /// Computes the sum of all elements in a 1D tensor.\n    fn sum_all_1d(t: \u0026Self::Tensor1D) -\u003e Self::Scalar;\n\n    // --- Scalar operations ---\n\n    /// Creates a backend-specific scalar from an f64 value.\n    ///\n    /// Used for loss gradients, learning rate updates, and other scalar computations.\n    fn scalar_f64(value: f64) -\u003e Self::Scalar;\n\n    // --- Data access ---\n\n    /// Converts a 1D tensor to a Vec of f64 values.\n    ///\n    /// Primarily used for metrics computation and debugging.\n    /// Not intended for hot paths due to allocation overhead.\n    fn to_vec_1d(t: \u0026Self::Tensor1D) -\u003e Vec\u003cf64\u003e;\n\n    /// Returns the number of elements in a 1D tensor.\n    fn len_1d(t: \u0026Self::Tensor1D) -\u003e usize;\n\n    /// Returns the total number of elements in a 2D tensor (rows Ã cols).\n    fn len_2d(t: \u0026Self::Tensor2D) -\u003e usize;\n\n    // --- Mathematical functions (1D) ---\n\n    /// Element-wise absolute value.\n    fn abs_1d(t: \u0026Self::Tensor1D) -\u003e Self::Tensor1D;\n\n    /// Element-wise sign function: returns -1.0, 0.0, or 1.0.\n    fn sign_1d(x: \u0026Self::Tensor1D) -\u003e Self::Tensor1D;\n\n    /// Element-wise maximum between two tensors.\n    ///\n    /// # Panics\n    /// If tensors have different lengths.\n    fn maximum_1d(x: \u0026Self::Tensor1D, other: \u0026Self::Tensor1D) -\u003e Self::Tensor1D;\n\n    /// Element-wise exponential function (e^x).\n    fn exp_1d(x: \u0026Self::Tensor1D) -\u003e Self::Tensor1D;\n\n    /// Element-wise natural logarithm (ln(x)).\n    ///\n    /// # Panics\n    /// If any element is â¤ 0.0.\n    fn log_1d(x: \u0026Self::Tensor1D) -\u003e Self::Tensor1D;\n\n    /// Element-wise sigmoid function: 1 / (1 + e^(-x)).\n    fn sigmoid_1d(x: \u0026Self::Tensor1D) -\u003e Self::Tensor1D;\n\n    // --- Mathematical functions (2D) ---\n\n    /// Element-wise absolute value for 2D tensors.\n    fn abs_2d(t: \u0026Self::Tensor2D) -\u003e Self::Tensor2D;\n\n    /// Element-wise sign function for 2D tensors.\n    fn sign_2d(x: \u0026Self::Tensor2D) -\u003e Self::Tensor2D;\n\n    /// Element-wise maximum between two 2D tensors.\n    ///\n    /// # Panics\n    /// If tensors have different shapes.\n    fn maximum_2d(x: \u0026Self::Tensor2D, other: \u0026Self::Tensor2D) -\u003e Self::Tensor2D;\n\n    /// Element-wise exponential function for 2D tensors.\n    fn exp_2d(x: \u0026Self::Tensor2D) -\u003e Self::Tensor2D;\n\n    /// Element-wise natural logarithm for 2D tensors.\n    ///\n    /// # Panics\n    /// If any element is â¤ 0.0.\n    fn log_2d(x: \u0026Self::Tensor2D) -\u003e Self::Tensor2D;\n\n    /// Element-wise sigmoid function for 2D tensors.\n    fn sigmoid_2d(x: \u0026Self::Tensor2D) -\u003e Self::Tensor2D;\n\n    // --- Linear algebra ---\n\n    /// Matrix-vector multiplication with shape checking.\n    ///\n    /// Computes `y = A * x` where `A` is (m Ã n) and `x` is (n,).\n    /// Returns a (m,) vector.\n    ///\n    /// # Panics\n    /// If `A.cols() != x.len()`.\n    fn matvec(a: \u0026Self::Tensor2D, x: \u0026Self::Tensor1D) -\u003e Self::Tensor1D;\n\n    /// Matrix-vector multiplication without shape checking.\n    ///\n    /// # Safety\n    /// Caller must ensure `A.cols() == x.len()`. Undefined behavior may occur\n    /// if shapes are incompatible.\n    fn _matvec_unchecked(a: \u0026Self::Tensor2D, x: \u0026Self::Tensor1D) -\u003e Self::Tensor1D;\n\n    /// Transposed matrix-vector multiplication with shape checking.\n    ///\n    /// Computes `y = A^T * x` where `A` is (m Ã n) and `x` is (m,).\n    /// Returns a (n,) vector.\n    ///\n    /// # Panics\n    /// If `A.rows() != x.len()`.\n    fn matvec_transposed(a: \u0026Self::Tensor2D, x: \u0026Self::Tensor1D) -\u003e Self::Tensor1D;\n\n    /// Transposed matrix-vector multiplication without shape checking.\n    ///\n    /// # Safety\n    /// Caller must ensure `A.rows() == x.len()`. Undefined behavior may occur\n    /// if shapes are incompatible.\n    fn _matvec_transposed_unchecked(a: \u0026Self::Tensor2D, x: \u0026Self::Tensor1D) -\u003e Self::Tensor1D;\n\n    /// Returns the transpose of a 2D tensor.\n    ///\n    /// Converts an (m Ã n) matrix to (n Ã m) with elements at (i,j) â (j,i).\n    fn transpose(t: \u0026Self::Tensor2D) -\u003e Self::Tensor2D;\n\n    /// Returns the shape of a 2D tensor as (rows, cols).\n    fn shape(t: \u0026Self::Tensor2D) -\u003e (usize, usize);\n\n    //Flattens 2d tensor into 1d tensor\n    fn ravel_2d(x: \u0026Self::Tensor2D) -\u003e Self::Tensor1D;\n\n    // --- Column-wise operations (for preprocessing) ---\n\n    /// Computes the mean of each column in a 2D tensor.\n    ///\n    /// Returns a 1D tensor of length `cols` where each element is the mean\n    /// of the corresponding column.\n    ///\n    /// For a tensor with shape (rows, cols), computes mean along axis 0.\n    fn col_mean_2d(t: \u0026Self::Tensor2D) -\u003e Self::Tensor1D;\n\n    /// Computes the standard deviation of each column in a 2D tensor.\n    ///\n    /// # Arguments\n    /// * `t` - Input 2D tensor\n    /// * `ddof` - Delta degrees of freedom (1 for sample std, 0 for population std)\n    ///\n    /// Returns a 1D tensor of length `cols`.\n    fn col_std_2d(t: \u0026Self::Tensor2D, ddof: usize) -\u003e Self::Tensor1D;\n\n    /// Computes the minimum value of each column in a 2D tensor.\n    ///\n    /// Returns a 1D tensor of length `cols`.\n    fn col_min_2d(t: \u0026Self::Tensor2D) -\u003e Self::Tensor1D;\n\n    /// Computes the maximum value of each column in a 2D tensor.\n    ///\n    /// Returns a 1D tensor of length `cols`.\n    fn col_max_2d(t: \u0026Self::Tensor2D) -\u003e Self::Tensor1D;\n\n    /// Computes the sum of each column in a 2D tensor.\n    ///\n    /// Returns a 1D tensor of length `cols`.\n    fn col_sum_2d(t: \u0026Self::Tensor2D) -\u003e Self::Tensor1D;\n\n    // --- Row-wise operations ---\n\n    /// Computes the sum of each row in a 2D tensor.\n    ///\n    /// Returns a 1D tensor of length `rows`.\n    fn row_sum_2d(t: \u0026Self::Tensor2D) -\u003e Self::Tensor1D;\n\n    // --- Broadcasting operations ---\n\n    /// Broadcasts a 1D tensor and subtracts from each row of a 2D tensor.\n    ///\n    /// For a 2D tensor with shape (rows, cols) and a 1D tensor with shape (cols,),\n    /// subtracts the 1D tensor from each row of the 2D tensor.\n    ///\n    /// Result[i, j] = t[i, j] - v[j]\n    fn broadcast_sub_1d_to_2d_rows(t: \u0026Self::Tensor2D, v: \u0026Self::Tensor1D) -\u003e Self::Tensor2D;\n\n    /// Broadcasts a 1D tensor and divides each row of a 2D tensor.\n    ///\n    /// For a 2D tensor with shape (rows, cols) and a 1D tensor with shape (cols,),\n    /// divides each row of the 2D tensor by the 1D tensor element-wise.\n    ///\n    /// Result[i, j] = t[i, j] / v[j]\n    fn broadcast_div_1d_to_2d_rows(t: \u0026Self::Tensor2D, v: \u0026Self::Tensor1D) -\u003e Self::Tensor2D;\n\n    /// Broadcasts a 1D tensor and multiplies each row of a 2D tensor.\n    ///\n    /// For a 2D tensor with shape (rows, cols) and a 1D tensor with shape (cols,),\n    /// multiplies each row of the 2D tensor by the 1D tensor element-wise.\n    ///\n    /// Result[i, j] = t[i, j] * v[j]\n    fn broadcast_mul_1d_to_2d_rows(t: \u0026Self::Tensor2D, v: \u0026Self::Tensor1D) -\u003e Self::Tensor2D;\n\n    /// Broadcasts a 1D tensor and adds to each row of a 2D tensor.\n    ///\n    /// For a 2D tensor with shape (rows, cols) and a 1D tensor with shape (cols,),\n    /// adds the 1D tensor to each row of the 2D tensor.\n    ///\n    /// Result[i, j] = t[i, j] + v[j]\n    fn broadcast_add_1d_to_2d_rows(t: \u0026Self::Tensor2D, v: \u0026Self::Tensor1D) -\u003e Self::Tensor2D;\n\n    /// Sqrt of all elements in a 1D tensor.\n    fn sqrt_1d(t: \u0026Self::Tensor1D) -\u003e Self::Tensor1D;\n\n    /// Sqrt of all elements in a 2D tensor.\n    fn sqrt_2d(t: \u0026Self::Tensor2D) -\u003e Self::Tensor2D;\n\n    // --- Column manipulation operations ---\n\n    /// Horizontally concatenate 2D tensors (stack columns side by side).\n    ///\n    /// All input tensors must have the same number of rows.\n    /// Returns a new tensor with shape (rows, sum of all cols).\n    ///\n    /// # Arguments\n    /// * `tensors` - Slice of 2D tensors to concatenate\n    ///\n    /// # Panics\n    /// Panics if tensors have different row counts or if the slice is empty.\n    ///\n    /// # Example\n    /// ```ignore\n    /// // [[1, 2]] + [[3]] -\u003e [[1, 2, 3]]\n    /// let a = Tensor2D::new(vec![1.0, 2.0], 1, 2);\n    /// let b = Tensor2D::new(vec![3.0], 1, 1);\n    /// let c = B::hcat_2d(\u0026[a, b]); // shape (1, 3)\n    /// ```\n    fn hcat_2d(tensors: \u0026[Self::Tensor2D]) -\u003e Result\u003cSelf::Tensor2D, PreprocessingError\u003e;\n\n    /// Extract specific columns from a 2D tensor.\n    ///\n    /// Returns a new tensor with only the specified columns, preserving row order.\n    ///\n    /// # Arguments\n    /// * `t` - Input 2D tensor\n    /// * `columns` - Indices of columns to extract (in order)\n    ///\n    /// # Panics\n    /// Panics if any column index is out of bounds.\n    fn select_columns_2d(t: \u0026Self::Tensor2D, columns: \u0026[usize]) -\u003e Self::Tensor2D;\n\n    /// Create a one-hot encoded matrix from integer indices.\n    ///\n    /// Each index becomes a row with a 1 at the index position and 0 elsewhere.\n    ///\n    /// # Arguments\n    /// * `indices` - 1D tensor of integer class indices (0 to num_classes-1)\n    /// * `num_classes` - Total number of classes (determines output column count)\n    ///\n    /// # Returns\n    /// A 2D tensor of shape (indices.len(), num_classes)\n    ///\n    /// # Panics\n    /// Panics if any index \u003e= num_classes.\n    fn one_hot_from_indices(indices: \u0026Self::Tensor1D, num_classes: usize) -\u003e Self::Tensor2D;\n}\n","traces":[],"covered":0,"coverable":0},{"path":["/","home","victor","ML","machinelearne-rs","lib","src","backend","ndarray_backend.rs"],"content":"use super::Backend;\nuse ndarray::{Array1, Array2, Ix1};\n\n/// CPU-based tensor backend implementation using the `ndarray` crate.\n///\n/// This backend provides efficient CPU tensor operations for machine learning workloads\n/// using ndarray's optimized linear algebra routines. It supports both 1D and 2D tensors\n/// with element-wise operations, reductions, and matrix-vector products.\n///\n/// # Type mappings\n/// - `Scalar`: `f64` (double-precision floating point)\n/// - `Tensor1D`: `ndarray::Array1\u003cf64\u003e` (1-dimensional array)\n/// - `Tensor2D`: `NdarrayTensor2D` wrapper around `ndarray::Array2\u003cf64\u003e`\n/// - `Device`: `()` (unit type, CPU-only execution)\n///\n/// # Numerical stability\n/// Non-linear operations like `sigmoid` use numerically stable implementations\n/// to avoid overflow/underflow for extreme input values (e.g., Â±100).\n#[derive(Clone, Debug, Copy)]\npub struct NdarrayBackend;\n\n/// Wrapper type for 2D tensors using ndarray's `Array2\u003cf64\u003e`.\n///\n/// This wrapper enables trait implementation for external types while providing\n/// convenient conversion from nested Vec representations commonly used in tests\n/// and data loading.\n///\n/// # Example\n/// ```\n/// use machinelearne_rs::backend::{NdarrayBackend, NdarrayTensor2D, Backend};\n/// use ndarray::{Array1, Array2, Ix1, Ix2};\n/// let tensor = NdarrayTensor2D::from(\u0026[\n///     vec![1.0f64, 2.0f64, 3.0f64],\n///     vec![4.0f64, 5.0f64, 6.0f64],\n/// ][..]);\n/// assert_eq!(tensor.0.shape(), \u0026[2, 3]);\n/// ```\n#[derive(Debug, Clone)]\npub struct NdarrayTensor2D(pub Array2\u003cf64\u003e);\n\nimpl From\u003c\u0026[Vec\u003cf64\u003e]\u003e for NdarrayTensor2D {\n    /// Converts a slice of row vectors into a 2D tensor.\n    ///\n    /// # Panics\n    /// Panics if rows have inconsistent lengths or if shape reconstruction fails.\n    ///\n    /// # Example\n    /// ```\n    /// use machinelearne_rs::backend::{NdarrayBackend, NdarrayTensor2D, Backend};\n    /// use ndarray::{Array1, Array2, Ix1, Ix2};\n    /// let data = \u0026[vec![1.0f64, 2.0f64], vec![3.0f64, 4.0f64]][..];\n    /// let tensor = NdarrayTensor2D::from(data);\n    /// assert_eq!(tensor.0[[0, 0]], 1.0);\n    /// assert_eq!(tensor.0[[1, 1]], 4.0);\n    /// ```\n    fn from(x: \u0026[Vec\u003cf64\u003e]) -\u003e Self {\n        let rows = x.len();\n        if rows == 0 {\n            return NdarrayTensor2D(Array2::from_shape_vec((0, 0), vec![]).unwrap());\n        }\n        let cols = x[0].len();\n        assert!(x.iter().all(|r| r.len() == cols));\n        let data: Vec\u003cf64\u003e = x.iter().flat_map(|r| r.iter()).copied().collect();\n        NdarrayTensor2D(Array2::from_shape_vec((rows, cols), data).unwrap())\n    }\n}\n\nimpl super::Backend for NdarrayBackend {\n    type Scalar = f64;\n    type Tensor1D = Array1\u003cf64\u003e;\n    type Tensor2D = NdarrayTensor2D;\n    type Device = (); // CPU-only for now\n\n    /// Returns the default device for this backend (unit type for CPU execution).\n    fn default_device() -\u003e Self::Device {\n        ()\n    }\n\n    /// Creates a 1D tensor filled with zeros.\n    ///\n    /// # Arguments\n    /// * `len` - Length of the resulting 1D tensor\n    ///\n    /// # Example\n    /// ```\n    /// use machinelearne_rs::backend::{NdarrayBackend, NdarrayTensor2D, Backend};\n    /// let zeros = NdarrayBackend::zeros_1d(3);\n    /// assert_eq!(zeros.to_vec(), vec![0.0, 0.0, 0.0]);\n    /// ```\n    fn zeros_1d(len: usize) -\u003e Self::Tensor1D {\n        Array1::zeros(len)\n    }\n\n    /// Creates a 2D tensor filled with zeros.\n    ///\n    /// # Arguments\n    /// * `rows` - Number of rows\n    /// * `cols` - Number of columns\n    ///\n    /// # Example\n    /// ```\n    /// use machinelearne_rs::backend::{NdarrayBackend, NdarrayTensor2D, Backend};\n    /// let zeros = NdarrayBackend::zeros_2d(2, 3);\n    /// assert_eq!(NdarrayBackend::shape(\u0026zeros), (2, 3));\n    /// ```\n    fn zeros_2d(rows: usize, cols: usize) -\u003e Self::Tensor2D {\n        NdarrayTensor2D(Array2::zeros((rows, cols)))\n    }\n\n    /// Converts a vector of f32 values to a 1D tensor (f64 precision).\n    ///\n    /// # Arguments\n    /// * `data` - Input vector with f32 values\n    ///\n    /// # Example\n    /// ```\n    /// use machinelearne_rs::backend::{NdarrayBackend, NdarrayTensor2D, Backend};\n    /// let tensor = NdarrayBackend::from_vec_1d(vec![1.0f32, 2.0, 3.0]);\n    /// assert_eq!(tensor.to_vec(), vec![1.0, 2.0, 3.0]);\n    /// ```\n    fn from_vec_1d(data: Vec\u003cf32\u003e) -\u003e Self::Tensor1D {\n        Array1::from_iter(data.into_iter().map(|x| x as f64))\n    }\n\n    /// Converts a flat vector of f32 values to a 2D tensor with specified shape.\n    ///\n    /// # Arguments\n    /// * `data` - Flat vector containing row-major ordered elements\n    /// * `rows` - Number of rows in output tensor\n    /// * `cols` - Number of columns in output tensor\n    ///\n    /// # Panics\n    /// Panics if `data.len() != rows * cols`.\n    ///\n    /// # Example\n    /// ```\n    /// use machinelearne_rs::backend::{NdarrayBackend, NdarrayTensor2D, Backend};\n    /// let tensor = NdarrayBackend::from_vec_2d(vec![1.0, 2.0, 3.0, 4.0], 2, 2);\n    /// assert_eq!(tensor.0[[0, 0]], 1.0);\n    /// assert_eq!(tensor.0[[1, 1]], 4.0);\n    /// ```\n    fn from_vec_2d(data: Vec\u003cf32\u003e, rows: usize, cols: usize) -\u003e Self::Tensor2D {\n        let data_f64: Vec\u003cf64\u003e = data.into_iter().map(|x| x as f64).collect();\n        NdarrayTensor2D(Array2::from_shape_vec((rows, cols), data_f64).unwrap())\n    }\n\n    /// Element-wise addition of two 1D tensors.\n    ///\n    /// # Panics\n    /// Panics if tensors have different lengths.\n    ///\n    /// # Example\n    /// ```\n    /// use machinelearne_rs::backend::{NdarrayBackend, NdarrayTensor2D, Backend};\n    /// use ndarray::{Array1, Array2, Ix1, Ix2};\n    /// let a = Array1::from_vec(vec![1.0, 2.0]);\n    /// let b = Array1::from_vec(vec![3.0, 4.0]);\n    /// let c = NdarrayBackend::add_1d(\u0026a, \u0026b);\n    /// assert_eq!(c.to_vec(), vec![4.0, 6.0]);\n    /// ```\n    fn add_1d(a: \u0026Self::Tensor1D, b: \u0026Self::Tensor1D) -\u003e Self::Tensor1D {\n        a + b\n    }\n\n    /// Multiplies a 2D tensor by a scalar value.\n    ///\n    /// # Arguments\n    /// * `t` - Input 2D tensor\n    /// * `s` - Scalar multiplier\n    ///\n    /// # Example\n    /// ```\n    /// use machinelearne_rs::backend::{NdarrayBackend, NdarrayTensor2D, Backend};\n    /// use ndarray::{Array1, Array2, Ix1, Ix2};\n    /// let t = NdarrayTensor2D(Array2::from_shape_vec((2, 2), vec![1.0, 2.0, 3.0, 4.0]).unwrap());\n    /// let scaled = NdarrayBackend::mul_scalar_2d(\u0026t, \u00262.0);\n    /// assert_eq!(scaled.0[[0, 1]], 4.0);\n    /// ```\n    fn mul_scalar_2d(t: \u0026Self::Tensor2D, s: \u0026Self::Scalar) -\u003e Self::Tensor2D {\n        NdarrayTensor2D(\u0026t.0 * *s)\n    }\n\n    /// Matrix-vector multiplication: `A @ x`.\n    ///\n    /// Computes the product of a 2D matrix and a 1D vector.\n    ///\n    /// # Arguments\n    /// * `a` - Matrix of shape (m, n)\n    /// * `x` - Vector of shape (n,)\n    ///\n    /// # Returns\n    /// Vector of shape (m,)\n    ///\n    /// # Panics\n    /// Panics if matrix columns != vector length.\n    ///\n    /// # Example\n    /// ```\n    /// // [[1, 2],    [1]   [5]\n    /// //  [3, 4]] @ [2] = [11]\n    /// use machinelearne_rs::backend::{NdarrayBackend, NdarrayTensor2D, Backend};\n    /// use ndarray::{Array1, Array2, Ix1, Ix2};\n    /// let a = NdarrayTensor2D::from(\u0026[vec![1.0, 2.0], vec![3.0, 4.0]][..]);\n    /// let x = Array1::from_vec(vec![1.0, 2.0]);\n    /// let y = NdarrayBackend::matvec(\u0026a, \u0026x);\n    /// assert_eq!(y.to_vec(), vec![5.0, 11.0]);\n    /// ```\n    fn matvec(a: \u0026Self::Tensor2D, x: \u0026Self::Tensor1D) -\u003e Self::Tensor1D {\n        a.0.dot(x) // ndarray has efficient matvec\n    }\n\n    /// Transposed matrix-vector multiplication: `A^T @ x`.\n    ///\n    /// Computes the product of a transposed matrix and a vector.\n    ///\n    /// # Arguments\n    /// * `a` - Matrix of shape (m, n)\n    /// * `x` - Vector of shape (m,)\n    ///\n    /// # Returns\n    /// Vector of shape (n,)\n    ///\n    /// # Example\n    /// ```\n    /// // [[1, 2],^T   [1]   [1]\n    /// //  [3, 4]]  @ [0] = [2]\n    /// use machinelearne_rs::backend::{NdarrayBackend, NdarrayTensor2D, Backend};\n    /// use ndarray::{Array1, Array2, Ix1, Ix2};\n    /// let a = NdarrayTensor2D::from(\u0026[vec![1.0, 2.0], vec![3.0, 4.0]][..]);\n    /// let x = Array1::from_vec(vec![1.0, 0.0]);\n    /// let y = NdarrayBackend::matvec_transposed(\u0026a, \u0026x);\n    /// assert_eq!(y.to_vec(), vec![1.0, 2.0]);\n    /// ```\n    fn matvec_transposed(a: \u0026Self::Tensor2D, x: \u0026Self::Tensor1D) -\u003e Self::Tensor1D {\n        a.0.t().dot(x)\n    }\n\n    /// Returns the transpose of a 2D tensor.\n    ///\n    /// # Arguments\n    /// * `a` - Input tensor of shape (m, n)\n    ///\n    /// # Returns\n    /// Tensor of shape (n, m)\n    ///\n    /// # Example\n    /// ```\n    /// use machinelearne_rs::backend::{NdarrayBackend, NdarrayTensor2D, Backend};\n    /// let a = NdarrayTensor2D::from(\u0026[vec![1.0, 2.0], vec![3.0, 4.0]][..]);\n    /// let at = NdarrayBackend::transpose(\u0026a);\n    /// assert_eq!(at.0[[0, 1]], 3.0); // Original [1,0] becomes [0,1]\n    /// ```\n    fn transpose(a: \u0026Self::Tensor2D) -\u003e Self::Tensor2D {\n        NdarrayTensor2D(a.0.t().to_owned())\n    }\n\n    /// Returns the shape of a 2D tensor as (rows, columns).\n    ///\n    /// # Arguments\n    /// * `t` - Input 2D tensor\n    ///\n    /// # Returns\n    /// Tuple `(rows, cols)`\n    ///\n    /// # Example\n    /// ```\n    /// use machinelearne_rs::backend::{NdarrayBackend, NdarrayTensor2D, Backend};\n    /// use ndarray::{Array1, Array2, Ix1, Ix2};\n    /// let t = NdarrayTensor2D::from(\u0026[vec![1.0, 2.0, 3.0]][..]);\n    /// assert_eq!(NdarrayBackend::shape(\u0026t), (1, 3));\n    /// ```\n    fn shape(t: \u0026Self::Tensor2D) -\u003e (usize, usize) {\n        let shape = t.0.shape();\n        (shape[0], shape[1])\n    }\n\n    // --- Element-wise non-linear ops (1D) ---\n\n    /// Computes element-wise exponential: `e^x`.\n    ///\n    /// # Example\n    /// ```\n    /// use machinelearne_rs::backend::{NdarrayBackend, NdarrayTensor2D, Backend};\n    /// use ndarray::{Array1, Array2, Ix1, Ix2};\n    /// let x = Array1::from_vec(vec![0.0, 1.0]);\n    /// let y = NdarrayBackend::exp_1d(\u0026x);\n    /// assert!((y[0] - 1.0).abs() \u003c 1e-6);\n    /// assert!((y[1] - std::f64::consts::E).abs() \u003c 1e-6);\n    /// ```\n    fn exp_1d(x: \u0026Self::Tensor1D) -\u003e Self::Tensor1D {\n        x.mapv(f64::exp)\n    }\n\n    /// Computes element-wise natural logarithm: `ln(x)`.\n    ///\n    /// # Panics\n    /// Panics for non-positive values (ndarray behavior).\n    ///\n    /// # Example\n    /// ```\n    /// use machinelearne_rs::backend::{NdarrayBackend, NdarrayTensor2D, Backend};\n    /// use ndarray::{Array1, Array2, Ix1, Ix2};\n    /// let x = Array1::from_vec(vec![1.0, std::f64::consts::E]);\n    /// let y = NdarrayBackend::log_1d(\u0026x);\n    /// assert!((y[0] - 0.0).abs() \u003c 1e-6);\n    /// assert!((y[1] - 1.0).abs() \u003c 1e-6);\n    /// ```\n    fn log_1d(x: \u0026Self::Tensor1D) -\u003e Self::Tensor1D {\n        x.mapv(f64::ln)\n    }\n\n    /// Computes numerically stable sigmoid activation: `1 / (1 + e^{-x})`.\n    ///\n    /// Uses a numerically stable implementation that avoids overflow for\n    /// extreme values (Â±100+).\n    ///\n    /// # Example\n    /// ```\n    /// use machinelearne_rs::backend::{NdarrayBackend, NdarrayTensor2D, Backend};\n    /// use ndarray::{Array1, Array2, Ix1, Ix2};\n    /// let x = Array1::from_vec(vec![-100.0, 0.0, 100.0]);\n    /// let y = NdarrayBackend::sigmoid_1d(\u0026x);\n    /// assert!((y[0] - 0.0).abs() \u003c 1e-6);   // â0\n    /// assert!((y[1] - 0.5).abs() \u003c 1e-6);   // =0.5\n    /// assert!((y[2] - 1.0).abs() \u003c 1e-6);   // â1\n    /// ```\n    fn sigmoid_1d(x: \u0026Self::Tensor1D) -\u003e Self::Tensor1D {\n        x.mapv(|z| {\n            if z \u003e= 0.0 {\n                1.0 / (1.0 + (-z).exp())\n            } else {\n                let ez = z.exp();\n                ez / (1.0 + ez)\n            }\n        })\n    }\n\n    /// Computes element-wise absolute value.\n    ///\n    /// # Example\n    /// ```\n    /// use machinelearne_rs::backend::{NdarrayBackend, NdarrayTensor2D, Backend};\n    /// use ndarray::{Array1, Array2, Ix1, Ix2};\n    /// let x = Array1::from_vec(vec![-2.0, 3.0]);\n    /// let y = NdarrayBackend::abs_1d(\u0026x);\n    /// assert_eq!(y.to_vec(), vec![2.0, 3.0]);\n    /// ```\n    fn abs_1d(x: \u0026Self::Tensor1D) -\u003e Self::Tensor1D {\n        x.mapv(f64::abs)\n    }\n\n    /// Computes element-wise sign function.\n    ///\n    /// Returns -1.0 for negative values, 0.0 for zero, 1.0 for positive values.\n    ///\n    /// # Example\n    /// ```\n    /// use machinelearne_rs::backend::{NdarrayBackend, NdarrayTensor2D, Backend};\n    /// use ndarray::{Array1, Array2, Ix1, Ix2};\n    /// let x = Array1::from_vec(vec![-2.0, 0.0, 3.0]);\n    /// let y = NdarrayBackend::sign_1d(\u0026x);\n    /// assert_eq!(y.to_vec(), vec![-1.0, 0.0, 1.0]);\n    /// ```\n    fn sign_1d(x: \u0026Self::Tensor1D) -\u003e Self::Tensor1D {\n        x.mapv(|x| {\n            if x \u003e 0.0 {\n                1.0\n            } else if x \u003c 0.0 {\n                -1.0\n            } else {\n                0.0\n            }\n        })\n    }\n\n    /// Element-wise maximum of two 1D tensors.\n    ///\n    /// # Panics\n    /// Panics if tensors have different lengths.\n    ///\n    /// # Example\n    /// ```\n    /// use machinelearne_rs::backend::{NdarrayBackend, NdarrayTensor2D, Backend};\n    /// use ndarray::{Array1, Array2, Ix1, Ix2};\n    /// let a = Array1::from_vec(vec![1.0, 5.0, 3.0]);\n    /// let b = Array1::from_vec(vec![2.0, 4.0, 6.0]);\n    /// let m = NdarrayBackend::maximum_1d(\u0026a, \u0026b);\n    /// assert_eq!(m.to_vec(), vec![2.0, 5.0, 6.0]);\n    /// ```\n    fn maximum_1d(a: \u0026Self::Tensor1D, b: \u0026Self::Tensor1D) -\u003e Self::Tensor1D {\n        assert_eq!(a.len(), b.len(), \"Shapes must match\");\n        a.iter().zip(b.iter()).map(|(\u0026x, \u0026y)| x.max(y)).collect()\n    }\n\n    // --- 2D versions (delegating to 1D via flat view or direct map) ---\n\n    /// Computes element-wise exponential for 2D tensors.\n    fn exp_2d(x: \u0026Self::Tensor2D) -\u003e Self::Tensor2D {\n        NdarrayTensor2D(x.0.mapv(f64::exp))\n    }\n\n    /// Computes element-wise natural logarithm for 2D tensors.\n    ///\n    /// # Panics\n    /// Panics for non-positive values.\n    fn log_2d(x: \u0026Self::Tensor2D) -\u003e Self::Tensor2D {\n        NdarrayTensor2D(x.0.mapv(f64::ln))\n    }\n\n    /// Computes numerically stable sigmoid activation for 2D tensors.\n    ///\n    /// Uses the same stable implementation as `sigmoid_1d` applied element-wise.\n    fn sigmoid_2d(x: \u0026Self::Tensor2D) -\u003e Self::Tensor2D {\n        NdarrayTensor2D(x.0.mapv(|z| {\n            if z \u003e= 0.0 {\n                1.0 / (1.0 + (-z).exp())\n            } else {\n                let ez = z.exp();\n                ez / (1.0 + ez)\n            }\n        }))\n    }\n\n    /// Computes element-wise absolute value for 2D tensors.\n    fn abs_2d(x: \u0026Self::Tensor2D) -\u003e Self::Tensor2D {\n        NdarrayTensor2D(x.0.mapv(f64::abs))\n    }\n\n    /// Computes element-wise sign function for 2D tensors.\n    fn sign_2d(x: \u0026Self::Tensor2D) -\u003e Self::Tensor2D {\n        NdarrayTensor2D(x.0.mapv(|x| {\n            if x \u003e 0.0 {\n                1.0\n            } else if x \u003c 0.0 {\n                -1.0\n            } else {\n                0.0\n            }\n        }))\n    }\n\n    /// Element-wise maximum of two 2D tensors.\n    ///\n    /// # Panics\n    /// Panics if tensors have different shapes.\n    fn maximum_2d(a: \u0026Self::Tensor2D, b: \u0026Self::Tensor2D) -\u003e Self::Tensor2D {\n        let (rows, cols) = a.0.dim();\n        assert_eq!(a.0.dim(), b.0.dim(), \"Shapes must match\");\n        let data: Vec\u003cf64\u003e =\n            a.0.iter()\n                .zip(b.0.iter())\n                .map(|(\u0026x, \u0026y)| x.max(y))\n                .collect();\n        NdarrayTensor2D(Array2::from_shape_vec((rows, cols), data).unwrap())\n    }\n\n    // --- Reductions (already partially covered, but for completeness) ---\n\n    /// Sums all elements in a 1D tensor.\n    fn sum_all_1d(t: \u0026Self::Tensor1D) -\u003e Self::Scalar {\n        t.sum()\n    }\n\n    /// Computes mean of all elements in a 1D tensor.\n    ///\n    /// # Panics\n    /// Panics if tensor is empty (ndarray behavior).\n    fn mean_all_1d(t: \u0026Self::Tensor1D) -\u003e Self::Scalar {\n        t.mean().unwrap()\n    }\n\n    /// Sums all elements in a 2D tensor.\n    fn sum_all_2d(t: \u0026Self::Tensor2D) -\u003e Self::Scalar {\n        t.0.sum()\n    }\n\n    /// Computes mean of all elements in a 2D tensor.\n    ///\n    /// # Panics\n    /// Panics if tensor is empty.\n    fn mean_all_2d(t: \u0026Self::Tensor2D) -\u003e Self::Scalar {\n        t.0.mean().unwrap()\n    }\n\n    // --- Scalar and access helpers ---\n\n    /// Creates a scalar value from f64.\n    ///\n    /// This is a trivial conversion since `Scalar = f64`.\n    fn scalar_f64(value: f64) -\u003e Self::Scalar {\n        value\n    }\n\n    /// Returns the length (number of elements) of a 1D tensor.\n    fn len_1d(t: \u0026Self::Tensor1D) -\u003e usize {\n        t.len()\n    }\n\n    /// Returns the number of rows in a 2D tensor.\n    ///\n    /// Note: This returns `nrows()`, not total element count.\n    fn len_2d(t: \u0026Self::Tensor2D) -\u003e usize {\n        t.0.nrows()\n    }\n\n    /// Converts a 1D tensor to a standard Vec\u003cf64\u003e.\n    fn to_vec_1d(t: \u0026Self::Tensor1D) -\u003e Vec\u003cf64\u003e {\n        t.to_vec()\n    }\n\n    // --- Element-wise binary ops (1D) ---\n\n    /// Element-wise subtraction of 1D tensors.\n    fn sub_1d(a: \u0026Self::Tensor1D, b: \u0026Self::Tensor1D) -\u003e Self::Tensor1D {\n        a - b\n    }\n\n    /// Element-wise multiplication of 1D tensors.\n    fn mul_1d(a: \u0026Self::Tensor1D, b: \u0026Self::Tensor1D) -\u003e Self::Tensor1D {\n        a * b\n    }\n\n    /// Element-wise division of 1D tensors.\n    fn div_1d(a: \u0026Self::Tensor1D, b: \u0026Self::Tensor1D) -\u003e Self::Tensor1D {\n        a / b\n    }\n\n    /// Multiplies each element of a 1D tensor by a scalar.\n    fn mul_scalar_1d(t: \u0026Self::Tensor1D, s: \u0026Self::Scalar) -\u003e Self::Tensor1D {\n        t.mapv(|x| x * *s)\n    }\n\n    /// Adds a scalar to each element of a 1D tensor.\n    fn add_scalar_1d(t: \u0026Self::Tensor1D, s: \u0026Self::Scalar) -\u003e Self::Tensor1D {\n        t.mapv(|x| x + *s)\n    }\n\n    // --- 2D scalar and binary ops ---\n\n    /// Adds a scalar to each element of a 2D tensor.\n    fn add_scalar_2d(t: \u0026Self::Tensor2D, s: \u0026Self::Scalar) -\u003e Self::Tensor2D {\n        NdarrayTensor2D(t.0.mapv(|x| x + *s))\n    }\n\n    /// Element-wise addition of 2D tensors.\n    fn add_2d(a: \u0026Self::Tensor2D, b: \u0026Self::Tensor2D) -\u003e Self::Tensor2D {\n        NdarrayTensor2D(\u0026a.0 + \u0026b.0)\n    }\n\n    /// Element-wise subtraction of 2D tensors.\n    fn sub_2d(a: \u0026Self::Tensor2D, b: \u0026Self::Tensor2D) -\u003e Self::Tensor2D {\n        NdarrayTensor2D(\u0026a.0 - \u0026b.0)\n    }\n\n    /// Element-wise multiplication of 2D tensors (Hadamard product).\n    fn mul_2d(a: \u0026Self::Tensor2D, b: \u0026Self::Tensor2D) -\u003e Self::Tensor2D {\n        NdarrayTensor2D(\u0026a.0 * \u0026b.0)\n    }\n\n    /// Element-wise division of 2D tensors.\n    fn div_2d(a: \u0026Self::Tensor2D, b: \u0026Self::Tensor2D) -\u003e Self::Tensor2D {\n        NdarrayTensor2D(\u0026a.0 / \u0026b.0)\n    }\n\n    // --- \"Unchecked\" matvec helpers (Ð´Ð»Ñ ÑÐ¾Ð²Ð¼ÐµÑÑÐ¸Ð¼Ð¾ÑÑÐ¸ Ñ CpuBackend) ---\n    // Ð ndarray Ð¾Ð½Ð¸ Ð½Ðµ Ð½ÑÐ¶Ð½Ñ, Ð½Ð¾ ÑÑÐµÐ¹Ñ ÑÑÐµÐ±ÑÐµÑ â Ð´ÐµÐ»Ð°ÐµÐ¼ Ð¿ÑÐ¾ÑÑÐ¾ Ð¾Ð±ÑÑÑÐºÐ¸\n\n    /// Unchecked matrix-vector multiplication (same as `matvec`).\n    ///\n    /// Provided for trait compatibility; delegates to `matvec`.\n    fn _matvec_unchecked(a: \u0026Self::Tensor2D, x: \u0026Self::Tensor1D) -\u003e Self::Tensor1D {\n        Self::matvec(a, x)\n    }\n\n    /// Unchecked transposed matrix-vector multiplication (same as `matvec_transposed`).\n    ///\n    /// Provided for trait compatibility; delegates to `matvec_transposed`.\n    fn _matvec_transposed_unchecked(a: \u0026Self::Tensor2D, x: \u0026Self::Tensor1D) -\u003e Self::Tensor1D {\n        Self::matvec_transposed(a, x)\n    }\n\n    //Returns copy of the inner 1d vector\n    fn ravel_2d(x: \u0026Self::Tensor2D) -\u003e Self::Tensor1D {\n        x.0.clone()\n            .into_dimensionality::\u003cIx1\u003e()\n            .expect(\"Failed to ravel 2D tensor: shape conversion error\")\n    }\n\n    // --- Column-wise operations ---\n\n    fn col_mean_2d(t: \u0026Self::Tensor2D) -\u003e Self::Tensor1D {\n        t.0.mean_axis(ndarray::Axis(0))\n            .unwrap_or_else(|| Array1::zeros(0))\n    }\n\n    fn col_std_2d(t: \u0026Self::Tensor2D, ddof: usize) -\u003e Self::Tensor1D {\n        let ncols = t.0.ncols();\n        if ncols == 0 {\n            return Array1::zeros(0);\n        }\n\n        let means = Self::col_mean_2d(t);\n        let nrows = t.0.nrows();\n\n        let mut stds = Array1::zeros(ncols);\n        for col in 0..ncols {\n            let mut var_sum = 0.0;\n            for row in 0..nrows {\n                let diff = t.0[[row, col]] - means[col];\n                var_sum += diff * diff;\n            }\n            let divisor = (nrows - ddof) as f64;\n            stds[col] = (var_sum / divisor).sqrt();\n        }\n        stds\n    }\n\n    fn col_min_2d(t: \u0026Self::Tensor2D) -\u003e Self::Tensor1D {\n        let ncols = t.0.ncols();\n        if ncols == 0 {\n            return Array1::zeros(0);\n        }\n\n        let mut mins = Array1::from_elem(ncols, f64::INFINITY);\n        for col in 0..ncols {\n            for row in 0..t.0.nrows() {\n                let val = t.0[[row, col]];\n                if val \u003c mins[col] {\n                    mins[col] = val;\n                }\n            }\n        }\n        mins\n    }\n\n    fn col_max_2d(t: \u0026Self::Tensor2D) -\u003e Self::Tensor1D {\n        let ncols = t.0.ncols();\n        if ncols == 0 {\n            return Array1::zeros(0);\n        }\n\n        let mut maxs = Array1::from_elem(ncols, f64::NEG_INFINITY);\n        for col in 0..ncols {\n            for row in 0..t.0.nrows() {\n                let val = t.0[[row, col]];\n                if val \u003e maxs[col] {\n                    maxs[col] = val;\n                }\n            }\n        }\n        maxs\n    }\n\n    fn col_sum_2d(t: \u0026Self::Tensor2D) -\u003e Self::Tensor1D {\n        t.0.sum_axis(ndarray::Axis(0))\n    }\n\n    // --- Row-wise operations ---\n\n    fn row_sum_2d(t: \u0026Self::Tensor2D) -\u003e Self::Tensor1D {\n        t.0.sum_axis(ndarray::Axis(1))\n    }\n\n    // --- Broadcasting operations ---\n\n    fn broadcast_sub_1d_to_2d_rows(t: \u0026Self::Tensor2D, v: \u0026Self::Tensor1D) -\u003e Self::Tensor2D {\n        NdarrayTensor2D(\u0026t.0 - \u0026v.view().insert_axis(ndarray::Axis(0)))\n    }\n\n    fn broadcast_div_1d_to_2d_rows(t: \u0026Self::Tensor2D, v: \u0026Self::Tensor1D) -\u003e Self::Tensor2D {\n        NdarrayTensor2D(\u0026t.0 / \u0026v.view().insert_axis(ndarray::Axis(0)))\n    }\n\n    fn broadcast_mul_1d_to_2d_rows(t: \u0026Self::Tensor2D, v: \u0026Self::Tensor1D) -\u003e Self::Tensor2D {\n        NdarrayTensor2D(\u0026t.0 * \u0026v.view().insert_axis(ndarray::Axis(0)))\n    }\n\n    fn broadcast_add_1d_to_2d_rows(t: \u0026Self::Tensor2D, v: \u0026Self::Tensor1D) -\u003e Self::Tensor2D {\n        NdarrayTensor2D(\u0026t.0 + \u0026v.view().insert_axis(ndarray::Axis(0)))\n    }\n\n    fn sqrt_1d(t: \u0026Self::Tensor1D) -\u003e Self::Tensor1D {\n        t.mapv(f64::sqrt)\n    }\n\n    fn sqrt_2d(t: \u0026Self::Tensor2D) -\u003e Self::Tensor2D {\n        NdarrayTensor2D(t.0.mapv(f64::sqrt))\n    }\n\n    // --- Column manipulation operations ---\n\n    fn hcat_2d(\n        tensors: \u0026[Self::Tensor2D],\n    ) -\u003e Result\u003cSelf::Tensor2D, crate::preprocessing::PreprocessingError\u003e {\n        if tensors.is_empty() {\n            return Err(crate::preprocessing::PreprocessingError::InvalidParameter(\n                \"Cannot horizontally concatenate empty slice of tensors\".to_string(),\n            ));\n        }\n\n        let rows = tensors[0].0.nrows();\n        if rows == 0 {\n            return Ok(NdarrayTensor2D(\n                Array2::from_shape_vec((0, 0), vec![]).unwrap(),\n            ));\n        }\n\n        // Verify all tensors have the same number of rows\n        for t in tensors.iter() {\n            if t.0.nrows() != rows {\n                return Err(crate::preprocessing::PreprocessingError::InvalidShape {\n                    expected: format!(\"({}, ?)\", rows),\n                    got: format!(\"({}, ?)\", t.0.nrows()),\n                });\n            }\n        }\n\n        // Calculate total columns\n        let total_cols: usize = tensors.iter().map(|t| t.0.ncols()).sum();\n\n        // Manually concatenate by copying data\n        let mut result = Array2::zeros((rows, total_cols));\n        let mut col_offset = 0;\n        for t in tensors {\n            let ncols = t.0.ncols();\n            for r in 0..rows {\n                for c in 0..ncols {\n                    result[[r, col_offset + c]] = t.0[[r, c]];\n                }\n            }\n            col_offset += ncols;\n        }\n\n        Ok(NdarrayTensor2D(result))\n    }\n\n    fn select_columns_2d(t: \u0026Self::Tensor2D, columns: \u0026[usize]) -\u003e Self::Tensor2D {\n        let (rows, ncols) = t.0.dim();\n        if columns.is_empty() {\n            return NdarrayTensor2D(Array2::from_shape_vec((rows, 0), vec![]).unwrap());\n        }\n\n        // Validate column indices\n        for \u0026col in columns {\n            assert!(\n                col \u003c ncols,\n                \"Column index {} out of bounds (max {})\",\n                col,\n                ncols - 1\n            );\n        }\n\n        // Use ndarray's select method\n        let selected = t.0.select(ndarray::Axis(1), columns);\n        NdarrayTensor2D(selected)\n    }\n\n    fn one_hot_from_indices(indices: \u0026Self::Tensor1D, num_classes: usize) -\u003e Self::Tensor2D {\n        let n_samples = indices.len();\n        if n_samples == 0 || num_classes == 0 {\n            return NdarrayTensor2D(\n                Array2::from_shape_vec((n_samples, num_classes), vec![]).unwrap(),\n            );\n        }\n\n        // Validate indices\n        for (i, \u0026idx) in indices.iter().enumerate() {\n            assert!(\n                idx \u003e= 0.0 \u0026\u0026 idx \u003c num_classes as f64 \u0026\u0026 idx.fract() == 0.0,\n                \"Index {} at position {} is not a valid integer in range [0, {})\",\n                idx,\n                i,\n                num_classes\n            );\n        }\n\n        let mut result = Array2::zeros((n_samples, num_classes));\n        for (i, \u0026idx) in indices.iter().enumerate() {\n            let col = idx as usize;\n            result[[i, col]] = 1.0;\n        }\n\n        NdarrayTensor2D(result)\n    }\n}\n\n#[cfg(test)]\n#[cfg(feature = \"ndarray\")]\nmod tests {\n    use super::*;\n    use ndarray::{Array1, Array2};\n\n    // Helper to create 2D tensor from nested vec\n    fn tensor2d_from(data: \u0026[Vec\u003cf64\u003e]) -\u003e NdarrayTensor2D {\n        NdarrayTensor2D::from(data)\n    }\n\n    #[test]\n    fn test_zeros_1d() {\n        let t = NdarrayBackend::zeros_1d(3);\n        assert_eq!(t.to_vec(), vec![0.0, 0.0, 0.0]);\n    }\n\n    #[test]\n    fn test_zeros_2d() {\n        let t = NdarrayBackend::zeros_2d(2, 3);\n        assert_eq!(NdarrayBackend::shape(\u0026t), (2, 3));\n        assert_eq!(t.0.iter().sum::\u003cf64\u003e(), 0.0);\n    }\n\n    #[test]\n    fn test_from_vec_1d() {\n        let t = NdarrayBackend::from_vec_1d(vec![1.0, 2.0, 3.0]);\n        assert_eq!(t.to_vec(), vec![1.0, 2.0, 3.0]);\n    }\n\n    #[test]\n    fn test_from_vec_2d() {\n        let t = NdarrayBackend::from_vec_2d(vec![1.0, 2.0, 3.0, 4.0], 2, 2);\n        assert_eq!(NdarrayBackend::shape(\u0026t), (2, 2));\n        assert_eq!(\n            t.0,\n            Array2::from_shape_vec((2, 2), vec![1.0, 2.0, 3.0, 4.0]).unwrap()\n        );\n    }\n\n    #[test]\n    fn test_from_nested_vec() {\n        let data = \u0026[vec![1.0, 2.0], vec![3.0, 4.0]];\n        let t = tensor2d_from(data);\n        assert_eq!(\n            t.0,\n            Array2::from_shape_vec((2, 2), vec![1.0, 2.0, 3.0, 4.0]).unwrap()\n        );\n    }\n\n    #[test]\n    fn test_add_1d() {\n        let a = Array1::from_vec(vec![1.0, 2.0]);\n        let b = Array1::from_vec(vec![3.0, 4.0]);\n        let c = NdarrayBackend::add_1d(\u0026a, \u0026b);\n        assert_eq!(c.to_vec(), vec![4.0, 6.0]);\n    }\n\n    #[test]\n    fn test_add_2d() {\n        let a = tensor2d_from(\u0026[vec![1.0, 2.0]]);\n        let b = tensor2d_from(\u0026[vec![3.0, 4.0]]);\n        let c = NdarrayBackend::add_2d(\u0026a, \u0026b);\n        assert_eq!(c.0, Array2::from_shape_vec((1, 2), vec![4.0, 6.0]).unwrap());\n    }\n\n    #[test]\n    fn test_sub_1d() {\n        let a = Array1::from_vec(vec![5.0, 6.0]);\n        let b = Array1::from_vec(vec![2.0, 1.0]);\n        let c = NdarrayBackend::sub_1d(\u0026a, \u0026b);\n        assert_eq!(c.to_vec(), vec![3.0, 5.0]);\n    }\n\n    #[test]\n    fn test_sub_2d() {\n        let a = tensor2d_from(\u0026[vec![5.0, 6.0]]);\n        let b = tensor2d_from(\u0026[vec![2.0, 1.0]]);\n        let c = NdarrayBackend::sub_2d(\u0026a, \u0026b);\n        assert_eq!(c.0, Array2::from_shape_vec((1, 2), vec![3.0, 5.0]).unwrap());\n    }\n\n    #[test]\n    fn test_mul_1d() {\n        let a = Array1::from_vec(vec![2.0, 3.0]);\n        let b = Array1::from_vec(vec![4.0, 5.0]);\n        let c = NdarrayBackend::mul_1d(\u0026a, \u0026b);\n        assert_eq!(c.to_vec(), vec![8.0, 15.0]);\n    }\n\n    #[test]\n    fn test_mul_2d() {\n        let a = tensor2d_from(\u0026[vec![2.0, 3.0]]);\n        let b = tensor2d_from(\u0026[vec![4.0, 5.0]]);\n        let c = NdarrayBackend::mul_2d(\u0026a, \u0026b);\n        assert_eq!(\n            c.0,\n            Array2::from_shape_vec((1, 2), vec![8.0, 15.0]).unwrap()\n        );\n    }\n\n    #[test]\n    fn test_div_1d() {\n        let a = Array1::from_vec(vec![8.0, 15.0]);\n        let b = Array1::from_vec(vec![2.0, 3.0]);\n        let c = NdarrayBackend::div_1d(\u0026a, \u0026b);\n        assert_eq!(c.to_vec(), vec![4.0, 5.0]);\n    }\n\n    #[test]\n    fn test_div_2d() {\n        let a = tensor2d_from(\u0026[vec![8.0, 15.0]]);\n        let b = tensor2d_from(\u0026[vec![2.0, 3.0]]);\n        let c = NdarrayBackend::div_2d(\u0026a, \u0026b);\n        assert_eq!(c.0, Array2::from_shape_vec((1, 2), vec![4.0, 5.0]).unwrap());\n    }\n\n    #[test]\n    fn test_add_scalar_1d() {\n        let t = Array1::from_vec(vec![1.0, 2.0]);\n        let s = 10.0;\n        let out = NdarrayBackend::add_scalar_1d(\u0026t, \u0026s);\n        assert_eq!(out.to_vec(), vec![11.0, 12.0]);\n    }\n\n    #[test]\n    fn test_add_scalar_2d() {\n        let t = tensor2d_from(\u0026[vec![1.0, 2.0]]);\n        let s = 10.0;\n        let out = NdarrayBackend::add_scalar_2d(\u0026t, \u0026s);\n        assert_eq!(\n            out.0,\n            Array2::from_shape_vec((1, 2), vec![11.0, 12.0]).unwrap()\n        );\n    }\n\n    #[test]\n    fn test_mul_scalar_1d() {\n        let t = Array1::from_vec(vec![2.0, 3.0]);\n        let s = 5.0;\n        let out = NdarrayBackend::mul_scalar_1d(\u0026t, \u0026s);\n        assert_eq!(out.to_vec(), vec![10.0, 15.0]);\n    }\n\n    #[test]\n    fn test_mul_scalar_2d() {\n        let t = tensor2d_from(\u0026[vec![2.0, 3.0]]);\n        let s = 5.0;\n        let out = NdarrayBackend::mul_scalar_2d(\u0026t, \u0026s);\n        assert_eq!(\n            out.0,\n            Array2::from_shape_vec((1, 2), vec![10.0, 15.0]).unwrap()\n        );\n    }\n\n    #[test]\n    fn test_matvec() {\n        let a = tensor2d_from(\u0026[vec![1.0, 2.0], vec![3.0, 4.0]]);\n        let x = Array1::from_vec(vec![1.0, 2.0]);\n        let y = NdarrayBackend::matvec(\u0026a, \u0026x);\n        assert_eq!(y.to_vec(), vec![5.0, 11.0]); // 1*1+2*2=5, 3*1+4*2=11\n    }\n\n    #[test]\n    fn test_matvec_transposed() {\n        let a = tensor2d_from(\u0026[vec![1.0, 2.0], vec![3.0, 4.0]]); // 2x2\n        let x = Array1::from_vec(vec![1.0, 0.0]); // shape (2,)\n        let y = NdarrayBackend::matvec_transposed(\u0026a, \u0026x); // A^T @ x â (2,)\n                                                           // A^T = [[1,3],[2,4]], so [1*1 + 3*0, 2*1 + 4*0] = [1, 2]\n        assert_eq!(y.to_vec(), vec![1.0, 2.0]);\n    }\n\n    #[test]\n    fn test_transpose() {\n        let a = tensor2d_from(\u0026[vec![1.0, 2.0], vec![3.0, 4.0]]);\n        let at = NdarrayBackend::transpose(\u0026a);\n        assert_eq!(NdarrayBackend::shape(\u0026at), (2, 2));\n        assert_eq!(\n            at.0,\n            Array2::from_shape_vec((2, 2), vec![1.0, 3.0, 2.0, 4.0]).unwrap()\n        );\n    }\n\n    #[test]\n    fn test_shape() {\n        let a = tensor2d_from(\u0026[vec![1.0, 2.0, 3.0]]);\n        assert_eq!(NdarrayBackend::shape(\u0026a), (1, 3));\n    }\n\n    #[test]\n    fn test_len_1d() {\n        let t = Array1::from_vec(vec![1.0, 2.0, 3.0]);\n        assert_eq!(NdarrayBackend::len_1d(\u0026t), 3);\n    }\n\n    #[test]\n    fn test_len_2d() {\n        let t = tensor2d_from(\u0026[vec![1.0, 2.0], vec![3.0, 4.0]]);\n        assert_eq!(NdarrayBackend::len_2d(\u0026t), 2); // nrows\n    }\n\n    #[test]\n    fn test_to_vec_1d() {\n        let t = Array1::from_vec(vec![1.0, 2.0, 3.0]);\n        assert_eq!(NdarrayBackend::to_vec_1d(\u0026t), vec![1.0, 2.0, 3.0]);\n    }\n\n    #[test]\n    fn test_exp_1d() {\n        let t = Array1::from_vec(vec![0.0, 1.0]);\n        let out = NdarrayBackend::exp_1d(\u0026t);\n        assert!((out[0] - 1.0).abs() \u003c 1e-6);\n        assert!((out[1] - std::f64::consts::E).abs() \u003c 1e-6);\n    }\n\n    #[test]\n    fn test_log_1d() {\n        let t = Array1::from_vec(vec![1.0, std::f64::consts::E]);\n        let out = NdarrayBackend::log_1d(\u0026t);\n        assert!((out[0] - 0.0).abs() \u003c 1e-6);\n        assert!((out[1] - 1.0).abs() \u003c 1e-6);\n    }\n\n    #[test]\n    fn test_sigmoid_stability() {\n        let input = Array1::from_vec(vec![-100.0, 0.0, 100.0]);\n        let out = NdarrayBackend::sigmoid_1d(\u0026input);\n        let expected = vec![0.0, 0.5, 1.0];\n        for (o, e) in out.iter().zip(expected) {\n            assert!((o - e).abs() \u003c 1e-6);\n        }\n    }\n\n    #[test]\n    fn test_abs_1d() {\n        let t = Array1::from_vec(vec![-2.0, 3.0]);\n        let out = NdarrayBackend::abs_1d(\u0026t);\n        assert_eq!(out.to_vec(), vec![2.0, 3.0]);\n    }\n\n    #[test]\n    fn test_sign_1d() {\n        let t = Array1::from_vec(vec![-2.0, 0.0, 3.0]);\n        let out = NdarrayBackend::sign_1d(\u0026t);\n        assert_eq!(out.to_vec(), vec![-1.0, 0.0, 1.0]);\n    }\n\n    #[test]\n    fn test_maximum_1d() {\n        let a = Array1::from_vec(vec![1.0, 5.0, 3.0]);\n        let b = Array1::from_vec(vec![2.0, 4.0, 6.0]);\n        let out = NdarrayBackend::maximum_1d(\u0026a, \u0026b);\n        assert_eq!(out.to_vec(), vec![2.0, 5.0, 6.0]);\n    }\n\n    #[test]\n    fn test_exp_2d() {\n        let t = tensor2d_from(\u0026[vec![0.0, 1.0]]);\n        let out = NdarrayBackend::exp_2d(\u0026t);\n        assert!((out.0[[0, 0]] - 1.0).abs() \u003c 1e-6);\n        assert!((out.0[[0, 1]] - std::f64::consts::E).abs() \u003c 1e-6);\n    }\n\n    #[test]\n    fn test_log_2d() {\n        let t = tensor2d_from(\u0026[vec![1.0, std::f64::consts::E]]);\n        let out = NdarrayBackend::log_2d(\u0026t);\n        assert!((out.0[[0, 0]] - 0.0).abs() \u003c 1e-6);\n        assert!((out.0[[0, 1]] - 1.0).abs() \u003c 1e-6);\n    }\n\n    #[test]\n    fn test_sigmoid_2d() {\n        let t = tensor2d_from(\u0026[vec![-100.0, 0.0, 100.0]]);\n        let out = NdarrayBackend::sigmoid_2d(\u0026t);\n        let expected = vec![0.0, 0.5, 1.0];\n        for (i, \u0026e) in expected.iter().enumerate() {\n            assert!((out.0[[0, i]] - e).abs() \u003c 1e-6);\n        }\n    }\n\n    #[test]\n    fn test_abs_2d() {\n        let t = tensor2d_from(\u0026[vec![-2.0, 3.0]]);\n        let out = NdarrayBackend::abs_2d(\u0026t);\n        assert_eq!(\n            out.0,\n            Array2::from_shape_vec((1, 2), vec![2.0, 3.0]).unwrap()\n        );\n    }\n\n    #[test]\n    fn test_sign_2d() {\n        let t = tensor2d_from(\u0026[vec![-2.0, 0.0, 3.0]]);\n        let out = NdarrayBackend::sign_2d(\u0026t);\n        assert_eq!(\n            out.0,\n            Array2::from_shape_vec((1, 3), vec![-1.0, 0.0, 1.0]).unwrap()\n        );\n    }\n\n    #[test]\n    fn test_maximum_2d() {\n        let a = tensor2d_from(\u0026[vec![1.0, 5.0, 3.0]]);\n        let b = tensor2d_from(\u0026[vec![2.0, 4.0, 6.0]]);\n        let out = NdarrayBackend::maximum_2d(\u0026a, \u0026b);\n        assert_eq!(\n            out.0,\n            Array2::from_shape_vec((1, 3), vec![2.0, 5.0, 6.0]).unwrap()\n        );\n    }\n\n    #[test]\n    fn test_sum_all_1d() {\n        let t = Array1::from_vec(vec![1.0, 2.0, 3.0]);\n        assert_eq!(NdarrayBackend::sum_all_1d(\u0026t), 6.0);\n    }\n\n    #[test]\n    fn test_mean_all_1d() {\n        let t = Array1::from_vec(vec![1.0, 2.0, 3.0]);\n        assert!((NdarrayBackend::mean_all_1d(\u0026t) - 2.0).abs() \u003c 1e-6);\n    }\n\n    #[test]\n    fn test_sum_all_2d() {\n        let t = tensor2d_from(\u0026[vec![1.0, 2.0], vec![3.0, 4.0]]);\n        assert_eq!(NdarrayBackend::sum_all_2d(\u0026t), 10.0);\n    }\n\n    #[test]\n    fn test_mean_all_2d() {\n        let t = tensor2d_from(\u0026[vec![1.0, 2.0], vec![3.0, 4.0]]);\n        assert!((NdarrayBackend::mean_all_2d(\u0026t) - 2.5).abs() \u003c 1e-6);\n    }\n\n    #[test]\n    fn test_scalar_f64() {\n        assert_eq!(NdarrayBackend::scalar_f64(42.0), 42.0);\n    }\n\n    #[test]\n    fn test_empty_tensor_2d() {\n        let empty: \u0026[Vec\u003cf64\u003e] = \u0026[];\n        let t = tensor2d_from(empty);\n        assert_eq!(NdarrayBackend::shape(\u0026t), (0, 0));\n    }\n\n    #[test]\n    #[should_panic(expected = \"Shapes must match\")]\n    fn test_maximum_1d_mismatch() {\n        let a = Array1::from_vec(vec![1.0]);\n        let b = Array1::from_vec(vec![1.0, 2.0]);\n        NdarrayBackend::maximum_1d(\u0026a, \u0026b);\n    }\n\n    #[test]\n    #[should_panic(expected = \"Shapes must match\")]\n    fn test_maximum_2d_mismatch() {\n        let a = tensor2d_from(\u0026[vec![1.0]]);\n        let b = tensor2d_from(\u0026[vec![1.0, 2.0]]);\n        NdarrayBackend::maximum_2d(\u0026a, \u0026b);\n    }\n}\n","traces":[{"line":56,"address":[3077328],"length":1,"stats":{"Line":3}},{"line":57,"address":[3077371],"length":1,"stats":{"Line":3}},{"line":58,"address":[3077379],"length":1,"stats":{"Line":3}},{"line":59,"address":[3077385],"length":1,"stats":{"Line":1}},{"line":61,"address":[3077499,3077472,3077575],"length":1,"stats":{"Line":4}},{"line":62,"address":[3077522,3077592],"length":1,"stats":{"Line":12}},{"line":63,"address":[3077627],"length":1,"stats":{"Line":14}},{"line":64,"address":[3077695],"length":1,"stats":{"Line":2}},{"line":90,"address":[3077104],"length":1,"stats":{"Line":1}},{"line":91,"address":[3077121],"length":1,"stats":{"Line":1}},{"line":106,"address":[3077136],"length":1,"stats":{"Line":1}},{"line":107,"address":[3077160],"length":1,"stats":{"Line":1}},{"line":121,"address":[3070400],"length":1,"stats":{"Line":1}},{"line":122,"address":[3070414],"length":1,"stats":{"Line":3}},{"line":142,"address":[3070464],"length":1,"stats":{"Line":1}},{"line":143,"address":[3079744,3079755],"length":1,"stats":{"Line":3}},{"line":144,"address":[3070555],"length":1,"stats":{"Line":1}},{"line":161,"address":[3073936],"length":1,"stats":{"Line":1}},{"line":162,"address":[3073957],"length":1,"stats":{"Line":1}},{"line":179,"address":[3070864],"length":1,"stats":{"Line":1}},{"line":180,"address":[3070888],"length":1,"stats":{"Line":1}},{"line":208,"address":[3074416],"length":1,"stats":{"Line":1}},{"line":209,"address":[3074437],"length":1,"stats":{"Line":1}},{"line":234,"address":[3071008],"length":1,"stats":{"Line":1}},{"line":235,"address":[3071037],"length":1,"stats":{"Line":1}},{"line":253,"address":[3077232],"length":1,"stats":{"Line":1}},{"line":254,"address":[3077257],"length":1,"stats":{"Line":1}},{"line":272,"address":[3073696],"length":1,"stats":{"Line":1}},{"line":273,"address":[3073705],"length":1,"stats":{"Line":1}},{"line":274,"address":[3073730,3073813],"length":1,"stats":{"Line":1}},{"line":290,"address":[3074192],"length":1,"stats":{"Line":1}},{"line":291,"address":[3074209],"length":1,"stats":{"Line":1}},{"line":308,"address":[3074320],"length":1,"stats":{"Line":1}},{"line":309,"address":[3074337],"length":1,"stats":{"Line":1}},{"line":327,"address":[3070192],"length":1,"stats":{"Line":1}},{"line":328,"address":[3070209],"length":1,"stats":{"Line":2}},{"line":329,"address":[3079428,3079475],"length":1,"stats":{"Line":2}},{"line":330,"address":[3079482],"length":1,"stats":{"Line":1}},{"line":332,"address":[3079442],"length":1,"stats":{"Line":1}},{"line":333,"address":[3079453],"length":1,"stats":{"Line":1}},{"line":348,"address":[3073840],"length":1,"stats":{"Line":1}},{"line":349,"address":[3073857],"length":1,"stats":{"Line":1}},{"line":364,"address":[3076816],"length":1,"stats":{"Line":1}},{"line":365,"address":[3076833],"length":1,"stats":{"Line":2}},{"line":366,"address":[3079961,3079921],"length":1,"stats":{"Line":2}},{"line":367,"address":[3079947],"length":1,"stats":{"Line":1}},{"line":368,"address":[3079972,3079936],"length":1,"stats":{"Line":2}},{"line":369,"address":[3079974],"length":1,"stats":{"Line":1}},{"line":371,"address":[3079963],"length":1,"stats":{"Line":1}},{"line":390,"address":[3069360],"length":1,"stats":{"Line":1}},{"line":391,"address":[3069409],"length":1,"stats":{"Line":2}},{"line":392,"address":[3069552],"length":1,"stats":{"Line":3}},{"line":398,"address":[3074224],"length":1,"stats":{"Line":1}},{"line":399,"address":[3074242],"length":1,"stats":{"Line":1}},{"line":406,"address":[3074352],"length":1,"stats":{"Line":1}},{"line":407,"address":[3074370],"length":1,"stats":{"Line":1}},{"line":413,"address":[3070224],"length":1,"stats":{"Line":1}},{"line":414,"address":[3070242],"length":1,"stats":{"Line":2}},{"line":415,"address":[3079619,3079572],"length":1,"stats":{"Line":2}},{"line":416,"address":[3079626],"length":1,"stats":{"Line":1}},{"line":418,"address":[3079586],"length":1,"stats":{"Line":1}},{"line":419,"address":[3079597],"length":1,"stats":{"Line":1}},{"line":425,"address":[3073872],"length":1,"stats":{"Line":1}},{"line":426,"address":[3073890],"length":1,"stats":{"Line":1}},{"line":430,"address":[3076848],"length":1,"stats":{"Line":1}},{"line":431,"address":[3076866],"length":1,"stats":{"Line":2}},{"line":432,"address":[3080017,3080057],"length":1,"stats":{"Line":2}},{"line":433,"address":[3080043],"length":1,"stats":{"Line":1}},{"line":434,"address":[3080032,3080068],"length":1,"stats":{"Line":2}},{"line":435,"address":[3080070],"length":1,"stats":{"Line":1}},{"line":437,"address":[3080059],"length":1,"stats":{"Line":1}},{"line":446,"address":[3069664],"length":1,"stats":{"Line":1}},{"line":447,"address":[3069715],"length":1,"stats":{"Line":1}},{"line":448,"address":[3069751],"length":1,"stats":{"Line":1}},{"line":449,"address":[3069917],"length":1,"stats":{"Line":1}},{"line":451,"address":[3069935],"length":1,"stats":{"Line":1}},{"line":452,"address":[3069977],"length":1,"stats":{"Line":3}},{"line":454,"address":[3070029],"length":1,"stats":{"Line":1}},{"line":460,"address":[3070288],"length":1,"stats":{"Line":1}},{"line":461,"address":[3070293],"length":1,"stats":{"Line":1}},{"line":468,"address":[3070640],"length":1,"stats":{"Line":1}},{"line":469,"address":[3070645],"length":1,"stats":{"Line":1}},{"line":473,"address":[3070304],"length":1,"stats":{"Line":1}},{"line":474,"address":[3070309],"length":1,"stats":{"Line":1}},{"line":481,"address":[3070672],"length":1,"stats":{"Line":1}},{"line":482,"address":[3070677],"length":1,"stats":{"Line":1}},{"line":490,"address":[3070176],"length":1,"stats":{"Line":1}},{"line":495,"address":[3074288],"length":1,"stats":{"Line":1}},{"line":496,"address":[3074293],"length":1,"stats":{"Line":1}},{"line":502,"address":[3074304],"length":1,"stats":{"Line":1}},{"line":503,"address":[3074309],"length":1,"stats":{"Line":1}},{"line":507,"address":[3077200],"length":1,"stats":{"Line":1}},{"line":508,"address":[3077217],"length":1,"stats":{"Line":1}},{"line":514,"address":[3074592],"length":1,"stats":{"Line":1}},{"line":515,"address":[3074613],"length":1,"stats":{"Line":1}},{"line":519,"address":[3074464],"length":1,"stats":{"Line":1}},{"line":520,"address":[3074485],"length":1,"stats":{"Line":1}},{"line":524,"address":[3074064],"length":1,"stats":{"Line":1}},{"line":525,"address":[3074085],"length":1,"stats":{"Line":1}},{"line":529,"address":[3070816],"length":1,"stats":{"Line":1}},{"line":530,"address":[3070837],"length":1,"stats":{"Line":3}},{"line":534,"address":[3070704],"length":1,"stats":{"Line":1}},{"line":535,"address":[3070725],"length":1,"stats":{"Line":3}},{"line":541,"address":[3070752],"length":1,"stats":{"Line":1}},{"line":542,"address":[3070776],"length":1,"stats":{"Line":3}},{"line":546,"address":[3073984],"length":1,"stats":{"Line":1}},{"line":547,"address":[3074008],"length":1,"stats":{"Line":1}},{"line":551,"address":[3074640],"length":1,"stats":{"Line":1}},{"line":552,"address":[3074664],"length":1,"stats":{"Line":1}},{"line":556,"address":[3074512],"length":1,"stats":{"Line":1}},{"line":557,"address":[3074536],"length":1,"stats":{"Line":1}},{"line":561,"address":[3074112],"length":1,"stats":{"Line":1}},{"line":562,"address":[3074136],"length":1,"stats":{"Line":1}},{"line":571,"address":[3070960],"length":1,"stats":{"Line":0}},{"line":572,"address":[3070981],"length":1,"stats":{"Line":0}},{"line":578,"address":[3073648],"length":1,"stats":{"Line":0}},{"line":579,"address":[3073669],"length":1,"stats":{"Line":0}},{"line":583,"address":[3077008],"length":1,"stats":{"Line":0}},{"line":584,"address":[3077032],"length":1,"stats":{"Line":0}},{"line":591,"address":[3070320],"length":1,"stats":{"Line":0}},{"line":592,"address":[3070338],"length":1,"stats":{"Line":0}},{"line":593,"address":[3070363],"length":1,"stats":{"Line":0}},{"line":596,"address":[3068304,3069287,3069281],"length":1,"stats":{"Line":0}},{"line":597,"address":[3068370],"length":1,"stats":{"Line":0}},{"line":598,"address":[3068391],"length":1,"stats":{"Line":0}},{"line":599,"address":[3068405],"length":1,"stats":{"Line":0}},{"line":602,"address":[3068432],"length":1,"stats":{"Line":0}},{"line":603,"address":[3068445,3068532],"length":1,"stats":{"Line":0}},{"line":605,"address":[3068548],"length":1,"stats":{"Line":0}},{"line":606,"address":[3068563,3069178,3068635],"length":1,"stats":{"Line":0}},{"line":607,"address":[3068749],"length":1,"stats":{"Line":0}},{"line":608,"address":[3068761,3068840,3069276],"length":1,"stats":{"Line":0}},{"line":609,"address":[3069193,3068954],"length":1,"stats":{"Line":0}},{"line":610,"address":[3069254],"length":1,"stats":{"Line":0}},{"line":612,"address":[3069111,3069016],"length":1,"stats":{"Line":0}},{"line":613,"address":[3069136,3069085],"length":1,"stats":{"Line":0}},{"line":615,"address":[3068794],"length":1,"stats":{"Line":0}},{"line":618,"address":[3067584,3068276,3068282],"length":1,"stats":{"Line":0}},{"line":619,"address":[3067625],"length":1,"stats":{"Line":0}},{"line":620,"address":[3067646],"length":1,"stats":{"Line":0}},{"line":621,"address":[3067657],"length":1,"stats":{"Line":0}},{"line":624,"address":[3067692],"length":1,"stats":{"Line":0}},{"line":625,"address":[3067705,3067796],"length":1,"stats":{"Line":0}},{"line":626,"address":[3067952,3067907],"length":1,"stats":{"Line":0}},{"line":627,"address":[3068102],"length":1,"stats":{"Line":0}},{"line":628,"address":[3068271,3068174],"length":1,"stats":{"Line":0}},{"line":629,"address":[3068231],"length":1,"stats":{"Line":0}},{"line":633,"address":[3067924],"length":1,"stats":{"Line":0}},{"line":636,"address":[3067558,3067552,3066864],"length":1,"stats":{"Line":0}},{"line":637,"address":[3066905],"length":1,"stats":{"Line":0}},{"line":638,"address":[3066926],"length":1,"stats":{"Line":0}},{"line":639,"address":[3066937],"length":1,"stats":{"Line":0}},{"line":642,"address":[3066972],"length":1,"stats":{"Line":0}},{"line":643,"address":[3067076,3066985],"length":1,"stats":{"Line":0}},{"line":644,"address":[3067232,3067187],"length":1,"stats":{"Line":0}},{"line":645,"address":[3067382],"length":1,"stats":{"Line":0}},{"line":646,"address":[3067547,3067454],"length":1,"stats":{"Line":0}},{"line":647,"address":[3067507],"length":1,"stats":{"Line":0}},{"line":651,"address":[3067204],"length":1,"stats":{"Line":0}},{"line":654,"address":[3069312],"length":1,"stats":{"Line":0}},{"line":655,"address":[3069329],"length":1,"stats":{"Line":0}},{"line":660,"address":[3070128],"length":1,"stats":{"Line":0}},{"line":661,"address":[3070145],"length":1,"stats":{"Line":0}},{"line":666,"address":[3073488],"length":1,"stats":{"Line":0}},{"line":667,"address":[3073539],"length":1,"stats":{"Line":0}},{"line":670,"address":[3073168],"length":1,"stats":{"Line":0}},{"line":671,"address":[3073219],"length":1,"stats":{"Line":0}},{"line":674,"address":[3073328],"length":1,"stats":{"Line":0}},{"line":675,"address":[3073379],"length":1,"stats":{"Line":0}},{"line":678,"address":[3073008],"length":1,"stats":{"Line":0}},{"line":679,"address":[3073059],"length":1,"stats":{"Line":0}},{"line":682,"address":[3076912],"length":1,"stats":{"Line":0}},{"line":683,"address":[3076929],"length":1,"stats":{"Line":0}},{"line":686,"address":[3076944],"length":1,"stats":{"Line":0}},{"line":687,"address":[3076962],"length":1,"stats":{"Line":0}},{"line":692,"address":[3074720,3076318,3076312],"length":1,"stats":{"Line":0}},{"line":695,"address":[3074797],"length":1,"stats":{"Line":0}},{"line":696,"address":[3074864],"length":1,"stats":{"Line":0}},{"line":697,"address":[3074830],"length":1,"stats":{"Line":0}},{"line":701,"address":[3074814,3075017,3074980],"length":1,"stats":{"Line":0}},{"line":702,"address":[3074993],"length":1,"stats":{"Line":0}},{"line":703,"address":[3075103],"length":1,"stats":{"Line":0}},{"line":704,"address":[3075034],"length":1,"stats":{"Line":0}},{"line":709,"address":[3075223,3075173],"length":1,"stats":{"Line":0}},{"line":710,"address":[3075299],"length":1,"stats":{"Line":0}},{"line":711,"address":[3076664],"length":1,"stats":{"Line":0}},{"line":712,"address":[3076331],"length":1,"stats":{"Line":0}},{"line":713,"address":[3076521,3076460],"length":1,"stats":{"Line":0}},{"line":719,"address":[3075336],"length":1,"stats":{"Line":0}},{"line":722,"address":[3075374],"length":1,"stats":{"Line":0}},{"line":723,"address":[3075411],"length":1,"stats":{"Line":0}},{"line":724,"address":[3075423,3075502,3075965],"length":1,"stats":{"Line":0}},{"line":725,"address":[3075612,3075718],"length":1,"stats":{"Line":0}},{"line":726,"address":[3075726],"length":1,"stats":{"Line":0}},{"line":727,"address":[3076307,3076000,3075895],"length":1,"stats":{"Line":0}},{"line":728,"address":[3076123],"length":1,"stats":{"Line":0}},{"line":731,"address":[3075928,3075970],"length":1,"stats":{"Line":0}},{"line":734,"address":[3075627],"length":1,"stats":{"Line":0}},{"line":737,"address":[3071088],"length":1,"stats":{"Line":0}},{"line":738,"address":[3071152],"length":1,"stats":{"Line":0}},{"line":739,"address":[3071193],"length":1,"stats":{"Line":0}},{"line":740,"address":[3071236],"length":1,"stats":{"Line":0}},{"line":744,"address":[3071212,3071329],"length":1,"stats":{"Line":0}},{"line":745,"address":[3071406],"length":1,"stats":{"Line":0}},{"line":754,"address":[3071433],"length":1,"stats":{"Line":0}},{"line":755,"address":[3071463],"length":1,"stats":{"Line":0}},{"line":758,"address":[3072641,3072635,3071744],"length":1,"stats":{"Line":0}},{"line":759,"address":[3071787],"length":1,"stats":{"Line":0}},{"line":760,"address":[3071912,3071805],"length":1,"stats":{"Line":0}},{"line":761,"address":[3071895],"length":1,"stats":{"Line":0}},{"line":762,"address":[3071811],"length":1,"stats":{"Line":0}},{"line":767,"address":[3071925,3072021],"length":1,"stats":{"Line":0}},{"line":768,"address":[3072118,3072910],"length":1,"stats":{"Line":0}},{"line":777,"address":[3072150],"length":1,"stats":{"Line":0}},{"line":778,"address":[3072244,3072181,3072630],"length":1,"stats":{"Line":0}},{"line":779,"address":[3072431],"length":1,"stats":{"Line":0}},{"line":780,"address":[3072508,3072618],"length":1,"stats":{"Line":0}},{"line":783,"address":[3072559],"length":1,"stats":{"Line":0}}],"covered":113,"coverable":218},{"path":["/","home","victor","ML","machinelearne-rs","lib","src","backend","scalar.rs"],"content":"use crate::backend::Backend;\nuse std::marker::PhantomData;\n\n/// Trait for scalar operations required by numerical backends.\n///\n/// Defines a minimal set of arithmetic and mathematical operations needed for\n/// machine learning computations. Implemented for primitive floating-point types\n/// used by backends (e.g., `f64`).\n///\n/// # Design rationale\n/// This trait abstracts scalar operations to enable backend-agnostic generic code\n/// while maintaining performance through `Copy` semantics and avoiding dynamic dispatch.\n///\n/// # Required operations\n/// - Basic arithmetic via standard library traits (`Add`, `Sub`, `Mul`, `Div`)\n/// - Mathematical functions: `sqrt`, `abs`, `exp`\n/// - Type conversion: `from_f64`/`to_f64` for interoperability\n/// - Constants: `zero()`, `one()` for initialization\n///\n/// # Safety guarantees\n/// Implementations must satisfy:\n/// - `Copy` + `Clone` for zero-cost abstractions\n/// - `Send` + `Sync` for thread-safe parallel computations\n/// - Numerical stability for edge cases (e.g., `sqrt` of negative numbers should panic)\n///\n/// # Example\n/// ```\n/// use machinelearne_rs::backend::ScalarOps;\n///\n/// let x = 4.0f64;\n/// assert_eq!(x.sqrt(), 2.0);\n/// assert_eq!(f64::zero(), 0.0);\n/// assert_eq!(f64::one(), 1.0);\n/// ```\npub trait ScalarOps:\n    Clone\n    + Copy\n    + Send\n    + Sync\n    + std::ops::Add\u003cOutput = Self\u003e\n    + std::ops::Mul\u003cOutput = Self\u003e\n    + std::ops::Sub\u003cOutput = Self\u003e\n    + std::ops::Div\u003cOutput = Self\u003e\n{\n    /// Computes the square root of the scalar.\n    ///\n    /// # Panics\n    /// Panics if called on a negative value (for real-number implementations).\n    fn sqrt(self) -\u003e Self;\n\n    /// Returns the absolute value of the scalar.\n    fn abs(self) -\u003e Self;\n\n    /// Returns the additive identity (zero) for this scalar type.\n    fn zero() -\u003e Self;\n\n    /// Returns the multiplicative identity (one) for this scalar type.\n    fn one() -\u003e Self;\n\n    /// Converts an `f64` value to this scalar type.\n    ///\n    /// Used for backend-agnostic initialization from host values.\n    fn from_f64(v: f64) -\u003e Self;\n\n    /// Converts this scalar to an `f64` value.\n    ///\n    /// Used for interoperability with host code and debugging.\n    fn to_f64(self) -\u003e f64;\n\n    /// Computes the exponential function `e^x`.\n    fn exp(self) -\u003e Self;\n}\n\n// === Implementations for primitive types ===\n\n/// `f64` implementation of `ScalarOps`.\n///\n/// Provides IEEE 754 double-precision floating point operations with\n/// hardware-accelerated math functions.\n///\n/// # Numerical behavior\n/// - Follows standard IEEE 754 semantics (NaN propagation, infinities)\n/// - `sqrt` panics on negative inputs via Rust's built-in `sqrt()` method\n/// - `exp` handles overflow by returning `INFINITY` per IEEE 754\nimpl ScalarOps for f64 {\n    fn sqrt(self) -\u003e Self {\n        self.sqrt()\n    }\n\n    fn abs(self) -\u003e Self {\n        self.abs()\n    }\n\n    fn zero() -\u003e Self {\n        0.0\n    }\n\n    fn one() -\u003e Self {\n        1.0\n    }\n\n    fn from_f64(v: f64) -\u003e Self {\n        v\n    }\n\n    fn to_f64(self) -\u003e f64 {\n        self\n    }\n\n    fn exp(self) -\u003e Self {\n        self.exp()\n    }\n}\n\n/// Backend-typed scalar wrapper providing compile-time type safety.\n///\n/// Wraps a backend's native scalar type (`B::Scalar`) while carrying phantom\n/// type information about its originating backend. This prevents accidental\n/// mixing of scalars from different backends at compile time.\n///\n/// # Type safety guarantees\n/// ```compile_fail\n/// use machinelearne_rs::backend::{CpuBackend, NdarrayBackend};\n/// use machinelearne_rs::backend::Scalar;\n///\n/// let cpu_scalar: Scalar\u003cCpuBackend\u003e = Scalar::new(1.0);\n/// let ndarray_scalar: Scalar\u003cNdarrayBackend\u003e = Scalar::new(2.0);\n/// let _ = cpu_scalar + ndarray_scalar; // COMPILE ERROR: mismatched backends\n/// ```\n///\n/// # Zero-cost abstraction\n/// - `PhantomData\u003cB\u003e` adds no runtime overhead (zero-sized type)\n/// - All operations delegate directly to backend's scalar type\n/// - Implements `Copy` for efficient pass-by-value semantics\n///\n/// # Example\n/// ```\n/// use machinelearne_rs::backend::CpuBackend;\n/// use machinelearne_rs::backend::{Scalar, ScalarOps};\n///\n/// let s: Scalar\u003cCpuBackend\u003e = Scalar::new(2.0);\n/// let squared = s * s; // Backend-safe multiplication\n/// assert_eq!(squared.to_f64(), 4.0);\n/// ```\n#[derive(Clone, Debug, Copy)]\npub struct Scalar\u003cB: Backend\u003e {\n    pub(crate) data: B::Scalar,\n    pub(crate) backend: PhantomData\u003cB\u003e,\n}\n\nimpl\u003cB: Backend\u003e Scalar\u003cB\u003e {\n    /// Creates a new scalar from an `f64` host value.\n    ///\n    /// Converts the host `f64` value to the backend's native scalar representation\n    /// using the backend's `scalar_f64` conversion function.\n    ///\n    /// # Example\n    /// ```\n    /// use machinelearne_rs::backend::CpuBackend;\n    /// use machinelearne_rs::backend::Scalar;\n    ///\n    /// let s: Scalar\u003cCpuBackend\u003e = Scalar::new(3.14);\n    /// assert_eq!(s.to_f64(), 3.14);\n    /// ```\n    pub fn new(f: f64) -\u003e Self {\n        Self {\n            data: B::scalar_f64(f),\n            backend: PhantomData,\n        }\n    }\n    /// Converts this backend scalar to a host `f64` value.\n    ///\n    /// Extracts the scalar value from the backend's native representation and\n    /// converts it to a standard Rust `f64` for interoperability with host code,\n    /// debugging, logging, or serialization.\n    ///\n    /// # Precision considerations\n    /// For backends using `f64` internally (e.g., `CpuBackend`, `NdarrayBackend`),\n    /// this is a zero-cost identity conversion. For hypothetical future backends\n    /// assert!((host_value - std::f64::consts::E).abs() \u003c 1e-5);\n    pub fn to_f64(\u0026self) -\u003e f64 {\n        self.data.to_f64()\n    }\n\n    /// Computes the exponential function `e^x` for this scalar.\n    ///\n    /// Delegates to the backend's scalar implementation of `exp`.\n    ///\n    /// # Example\n    /// ```\n    /// use machinelearne_rs::backend::CpuBackend;\n    /// use machinelearne_rs::backend::Scalar;\n    ///\n    /// let s: Scalar\u003cCpuBackend\u003e = Scalar::new(1.0);\n    /// let e = s.exp();\n    /// assert!((e.to_f64() - std::f64::consts::E).abs() \u003c 1e-12);\n    /// ```\n    pub fn exp(\u0026self) -\u003e Self {\n        Self {\n            data: self.data.exp(),\n            backend: PhantomData,\n        }\n    }\n}\n\n// === Standard arithmetic trait implementations ===\n\n/// Implements addition for backend-typed scalars.\n///\n/// Enables ergonomic `a + b` syntax while preserving backend type safety.\n/// Only scalars from the *same* backend can be added together.\n///\n/// # Example\n/// ```\n/// use machinelearne_rs::backend::CpuBackend;\n/// use machinelearne_rs::backend::Scalar;\n///\n/// let a: Scalar\u003cCpuBackend\u003e = Scalar::new(2.0);\n/// let b: Scalar\u003cCpuBackend\u003e = Scalar::new(3.0);\n/// let sum = a + b;\n/// assert_eq!(sum.to_f64(), 5.0);\n/// ```\nimpl\u003cB: Backend + Copy\u003e std::ops::Add for Scalar\u003cB\u003e {\n    type Output = Self;\n\n    fn add(self, rhs: Self) -\u003e Self {\n        Self {\n            data: self.data + rhs.data,\n            backend: PhantomData,\n        }\n    }\n}\n\n/// Implements subtraction for backend-typed scalars.\nimpl\u003cB: Backend\u003e std::ops::Sub for Scalar\u003cB\u003e {\n    type Output = Self;\n\n    fn sub(self, rhs: Self) -\u003e Self {\n        Self {\n            data: self.data - rhs.data,\n            backend: PhantomData,\n        }\n    }\n}\n\n/// Implements multiplication for backend-typed scalars.\nimpl\u003cB: Backend\u003e std::ops::Mul for Scalar\u003cB\u003e {\n    type Output = Self;\n\n    fn mul(self, rhs: Self) -\u003e Self {\n        Self {\n            data: self.data * rhs.data,\n            backend: PhantomData,\n        }\n    }\n}\n\n/// Implements division for backend-typed scalars.\n///\n/// # Panics\n/// Panics on division by zero according to backend's scalar implementation.\nimpl\u003cB: Backend\u003e std::ops::Div for Scalar\u003cB\u003e {\n    type Output = Self;\n\n    fn div(self, rhs: Self) -\u003e Self {\n        Self {\n            data: self.data / rhs.data,\n            backend: PhantomData,\n        }\n    }\n}\n\n#[cfg(test)]\nmod tests {\n    use super::*;\n    use crate::backend::CpuBackend;\n\n    #[test]\n    fn test_scalar_ops_f64() {\n        let a = 4.0f64;\n        assert_eq!(a.sqrt(), 2.0);\n        assert_eq!(a.abs(), 4.0);\n        assert_eq!(f64::zero(), 0.0);\n        assert_eq!(f64::one(), 1.0);\n        assert_eq!(f64::from_f64(3.14), 3.14);\n        assert_eq!(3.14f64.to_f64(), 3.14);\n        assert_eq!(1.0f64.exp(), std::f64::consts::E);\n    }\n\n    #[test]\n    fn test_scalar_new_and_exp() {\n        let s: Scalar\u003cCpuBackend\u003e = Scalar::new(1.0);\n        assert_eq!(s.data, 1.0);\n\n        let e = s.exp();\n        assert!((e.data - std::f64::consts::E).abs() \u003c 1e-12);\n    }\n\n    #[test]\n    fn test_scalar_arithmetic() {\n        let a: Scalar\u003cCpuBackend\u003e = Scalar::new(5.0);\n        let b: Scalar\u003cCpuBackend\u003e = Scalar::new(2.0);\n\n        let sum = a + b;\n        assert_eq!(sum.data, 7.0);\n\n        let diff = sum - Scalar::new(3.0);\n        assert_eq!(diff.data, 4.0);\n\n        let prod = diff * Scalar::new(0.5);\n        assert_eq!(prod.data, 2.0);\n\n        let quot = prod / Scalar::new(4.0);\n        assert_eq!(quot.data, 0.5);\n    }\n\n    #[test]\n    fn test_scalar_type_safety() {\n        // Verify that scalars from the same backend compose correctly\n        let x: Scalar\u003cCpuBackend\u003e = Scalar::new(10.0);\n        let y: Scalar\u003cCpuBackend\u003e = Scalar::new(3.0);\n        let _ = x / y; // Must compile successfully\n\n        // Note: Cross-backend operations are prevented at compile time:\n        // let z: Scalar\u003cNdarrayBackend\u003e = Scalar::new(2.0);\n        // let _ = x + z; // \u003c-- Would fail to compile with type mismatch error\n    }\n}\n","traces":[{"line":86,"address":[2825440],"length":1,"stats":{"Line":0}},{"line":87,"address":[2825446],"length":1,"stats":{"Line":0}},{"line":90,"address":[2825392],"length":1,"stats":{"Line":0}},{"line":91,"address":[2825398],"length":1,"stats":{"Line":0}},{"line":102,"address":[2825488],"length":1,"stats":{"Line":1}},{"line":106,"address":[2825472],"length":1,"stats":{"Line":1}},{"line":110,"address":[2825408],"length":1,"stats":{"Line":1}},{"line":111,"address":[2825414],"length":1,"stats":{"Line":1}},{"line":165,"address":[1755872],"length":1,"stats":{"Line":1}},{"line":167,"address":[1755878],"length":1,"stats":{"Line":1}},{"line":181,"address":[1755888],"length":1,"stats":{"Line":3}},{"line":182,"address":[1755893],"length":1,"stats":{"Line":2}},{"line":198,"address":[1755856],"length":1,"stats":{"Line":1}},{"line":200,"address":[1755861],"length":1,"stats":{"Line":1}},{"line":226,"address":[1755952],"length":1,"stats":{"Line":1}},{"line":228,"address":[1755968],"length":1,"stats":{"Line":1}},{"line":238,"address":[1756080],"length":1,"stats":{"Line":1}},{"line":240,"address":[1756096],"length":1,"stats":{"Line":1}},{"line":250,"address":[1756032],"length":1,"stats":{"Line":1}},{"line":252,"address":[1756048],"length":1,"stats":{"Line":1}},{"line":265,"address":[1756000],"length":1,"stats":{"Line":1}},{"line":267,"address":[1756016],"length":1,"stats":{"Line":1}}],"covered":18,"coverable":22},{"path":["/","home","victor","ML","machinelearne-rs","lib","src","backend","tensor1d.rs"],"content":"use super::scalar::Scalar;\nuse crate::backend::Backend;\nuse std::marker::PhantomData;\n\n/// Backend-typed 1D tensor providing compile-time type safety and zero-cost abstractions.\n///\n/// Wraps a backend's native 1D tensor representation (`B::Tensor1D`) while carrying phantom\n/// type information about its originating backend. This prevents accidental mixing of tensors\n/// from different backends at compile time while maintaining performance through zero-sized\n/// `PhantomData` overhead.\n///\n/// # Type safety guarantees\n/// ```compile_fail\n/// use machinelearne_rs::backend::{CpuBackend, NdarrayBackend};\n/// use machinelearne_rs::backend::Tensor1D;\n///\n/// let cpu_tensor: Tensor1D\u003cCpuBackend\u003e = Tensor1D::zeros(3);\n/// let ndarray_tensor: Tensor1D\u003cNdarrayBackend\u003e = Tensor1D::zeros(3);\n/// let _ = cpu_tensor.sub(\u0026ndarray_tensor); // COMPILE ERROR: mismatched backends\n/// ```\n///\n/// # Precision semantics\n/// - Constructors accept `Vec\u003cf32\u003e` for ergonomic data loading from common sources\n/// - Values are immediately converted to backend's native precision (typically `f64`)\n/// - All operations occur in native backend precision\n/// - `to_vec()` returns `Vec\u003cf64\u003e` for host interoperability\n///\n/// # Zero-cost design\n/// - `PhantomData\u003cB\u003e` adds no runtime memory overhead\n/// - All operations delegate directly to backend implementations\n/// - Implements `Clone` (but not `Copy`) due to potential heap allocation in underlying tensors\n///\n/// # Example\n/// ```\n/// use machinelearne_rs::backend::CpuBackend;\n/// use machinelearne_rs::backend::{Tensor1D, Scalar};\n///\n/// // Create tensor from f32 data (converted to f64 internally)\n/// let x: Tensor1D\u003cCpuBackend\u003e = Tensor1D::new(vec![1.0f32, 2.0, 3.0]);\n/// assert_eq!(x.len(), 3);\n///\n/// // Element-wise operations\n/// let y = x.scale(\u0026Scalar::\u003cCpuBackend\u003e::new(2.0));\n/// assert_eq!(y.to_vec(), vec![2.0, 4.0, 6.0]);\n/// ```\n#[derive(Clone)]\npub struct Tensor1D\u003cB: Backend\u003e {\n    pub(crate) data: B::Tensor1D,\n    pub(crate) backend: PhantomData\u003cB\u003e,\n}\n\nimpl\u003cB: Backend\u003e Tensor1D\u003cB\u003e {\n    /// Creates a new 1D tensor from a vector of `f32` values.\n    ///\n    /// Converts the input `Vec\u003cf32\u003e` to the backend's native scalar representation\n    /// (typically `f64`) using the backend's `from_vec_1d` conversion function.\n    ///\n    /// # Precision note\n    /// Input values are converted from `f32` to backend precision (usually `f64`).\n    /// This conversion is lossless for values representable in both types, but extremely\n    /// large `f32` values near `f32::MAX` may lose precision when converted to `f64`.\n    ///\n    /// # Example\n    /// ```\n    /// use machinelearne_rs::backend::CpuBackend;\n    /// use machinelearne_rs::backend::Tensor1D;\n    ///\n    /// let t: Tensor1D\u003cCpuBackend\u003e = Tensor1D::new(vec![1.0f32, 2.5, 3.75]);\n    /// assert_eq!(t.to_vec(), vec![1.0, 2.5, 3.75]);\n    /// ```\n    pub fn new(data: Vec\u003cf32\u003e) -\u003e Self {\n        Self {\n            data: B::from_vec_1d(data),\n            backend: PhantomData,\n        }\n    }\n\n    /// Creates a 1D tensor filled with zeros of specified length.\n    ///\n    /// # Arguments\n    /// * `len` - Number of elements in the resulting tensor\n    ///\n    /// # Example\n    /// ```\n    /// use machinelearne_rs::backend::CpuBackend;\n    /// use machinelearne_rs::backend::Tensor1D;\n    ///\n    /// let zeros: Tensor1D\u003cCpuBackend\u003e = Tensor1D::zeros(4);\n    /// assert_eq!(zeros.to_vec(), vec![0.0, 0.0, 0.0, 0.0]);\n    /// assert_eq!(zeros.len(), 4);\n    /// ```\n    pub fn zeros(len: usize) -\u003e Self {\n        Self {\n            data: B::zeros_1d(len),\n            backend: PhantomData,\n        }\n    }\n\n    /// Computes element-wise subtraction: `self - other`.\n    ///\n    /// # Panics\n    /// Panics if tensors have different lengths (backend-dependent behavior).\n    ///\n    /// # Example\n    /// ```\n    /// use machinelearne_rs::backend::CpuBackend;\n    /// use machinelearne_rs::backend::Tensor1D;\n    ///\n    /// let a = Tensor1D::\u003cCpuBackend\u003e::new(vec![5.0f32, 7.0, 9.0]);\n    /// let b = Tensor1D::\u003cCpuBackend\u003e::new(vec![2.0f32, 3.0, 4.0]);\n    /// let diff = a.sub(\u0026b);\n    /// assert_eq!(diff.to_vec(), vec![3.0, 4.0, 5.0]);\n    /// ```\n    pub fn sub(\u0026self, other: \u0026Self) -\u003e Self {\n        Self {\n            data: B::sub_1d(\u0026self.data, \u0026other.data),\n            backend: PhantomData,\n        }\n    }\n\n    /// Computes the arithmetic mean of all elements in the tensor.\n    ///\n    /// # Returns\n    /// A `Scalar\u003cB\u003e` containing the mean value.\n    ///\n    /// # Panics\n    /// Panics if the tensor is empty (backend-dependent behavior).\n    ///\n    /// # Example\n    /// ```\n    /// use machinelearne_rs::backend::CpuBackend;\n    /// use machinelearne_rs::backend::Tensor1D;\n    ///\n    /// let t = Tensor1D::\u003cCpuBackend\u003e::new(vec![1.0f32, 2.0, 3.0, 4.0]);\n    /// let mean = t.mean();\n    /// assert!((mean.to_f64() - 2.5).abs() \u003c 1e-12);\n    /// ```\n    pub fn mean(\u0026self) -\u003e Scalar\u003cB\u003e {\n        Scalar {\n            data: B::mean_all_1d(\u0026self.data),\n            backend: PhantomData,\n        }\n    }\n\n    /// Converts the tensor to a standard Rust `Vec\u003cf64\u003e` for host interoperability.\n    ///\n    /// # Use cases\n    /// - Debugging and logging\n    /// - Serialization to external formats\n    /// - Interfacing with non-backend-aware code\n    /// - Test assertions\n    ///\n    /// # Example\n    /// ```\n    /// use machinelearne_rs::backend::CpuBackend;\n    /// use machinelearne_rs::backend::Tensor1D;\n    ///\n    /// let t = Tensor1D::\u003cCpuBackend\u003e::new(vec![1.5f32, -2.5, 3.5]);\n    /// let host_vec = t.to_vec();\n    /// assert_eq!(host_vec, vec![1.5, -2.5, 3.5]);\n    /// ```\n    pub fn to_vec(\u0026self) -\u003e Vec\u003cf64\u003e {\n        B::to_vec_1d(\u0026self.data)\n    }\n\n    /// Computes the dot product (inner product) between two tensors.\n    ///\n    /// Equivalent to `sum(self * other)` where `*` denotes element-wise multiplication.\n    ///\n    /// # Formula\n    /// `dot(a, b) = Î£áµ¢ aáµ¢ * báµ¢`\n    ///\n    /// # Panics\n    /// Panics if tensors have different lengths.\n    ///\n    /// # Example\n    /// ```\n    /// use machinelearne_rs::backend::CpuBackend;\n    /// use machinelearne_rs::backend::Tensor1D;\n    ///\n    /// let a = Tensor1D::\u003cCpuBackend\u003e::new(vec![1.0f32, 2.0, 3.0]);\n    /// let b = Tensor1D::\u003cCpuBackend\u003e::new(vec![4.0f32, 5.0, 6.0]);\n    /// let dot = a.dot(\u0026b);\n    /// // 1*4 + 2*5 + 3*6 = 4 + 10 + 18 = 32\n    /// assert_eq!(dot.to_f64(), 32.0);\n    /// ```\n    pub fn dot(\u0026self, other: \u0026Self) -\u003e Scalar\u003cB\u003e {\n        let prod = B::mul_1d(\u0026self.data, \u0026other.data);\n        let sum = B::sum_all_1d(\u0026prod);\n        Scalar {\n            data: sum,\n            backend: PhantomData,\n        }\n    }\n\n    /// Computes element-wise absolute value.\n    ///\n    /// # Example\n    /// ```\n    /// use machinelearne_rs::backend::CpuBackend;\n    /// use machinelearne_rs::backend::Tensor1D;\n    ///\n    /// let t = Tensor1D::\u003cCpuBackend\u003e::new(vec![-1.0f32, 2.0, -3.0]);\n    /// let abs_t = t.abs();\n    /// assert_eq!(abs_t.to_vec(), vec![1.0, 2.0, 3.0]);\n    /// ```\n    pub fn abs(\u0026self) -\u003e Self {\n        Self {\n            data: B::abs_1d(\u0026self.data),\n            backend: PhantomData,\n        }\n    }\n\n    /// Computes element-wise sign function.\n    ///\n    /// Returns:\n    /// - `1.0` for positive values\n    /// - `-1.0` for negative values\n    /// - `0.0` for zero values\n    ///\n    /// # Example\n    /// ```\n    /// use machinelearne_rs::backend::CpuBackend;\n    /// use machinelearne_rs::backend::Tensor1D;\n    ///\n    /// let t = Tensor1D::\u003cCpuBackend\u003e::new(vec![-2.0f32, 0.0, 3.0]);\n    /// let sign_t = t.sign();\n    /// assert_eq!(sign_t.to_vec(), vec![-1.0, 0.0, 1.0]);\n    /// ```\n    pub fn sign(\u0026self) -\u003e Self {\n        Self {\n            data: B::sign_1d(\u0026self.data),\n            backend: PhantomData,\n        }\n    }\n\n    /// Returns the number of elements in the tensor.\n    ///\n    /// # Example\n    /// ```\n    /// use machinelearne_rs::backend::CpuBackend;\n    /// use machinelearne_rs::backend::Tensor1D;\n    ///\n    /// let t = Tensor1D::\u003cCpuBackend\u003e::new(vec![1.0f32, 2.0, 3.0]);\n    /// assert_eq!(t.len(), 3);\n    /// ```\n    pub fn len(\u0026self) -\u003e usize {\n        B::len_1d(\u0026self.data)\n    }\n\n    /// Returns `true` if the tensor contains no elements.\n    ///\n    /// # Example\n    /// ```\n    /// use machinelearne_rs::backend::CpuBackend;\n    /// use machinelearne_rs::backend::Tensor1D;\n    ///\n    /// let empty = Tensor1D::\u003cCpuBackend\u003e::zeros(0);\n    /// assert!(empty.is_empty());\n    ///\n    /// let non_empty = Tensor1D::\u003cCpuBackend\u003e::zeros(1);\n    /// assert!(!non_empty.is_empty());\n    /// ```\n    pub fn is_empty(\u0026self) -\u003e bool {\n        B::len_1d(\u0026self.data) == 0\n    }\n\n    /// Scales the tensor by multiplying each element by a scalar value.\n    ///\n    /// Equivalent to element-wise multiplication: `self * scalar`.\n    ///\n    /// # Example\n    /// ```\n    /// use machinelearne_rs::backend::CpuBackend;\n    /// use machinelearne_rs::backend::Tensor1D;\n    /// use machinelearne_rs::backend::Scalar;\n    ///\n    /// let t = Tensor1D::\u003cCpuBackend\u003e::new(vec![1.0f32, 2.0, 3.0]);\n    /// let s = Scalar::\u003cCpuBackend\u003e::new(2.5);\n    /// let scaled = t.scale(\u0026s);\n    /// assert_eq!(scaled.to_vec(), vec![2.5, 5.0, 7.5]);\n    /// ```\n    pub fn scale(\u0026self, a: \u0026Scalar\u003cB\u003e) -\u003e Self {\n        Self {\n            data: B::mul_scalar_1d(\u0026self.data, \u0026a.data),\n            backend: PhantomData,\n        }\n    }\n\n    /// Adds a scalar value to each element of the tensor.\n    ///\n    /// Equivalent to element-wise addition: `self + scalar`.\n    ///\n    /// # Example\n    /// ```\n    /// use machinelearne_rs::backend::CpuBackend;\n    /// use machinelearne_rs::backend::Tensor1D;\n    /// use machinelearne_rs::backend::Scalar;\n    ///\n    /// let t = Tensor1D::\u003cCpuBackend\u003e::new(vec![1.0f32, 2.0, 3.0]);\n    /// let s = Scalar::\u003cCpuBackend\u003e::new(10.0);\n    /// let shifted = t.add_scalar(\u0026s);\n    /// assert_eq!(shifted.to_vec(), vec![11.0, 12.0, 13.0]);\n    /// ```\n    pub fn add_scalar(\u0026self, a: \u0026Scalar\u003cB\u003e) -\u003e Self {\n        Self {\n            data: B::add_scalar_1d(\u0026self.data, \u0026a.data),\n            backend: PhantomData,\n        }\n    }\n\n    /// Computes element-wise maximum between two tensors.\n    ///\n    /// For each index `i`, returns `max(self[i], other[i])`.\n    ///\n    /// # Panics\n    /// Panics if tensors have different lengths.\n    ///\n    /// # Example\n    /// ```\n    /// use machinelearne_rs::backend::CpuBackend;\n    /// use machinelearne_rs::backend::Tensor1D;\n    ///\n    /// let a = Tensor1D::\u003cCpuBackend\u003e::new(vec![1.0f32, 5.0, 3.0]);\n    /// let b = Tensor1D::\u003cCpuBackend\u003e::new(vec![2.0f32, 4.0, 6.0]);\n    /// let max_ab = a.maximum(b);\n    /// assert_eq!(max_ab.to_vec(), vec![2.0, 5.0, 6.0]);\n    /// ```\n    pub fn maximum(\u0026self, other: Self) -\u003e Self {\n        Self {\n            data: B::maximum_1d(\u0026self.data, \u0026other.data),\n            backend: PhantomData,\n        }\n    }\n\n    /// Computes element-wise exponential function: `e^x`.\n    ///\n    /// # Numerical behavior\n    /// Follows IEEE 754 semantics:\n    /// - `exp(0.0)` = `1.0`\n    /// - `exp(+â)` = `+â`\n    /// - `exp(-â)` = `0.0`\n    /// - Large positive inputs may return `INFINITY` on overflow\n    ///\n    /// # Example\n    /// ```\n    /// use machinelearne_rs::backend::CpuBackend;\n    /// use machinelearne_rs::backend::Tensor1D;\n    ///\n    /// let t = Tensor1D::\u003cCpuBackend\u003e::new(vec![0.0f32, 1.0]);\n    /// let exp_t = t.exp();\n    /// assert!((exp_t.to_vec()[0] - 1.0).abs() \u003c 1e-12);\n    /// assert!((exp_t.to_vec()[1] - std::f64::consts::E).abs() \u003c 1e-12);\n    /// ```\n    pub fn exp(\u0026self) -\u003e Self {\n        Self {\n            data: B::exp_1d(\u0026self.data),\n            backend: PhantomData,\n        }\n    }\n\n    /// Computes element-wise natural logarithm: `ln(x)`.\n    ///\n    /// # Panics\n    /// Panics for non-positive values (backend-dependent behavior).\n    ///\n    /// # Example\n    /// ```\n    /// use machinelearne_rs::backend::CpuBackend;\n    /// use machinelearne_rs::backend::Tensor1D;\n    ///\n    /// let t = Tensor1D::\u003cCpuBackend\u003e::new(vec![1.0f32, std::f64::consts::E as f32]);\n    /// let log_t = t.log();\n    /// assert!((log_t.to_vec()[0] - 0.0).abs() \u003c 1e-6);\n    /// assert!((log_t.to_vec()[1] - 1.0).abs() \u003c 1e-6);\n    /// ```\n    pub fn log(\u0026self) -\u003e Self {\n        Self {\n            data: B::log_1d(\u0026self.data),\n            backend: PhantomData,\n        }\n    }\n\n    /// Computes numerically stable sigmoid activation element-wise.\n    ///\n    /// Implements the logistic function: `Ï(x) = 1 / (1 + e^(-x))`\n    ///\n    /// # Numerical stability\n    /// Uses a numerically stable implementation that avoids overflow/underflow\n    /// for extreme input values (e.g., Â±100):\n    /// - For `x \u003e= 0`: `1 / (1 + e^(-x))`\n    /// - For `x \u003c 0`: `e^x / (1 + e^x)`\n    ///\n    /// # Output range\n    /// Always returns values in the open interval `(0, 1)`.\n    ///\n    /// # Example\n    /// ```\n    /// use machinelearne_rs::backend::CpuBackend;\n    /// use machinelearne_rs::backend::Tensor1D;\n    ///\n    /// let t = Tensor1D::\u003cCpuBackend\u003e::new(vec![-100.0f32, 0.0, 100.0]);\n    /// let sig = t.sigmoid();\n    /// let output = sig.to_vec();\n    ///\n    /// // Extreme negative â â0.0\n    /// assert!(output[0] \u003c 1e-10);\n    /// // Zero â 0.5 exactly\n    /// assert!((output[1] - 0.5).abs() \u003c 1e-12);\n    /// // Extreme positive â â1.0\n    /// assert!(output[2] \u003e 1.0 - 1e-10);\n    /// ```\n    pub fn sigmoid(\u0026self) -\u003e Self {\n        Self {\n            data: B::sigmoid_1d(\u0026self.data),\n            backend: PhantomData,\n        }\n    }\n}\n\n#[cfg(test)]\nmod tests {\n    use super::*;\n    use crate::backend::CpuBackend;\n\n    // === Constructor Tests ===\n\n    #[test]\n    fn test_tensor1d_new() {\n        let t = Tensor1D::\u003cCpuBackend\u003e::new(vec![1.0, 2.0, 3.0]);\n        assert_eq!(t.to_vec(), vec![1.0, 2.0, 3.0]);\n        assert_eq!(t.len(), 3);\n    }\n\n    #[test]\n    fn test_tensor1d_new_empty() {\n        let t = Tensor1D::\u003cCpuBackend\u003e::new(vec![]);\n        assert_eq!(t.to_vec(), vec![]);\n        assert_eq!(t.len(), 0);\n        assert!(t.is_empty());\n    }\n\n    #[test]\n    fn test_tensor1d_new_single() {\n        let t = Tensor1D::\u003cCpuBackend\u003e::new(vec![42.0]);\n        assert_eq!(t.to_vec(), vec![42.0]);\n        assert_eq!(t.len(), 1);\n        assert!(!t.is_empty());\n    }\n\n    #[test]\n    fn test_tensor1d_new_negative() {\n        let t = Tensor1D::\u003cCpuBackend\u003e::new(vec![-1.0, -2.5, -0.5]);\n        assert_eq!(t.to_vec(), vec![-1.0, -2.5, -0.5]);\n    }\n\n    #[test]\n    fn test_tensor1d_zeros() {\n        let t = Tensor1D::\u003cCpuBackend\u003e::zeros(5);\n        assert_eq!(t.to_vec(), vec![0.0, 0.0, 0.0, 0.0, 0.0]);\n        assert_eq!(t.len(), 5);\n        assert!(!t.is_empty());\n    }\n\n    #[test]\n    fn test_tensor1d_zeros_empty() {\n        let t = Tensor1D::\u003cCpuBackend\u003e::zeros(0);\n        assert_eq!(t.to_vec(), vec![]);\n        assert_eq!(t.len(), 0);\n        assert!(t.is_empty());\n    }\n\n    #[test]\n    fn test_tensor1d_zeros_single() {\n        let t = Tensor1D::\u003cCpuBackend\u003e::zeros(1);\n        assert_eq!(t.to_vec(), vec![0.0]);\n        assert_eq!(t.len(), 1);\n    }\n\n    #[test]\n    fn test_tensor1d_zeros_large() {\n        let n = 10000;\n        let t = Tensor1D::\u003cCpuBackend\u003e::zeros(n);\n        assert_eq!(t.len(), n);\n        assert!(t.to_vec().iter().all(|\u0026x| x == 0.0));\n    }\n\n    // === len and is_empty Tests ===\n\n    #[test]\n    fn test_tensor1d_len() {\n        let t = Tensor1D::\u003cCpuBackend\u003e::new(vec![1.0, 2.0, 3.0, 4.0, 5.0]);\n        assert_eq!(t.len(), 5);\n    }\n\n    #[test]\n    fn test_tensor1d_is_empty_true() {\n        let t = Tensor1D::\u003cCpuBackend\u003e::new(vec![]);\n        assert!(t.is_empty());\n    }\n\n    #[test]\n    fn test_tensor1d_is_empty_false() {\n        let t = Tensor1D::\u003cCpuBackend\u003e::new(vec![1.0]);\n        assert!(!t.is_empty());\n    }\n\n    // === to_vec Tests ===\n\n    #[test]\n    fn test_tensor1d_to_vec_simple() {\n        let t = Tensor1D::\u003cCpuBackend\u003e::new(vec![1.5, 2.5, 3.5]);\n        assert_eq!(t.to_vec(), vec![1.5, 2.5, 3.5]);\n    }\n\n    #[test]\n    fn test_tensor1d_to_vec_negative() {\n        let t = Tensor1D::\u003cCpuBackend\u003e::new(vec![-1.0, -2.0, -3.0]);\n        assert_eq!(t.to_vec(), vec![-1.0, -2.0, -3.0]);\n    }\n\n    #[test]\n    fn test_tensor1d_to_vec_fractional() {\n        let t = Tensor1D::\u003cCpuBackend\u003e::new(vec![0.1, 0.25, 0.75]);\n        assert!((t.to_vec()[0] - 0.1).abs() \u003c 1e-6);\n        assert!((t.to_vec()[1] - 0.25).abs() \u003c 1e-6);\n        assert!((t.to_vec()[2] - 0.75).abs() \u003c 1e-6);\n    }\n\n    // === sub Tests ===\n\n    #[test]\n    fn test_tensor1d_sub() {\n        let a = Tensor1D::\u003cCpuBackend\u003e::new(vec![5.0, 7.0]);\n        let b = Tensor1D::\u003cCpuBackend\u003e::new(vec![2.0, 3.0]);\n        let diff = a.sub(\u0026b);\n        assert_eq!(diff.to_vec(), vec![3.0, 4.0]);\n    }\n\n    #[test]\n    fn test_tensor1d_sub_negative_result() {\n        let a = Tensor1D::\u003cCpuBackend\u003e::new(vec![1.0]);\n        let b = Tensor1D::\u003cCpuBackend\u003e::new(vec![5.0]);\n        let diff = a.sub(\u0026b);\n        assert_eq!(diff.to_vec(), vec![-4.0]);\n    }\n\n    #[test]\n    fn test_tensor1d_sub_with_negatives() {\n        let a = Tensor1D::\u003cCpuBackend\u003e::new(vec![-1.0, -5.0]);\n        let b = Tensor1D::\u003cCpuBackend\u003e::new(vec![-3.0, -2.0]);\n        let diff = a.sub(\u0026b);\n        assert_eq!(diff.to_vec(), vec![2.0, -3.0]);\n    }\n\n    // === mean Tests ===\n\n    #[test]\n    fn test_tensor1d_mean() {\n        let t = Tensor1D::\u003cCpuBackend\u003e::new(vec![1.0, 2.0, 3.0, 4.0]);\n        let mean = t.mean();\n        assert_eq!(mean.to_f64(), 2.5);\n    }\n\n    #[test]\n    fn test_tensor1d_mean_negatives() {\n        let t = Tensor1D::\u003cCpuBackend\u003e::new(vec![-1.0, 1.0, -1.0, 1.0]);\n        let mean = t.mean();\n        assert_eq!(mean.to_f64(), 0.0);\n    }\n\n    #[test]\n    fn test_tensor1d_mean_single() {\n        let t = Tensor1D::\u003cCpuBackend\u003e::new(vec![5.0]);\n        let mean = t.mean();\n        assert_eq!(mean.to_f64(), 5.0);\n    }\n\n    // === dot Tests ===\n\n    #[test]\n    fn test_tensor1d_dot_simple() {\n        let a = Tensor1D::\u003cCpuBackend\u003e::new(vec![1.0, 2.0, 3.0]);\n        let b = Tensor1D::\u003cCpuBackend\u003e::new(vec![4.0, 5.0, 6.0]);\n        let dot = a.dot(\u0026b);\n        // 1*4 + 2*5 + 3*6 = 4 + 10 + 18 = 32\n        assert_eq!(dot.to_f64(), 32.0);\n    }\n\n    #[test]\n    fn test_tensor1d_dot_single() {\n        let a = Tensor1D::\u003cCpuBackend\u003e::new(vec![5.0]);\n        let b = Tensor1D::\u003cCpuBackend\u003e::new(vec![3.0]);\n        let dot = a.dot(\u0026b);\n        assert_eq!(dot.to_f64(), 15.0);\n    }\n\n    #[test]\n    fn test_tensor1d_dot_with_negatives() {\n        let a = Tensor1D::\u003cCpuBackend\u003e::new(vec![1.0, -1.0]);\n        let b = Tensor1D::\u003cCpuBackend\u003e::new(vec![1.0, 1.0]);\n        let dot = a.dot(\u0026b);\n        // 1*1 + (-1)*1 = 0\n        assert_eq!(dot.to_f64(), 0.0);\n    }\n\n    #[test]\n    fn test_tensor1d_dot_all_zeros() {\n        let a = Tensor1D::\u003cCpuBackend\u003e::new(vec![0.0, 0.0, 0.0]);\n        let b = Tensor1D::\u003cCpuBackend\u003e::new(vec![1.0, 2.0, 3.0]);\n        let dot = a.dot(\u0026b);\n        assert_eq!(dot.to_f64(), 0.0);\n    }\n\n    #[test]\n    fn test_tensor1d_dot_fractional() {\n        let a = Tensor1D::\u003cCpuBackend\u003e::new(vec![0.5, 0.5]);\n        let b = Tensor1D::\u003cCpuBackend\u003e::new(vec![1.0, 1.0]);\n        let dot = a.dot(\u0026b);\n        // 0.5*1 + 0.5*1 = 1.0\n        assert!((dot.to_f64() - 1.0).abs() \u003c 1e-10);\n    }\n\n    // === abs Tests ===\n\n    #[test]\n    fn test_tensor1d_abs() {\n        let t = Tensor1D::\u003cCpuBackend\u003e::new(vec![-1.0, 2.0, -3.0]);\n        let abs_t = t.abs();\n        assert_eq!(abs_t.to_vec(), vec![1.0, 2.0, 3.0]);\n    }\n\n    #[test]\n    fn test_tensor1d_abs_all_negative() {\n        let t = Tensor1D::\u003cCpuBackend\u003e::new(vec![-5.0, -10.0, -0.5]);\n        let abs_t = t.abs();\n        assert_eq!(abs_t.to_vec(), vec![5.0, 10.0, 0.5]);\n    }\n\n    #[test]\n    fn test_tensor1d_abs_all_positive() {\n        let t = Tensor1D::\u003cCpuBackend\u003e::new(vec![1.0, 2.0, 3.0]);\n        let abs_t = t.abs();\n        assert_eq!(abs_t.to_vec(), vec![1.0, 2.0, 3.0]);\n    }\n\n    // === sign Tests ===\n\n    #[test]\n    fn test_tensor1d_sign() {\n        let t = Tensor1D::\u003cCpuBackend\u003e::new(vec![-2.0, 0.0, 3.0]);\n        let sign_t = t.sign();\n        assert_eq!(sign_t.to_vec(), vec![-1.0, 0.0, 1.0]);\n    }\n\n    #[test]\n    fn test_tensor1d_sign_fractional() {\n        let t = Tensor1D::\u003cCpuBackend\u003e::new(vec![-0.5, 0.5, 0.0]);\n        let sign_t = t.sign();\n        assert_eq!(sign_t.to_vec(), vec![-1.0, 1.0, 0.0]);\n    }\n\n    // === scale Tests ===\n\n    #[test]\n    fn test_tensor1d_scale() {\n        let t = Tensor1D::\u003cCpuBackend\u003e::new(vec![1.0, 2.0, 3.0]);\n        let s = Scalar::\u003cCpuBackend\u003e::new(2.0);\n        let scaled = t.scale(\u0026s);\n        assert_eq!(scaled.to_vec(), vec![2.0, 4.0, 6.0]);\n    }\n\n    #[test]\n    fn test_tensor1d_scale_fractional() {\n        let t = Tensor1D::\u003cCpuBackend\u003e::new(vec![1.0, 2.0]);\n        let s = Scalar::\u003cCpuBackend\u003e::new(0.5);\n        let scaled = t.scale(\u0026s);\n        assert_eq!(scaled.to_vec(), vec![0.5, 1.0]);\n    }\n\n    #[test]\n    fn test_tensor1d_scale_negative() {\n        let t = Tensor1D::\u003cCpuBackend\u003e::new(vec![1.0, 2.0]);\n        let s = Scalar::\u003cCpuBackend\u003e::new(-1.0);\n        let scaled = t.scale(\u0026s);\n        assert_eq!(scaled.to_vec(), vec![-1.0, -2.0]);\n    }\n\n    #[test]\n    fn test_tensor1d_scale_zero() {\n        let t = Tensor1D::\u003cCpuBackend\u003e::new(vec![1.0, 2.0, 3.0]);\n        let s = Scalar::\u003cCpuBackend\u003e::new(0.0);\n        let scaled = t.scale(\u0026s);\n        assert_eq!(scaled.to_vec(), vec![0.0, 0.0, 0.0]);\n    }\n\n    // === add_scalar Tests ===\n\n    #[test]\n    fn test_tensor1d_add_scalar() {\n        let t = Tensor1D::\u003cCpuBackend\u003e::new(vec![1.0, 2.0, 3.0]);\n        let s = Scalar::\u003cCpuBackend\u003e::new(10.0);\n        let result = t.add_scalar(\u0026s);\n        assert_eq!(result.to_vec(), vec![11.0, 12.0, 13.0]);\n    }\n\n    #[test]\n    fn test_tensor1d_add_scalar_negative() {\n        let t = Tensor1D::\u003cCpuBackend\u003e::new(vec![5.0, 10.0]);\n        let s = Scalar::\u003cCpuBackend\u003e::new(-3.0);\n        let result = t.add_scalar(\u0026s);\n        assert_eq!(result.to_vec(), vec![2.0, 7.0]);\n    }\n\n    // === maximum Tests ===\n\n    #[test]\n    fn test_tensor1d_maximum() {\n        let a = Tensor1D::\u003cCpuBackend\u003e::new(vec![1.0, 5.0, 3.0]);\n        let b = Tensor1D::\u003cCpuBackend\u003e::new(vec![2.0, 4.0, 6.0]);\n        let max_ab = a.maximum(b);\n        assert_eq!(max_ab.to_vec(), vec![2.0, 5.0, 6.0]);\n    }\n\n    #[test]\n    fn test_tensor1d_maximum_equal() {\n        let a = Tensor1D::\u003cCpuBackend\u003e::new(vec![1.0, 2.0, 3.0]);\n        let b = Tensor1D::\u003cCpuBackend\u003e::new(vec![1.0, 2.0, 3.0]);\n        let max_ab = a.maximum(b);\n        assert_eq!(max_ab.to_vec(), vec![1.0, 2.0, 3.0]);\n    }\n\n    #[test]\n    fn test_tensor1d_maximum_with_negatives() {\n        let a = Tensor1D::\u003cCpuBackend\u003e::new(vec![-1.0, 5.0]);\n        let b = Tensor1D::\u003cCpuBackend\u003e::new(vec![1.0, -5.0]);\n        let max_ab = a.maximum(b);\n        assert_eq!(max_ab.to_vec(), vec![1.0, 5.0]);\n    }\n\n    // === exp Tests ===\n\n    #[test]\n    fn test_tensor1d_exp_zero() {\n        let t = Tensor1D::\u003cCpuBackend\u003e::new(vec![0.0]);\n        let exp_t = t.exp();\n        assert!((exp_t.to_vec()[0] - 1.0).abs() \u003c 1e-12);\n    }\n\n    #[test]\n    fn test_tensor1d_exp_one() {\n        let t = Tensor1D::\u003cCpuBackend\u003e::new(vec![1.0]);\n        let exp_t = t.exp();\n        assert!((exp_t.to_vec()[0] - std::f64::consts::E).abs() \u003c 1e-12);\n    }\n\n    #[test]\n    fn test_tensor1d_exp_negative() {\n        let t = Tensor1D::\u003cCpuBackend\u003e::new(vec![-1.0]);\n        let exp_t = t.exp();\n        assert!((exp_t.to_vec()[0] - (1.0 / std::f64::consts::E)).abs() \u003c 1e-12);\n    }\n\n    // === log Tests ===\n\n    #[test]\n    fn test_tensor1d_log() {\n        let t = Tensor1D::\u003cCpuBackend\u003e::new(vec![1.0, std::f64::consts::E as f32]);\n        let log_t = t.log();\n        assert!((log_t.to_vec()[0] - 0.0).abs() \u003c 1e-6);\n        assert!((log_t.to_vec()[1] - 1.0).abs() \u003c 1e-6);\n    }\n\n    // === sigmoid Tests ===\n\n    #[test]\n    fn test_tensor1d_sigmoid_zero() {\n        let t = Tensor1D::\u003cCpuBackend\u003e::new(vec![0.0]);\n        let sig = t.sigmoid();\n        assert!((sig.to_vec()[0] - 0.5).abs() \u003c 1e-12);\n    }\n\n    #[test]\n    fn test_tensor1d_sigmoid_large_positive() {\n        let t = Tensor1D::\u003cCpuBackend\u003e::new(vec![100.0]);\n        let sig = t.sigmoid();\n        // sigmoid(100) is approximately 1.0, which satisfies \u003e 1.0 - 1e-10\n        // and equals 1.0 exactly due to floating point saturation\n        assert!(sig.to_vec()[0] \u003e= 1.0 - 1e-10);\n        assert!(sig.to_vec()[0] \u003c= 1.0);\n    }\n\n    #[test]\n    fn test_tensor1d_sigmoid_large_negative() {\n        let t = Tensor1D::\u003cCpuBackend\u003e::new(vec![-100.0]);\n        let sig = t.sigmoid();\n        assert!(sig.to_vec()[0] \u003c 1e-10);\n        assert!(sig.to_vec()[0] \u003e 0.0);\n    }\n\n    // === Method Chaining Tests ===\n\n    #[test]\n    fn test_tensor1d_chaining_scale_add() {\n        let t = Tensor1D::\u003cCpuBackend\u003e::new(vec![1.0, 2.0]);\n        let result = t\n            .scale(\u0026Scalar::\u003cCpuBackend\u003e::new(2.0))\n            .add_scalar(\u0026Scalar::\u003cCpuBackend\u003e::new(1.0));\n        assert_eq!(result.to_vec(), vec![3.0, 5.0]);\n    }\n\n    #[test]\n    fn test_tensor1d_chaining_abs_scale() {\n        let t = Tensor1D::\u003cCpuBackend\u003e::new(vec![-1.0, -2.0]);\n        let result = t.abs().scale(\u0026Scalar::\u003cCpuBackend\u003e::new(2.0));\n        assert_eq!(result.to_vec(), vec![2.0, 4.0]);\n    }\n\n    #[test]\n    fn test_tensor1d_chaining_sub_scale() {\n        let a = Tensor1D::\u003cCpuBackend\u003e::new(vec![5.0, 10.0]);\n        let b = Tensor1D::\u003cCpuBackend\u003e::new(vec![1.0, 2.0]);\n        let result = a.sub(\u0026b).scale(\u0026Scalar::\u003cCpuBackend\u003e::new(0.5));\n        assert_eq!(result.to_vec(), vec![2.0, 4.0]);\n    }\n\n    // === Clone Tests ===\n\n    #[test]\n    fn test_tensor1d_clone() {\n        let t = Tensor1D::\u003cCpuBackend\u003e::new(vec![1.0, 2.0, 3.0]);\n        let t_clone = t.clone();\n\n        assert_eq!(t.to_vec(), t_clone.to_vec());\n        assert_eq!(t.len(), t_clone.len());\n    }\n\n    #[test]\n    fn test_tensor1d_clone_independence() {\n        let t = Tensor1D::\u003cCpuBackend\u003e::new(vec![1.0, 2.0]);\n        let t_clone = t.clone();\n        let modified = t_clone.scale(\u0026Scalar::\u003cCpuBackend\u003e::new(2.0));\n\n        // Original should be unchanged\n        assert_eq!(t.to_vec(), vec![1.0, 2.0]);\n        assert_eq!(modified.to_vec(), vec![2.0, 4.0]);\n    }\n}\n","traces":[{"line":71,"address":[2547024],"length":1,"stats":{"Line":1}},{"line":73,"address":[2547037],"length":1,"stats":{"Line":1}},{"line":92,"address":[2547344],"length":1,"stats":{"Line":3}},{"line":94,"address":[2547363],"length":1,"stats":{"Line":4}},{"line":114,"address":[2547088],"length":1,"stats":{"Line":1}},{"line":116,"address":[2547111],"length":1,"stats":{"Line":1}},{"line":138,"address":[2547168],"length":1,"stats":{"Line":3}},{"line":140,"address":[2547173],"length":1,"stats":{"Line":3}},{"line":162,"address":[2547424],"length":1,"stats":{"Line":3}},{"line":163,"address":[2547441],"length":1,"stats":{"Line":3}},{"line":187,"address":[2546704,2546824,2546818],"length":1,"stats":{"Line":1}},{"line":188,"address":[2546734],"length":1,"stats":{"Line":2}},{"line":189,"address":[2546744,2546791],"length":1,"stats":{"Line":5}},{"line":207,"address":[2546624],"length":1,"stats":{"Line":1}},{"line":209,"address":[2546643],"length":1,"stats":{"Line":2}},{"line":230,"address":[2547184],"length":1,"stats":{"Line":2}},{"line":232,"address":[2547203],"length":1,"stats":{"Line":2}},{"line":247,"address":[2546928],"length":1,"stats":{"Line":1}},{"line":248,"address":[2546933],"length":1,"stats":{"Line":1}},{"line":264,"address":[2547664],"length":1,"stats":{"Line":2}},{"line":265,"address":[2547669],"length":1,"stats":{"Line":1}},{"line":283,"address":[2547264],"length":1,"stats":{"Line":3}},{"line":285,"address":[2547287],"length":1,"stats":{"Line":3}},{"line":305,"address":[2546544],"length":1,"stats":{"Line":1}},{"line":307,"address":[2546567],"length":1,"stats":{"Line":2}},{"line":329,"address":[2547568,2547456],"length":1,"stats":{"Line":3}},{"line":331,"address":[2547484],"length":1,"stats":{"Line":3}},{"line":355,"address":[2546848],"length":1,"stats":{"Line":1}},{"line":357,"address":[2546867],"length":1,"stats":{"Line":1}},{"line":377,"address":[2546944],"length":1,"stats":{"Line":1}},{"line":379,"address":[2546963],"length":1,"stats":{"Line":1}},{"line":413,"address":[2547584],"length":1,"stats":{"Line":3}},{"line":415,"address":[2547603],"length":1,"stats":{"Line":3}}],"covered":33,"coverable":33},{"path":["/","home","victor","ML","machinelearne-rs","lib","src","backend","tensor2d.rs"],"content":"use super::scalar::Scalar;\nuse super::tensor1d::Tensor1D;\nuse crate::backend::Backend;\nuse std::marker::PhantomData;\n\n/// Backend-typed 2D tensor (matrix) providing compile-time type safety and zero-cost abstractions.\n///\n/// Wraps a backend's native 2D tensor representation (`B::Tensor2D`) while carrying phantom\n/// type information about its originating backend. This prevents accidental mixing of tensors\n/// from different backends at compile time while maintaining performance through zero-sized\n/// `PhantomData` overhead.\n///\n/// # Type safety guarantees\n/// ```compile_fail\n/// use machinelearne_rs::backend::{CpuBackend, NdarrayBackend};\n/// use machinelearne_rs::backend::Tensor2D;\n///\n/// let cpu_mat: Tensor2D\u003cCpuBackend\u003e = Tensor2D::zeros(2, 2);\n/// let ndarray_mat: Tensor2D\u003cNdarrayBackend\u003e = Tensor2D::zeros(2, 2);\n/// let _ = cpu_mat.sub(\u0026ndarray_mat); // COMPILE ERROR: mismatched backends\n/// ```\n///\n/// # Precision semantics\n/// - Constructors accept `Vec\u003cf32\u003e` for ergonomic data loading (row-major order)\n/// - Values are immediately converted to backend's native precision (typically `f64`)\n/// - All operations occur in native backend precision\n/// - Row-major layout: `[aââ, aââ, ..., aââ, aââ, ..., aââ]` for an (mÃn) matrix\n///\n/// # Matrix-vector operations\n/// This tensor provides two fundamental linear algebra operations:\n/// - `dot(x)`: Computes `A @ x` where `A` is (mÃn) and `x` is (n,) â result (m,)\n/// - `tdot(x)`: Computes `Aáµ @ x` where `A` is (mÃn) and `x` is (m,) â result (n,)\n///\n/// # Zero-cost design\n/// - `PhantomData\u003cB\u003e` adds no runtime memory overhead\n/// - All operations delegate directly to backend implementations\n/// - Implements `Clone` (but not `Copy`) due to potential heap allocation in underlying tensors\n///\n/// # Example\n/// ```\n/// use machinelearne_rs::backend::CpuBackend;\n/// use machinelearne_rs::backend::{Tensor2D, Tensor1D};\n///\n/// // Create 2Ã2 matrix: [[1.0, 2.0], [3.0, 4.0]]\n/// let a: Tensor2D\u003cCpuBackend\u003e = Tensor2D::new(vec![1.0f32, 2.0, 3.0, 4.0], 2, 2);\n/// assert_eq!(a.shape(), (2, 2));\n///\n/// // Matrix-vector multiplication: A @ [1, 0]áµ = [1, 3]áµ\n/// let x = Tensor1D::\u003cCpuBackend\u003e::new(vec![1.0f32, 0.0]);\n/// let y = a.dot(\u0026x);\n/// assert_eq!(y.to_vec(), vec![1.0, 3.0]);\n/// ```\n#[derive(Clone)]\npub struct Tensor2D\u003cB: Backend\u003e {\n    pub(crate) data: B::Tensor2D,\n    pub(crate) backend: PhantomData\u003cB\u003e,\n}\n\nimpl\u003cB: Backend\u003e Tensor2D\u003cB\u003e {\n    /// Creates a new 2D tensor from a flat vector of `f32` values in row-major order.\n    ///\n    /// The input vector must contain exactly `rows * cols` elements arranged as:\n    /// `[rowâ_colâ, rowâ_colâ, ..., rowâ_colâââ, rowâ_colâ, ..., rowâââ_colâââ]`\n    ///\n    /// # Arguments\n    /// * `data` - Flat vector containing matrix elements in row-major order\n    /// * `rows` - Number of rows in the resulting matrix\n    /// * `cols` - Number of columns in the resulting matrix\n    ///\n    /// # Panics\n    /// Panics if `data.len() != rows * cols` (backend-dependent behavior).\n    ///\n    /// # Precision note\n    /// Input values are converted from `f32` to backend precision (usually `f64`).\n    /// Extremely large `f32` values near `f32::MAX` may lose precision during conversion.\n    ///\n    /// # Example\n    /// ```\n    /// use machinelearne_rs::backend::CpuBackend;\n    /// use machinelearne_rs::backend::Tensor2D;\n    ///\n    /// // Matrix: [[1.0, 2.0, 3.0],\n    /// //          [4.0, 5.0, 6.0]]\n    /// let t: Tensor2D\u003cCpuBackend\u003e = Tensor2D::new(vec![1.0f32, 2.0, 3.0, 4.0, 5.0, 6.0], 2, 3);\n    /// assert_eq!(t.shape(), (2, 3));\n    /// ```\n    pub fn new(data: Vec\u003cf32\u003e, rows: usize, cols: usize) -\u003e Self {\n        Self {\n            data: B::from_vec_2d(data, rows, cols),\n            backend: PhantomData,\n        }\n    }\n\n    /// Creates a 2D tensor filled with zeros of specified dimensions.\n    ///\n    /// # Arguments\n    /// * `rows` - Number of rows\n    /// * `cols` - Number of columns\n    ///\n    /// # Example\n    /// ```\n    /// use machinelearne_rs::backend::CpuBackend;\n    /// use machinelearne_rs::backend::Tensor2D;\n    ///\n    /// let zeros: Tensor2D\u003cCpuBackend\u003e = Tensor2D::zeros(3, 4);\n    /// assert_eq!(zeros.shape(), (3, 4));\n    /// assert_eq!(zeros.mean().to_f64(), 0.0);\n    /// ```\n    pub fn zeros(rows: usize, cols: usize) -\u003e Self {\n        Self {\n            data: B::zeros_2d(rows, cols),\n            backend: PhantomData,\n        }\n    }\n\n    /// Computes element-wise subtraction: `self - other`.\n    ///\n    /// # Panics\n    /// Panics if tensors have different shapes (backend-dependent behavior).\n    ///\n    /// # Example\n    /// ```\n    /// use machinelearne_rs::backend::CpuBackend;\n    /// use machinelearne_rs::backend::Tensor2D;\n    ///\n    /// let a = Tensor2D::\u003cCpuBackend\u003e::new(vec![5.0f32, 7.0, 9.0, 11.0], 2, 2);\n    /// let b = Tensor2D::\u003cCpuBackend\u003e::new(vec![2.0f32, 3.0, 4.0, 5.0], 2, 2);\n    /// let diff = a.sub(\u0026b);\n    /// // Result: [[3.0, 4.0], [5.0, 6.0]]\n    /// assert_eq!(diff.mean().to_f64(), 4.5);\n    /// ```\n    pub fn sub(\u0026self, other: \u0026Self) -\u003e Self {\n        Self {\n            data: B::sub_2d(\u0026self.data, \u0026other.data),\n            backend: PhantomData,\n        }\n    }\n\n    /// Computes the arithmetic mean of all elements in the tensor.\n    ///\n    /// # Returns\n    /// A `Scalar\u003cB\u003e` containing the mean value: `sum(elements) / (rows * cols)`\n    ///\n    /// # Panics\n    /// Panics if the tensor is empty (0 rows or 0 columns).\n    ///\n    /// # Example\n    /// ```\n    /// use machinelearne_rs::backend::CpuBackend;\n    /// use machinelearne_rs::backend::Tensor2D;\n    ///\n    /// // Matrix: [[1.0, 2.0],\n    /// //          [3.0, 4.0]]\n    /// let t = Tensor2D::\u003cCpuBackend\u003e::new(vec![1.0f32, 2.0, 3.0, 4.0], 2, 2);\n    /// let mean = t.mean();\n    /// assert!((mean.to_f64() - 2.5).abs() \u003c 1e-12);\n    /// ```\n    pub fn mean(\u0026self) -\u003e Scalar\u003cB\u003e {\n        Scalar {\n            data: B::mean_all_2d(\u0026self.data),\n            backend: PhantomData,\n        }\n    }\n\n    /// Matrix-vector multiplication: computes `A @ x` (no transpose).\n    ///\n    /// Multiplies this (mÃn) matrix by a vector of length n to produce a vector of length m.\n    ///\n    /// # Formula\n    /// `yáµ¢ = Î£â±¼ Aáµ¢â±¼ * xâ±¼` for i â [0, m)\n    ///\n    /// # Arguments\n    /// * `other` - Vector of length n (must match matrix columns)\n    ///\n    /// # Returns\n    /// `Tensor1D\u003cB\u003e` of length m\n    ///\n    /// # Panics\n    /// Panics if `self.shape().1 != other.len()` (columns â  vector length).\n    ///\n    /// # Example\n    /// ```\n    /// use machinelearne_rs::backend::CpuBackend;\n    /// use machinelearne_rs::backend::{Tensor2D, Tensor1D};\n    ///\n    /// // A = [[1.0, 2.0],\n    /// //      [3.0, 4.0]]\n    /// let a = Tensor2D::\u003cCpuBackend\u003e::new(vec![1.0f32, 2.0, 3.0, 4.0], 2, 2);\n    ///\n    /// // x = [1.0, 0.0]áµ\n    /// let x = Tensor1D::\u003cCpuBackend\u003e::new(vec![1.0f32, 0.0]);\n    ///\n    /// // A @ x = [1.0, 3.0]áµ\n    /// let y = a.dot(\u0026x);\n    /// assert_eq!(y.to_vec(), vec![1.0, 3.0]);\n    /// ```\n    pub fn dot(\u0026self, other: \u0026Tensor1D\u003cB\u003e) -\u003e Tensor1D\u003cB\u003e {\n        Tensor1D {\n            data: B::matvec(\u0026self.data, \u0026other.data),\n            backend: PhantomData,\n        }\n    }\n\n    /// Transposed matrix-vector multiplication: computes `Aáµ @ x`.\n    ///\n    /// Multiplies the transpose of this (mÃn) matrix by a vector of length m\n    /// to produce a vector of length n.\n    ///\n    /// # Formula\n    /// `yâ±¼ = Î£áµ¢ Aáµ¢â±¼ * xáµ¢` for j â [0, n)\n    ///\n    /// # Arguments\n    /// * `other` - Vector of length m (must match matrix rows)\n    ///\n    /// # Returns\n    /// `Tensor1D\u003cB\u003e` of length n\n    ///\n    /// # Panics\n    /// Panics if `self.shape().0 != other.len()` (rows â  vector length).\n    ///\n    /// # Example\n    /// ```\n    /// use machinelearne_rs::backend::CpuBackend;\n    /// use machinelearne_rs::backend::{Tensor2D, Tensor1D};\n    ///\n    /// // A = [[1.0, 2.0],\n    /// //      [3.0, 4.0]]\n    /// let a = Tensor2D::\u003cCpuBackend\u003e::new(vec![1.0f32, 2.0, 3.0, 4.0], 2, 2);\n    ///\n    /// // x = [1.0, 0.0]áµ\n    /// let x = Tensor1D::\u003cCpuBackend\u003e::new(vec![1.0f32, 0.0]);\n    ///\n    /// // Aáµ @ x = [1.0*1 + 3.0*0, 2.0*1 + 4.0*0]áµ = [1.0, 2.0]áµ\n    /// let y = a.tdot(\u0026x);\n    /// assert_eq!(y.to_vec(), vec![1.0, 2.0]);\n    /// ```\n    pub fn tdot(\u0026self, other: \u0026Tensor1D\u003cB\u003e) -\u003e Tensor1D\u003cB\u003e {\n        Tensor1D {\n            data: B::matvec_transposed(\u0026self.data, \u0026other.data),\n            backend: PhantomData,\n        }\n    }\n\n    /// Computes element-wise absolute value.\n    ///\n    /// # Example\n    /// ```\n    /// use machinelearne_rs::backend::CpuBackend;\n    /// use machinelearne_rs::backend::Tensor2D;\n    ///\n    /// let t = Tensor2D::\u003cCpuBackend\u003e::new(vec![-1.0f32, 2.0, -3.0, 4.0], 2, 2);\n    /// let abs_t = t.abs();\n    /// // Result: [[1.0, 2.0], [3.0, 4.0]]\n    /// assert_eq!(abs_t.mean().to_f64(), 2.5);\n    /// ```\n    pub fn abs(\u0026self) -\u003e Self {\n        Self {\n            data: B::abs_2d(\u0026self.data),\n            backend: PhantomData,\n        }\n    }\n\n    /// Computes element-wise sign function.\n    ///\n    /// Returns:\n    /// - `1.0` for positive values\n    /// - `-1.0` for negative values\n    /// - `0.0` for zero values\n    ///\n    /// # Example\n    /// ```\n    /// use machinelearne_rs::backend::CpuBackend;\n    /// use machinelearne_rs::backend::Tensor2D;\n    ///\n    /// let t = Tensor2D::\u003cCpuBackend\u003e::new(vec![-2.0f32, 0.0, 3.0, -4.0], 2, 2);\n    /// let sign_t = t.sign();\n    /// // Result: [[-1.0, 0.0], [1.0, -1.0]]\n    /// let vec = sign_t.ravel().to_vec(); // Note: to_vec() flattens the matrix\n    /// // We can verify specific elements via mean or custom checks\n    /// assert!((sign_t.mean().to_f64() + 0.25).abs() \u003c 1e-12); // (-1+0+1-1)/4 = -0.25\n    /// ```\n    pub fn sign(\u0026self) -\u003e Self {\n        Self {\n            data: B::sign_2d(\u0026self.data),\n            backend: PhantomData,\n        }\n    }\n\n    /// Returns the number of rows in the tensor as a scalar value.\n    ///\n    /// # Note\n    /// This returns a `Scalar\u003cB\u003e` rather than `usize` to maintain a uniform\n    /// tensor/scalar API for backend-agnostic generic code. For most use cases\n    /// requiring integer dimensions, prefer the `shape()` method.\n    ///\n    /// # Example\n    /// ```\n    /// use machinelearne_rs::backend::CpuBackend;\n    /// use machinelearne_rs::backend::Tensor2D;\n    ///\n    /// let t = Tensor2D::\u003cCpuBackend\u003e::zeros(5, 3);\n    /// let rows = t.len();\n    /// assert_eq!(rows.to_f64(), 5.0);\n    /// ```\n    pub fn len(\u0026self) -\u003e Scalar\u003cB\u003e {\n        Scalar {\n            data: B::scalar_f64(B::len_2d(\u0026self.data) as f64),\n            backend: PhantomData,\n        }\n    }\n\n    /// Scales the tensor by multiplying each element by a scalar value.\n    ///\n    /// Equivalent to element-wise multiplication: `self * scalar`.\n    ///\n    /// # Example\n    /// ```\n    /// use machinelearne_rs::backend::CpuBackend;\n    /// use machinelearne_rs::backend::Tensor2D;\n    /// use machinelearne_rs::backend::Scalar;\n    ///\n    /// let t = Tensor2D::\u003cCpuBackend\u003e::new(vec![1.0f32, 2.0, 3.0, 4.0], 2, 2);\n    /// let s = Scalar::\u003cCpuBackend\u003e::new(2.5);\n    /// let scaled = t.scale(s);\n    /// // Result: [[2.5, 5.0], [7.5, 10.0]]\n    /// assert!((scaled.mean().to_f64() - 6.25).abs() \u003c 1e-12);\n    /// ```\n    pub fn scale(\u0026self, a: Scalar\u003cB\u003e) -\u003e Self {\n        Self {\n            data: B::mul_scalar_2d(\u0026self.data, \u0026a.data),\n            backend: PhantomData,\n        }\n    }\n\n    /// Adds a scalar value to each element of the tensor.\n    ///\n    /// Equivalent to element-wise addition: `self + scalar`.\n    ///\n    /// # Example\n    /// ```\n    /// use machinelearne_rs::backend::CpuBackend;\n    /// use machinelearne_rs::backend::Tensor2D;\n    /// use machinelearne_rs::backend::Scalar;\n    ///\n    /// let t = Tensor2D::\u003cCpuBackend\u003e::new(vec![1.0f32, 2.0, 3.0, 4.0], 2, 2);\n    /// let s = Scalar::\u003cCpuBackend\u003e::new(10.0);\n    /// let shifted = t.add_scalar(s);\n    /// // Result: [[11.0, 12.0], [13.0, 14.0]]\n    /// assert_eq!(shifted.mean().to_f64(), 12.5);\n    /// ```\n    pub fn add_scalar(\u0026self, a: Scalar\u003cB\u003e) -\u003e Self {\n        Self {\n            data: B::add_scalar_2d(\u0026self.data, \u0026a.data),\n            backend: PhantomData,\n        }\n    }\n\n    /// Computes element-wise maximum between two tensors.\n    ///\n    /// For each index `(i, j)`, returns `max(self[i,j], other[i,j])`.\n    ///\n    /// # Panics\n    /// Panics if tensors have different shapes.\n    ///\n    /// # Example\n    /// ```\n    /// use machinelearne_rs::backend::CpuBackend;\n    /// use machinelearne_rs::backend::Tensor2D;\n    ///\n    /// let a = Tensor2D::\u003cCpuBackend\u003e::new(vec![1.0f32, 5.0, 3.0, 2.0], 2, 2);\n    /// let b = Tensor2D::\u003cCpuBackend\u003e::new(vec![2.0f32, 4.0, 6.0, 1.0], 2, 2);\n    /// let max_ab = a.maximum(b);\n    /// // Result: [[2.0, 5.0], [6.0, 2.0]]\n    /// assert_eq!(max_ab.mean().to_f64(), 3.75);\n    /// ```\n    pub fn maximum(\u0026self, other: Self) -\u003e Self {\n        Self {\n            data: B::maximum_2d(\u0026self.data, \u0026other.data),\n            backend: PhantomData,\n        }\n    }\n\n    /// Computes element-wise exponential function: `e^x`.\n    ///\n    /// # Numerical behavior\n    /// Follows IEEE 754 semantics:\n    /// - `exp(0.0)` = `1.0`\n    /// - `exp(+â)` = `+â`\n    /// - `exp(-â)` = `0.0`\n    /// - Large positive inputs may return `INFINITY` on overflow\n    ///\n    /// # Example\n    /// ```\n    /// use machinelearne_rs::backend::CpuBackend;\n    /// use machinelearne_rs::backend::Tensor2D;\n    ///\n    /// let t = Tensor2D::\u003cCpuBackend\u003e::new(vec![0.0f32, 1.0], 1, 2);\n    /// let exp_t = t.exp();\n    /// let values = exp_t.ravel().to_vec();\n    /// assert!((values[0] - 1.0).abs() \u003c 1e-12);\n    /// assert!((values[1] - std::f64::consts::E).abs() \u003c 1e-6); // f32âf64 conversion error\n    /// ```\n    pub fn exp(\u0026self) -\u003e Self {\n        Self {\n            data: B::exp_2d(\u0026self.data),\n            backend: PhantomData,\n        }\n    }\n\n    /// Computes element-wise natural logarithm: `ln(x)`.\n    ///\n    /// # Behavior for edge cases (IEEE 754 compliant)\n    /// - `x \u003e 0.0` â `ln(x)` (finite value)\n    /// - `x == 0.0` â `-â` (`f64::NEG_INFINITY`)\n    /// - `x \u003c 0.0` â `NaN` (`f64::NAN`)\n    ///\n    /// Does **not** panic â follows standard floating-point semantics used by\n    /// NumPy, PyTorch, and TensorFlow.\n    ///\n    /// # Numerical stability in ML\n    /// For loss functions involving logarithms (e.g., cross-entropy):\n    /// - Avoid raw `log(x)` on unnormalized probabilities â use `log_softmax` instead\n    /// - Clip inputs when necessary: `log(max(x, Îµ))` with `Îµ = 1e-12` to avoid `-inf`\n    /// - `-inf` propagates through computations and will cause `NaN` in gradients\n    ///\n    /// # Example\n    /// ```\n    /// use machinelearne_rs::backend::{CpuBackend, Tensor2D};\n    ///\n    /// let t = Tensor2D::\u003cCpuBackend\u003e::new(vec![1.0f32, 0.0, -1.0], 1, 3);\n    /// let log_t = t.log();\n    /// let values = log_t.ravel().to_vec();\n    ///\n    /// assert!((values[0] - 0.0).abs() \u003c 1e-12);      // ln(1) = 0\n    /// assert!(values[1].is_infinite() \u0026\u0026 values[1] \u003c 0.0); // ln(0) = -inf\n    /// assert!(values[2].is_nan());                   // ln(-1) = NaN\n    /// ```\n    pub fn log(\u0026self) -\u003e Self {\n        Self {\n            data: B::log_2d(\u0026self.data),\n            backend: PhantomData,\n        }\n    }\n\n    /// Computes numerically stable sigmoid activation element-wise.\n    ///\n    /// Implements the logistic function: `Ï(x) = 1 / (1 + e^(-x))`\n    ///\n    /// # Numerical stability\n    /// Uses a numerically stable implementation that avoids overflow/underflow\n    /// for extreme input values (e.g., Â±100):\n    /// - For `x \u003e= 0`: `1 / (1 + e^(-x))`\n    /// - For `x \u003c 0`: `e^x / (1 + e^x)`\n    ///\n    /// # Output range\n    /// Always returns values in the open interval `(0, 1)`.\n    ///\n    /// # Example\n    /// ```\n    /// use machinelearne_rs::backend::CpuBackend;\n    /// use machinelearne_rs::backend::Tensor2D;\n    ///\n    /// let t = Tensor2D::\u003cCpuBackend\u003e::new(vec![-100.0f32, 0.0, 100.0], 1, 3);\n    /// let sig = t.sigmoid();\n    /// let values = sig.ravel().to_vec();\n    ///\n    /// // Extreme negative â â0.0\n    /// assert!(values[0] \u003c 1e-10);\n    /// // Zero â 0.5 exactly\n    /// assert!((values[1] - 0.5).abs() \u003c 1e-12);\n    /// // Extreme positive â â1.0\n    /// assert!(values[2] \u003e 1.0 - 1e-10);\n    /// ```\n    pub fn sigmoid(\u0026self) -\u003e Self {\n        Self {\n            data: B::sigmoid_2d(\u0026self.data),\n            backend: PhantomData,\n        }\n    }\n\n    /// Returns the shape of the tensor as `(rows, columns)`.\n    ///\n    /// # Returns\n    /// Tuple `(m, n)` where `m` is the number of rows and `n` is the number of columns.\n    ///\n    /// # Example\n    /// ```\n    /// use machinelearne_rs::backend::CpuBackend;\n    /// use machinelearne_rs::backend::Tensor2D;\n    ///\n    /// let t = Tensor2D::\u003cCpuBackend\u003e::new(vec![1.0f32, 2.0, 3.0], 3, 1);\n    /// assert_eq!(t.shape(), (3, 1));\n    ///\n    /// let empty = Tensor2D::\u003cCpuBackend\u003e::zeros(0, 5);\n    /// assert_eq!(empty.shape(), (0, 5));\n    /// ```\n    pub fn shape(\u0026self) -\u003e (usize, usize) {\n        B::shape(\u0026self.data)\n    }\n\n    pub fn ravel(\u0026self) -\u003e Tensor1D\u003cB\u003e {\n        Tensor1D::\u003cB\u003e {\n            data: B::ravel_2d(\u0026self.data),\n            backend: PhantomData,\n        }\n    }\n}\n\n#[cfg(test)]\nmod tests {\n    use super::*;\n    use crate::backend::{CpuBackend, Tensor1D};\n\n    #[test]\n    fn test_new_constructor_valid() {\n        // Standard 2x3 matrix\n        let t = Tensor2D::\u003cCpuBackend\u003e::new(vec![1.0f32, 2.0, 3.0, 4.0, 5.0, 6.0], 2, 3);\n        assert_eq!(t.shape(), (2, 3));\n        assert_eq!(t.ravel().to_vec(), vec![1.0, 2.0, 3.0, 4.0, 5.0, 6.0]);\n    }\n\n    #[test]\n    #[should_panic(expected = \"Inconsistent shape\")]\n    fn test_new_constructor_invalid_shape() {\n        // Should panic when data length != rows * cols\n        let _ = Tensor2D::\u003cCpuBackend\u003e::new(vec![1.0f32, 2.0, 3.0], 2, 2); // 3 != 4\n    }\n\n    #[test]\n    fn test_zeros_constructor() {\n        let t = Tensor2D::\u003cCpuBackend\u003e::zeros(3, 4);\n        assert_eq!(t.shape(), (3, 4));\n        assert_eq!(t.ravel().to_vec(), vec![0.0; 12]);\n        assert_eq!(t.mean().to_f64(), 0.0);\n    }\n\n    #[test]\n    fn test_sub_shape_mismatch_panics() {\n        let a = Tensor2D::\u003cCpuBackend\u003e::zeros(2, 3);\n        let b = Tensor2D::\u003cCpuBackend\u003e::zeros(3, 2);\n\n        // Should panic when shapes don't match\n        std::panic::catch_unwind(|| {\n            let _ = a.sub(\u0026b);\n        })\n        .unwrap_err();\n    }\n\n    #[test]\n    fn test_dot_shape_validation() {\n        let mat = Tensor2D::\u003cCpuBackend\u003e::zeros(3, 4); // 3x4 matrix\n        let vec_wrong = Tensor1D::\u003cCpuBackend\u003e::zeros(3); // Should be length 4\n\n        // Should panic: columns (4) != vector length (3)\n        std::panic::catch_unwind(|| {\n            let _ = mat.dot(\u0026vec_wrong);\n        })\n        .unwrap_err();\n    }\n\n    #[test]\n    fn test_tdot_shape_validation() {\n        let mat = Tensor2D::\u003cCpuBackend\u003e::zeros(3, 4); // 3x4 matrix\n        let vec_wrong = Tensor1D::\u003cCpuBackend\u003e::zeros(4); // Should be length 3\n\n        // Should panic: rows (3) != vector length (4)\n        std::panic::catch_unwind(|| {\n            let _ = mat.tdot(\u0026vec_wrong);\n        })\n        .unwrap_err();\n    }\n\n    #[test]\n    fn test_abs_negative_values() {\n        let t = Tensor2D::\u003cCpuBackend\u003e::new(vec![-5.0f32, -0.0, 3.0, -2.5], 2, 2);\n        let abs_t = t.abs();\n\n        let values = abs_t.ravel().to_vec();\n        assert_eq!(values[0], 5.0);\n        assert_eq!(values[1], 0.0); // -0.0 becomes +0.0\n        assert_eq!(values[2], 3.0);\n        assert_eq!(values[3], 2.5);\n    }\n\n    #[test]\n    fn test_sign_edge_cases() {\n        let t = Tensor2D::\u003cCpuBackend\u003e::new(vec![-3.0f32, -0.0, 0.0, 4.2], 2, 2);\n        let sign_t = t.sign();\n\n        let values = sign_t.ravel().to_vec();\n        assert_eq!(values[0], -1.0);\n        assert_eq!(values[1], -0.0); // sign of -0.0 is -0.0 in IEEE 754\n        assert_eq!(values[2], 0.0);\n        assert_eq!(values[3], 1.0);\n    }\n\n    #[test]\n    fn test_len_returns_row_count() {\n        let t = Tensor2D::\u003cCpuBackend\u003e::zeros(7, 3);\n        let len_scalar = t.len();\n        assert_eq!(len_scalar.to_f64(), 7.0); // Returns ROW count, not total elements\n    }\n\n    #[test]\n    fn test_scale_negative_scalar() {\n        let t = Tensor2D::\u003cCpuBackend\u003e::new(vec![1.0f32, 2.0, 3.0, 4.0], 2, 2);\n        let s = Scalar::\u003cCpuBackend\u003e::new(-2.0);\n        let scaled = t.scale(s);\n\n        let values = scaled.ravel().to_vec();\n        assert_eq!(values, vec![-2.0, -4.0, -6.0, -8.0]);\n    }\n\n    #[test]\n    fn test_add_scalar_negative() {\n        let t = Tensor2D::\u003cCpuBackend\u003e::new(vec![5.0f32, -3.0, 0.0, 2.0], 2, 2);\n        let s = Scalar::\u003cCpuBackend\u003e::new(-10.0);\n        let shifted = t.add_scalar(s);\n\n        let values = shifted.ravel().to_vec();\n        assert_eq!(values, vec![-5.0, -13.0, -10.0, -8.0]);\n    }\n\n    #[test]\n    fn test_maximum_elementwise() {\n        let a = Tensor2D::\u003cCpuBackend\u003e::new(vec![1.0f32, 5.0, -2.0, 0.0], 2, 2);\n        let b = Tensor2D::\u003cCpuBackend\u003e::new(vec![3.0f32, 2.0, -5.0, 4.0], 2, 2);\n        let max_ab = a.maximum(b);\n\n        let values = max_ab.ravel().to_vec();\n        assert_eq!(values, vec![3.0, 5.0, -2.0, 4.0]); // Element-wise max\n    }\n\n    #[test]\n    fn test_exp_numerical_behavior() {\n        let t = Tensor2D::\u003cCpuBackend\u003e::new(vec![0.0f32, 1.0, -1.0, 1000.0], 2, 2);\n        let exp_t = t.exp();\n\n        let values = exp_t.ravel().to_vec();\n        assert!((values[0] - 1.0).abs() \u003c 1e-12); // e^0 = 1\n        assert!((values[1] - std::f64::consts::E).abs() \u003c 1e-6); // e^1 â 2.718\n        assert!((values[2] - 0.367879).abs() \u003c 1e-6); // e^-1 â 0.367879\n        assert!(values[3].is_infinite()); // Overflow to INF\n    }\n\n    #[test]\n    fn test_log_zero_returns_negative_infinity() {\n        let t = Tensor2D::\u003cCpuBackend\u003e::new(vec![0.0f32], 1, 1);\n        let result = t.log();\n        let values = result.ravel().to_vec();\n        assert!(\n            values[0].is_infinite(),\n            \"log(0.0) should return -inf, got {}\",\n            values[0]\n        );\n        assert!(values[0] \u003c 0.0, \"log(0.0) should be negative infinity\");\n    }\n\n    #[test]\n    fn test_log_negative_returns_nan() {\n        let t = Tensor2D::\u003cCpuBackend\u003e::new(vec![-1.0f32, -0.001, -100.0], 1, 3);\n        let result = t.log();\n        let values = result.ravel().to_vec();\n\n        assert!(\n            values[0].is_nan(),\n            \"log(-1.0) should return NaN, got {}\",\n            values[0]\n        );\n        assert!(values[1].is_nan(), \"log(-0.001) should return NaN\");\n        assert!(values[2].is_nan(), \"log(-100.0) should return NaN\");\n    }\n\n    #[test]\n    fn test_log_positive_values() {\n        let t = Tensor2D::\u003cCpuBackend\u003e::new(vec![1.0f32, std::f32::consts::E, 10.0], 1, 3);\n        let result = t.log();\n        let values = result.ravel().to_vec();\n\n        assert!((values[0] - 0.0).abs() \u003c 1e-12, \"log(1.0) = 0.0\");\n        assert!((values[1] - 1.0).abs() \u003c 1e-6, \"log(e) = 1.0\");\n        assert!((values[2] - 2.302585).abs() \u003c 1e-6, \"log(10.0) â 2.302585\");\n    }\n\n    #[test]\n    fn test_log_valid_inputs() {\n        let t = Tensor2D::\u003cCpuBackend\u003e::new(vec![1.0f32, std::f32::consts::E, 10.0], 1, 3);\n        let result = t.log();\n        let values = result.ravel().to_vec();\n\n        assert!((values[0] - 0.0).abs() \u003c 1e-12, \"log(1.0) = 0.0\");\n        assert!((values[1] - 1.0).abs() \u003c 1e-6, \"log(e) = 1.0\");\n        assert!((values[2] - 2.302585).abs() \u003c 1e-6, \"log(10.0) â 2.302585\");\n    }\n\n    #[test]\n    fn test_log_small_positive_values() {\n        // Critical for numerical stability in ML (e.g., log-probabilities)\n        let t = Tensor2D::\u003cCpuBackend\u003e::new(vec![1e-20f32, 1e-10, 1e-5], 1, 3);\n        let result = t.log();\n        let values = result.ravel().to_vec();\n\n        // Should not panic or return NaN for tiny positive values\n        assert!(!values[0].is_nan(), \"log(1e-20) should not be NaN\");\n        assert!(!values[1].is_nan(), \"log(1e-10) should not be NaN\");\n        assert!(!values[2].is_nan(), \"log(1e-5) should not be NaN\");\n\n        // Verify approximate values\n        assert!((values[0] + 46.0517).abs() \u003c 0.1, \"log(1e-20) â -46.05\");\n    }\n\n    #[test]\n    fn test_sigmoid_numerical_stability() {\n        let t = Tensor2D::\u003cCpuBackend\u003e::new(vec![-1000.0f32, 0.0, 1000.0], 1, 3);\n        let sig = t.sigmoid();\n\n        let values = sig.ravel().to_vec();\n        assert!(values[0] \u003c 1e-15); // â0.0 for large negative\n        assert!((values[1] - 0.5).abs() \u003c 1e-12); // Exactly 0.5 at zero\n        assert!(values[2] \u003e 1.0 - 1e-15); // â1.0 for large positive\n    }\n\n    #[test]\n    fn test_ravel_row_major_order() {\n        // 3x2 matrix: [[1,2],\n        //              [3,4],\n        //              [5,6]]\n        let t = Tensor2D::\u003cCpuBackend\u003e::new(vec![1.0f32, 2.0, 3.0, 4.0, 5.0, 6.0], 3, 2);\n        let flat = t.ravel();\n\n        // Row-major flattening: [1,2,3,4,5,6]\n        assert_eq!(flat.to_vec(), vec![1.0, 2.0, 3.0, 4.0, 5.0, 6.0]);\n    }\n\n    #[test]\n    fn test_empty_tensor_constructors() {\n        // Valid empty tensors (0 rows or 0 cols)\n        let t1 = Tensor2D::\u003cCpuBackend\u003e::zeros(0, 5);\n        assert_eq!(t1.shape(), (0, 5));\n\n        let t2 = Tensor2D::\u003cCpuBackend\u003e::zeros(5, 0);\n        assert_eq!(t2.shape(), (5, 0));\n\n        let t3 = Tensor2D::\u003cCpuBackend\u003e::zeros(0, 0);\n        assert_eq!(t3.shape(), (0, 0));\n    }\n\n    #[test]\n    #[should_panic(expected = \"empty\")]\n    fn test_mean_empty_tensor_panics() {\n        let t = Tensor2D::\u003cCpuBackend\u003e::zeros(0, 0);\n        let _ = t.mean(); // Should panic on empty tensor\n    }\n\n    #[test]\n    fn test_clone_semantics() {\n        let original = Tensor2D::\u003cCpuBackend\u003e::new(vec![1.0f32, 2.0, 3.0, 4.0], 2, 2);\n        let clone = original.clone();\n\n        // Modify clone via backend operations (simulated by creating new tensor)\n        let modified = clone.sub(\u0026Tensor2D::\u003cCpuBackend\u003e::zeros(2, 2));\n\n        // Original should remain unchanged\n        assert_eq!(original.ravel().to_vec(), vec![1.0, 2.0, 3.0, 4.0]);\n        assert_eq!(modified.ravel().to_vec(), vec![1.0, 2.0, 3.0, 4.0]); // Still same values\n    }\n\n    #[test]\n    fn test_dot_and_tdot_consistency() {\n        // Verify that dot and tdot produce transposed results\n        let a = Tensor2D::\u003cCpuBackend\u003e::new(\n            vec![1.0f32, 2.0, 3.0, 4.0, 5.0, 6.0], // 2x3 matrix\n            2,\n            3,\n        );\n        let x = Tensor1D::\u003cCpuBackend\u003e::new(vec![1.0f32, 0.0, 0.0]); // Length 3\n\n        // A @ x = [1*1 + 2*0 + 3*0, 4*1 + 5*0 + 6*0] = [1, 4]\n        let y = a.dot(\u0026x);\n        assert_eq!(y.to_vec(), vec![1.0, 4.0]);\n\n        // Aáµ @ y should reconstruct x scaled by row norms (not exact inverse, but dimensionally correct)\n        let x_recon = a.tdot(\u0026y);\n        assert_eq!(x_recon.len(), 3); // Should be length 3 (columns of original)\n    }\n}\n","traces":[{"line":87,"address":[2020848],"length":1,"stats":{"Line":1}},{"line":89,"address":[2020871],"length":1,"stats":{"Line":1}},{"line":109,"address":[2021312],"length":1,"stats":{"Line":1}},{"line":111,"address":[2021335],"length":1,"stats":{"Line":1}},{"line":132,"address":[2020912],"length":1,"stats":{"Line":1}},{"line":134,"address":[2020935],"length":1,"stats":{"Line":1}},{"line":158,"address":[2020976],"length":1,"stats":{"Line":1}},{"line":160,"address":[2020981],"length":1,"stats":{"Line":1}},{"line":197,"address":[2020576],"length":1,"stats":{"Line":1}},{"line":199,"address":[2020599],"length":1,"stats":{"Line":1}},{"line":237,"address":[2021056],"length":1,"stats":{"Line":1}},{"line":239,"address":[2021079],"length":1,"stats":{"Line":1}},{"line":256,"address":[2020512],"length":1,"stats":{"Line":1}},{"line":258,"address":[2020531],"length":1,"stats":{"Line":1}},{"line":282,"address":[2020992],"length":1,"stats":{"Line":1}},{"line":284,"address":[2021011],"length":1,"stats":{"Line":1}},{"line":305,"address":[2020720],"length":1,"stats":{"Line":1}},{"line":307,"address":[2020725],"length":1,"stats":{"Line":1}},{"line":328,"address":[2021216],"length":1,"stats":{"Line":1}},{"line":330,"address":[2021240],"length":1,"stats":{"Line":1}},{"line":351,"address":[2020432],"length":1,"stats":{"Line":1}},{"line":353,"address":[2020456],"length":1,"stats":{"Line":1}},{"line":376,"address":[2021477,2021376],"length":1,"stats":{"Line":1}},{"line":378,"address":[2021404],"length":1,"stats":{"Line":1}},{"line":403,"address":[2020656],"length":1,"stats":{"Line":1}},{"line":405,"address":[2020675],"length":1,"stats":{"Line":1}},{"line":438,"address":[2020784],"length":1,"stats":{"Line":4}},{"line":440,"address":[2020803],"length":1,"stats":{"Line":5}},{"line":474,"address":[2021504],"length":1,"stats":{"Line":1}},{"line":476,"address":[2021523],"length":1,"stats":{"Line":1}},{"line":497,"address":[2021296],"length":1,"stats":{"Line":1}},{"line":498,"address":[2021301],"length":1,"stats":{"Line":1}},{"line":501,"address":[2021136],"length":1,"stats":{"Line":1}},{"line":503,"address":[2021155],"length":1,"stats":{"Line":2}}],"covered":34,"coverable":34},{"path":["/","home","victor","ML","machinelearne-rs","lib","src","backend","tensorlike.rs"],"content":"use crate::backend::scalar::Scalar;\nuse std::marker::PhantomData;\n\nuse super::tensor1d::Tensor1D;\nuse super::Backend;\n\n/// Trait for tensor-like structures supporting element-wise arithmetic operations.\n///\n/// Provides a unified interface for performing element-wise operations, aggregations,\n/// and scalar transformations over tensor implementations.\n///\n/// # Design Rationale\n/// - All binary operations (`sub`, `add`, `mul`, `div`) are performed **element-wise**.\n/// - Aggregation methods return scalar values wrapped in [`Scalar\u003cB\u003e`].\n/// - Backend-generic: parameterized over `B: Backend` to abstract away concrete\n///   computation backends (CPU, GPU, etc.).\n/// - Enables generic algorithms: users can write functions that work with any tensor\n///   dimensionality implementing this trait (e.g., `Tensor1D`, `Tensor2D`).\n///\n/// # Example\n/// ```rust\n/// # use machinelearne_rs::backend::{Scalar, Backend, tensorlike::TensorLike};\n/// fn mean_squared_error\u003cT: TensorLike\u003cB\u003e, B: Backend\u003e(\n///     pred: \u0026T,\n///     target: \u0026T,\n/// ) -\u003e Scalar\u003cB\u003e {\n///     let diff = pred.sub(target);\n///     let sq = diff.mul(\u0026diff);\n///     sq.mean_all()  // Returns mean of squared errors\n/// }\n/// ```\npub trait TensorLike\u003cB: Backend\u003e {\n    /// Element-wise subtraction: `self - other`.\n    fn sub(\u0026self, other: \u0026Self) -\u003e Self;\n\n    /// Element-wise addition: `self + other`.\n    fn add(\u0026self, other: \u0026Self) -\u003e Self;\n\n    /// Element-wise multiplication: `self * other`.\n    fn mul(\u0026self, other: \u0026Self) -\u003e Self;\n\n    /// Element-wise division: `self / other`.\n    fn div(\u0026self, other: \u0026Self) -\u003e Self;\n\n    /// Computes the arithmetic mean across all elements of the tensor.\n    ///\n    /// Returns a scalar value:\n    /// ```text\n    /// mean = (Î£ x_i) / N\n    /// ```\n    fn mean_all(\u0026self) -\u003e Scalar\u003cB\u003e;\n\n    /// Sums all elements of the tensor.\n    ///\n    /// Returns a scalar value:\n    /// ```text\n    /// sum = Î£ x_i\n    /// ```\n    fn sum(\u0026self) -\u003e Scalar\u003cB\u003e;\n\n    /// Scales all elements of the tensor by a scalar value.\n    ///\n    /// Equivalent to element-wise multiplication: `self * scalar`.\n    fn scale(\u0026self, other: Scalar\u003cB\u003e) -\u003e Self;\n}\n\nimpl\u003cB: Backend\u003e TensorLike\u003cB\u003e for Tensor1D\u003cB\u003e {\n    fn sub(\u0026self, other: \u0026Self) -\u003e Self {\n        Self {\n            data: B::sub_1d(\u0026self.data, \u0026other.data),\n            backend: PhantomData,\n        }\n    }\n\n    fn add(\u0026self, other: \u0026Self) -\u003e Self {\n        Self {\n            data: B::add_1d(\u0026self.data, \u0026other.data),\n            backend: PhantomData,\n        }\n    }\n\n    fn mul(\u0026self, other: \u0026Self) -\u003e Self {\n        Self {\n            data: B::mul_1d(\u0026self.data, \u0026other.data),\n            backend: PhantomData,\n        }\n    }\n\n    fn div(\u0026self, other: \u0026Self) -\u003e Self {\n        Self {\n            data: B::div_1d(\u0026self.data, \u0026other.data),\n            backend: PhantomData,\n        }\n    }\n\n    fn mean_all(\u0026self) -\u003e Scalar\u003cB\u003e {\n        Scalar {\n            data: B::mean_all_1d(\u0026self.data),\n            backend: PhantomData,\n        }\n    }\n\n    fn scale(\u0026self, other: Scalar\u003cB\u003e) -\u003e Self {\n        Self {\n            data: B::mul_scalar_1d(\u0026self.data, \u0026other.data),\n            backend: PhantomData,\n        }\n    }\n\n    fn sum(\u0026self) -\u003e Scalar\u003cB\u003e {\n        Scalar {\n            data: B::sum_all_1d(\u0026self.data),\n            backend: PhantomData,\n        }\n    }\n}\n\n#[cfg(test)]\nmod tests {\n    use super::*;\n    use crate::backend::tensor1d::Tensor1D;\n    use crate::backend::CpuBackend;\n\n    /// Tests basic `TensorLike` operations for one-dimensional tensors.\n    ///\n    /// Verifies correctness of:\n    /// - Element-wise arithmetic operations (sub, add, mul, div)\n    /// - Aggregation methods (mean_all, sum)\n    /// - Scalar multiplication (scale)\n    #[test]\n    fn test_tensorlike_for_tensor1d() {\n        let a: Tensor1D\u003cCpuBackend\u003e = Tensor1D::new(vec![4.0f32, 6.0]);\n        let b: Tensor1D\u003cCpuBackend\u003e = Tensor1D::new(vec![1.0f32, 2.0]);\n\n        // sub: [4-1, 6-2] = [3, 4]\n        let diff = a.sub(\u0026b);\n        assert_eq!(diff.to_vec(), vec![3.0, 4.0]);\n\n        // add: [4+1, 6+2] = [5, 8]\n        let sum = a.add(\u0026b);\n        assert_eq!(sum.to_vec(), vec![5.0, 8.0]);\n\n        // mul: [4*1, 6*2] = [4, 12]\n        let prod = a.mul(\u0026b);\n        assert_eq!(prod.to_vec(), vec![4.0, 12.0]);\n\n        // div: [4/1, 6/2] = [4, 3]\n        let quot = a.div(\u0026b);\n        assert_eq!(quot.to_vec(), vec![4.0, 3.0]);\n\n        // mean_all: (4 + 6) / 2 = 5.0\n        let mean = a.mean_all();\n        assert!((mean.data - 5.0).abs() \u003c 1e-12);\n\n        // sum: 4 + 6 = 10.0\n        let total = a.sum();\n        assert_eq!(total.data, 10.0);\n\n        // scale: [4*0.5, 6*0.5] = [2, 3]\n        let s = Scalar::\u003cCpuBackend\u003e::new(0.5);\n        let scaled = a.scale(\u0026s);\n        assert_eq!(scaled.to_vec(), vec![2.0, 3.0]);\n    }\n\n    /// Tests a generic function operating on any type implementing `TensorLike`.\n    ///\n    /// Demonstrates the key advantage of the trait: writing algorithms agnostic\n    /// to tensor dimensionality or concrete backend implementation.\n    #[test]\n    fn test_generic_function_over_tensorlike() {\n        fn mean_squared_error\u003cT: TensorLike\u003cCpuBackend\u003e\u003e(\n            pred: \u0026T,\n            target: \u0026T,\n        ) -\u003e Scalar\u003cCpuBackend\u003e {\n            let diff = pred.sub(target);\n            let sq = diff.mul(\u0026diff);\n            sq.mean_all()\n        }\n\n        let pred = Tensor1D::\u003cCpuBackend\u003e::new(vec![3.0f32, 5.0]);\n        let target = Tensor1D::\u003cCpuBackend\u003e::new(vec![1.0f32, 2.0]);\n\n        // MSE = ((3-1)Â² + (5-2)Â²) / 2 = (4 + 9) / 2 = 6.5\n        let mse = mean_squared_error(\u0026pred, \u0026target);\n        assert!((mse.data - 6.5).abs() \u003c 1e-12);\n    }\n}\n","traces":[{"line":68,"address":[2546432],"length":1,"stats":{"Line":1}},{"line":70,"address":[2546455],"length":1,"stats":{"Line":1}},{"line":75,"address":[2546192],"length":1,"stats":{"Line":1}},{"line":77,"address":[2546215],"length":1,"stats":{"Line":1}},{"line":82,"address":[2546352],"length":1,"stats":{"Line":2}},{"line":84,"address":[2546375],"length":1,"stats":{"Line":2}},{"line":89,"address":[2546272],"length":1,"stats":{"Line":1}},{"line":91,"address":[2546295],"length":1,"stats":{"Line":1}},{"line":96,"address":[2546528],"length":1,"stats":{"Line":1}},{"line":98,"address":[2546533],"length":1,"stats":{"Line":1}},{"line":103,"address":[],"length":0,"stats":{"Line":0}},{"line":105,"address":[],"length":0,"stats":{"Line":0}},{"line":110,"address":[2546512],"length":1,"stats":{"Line":1}},{"line":112,"address":[2546517],"length":1,"stats":{"Line":1}}],"covered":12,"coverable":14},{"path":["/","home","victor","ML","machinelearne-rs","lib","src","dataset","memory.rs"],"content":"//! In-memory dataset implementation for tabular data.\n//!\n//! Provides [`InMemoryDataset`], a simple dataset backed by `Vec\u003cVec\u003cf32\u003e\u003e` for features\n//! and `Vec\u003cf32\u003e` for targets. Suitable for small-to-medium datasets that fit entirely in RAM.\n//!\n//! # Design Philosophy\n//!\n//! Following the separation of concerns principle (see ADR: *separate-trainer-losses*):\n//! - Datasets are **pure data containers** without training logic or hyperparameters\n//! - Training loops, optimizers, and loss functions live in separate components\n//! - Fitted models contain only inference parameters (`predict` method), not training state\n//!\n//! # Example\n//!\n//! ```rust\n//! # use machinelearne_rs::dataset::memory::InMemoryDataset;\n//! # use machinelearne_rs::backend::CpuBackend;\n//! # use machinelearne_rs::dataset::Dataset;\n//! # fn main() {\n//! // Create dataset: 3 samples Ã 2 features\n//! let x = vec![\n//!     vec![1.0, 0.0],\n//!     vec![0.0, 1.0],\n//!     vec![1.0, 1.0],\n//! ];\n//! let y = vec![1.0, 0.0, 1.0];\n//!\n//! let dataset = InMemoryDataset::new(x, y).unwrap();\n//!\n//! // Iterate over batches\n//! for batch in dataset.batches::\u003cCpuBackend\u003e(2) {\n//!     let (x_batch, y_batch) = batch.unwrap();\n//!     // ... use tensors for training/inference\n//! }\n//! # }\n//! ```\n\nuse crate::backend::{Backend, Tensor1D, Tensor2D};\nuse crate::dataset::Dataset;\nuse std::ops::Range;\n\n/// An in-memory dataset storing tabular data as nested vectors.\n///\n/// Holds features `X` as `Vec\u003cVec\u003cf32\u003e\u003e` (rows Ã features) and targets `y` as `Vec\u003cf32\u003e`.\n/// Validates structural invariants at construction time:\n/// - Equal length of `X` and `y`\n/// - Non-empty dataset\n/// - Uniform feature dimensionality across all samples\n///\n/// # Memory Layout\n///\n/// Data is stored in row-major order (each inner `Vec` is a sample). When converted\n/// to tensors via [`get_batch`], data is flattened into contiguous column-major layout\n/// required by most ML backends.\n///\n/// # Thread Safety\n///\n/// `InMemoryDataset` is `Send + Sync` (via inherent `Vec` properties) and can be\n/// safely shared across threads for read-only access during training.\n#[derive(Debug, Clone)]\npub struct InMemoryDataset {\n    /// Feature matrix: outer vector = samples, inner vector = features per sample.\n    x: Vec\u003cVec\u003cf32\u003e\u003e,\n    /// Target vector: one value per sample.\n    y: Vec\u003cf32\u003e,\n}\n\nimpl InMemoryDataset {\n    /// Constructs a validated in-memory dataset from feature and target vectors.\n    ///\n    /// # Validation\n    ///\n    /// Returns `Err` if any of the following conditions are violated:\n    /// - `x.len() != y.len()` â mismatched sample counts\n    /// - `x.is_empty()` â empty dataset (no samples)\n    /// - Non-uniform feature dimensions â rows in `x` have different lengths\n    ///\n    /// # Parameters\n    ///\n    /// - `x`: Feature matrix where each inner vector represents one sample's features\n    /// - `y`: Target values corresponding to each sample in `x`\n    ///\n    /// # Returns\n    ///\n    /// - `Ok(InMemoryDataset)` â validated dataset ready for training/inference\n    /// - `Err(String)` â descriptive validation error\n    ///\n    /// # Example\n    ///\n    /// ```rust\n    /// use machinelearne_rs::dataset::memory::InMemoryDataset;\n    ///\n    /// // Valid dataset: 2 samples Ã 2 features\n    /// let x = vec![vec![1.0, 2.0], vec![3.0, 4.0]];\n    /// let y = vec![0.0, 1.0];\n    /// let dataset = InMemoryDataset::new(x, y).unwrap();\n    ///\n    /// // Invalid: mismatched lengths\n    /// let x = vec![vec![1.0]];\n    /// let y = vec![0.0, 1.0];\n    /// assert!(InMemoryDataset::new(x, y).is_err());\n    /// ```\n    pub fn new(x: Vec\u003cVec\u003cf32\u003e\u003e, y: Vec\u003cf32\u003e) -\u003e Result\u003cSelf, String\u003e {\n        if x.len() != y.len() {\n            return Err(\"x and y must have same length\".into());\n        }\n        if x.is_empty() {\n            return Err(\"Dataset is empty\".into());\n        }\n        let n_features = x[0].len();\n        if !x.iter().all(|row| row.len() == n_features) {\n            return Err(\"All rows must have the same number of features\".into());\n        }\n        Ok(Self { x, y })\n    }\n}\n\nimpl Dataset for InMemoryDataset {\n    /// Error type for data access operations.\n    ///\n    /// Uses [`Infallible`](std::convert::Infallible) because:\n    /// - Structural validation happens at construction time\n    /// - Range checks are handled by slice indexing (panics on OOB, not recoverable errors)\n    /// - No I/O operations that could fail at runtime\n    type Error = std::convert::Infallible;\n\n    /// Type of a single dataset item: `(feature_vector, target)`.\n    type Item = (Vec\u003cf32\u003e, f32);\n\n    /// Returns the exact number of samples in the dataset.\n    ///\n    /// Always `Some(n)` since in-memory datasets have known size.\n    fn len(\u0026self) -\u003e Option\u003cusize\u003e {\n        Some(self.x.len())\n    }\n\n    /// Loads a contiguous range of samples as backend-specific tensors.\n    ///\n    /// # Parameters\n    ///\n    /// - `range`: Sample index range `[start, end)` (half-open interval)\n    ///\n    /// # Returns\n    ///\n    /// - `Ok((X, y))` where:\n    ///   - `X` is a `(batch_size, n_features)` tensor in column-major layout\n    ///   - `y` is a `(batch_size,)` tensor\n    ///\n    /// # Panics\n    ///\n    /// Panics if `range` is out of bounds (caller should ensure valid ranges via `len()`).\n    /// This is intentional: boundary checks belong to the iterator layer ([`DatasetBatchIter`]),\n    /// not the dataset implementation itself.\n    ///\n    /// # Implementation Notes\n    ///\n    /// - Flattens row-major `Vec\u003cVec\u003cf32\u003e\u003e` into contiguous column-major buffer\n    /// - Preserves backend abstraction: tensors are constructed generically for any `B: Backend`\n    fn get_batch\u003cB: Backend\u003e(\n        \u0026self,\n        range: Range\u003cusize\u003e,\n    ) -\u003e Result\u003c(Tensor2D\u003cB\u003e, Tensor1D\u003cB\u003e), Self::Error\u003e {\n        let batch_x = \u0026self.x[range.clone()];\n        let batch_y = \u0026self.y[range];\n\n        let batch_size = batch_x.len();\n        let n_features = batch_x[0].len();\n\n        let data = batch_x.iter().flat_map(|row| row.iter()).copied().collect();\n        let x_tensor = Tensor2D::\u003cB\u003e::new(data, batch_size, n_features);\n\n        let y_tensor = Tensor1D::\u003cB\u003e::new(batch_y.to_vec());\n\n        Ok((x_tensor, y_tensor))\n    }\n}\n\n#[cfg(test)]\nmod tests {\n    use super::*;\n    use crate::backend::CpuBackend;\n\n    #[test]\n    fn test_in_memory_dataset_new_success() {\n        let x = vec![vec![1.0, 2.0], vec![3.0, 4.0]];\n        let y = vec![0.0, 1.0];\n        let dataset = InMemoryDataset::new(x, y);\n        assert!(dataset.is_ok());\n    }\n\n    #[test]\n    fn test_in_memory_dataset_new_mismatched_lengths() {\n        let x = vec![vec![1.0, 2.0]];\n        let y = vec![0.0, 1.0];\n        let dataset = InMemoryDataset::new(x, y);\n        assert!(dataset.is_err());\n        assert_eq!(dataset.unwrap_err(), \"x and y must have same length\");\n    }\n\n    #[test]\n    fn test_in_memory_dataset_new_empty() {\n        let x = vec![];\n        let y = vec![];\n        let dataset = InMemoryDataset::new(x, y);\n        assert!(dataset.is_err());\n        assert_eq!(dataset.unwrap_err(), \"Dataset is empty\");\n    }\n\n    #[test]\n    fn test_in_memory_dataset_new_uneven_rows() {\n        let x = vec![vec![1.0, 2.0], vec![3.0]]; // second row shorter\n        let y = vec![0.0, 1.0];\n        let dataset = InMemoryDataset::new(x, y);\n        assert!(dataset.is_err());\n        assert_eq!(\n            dataset.unwrap_err(),\n            \"All rows must have the same number of features\"\n        );\n    }\n\n    #[test]\n    fn test_in_memory_dataset_len() {\n        let x = vec![vec![1.0], vec![2.0]];\n        let y = vec![0.0, 1.0];\n        let dataset = InMemoryDataset::new(x, y).unwrap();\n        assert_eq!(dataset.len(), Some(2));\n    }\n\n    #[test]\n    fn test_in_memory_dataset_batches_integration() {\n        let x = vec![vec![1.0, 0.0], vec![0.0, 1.0], vec![1.0, 1.0]];\n        let y = vec![1.0, 0.0, 1.0];\n        let dataset = InMemoryDataset::new(x, y).unwrap();\n\n        let mut batches = dataset.batches::\u003cCpuBackend\u003e(2);\n        let batch1 = batches.next().unwrap().unwrap();\n        assert_eq!(batch1.0.shape(), (2, 2));\n        assert_eq!(batch1.1.to_vec(), vec![1.0, 0.0]);\n\n        let batch2 = batches.next().unwrap().unwrap();\n        assert_eq!(batch2.0.shape(), (1, 2));\n        assert_eq!(batch2.1.to_vec(), vec![1.0]);\n\n        assert!(batches.next().is_none());\n    }\n}\n","traces":[{"line":103,"address":[3083040,3083904],"length":1,"stats":{"Line":2}},{"line":104,"address":[3083129,3083075],"length":1,"stats":{"Line":3}},{"line":105,"address":[3083804,3083172],"length":1,"stats":{"Line":2}},{"line":107,"address":[3083161,3083210],"length":1,"stats":{"Line":3}},{"line":108,"address":[3083748,3083244],"length":1,"stats":{"Line":2}},{"line":110,"address":[3083221,3083286],"length":1,"stats":{"Line":3}},{"line":111,"address":[1838992,1839024],"length":1,"stats":{"Line":7}},{"line":112,"address":[3083423,3083666],"length":1,"stats":{"Line":2}},{"line":114,"address":[3083475],"length":1,"stats":{"Line":1}},{"line":133,"address":[3083008],"length":1,"stats":{"Line":1}},{"line":134,"address":[3083013],"length":1,"stats":{"Line":1}},{"line":159,"address":[1838931,1838368,1838925],"length":1,"stats":{"Line":1}},{"line":163,"address":[1838408],"length":1,"stats":{"Line":1}},{"line":164,"address":[1838469],"length":1,"stats":{"Line":1}},{"line":166,"address":[1838532],"length":1,"stats":{"Line":1}},{"line":167,"address":[1838709,1838540],"length":1,"stats":{"Line":1}},{"line":169,"address":[1838969,1838584,1838944],"length":1,"stats":{"Line":3}},{"line":170,"address":[1838674],"length":1,"stats":{"Line":1}},{"line":172,"address":[1838697,1838782],"length":1,"stats":{"Line":2}},{"line":174,"address":[1838789],"length":1,"stats":{"Line":1}}],"covered":20,"coverable":20},{"path":["/","home","victor","ML","machinelearne-rs","lib","src","dataset","mod.rs"],"content":"//! Dataset abstractions for machine learning workloads.\n//!\n//! This module provides a generic [`Dataset`] trait for uniform access to training data\n//! and a [`DatasetBatchIter`] iterator for efficient batch loading across different backends.\n//!\n//! # Core Concepts\n//!\n//! - **Dataset** â A source of `(X, y)` pairs where `X` is a feature matrix of shape `(n_samples, n_features)`\n//!   and `y` is a target vector of shape `(n_samples,)`.\n//! - **Backend** â Tensor implementation (`CPU`, `CUDA`, etc.) defined by the [`Backend`] trait.\n//! - **Batch** â A contiguous subset of samples for mini-batch gradient descent.\n//!\n//! # Example\n//!\n//! ```rust\n//! use machinelearne_rs::dataset::{Dataset, InMemoryDataset};\n//! use machinelearne_rs::backend::CpuBackend;\n//!\n//! // Create an in-memory dataset: 2 samples Ã 2 features\n//! let x = vec![vec![1.0], vec![2.0]];\n//! let y = vec![0.0, 1.0];\n//! let dataset = InMemoryDataset::new(x, y).unwrap();\n//!\n//! // Iterate over batches of size 1\n//! for batch in dataset.batches::\u003cCpuBackend\u003e(1) {\n//!     let (x_batch, y_batch) = batch.unwrap();\n//!     // ... train your model\n//! }\n//! ```\n\nuse crate::backend::{Backend, Tensor1D, Tensor2D};\nuse std::{fmt::Debug, ops::Range};\n\npub mod memory;\npub use self::memory::InMemoryDataset;\n\n/// Abstract interface for a machine learning dataset.\n///\n/// Defines a contract for loading data in `(X, y)` format where:\n/// - `X` â Feature matrix with shape `(n_samples, n_features)`\n/// - `y` â Target vector with shape `(n_samples,)`\n///\n/// # Associated Types\n///\n/// - `Error` â Error type returned when accessing data (must implement [`Debug`])\n/// - `Item` â Type of a single dataset item (rarely used directly; often `()` as a placeholder)\n///\n/// # Example Implementation\n///\n/// ```rust\n/// use machinelearne_rs::dataset::Dataset;\n/// use machinelearne_rs::backend::{Backend, Tensor1D, Tensor2D};\n/// use std::ops::Range;\n///\n/// struct MyDataset { /* ... */ }\n///\n/// impl Dataset for MyDataset {\n///     type Error = String;\n///     type Item = ();\n///\n///     fn len(\u0026self) -\u003e Option\u003cusize\u003e {\n///         Some(1000) // or None if size is unknown (e.g., streaming data)\n///     }\n///\n///     fn get_batch\u003cB: Backend\u003e(\n///         \u0026self,\n///         range: Range\u003cusize\u003e,\n///     ) -\u003e Result\u003c(Tensor2D\u003cB\u003e, Tensor1D\u003cB\u003e), Self::Error\u003e {\n///         // Implement range-based data loading\n///         # unimplemented!()\n///     }\n/// }\n/// ```\npub trait Dataset {\n    /// Error type returned when accessing data.\n    type Error: Debug + 'static;\n\n    /// Type of a single dataset item (typically unused directly).\n    type Item: ?Sized;\n\n    /// Returns the total number of samples in the dataset, if known.\n    ///\n    /// # Returns\n    ///\n    /// - `Some(n)` â Exact number of samples\n    /// - `None` â Size is unknown (e.g., infinite streams, lazy-loaded sources)\n    ///\n    /// # Example\n    ///\n    /// ```rust\n    /// # use machinelearne_rs::dataset::Dataset;\n    /// # use machinelearne_rs::backend::{Backend, Tensor1D, Tensor2D};\n    /// # struct MyDataset;\n    /// # impl Dataset for MyDataset {\n    /// #     type Error = ();\n    /// #     type Item = ();\n    /// #     fn len(\u0026self) -\u003e Option\u003cusize\u003e { Some(42) }\n    /// #     fn get_batch\u003cB: machinelearne_rs::backend::Backend\u003e(\u0026self, _: std::ops::Range\u003cusize\u003e) -\u003e Result\u003c(Tensor2D\u003cB\u003e, Tensor1D\u003cB\u003e), Self::Error\u003e { unimplemented!() }\n    /// # }\n    /// let ds = MyDataset;\n    /// assert_eq!(ds.len(), Some(42));\n    /// ```\n    fn len(\u0026self) -\u003e Option\u003cusize\u003e;\n\n    /// Checks whether the dataset is empty.\n    ///\n    /// Default implementation checks if `len() == Some(0)`.\n    ///\n    /// # Example\n    ///\n    /// ```rust\n    /// # use machinelearne_rs::dataset::Dataset;\n    /// # use machinelearne_rs::backend::{Backend, Tensor1D, Tensor2D};\n    /// # struct EmptyDataset;\n    /// # impl Dataset for EmptyDataset {\n    /// #     type Error = ();\n    /// #     type Item = ();\n    /// #     fn len(\u0026self) -\u003e Option\u003cusize\u003e { Some(0) }\n    /// #     fn get_batch\u003cB: Backend\u003e(\u0026self, _: std::ops::Range\u003cusize\u003e) -\u003e Result\u003c(Tensor2D\u003cB\u003e, Tensor1D\u003cB\u003e), Self::Error\u003e { unimplemented!() }\n    /// # }\n    /// let ds = EmptyDataset;\n    /// assert!(ds.is_empty());\n    /// ```\n    fn is_empty(\u0026self) -\u003e bool {\n        self.len() == Some(0)\n    }\n\n    /// Creates an iterator over fixed-size batches.\n    ///\n    /// # Parameters\n    ///\n    /// - `batch_size` â Desired batch size (last batch may be smaller)\n    ///\n    /// # Behavior\n    ///\n    /// - Returns batches of size `batch_size`, except possibly the last one\n    /// - Iterator yields `Result` to propagate errors from [`get_batch`]\n    /// - Requires `Sized` bound because the iterator holds a reference to `self`\n    ///\n    /// # Example\n    ///\n    /// ```rust\n    /// # use machinelearne_rs::dataset::{Dataset, InMemoryDataset};\n    /// # use machinelearne_rs::backend::CpuBackend;\n    /// let x = vec![vec![1.0], vec![2.0]];\n    /// let y = vec![0.0, 1.0];\n    /// let ds = InMemoryDataset::new(x, y).unwrap();\n    ///\n    /// let batches: Vec\u003c_\u003e = ds.batches::\u003cCpuBackend\u003e(1).collect();\n    /// assert_eq!(batches.len(), 2); // 2 batches of 1 sample each\n    /// ```\n    fn batches\u003c'a, B: Backend\u003e(\u0026'a self, batch_size: usize) -\u003e DatasetBatchIter\u003c'a, B, Self\u003e\n    where\n        Self: Sized,\n    {\n        DatasetBatchIter {\n            dataset: self,\n            batch_size,\n            current: 0,\n            _backend: std::marker::PhantomData,\n        }\n    }\n\n    /// Loads a subset of data as tensors for the given index range.\n    ///\n    /// # Parameters\n    ///\n    /// - `range` â Sample index range `[start, end)`\n    ///\n    /// # Returns\n    ///\n    /// - `Ok((X, y))` â Feature matrix `(n, n_features)` and target vector `(n,)`\n    /// - `Err(e)` â Data access error (e.g., out-of-bounds access)\n    ///\n    /// # Panics\n    ///\n    /// Does not panic if implemented correctly. Boundary checks are the implementor's responsibility.\n    ///\n    /// # Example\n    ///\n    /// ```rust\n    /// # use machinelearne_rs::dataset::Dataset;\n    /// # use machinelearne_rs::backend::CpuBackend;\n    /// # use machinelearne_rs::backend::{Backend, Tensor1D, Tensor2D};\n    /// # struct MyDataset;\n    /// # impl Dataset for MyDataset {\n    /// #     type Error = String;\n    /// #     type Item = ();\n    /// #     fn len(\u0026self) -\u003e Option\u003cusize\u003e { Some(10) }\n    /// #     fn get_batch\u003cB: Backend\u003e(\u0026self, range: std::ops::Range\u003cusize\u003e) -\u003e Result\u003c(Tensor2D\u003cB\u003e, Tensor1D\u003cB\u003e), Self::Error\u003e {\n    /// #         Ok((Tensor2D::new(vec![0.0; (range.end - range.start) * 2], range.end - range.start, 2), Tensor1D::new(vec![0.0; range.end - range.start])))\n    /// #     }\n    /// # }\n    /// let ds = MyDataset;\n    /// let (x, y) = ds.get_batch::\u003cCpuBackend\u003e(0..5).unwrap();\n    /// assert_eq!(x.shape().0, 5); // 5 samples\n    /// ```\n    fn get_batch\u003cB: Backend\u003e(\n        \u0026self,\n        range: Range\u003cusize\u003e,\n    ) -\u003e Result\u003c(Tensor2D\u003cB\u003e, Tensor1D\u003cB\u003e), Self::Error\u003e;\n}\n\n/// Iterator over dataset batches.\n///\n/// Created by [`Dataset::batches`], yields consecutive batches of fixed size\n/// (last batch may be smaller than requested).\n///\n/// # Type Parameters\n///\n/// - `'a` â Lifetime of the reference to the dataset\n/// - `B` â Backend type for tensors (implements [`Backend`])\n/// - `D` â Dataset type (implements [`Dataset`])\n///\n/// # Characteristics\n///\n/// - **Lazy loading**: Data is fetched only when `next()` is called\n/// - **Error propagation**: Returns `Result` to forward errors from `get_batch`\n/// - **Partial batches**: Last batch may contain fewer samples than `batch_size`\npub struct DatasetBatchIter\u003c'a, B: Backend, D: ?Sized\u003e {\n    /// Reference to the source dataset.\n    dataset: \u0026'a D,\n    /// Desired batch size (actual size may be smaller for the last batch).\n    batch_size: usize,\n    /// Current position in the dataset (index of next sample to yield).\n    current: usize,\n    /// Phantom marker for the backend type parameter.\n    _backend: std::marker::PhantomData\u003cB\u003e,\n}\n\nimpl\u003c'a, B: Backend, D: Dataset\u003e Iterator for DatasetBatchIter\u003c'a, B, D\u003e {\n    /// Iterator item type: `Result` containing tensor pair or dataset error.\n    type Item = Result\u003c(Tensor2D\u003cB\u003e, Tensor1D\u003cB\u003e), D::Error\u003e;\n\n    /// Returns the next batch or `None` if all samples have been consumed.\n    ///\n    /// # Algorithm\n    ///\n    /// 1. Checks dataset size via `len()`; returns `None` if unknown\n    /// 2. Returns `None` if `current \u003e= total_samples`\n    /// 3. Computes range `[current, min(current + batch_size, total))`\n    /// 4. Calls `dataset.get_batch(range)` and returns the result wrapped in `Some`\n    ///\n    /// # Error Handling\n    ///\n    /// Errors from `get_batch` are propagated as `Some(Err(e))`. The iterator\n    /// terminates only when all samples are consumed (`None`).\n    fn next(\u0026mut self) -\u003e Option\u003cSelf::Item\u003e {\n        let total = self.dataset.len()?;\n        if self.current \u003e= total {\n            return None;\n        }\n\n        let end = (self.current + self.batch_size).min(total);\n        let range = self.current..end;\n        self.current = end;\n\n        // Fetch subset and convert to tensors\n        match self.dataset.get_batch::\u003cB\u003e(range) {\n            Ok((x, y)) =\u003e Some(Ok((x, y))),\n            Err(e) =\u003e Some(Err(e)),\n        }\n    }\n}\n\n#[cfg(test)]\nmod tests {\n    use super::*;\n    use crate::backend::CpuBackend;\n    use std::ops::Range;\n\n    // Mock dataset for iterator logic testing\n    struct MockDataset {\n        len: usize,\n    }\n\n    impl Dataset for MockDataset {\n        type Error = \u0026'static str;\n        type Item = ();\n\n        fn len(\u0026self) -\u003e Option\u003cusize\u003e {\n            Some(self.len)\n        }\n\n        fn get_batch\u003cB: Backend\u003e(\n            \u0026self,\n            range: Range\u003cusize\u003e,\n        ) -\u003e Result\u003c(Tensor2D\u003cB\u003e, Tensor1D\u003cB\u003e), Self::Error\u003e {\n            if range.start \u003e= self.len || range.end \u003e self.len {\n                return Err(\"range out of bounds\");\n            }\n\n            let n = range.len();\n            let start = range.start;\n\n            // X: (n, 2) â unique values per sample: [start*2, start*2+1, ...]\n            let x_data: Vec\u003cf32\u003e = (0..n * 2).map(|i| (start * 2 + i) as f32).collect();\n            let x = Tensor2D::\u003cB\u003e::new(x_data, n, 2);\n\n            // y: (n,) â sequential values starting from `start`\n            let y_data: Vec\u003cf32\u003e = (start..range.end).map(|i| i as f32).collect();\n            let y = Tensor1D::\u003cB\u003e::new(y_data);\n\n            Ok((x, y))\n        }\n    }\n\n    #[test]\n    fn test_dataset_is_empty() {\n        let empty = MockDataset { len: 0 };\n        assert!(empty.is_empty());\n\n        let non_empty = MockDataset { len: 1 };\n        assert!(!non_empty.is_empty());\n    }\n\n    #[test]\n    fn test_batches_full() {\n        let dataset = MockDataset { len: 6 };\n        let mut iter = dataset.batches::\u003cCpuBackend\u003e(2);\n\n        // Should yield 3 full batches\n        for i in 0..3 {\n            let batch = iter.next().unwrap().unwrap();\n            let (x, y) = batch;\n            assert_eq!(x.shape(), (2, 2));\n            assert_eq!(y.to_vec(), vec![i as f64 * 2.0, i as f64 * 2.0 + 1.0]);\n        }\n        assert!(iter.next().is_none());\n    }\n\n    #[test]\n    fn test_batches_partial_last() {\n        let dataset = MockDataset { len: 5 };\n        let mut iter = dataset.batches::\u003cCpuBackend\u003e(2);\n\n        // 2 full batches + 1 partial batch\n        assert_eq!(iter.next().unwrap().unwrap().0.shape(), (2, 2));\n        assert_eq!(iter.next().unwrap().unwrap().0.shape(), (2, 2));\n        assert_eq!(iter.next().unwrap().unwrap().0.shape(), (1, 2));\n        assert!(iter.next().is_none());\n    }\n\n    #[test]\n    fn test_batches_larger_than_dataset() {\n        let dataset = MockDataset { len: 3 };\n        let mut iter = dataset.batches::\u003cCpuBackend\u003e(10);\n\n        // Single batch covering entire dataset\n        let batch = iter.next().unwrap().unwrap();\n        assert_eq!(batch.0.shape(), (3, 2));\n        assert!(iter.next().is_none());\n    }\n\n    #[test]\n    fn test_batches_empty_dataset() {\n        let dataset = MockDataset { len: 0 };\n        let mut iter = dataset.batches::\u003cCpuBackend\u003e(2);\n        assert!(iter.next().is_none());\n    }\n}\n","traces":[{"line":124,"address":[1839088],"length":1,"stats":{"Line":1}},{"line":125,"address":[1839097],"length":1,"stats":{"Line":1}},{"line":152,"address":[1839056],"length":1,"stats":{"Line":2}},{"line":248,"address":[2768928,2768320,2769376],"length":1,"stats":{"Line":3}},{"line":249,"address":[2768958,2769406,2768350],"length":1,"stats":{"Line":2}},{"line":250,"address":[2769485,2769037,2768429],"length":1,"stats":{"Line":3}},{"line":251,"address":[2768471,2769075,2769528],"length":1,"stats":{"Line":2}},{"line":254,"address":[2769556,2769496,2769163,2769654,2768593,2768440,2769102,2768498,2769048],"length":1,"stats":{"Line":4}},{"line":255,"address":[2768520,2769124,2769578],"length":1,"stats":{"Line":3}},{"line":256,"address":[2769598,2768540,2769144],"length":1,"stats":{"Line":2}},{"line":259,"address":[2769148,2768544,2769602],"length":1,"stats":{"Line":3}},{"line":260,"address":[2768694,2769800,2769176],"length":1,"stats":{"Line":2}},{"line":261,"address":[2769672,2768611],"length":1,"stats":{"Line":0}}],"covered":12,"coverable":13},{"path":["/","home","victor","ML","machinelearne-rs","lib","src","lib.rs"],"content":"//! # rust-ml\n//!\n//! A type-safe machine learning library in Rust with pluggable backends and strict\n//! separation between training and inference phases.\n//!\n//! ## Core Design Principles\n//!\n//! - **Stateful Type Safety**: Models carry their training state in the type system\n//!   (`Unfitted` vs `Fitted`), preventing invalid operations at compile time.\n//! - **Training/Inference Separation**: Trained models contain only prediction parameters;\n//!   training logic lives in separate components (losses, optimizers, trainers).\n//! - **Backend Agnosticism**: Abstract `Backend` trait enables CPU/GPU implementations\n//!   without changing model code.\n//! - **Zero-Cost Abstractions**: Generics and traits provide flexibility without runtime overhead.\n//!\n//! ## Quick Start\n//!\n//! ```rust\n//! use machinelearne_rs::backend::CpuBackend;\n//! use machinelearne_rs::model::linear::{LinearModel, Unfitted};\n//! use machinelearne_rs::loss::MSELoss;\n//! use machinelearne_rs::optimizer::SGD;\n//!\n//! // Create an untrained linear model (1 input feature)\n//! let mut model = LinearModel::\u003cCpuBackend, Unfitted\u003e::new(1);\n//!\n//! // Training loop (simplified)\n//! // for epoch in 0..100 {\n//! //     let pred = model.forward(\u0026x_tensor);\n//! //     let grad = MSELoss::grad_wrt_prediction(\u0026pred, \u0026y_tensor);\n//! //     let grads = model.backward(\u0026x_tensor, \u0026grad);\n//! //     let new_params = SGD::new(0.01).step(model.params(), \u0026grads);\n//! //     model.update_params(\u0026new_params);\n//! // }\n//!\n//! // Convert to inference-optimized model\n//! // let fitted = model.into_fitted();\n//! // let prediction = fitted.predict(\u0026input_tensor);\n//! ```\n//!\n//! ## Module Structure\n//!\n//! - `backend` â Tensor abstractions and computation primitives (`Tensor1D`, `Tensor2D`)\n//! - `model` â ML model implementations with stateful type parameters\n//! - `loss` â Differentiable loss functions (MSE, CrossEntropy, etc.)\n//! - `optimizer` â Parameter update algorithms (SGD, Adam)\n//! - `trainer` â High-level training loop orchestration\n//! - `regularizers` â Weight regularization strategies (L1, L2)\n//! - `dataset` â Data loading and preprocessing utilities\n//! - `serialization` â Model persistence formats\n//!\n//! ## Example Projects\n//!\n//! See the `examples/` directory for complete training pipelines demonstrating:\n//! - Linear regression with SGD\n//! - Regularization techniques\n//! - Custom backend integration\n//! - Model serialization workflows\n\npub mod backend;\n\n/// Data loading utilities and dataset abstractions.\npub mod dataset;\n\n/// Data preprocessing transformers for ML pipelines.\npub mod preprocessing;\n\n/// Differentiable loss functions for model training.\npub mod loss;\n\n/// Machine learning models with compile-time state safety.\npub mod model;\n\n/// Optimization algorithms for parameter updates.\npub mod optimizer;\n\n/// Weight regularization strategies to prevent overfitting.\npub mod regularizers;\n\n/// Model persistence and format conversion utilities.\npub mod serialization;\n\n/// High-level training loop orchestration.\npub mod trainer;\n\n/// Re-export of core backend types for convenient usage.\npub use backend::{Backend, CpuBackend, ScalarOps, Tensor1D, Tensor2D};\n\n#[cfg(test)]\nmod tests {\n    use super::*;\n    use crate::backend::CpuBackend;\n    use crate::loss::{Loss, MSELoss};\n    use crate::model::linear::{InferenceModel, LinearModel, TrainableModel, Unfitted};\n    use crate::optimizer::{Optimizer, SGD};\n\n    // Helper function to create (n, 1) matrix from column data\n    fn col_to_tensor2d\u003cB: Backend\u003e(col: \u0026[f32]) -\u003e Tensor2D\u003cB\u003e {\n        let n = col.len();\n        let mut data = vec![0.0; n];\n        data.copy_from_slice(col);\n        Tensor2D::\u003cB\u003e::new(data, n, 1)\n    }\n\n    fn slice_to_tensor1d\u003cB: Backend\u003e(slice: \u0026[f32]) -\u003e Tensor1D\u003cB\u003e {\n        Tensor1D::\u003cB\u003e::new(slice.to_vec())\n    }\n\n    #[test]\n    fn test_linear_regression_identity() {\n        // y = x\n        let x_data: \u0026[f32; 4] = \u0026[1.0, 2.0, 3.0, 4.0];\n        let y_data: \u0026[f32; 4] = \u0026[1.0, 2.0, 3.0, 4.0];\n\n        let x_tensor = col_to_tensor2d(x_data);\n        let y_tensor = slice_to_tensor1d(y_data);\n\n        let mut model = LinearModel::\u003cCpuBackend, Unfitted\u003e::new(1);\n        let loss_fn = MSELoss;\n        let optimizer = SGD::new(0.01);\n\n        for epoch in 0..200 {\n            let pred = model.forward(\u0026x_tensor);\n            let grad_pred = Loss::\u003cCpuBackend\u003e::grad_wrt_prediction(\u0026loss_fn, \u0026pred, \u0026y_tensor);\n            let grads = model.backward(\u0026x_tensor, \u0026grad_pred);\n            let new_params = optimizer.step(\u0026model.params(), \u0026grads);\n            model.update_params(\u0026new_params);\n            if epoch % 5 == 0 {\n                let loss_val = Loss::\u003cCpuBackend\u003e::loss(\u0026loss_fn, \u0026pred, \u0026y_tensor);\n                let w = \u0026model.params().weights.to_vec()[0];\n                let b = model.params().bias.data.to_f64();\n                println!(\n                    \"Epoch {}: loss={:.6}, w={:.4}, b={:.4}, pred={:?}\",\n                    epoch,\n                    loss_val.data.to_f64(),\n                    w,\n                    b,\n                    pred.to_vec()\n                );\n            }\n        }\n\n        let fitted = model.into_fitted();\n        let inp = slice_to_tensor1d::\u003cCpuBackend\u003e(\u0026[2.5]);\n        let pred = fitted.predict(\u0026inp).data.to_f64();\n        assert!((pred - 2.5).abs() \u003c 0.1, \"Expected ~2.5, got {}\", pred);\n    }\n\n    #[test]\n    fn test_linear_regression_with_bias() {\n        // y = 2*x + 1\n        let x_data = \u0026[0.0, 1.0, 2.0, 3.0];\n        let y_data = \u0026[1.0, 3.0, 5.0, 7.0];\n\n        let x_tensor = col_to_tensor2d(x_data);\n        let y_tensor = slice_to_tensor1d(y_data);\n\n        let mut model = LinearModel::\u003cCpuBackend, Unfitted\u003e::new(1);\n\n        let loss_fn = MSELoss;\n        let optimizer = SGD::new(0.01);\n\n        for _ in 0..3000 {\n            let pred = model.forward(\u0026x_tensor);\n            let grad_pred = Loss::\u003cCpuBackend\u003e::grad_wrt_prediction(\u0026loss_fn, \u0026pred, \u0026y_tensor);\n            let grads = model.backward(\u0026x_tensor, \u0026grad_pred);\n            let new_params = optimizer.step(model.params(), \u0026grads);\n            model.update_params(\u0026new_params);\n        }\n\n        let fitted = model.into_fitted();\n        let p0 = fitted.predict(\u0026slice_to_tensor1d::\u003cCpuBackend\u003e(\u0026[0.0]));\n        let p1 = fitted.predict(\u0026slice_to_tensor1d::\u003cCpuBackend\u003e(\u0026[1.0]));\n        let p3 = fitted.predict(\u0026slice_to_tensor1d::\u003cCpuBackend\u003e(\u0026[3.0]));\n\n        assert!(\n            (p0.data.to_f64() - 1.0).abs() \u003c 0.2,\n            \"p0 = {}\",\n            p0.data.to_f64()\n        );\n        assert!(\n            (p1.data.to_f64() - 3.0).abs() \u003c 0.2,\n            \"p1 = {}\",\n            p1.data.to_f64()\n        );\n        assert!(\n            (p3.data.to_f64() - 7.0).abs() \u003c 0.3,\n            \"p3 = {}\",\n            p3.data.to_f64()\n        );\n    }\n}\n","traces":[],"covered":0,"coverable":0},{"path":["/","home","victor","ML","machinelearne-rs","lib","src","loss","mod.rs"],"content":"pub use crate::backend::scalar::{Scalar, ScalarOps};\npub use crate::backend::tensor1d::Tensor1D;\npub use crate::backend::tensorlike::TensorLike;\npub use crate::backend::Backend;\n\npub use crate::model::linear::{LinearModel, LinearParams, Unfitted};\npub use crate::model::TrainableModel;\n\n/// A trait for differentiable loss functions used during model training.\n///\n/// Implementors must define:\n/// - How to compute the scalar loss value (for logging/metrics).\n/// - How to compute the gradient of the loss w.r.t. the model's predictions.\n///\n/// This gradient is passed to the model's `backward()` method to update parameters.\npub trait Loss\u003cB: Backend\u003e {\n    type Prediction: TensorLike\u003cB\u003e;\n    type Target: TensorLike\u003cB\u003e;\n\n    /// Computes the scalar loss value (for logging/metrics).\n    fn loss(\u0026self, prediction: \u0026Self::Prediction, target: \u0026Self::Target) -\u003e Scalar\u003cB\u003e;\n\n    /// Computes the gradient of the loss w.r.t. the prediction: âL/âpred.\n    /// This is what gets passed to `model.backward()`.\n    fn grad_wrt_prediction(\n        \u0026self,\n        prediction: \u0026Self::Prediction,\n        target: \u0026Self::Target,\n    ) -\u003e Self::Prediction;\n}\n/// Mean Squared Error (MSE) loss: `L = (1/n) * Î£(pred_i - target_i)^2`\n///\n/// Gradient w.r.t. prediction: `âL/âpred = (pred - target) / n`\n///\n/// Note: The factor of 2 is omitted, as it can be absorbed into the learning rate.\npub struct MSELoss;\n\nimpl\u003cB: Backend\u003e Loss\u003cB\u003e for MSELoss\nwhere\n    B::Tensor1D: Clone,\n{\n    type Prediction = Tensor1D\u003cB\u003e;\n    type Target = Tensor1D\u003cB\u003e;\n\n    fn loss(\u0026self, pred: \u0026Self::Prediction, target: \u0026Self::Target) -\u003e Scalar\u003cB\u003e {\n        let diff = pred.sub(target);\n        diff.dot(\u0026diff) / Scalar::\u003cB\u003e::new(B::len_1d(\u0026diff.data) as f64)\n    }\n\n    fn grad_wrt_prediction(\u0026self, pred: \u0026Self::Prediction, target: \u0026Self::Target) -\u003e Tensor1D\u003cB\u003e {\n        // d/dp ( (p - y)^2 ) = 2(p - y)\n        // But commonly: MSE = (1/n) * sum(...), so grad = (2/n)(p - y)\n        // However, in practice, we often omit 2 and let LR absorb it.\n        // We'll return (pred - target) â standard in many frameworks.\n        let diff = pred.sub(target);\n        let n = Scalar::\u003cB\u003e::new(1. / pred.len() as f64); // or use backend method\n        diff.scale(\u0026n)\n    }\n}\n\n/// Mean Absolute Error (MAE) loss: `L = (1/n) * Î£|pred_i - target_i|`\n///\n/// Gradient w.r.t. prediction: `âL/âpred = sign(pred - target) / n`\n/// (subgradient is used at zero).\npub struct MAELoss;\n\nimpl\u003cB: Backend\u003e Loss\u003cB\u003e for MAELoss\nwhere\n    B::Tensor1D: Clone,\n{\n    type Prediction = Tensor1D\u003cB\u003e;\n    type Target = Tensor1D\u003cB\u003e;\n\n    fn loss(\u0026self, pred: \u0026Self::Prediction, target: \u0026Self::Target) -\u003e Scalar\u003cB\u003e {\n        pred.sub(target).abs().mean()\n    }\n\n    fn grad_wrt_prediction(\u0026self, pred: \u0026Self::Prediction, target: \u0026Self::Target) -\u003e Tensor1D\u003cB\u003e {\n        let diff = pred.sub(target);\n        let sign = diff.sign();\n        let n = Scalar::\u003cB\u003e::new(1.0 / pred.len() as f64);\n        sign.scale(\u0026n)\n    }\n}\n/// Binary Cross-Entropy loss with logits input (numerically stable).\n///\n/// Computes: `L = -(t * log(Ï(z)) + (1-t) * log(1 - Ï(z)))`\n/// using the stable formulation: `max(z,0) - z*t + log(1 + exp(-|z|))`\n///\n/// Gradient w.r.t. logits: `âL/âz = (Ï(z) - t) / n`\npub struct BCEWithLogitsLoss;\n\nimpl\u003cB: Backend\u003e Loss\u003cB\u003e for BCEWithLogitsLoss\nwhere\n    B::Tensor1D: Clone,\n{\n    type Prediction = Tensor1D\u003cB\u003e;\n    type Target = Tensor1D\u003cB\u003e;\n\n    fn loss(\u0026self, logits: \u0026Self::Prediction, targets: \u0026Self::Target) -\u003e Scalar\u003cB\u003e {\n        // Numerically stable BCE: -(t * log(s(z)) + (1-t) * log(1 - s(z)))\n        // = max(z, 0) - z * t + log(1 + exp(-|z|))\n        let max_logits = logits.maximum(Self::Prediction::zeros(logits.len()));\n        let term2 = logits\n            .abs()\n            .scale(\u0026Scalar::\u003cB\u003e::new(-1.))\n            .exp()\n            .add_scalar(\u0026Scalar::\u003cB\u003e::new(1.))\n            .log();\n\n        let term1 = max_logits.sub(\u0026logits.mul(targets));\n        let total = term1.add(\u0026term2);\n        total.mean()\n    }\n\n    fn grad_wrt_prediction(\n        \u0026self,\n        logits: \u0026Self::Prediction,\n        targets: \u0026Self::Target,\n    ) -\u003e Self::Prediction {\n        // d/dz BCE = sigmoid(z) - t\n        let n = Scalar::\u003cB\u003e::new(1.0 / logits.len() as f64);\n        logits.sigmoid().sub(targets).scale(\u0026n)\n    }\n}\n\n#[cfg(test)]\nmod tests {\n    use super::*;\n    use crate::backend::CpuBackend;\n\n    #[test]\n    fn test_mse_loss() {\n        let pred = Tensor1D::\u003cCpuBackend\u003e::new(vec![3.0f32, 5.0]);\n        let target = Tensor1D::\u003cCpuBackend\u003e::new(vec![1.0f32, 2.0]);\n\n        let mse = MSELoss;\n        let loss_val = mse.loss(\u0026pred, \u0026target);\n        // ((3-1)^2 + (5-2)^2) / 2 = (4 + 9) / 2 = 6.5\n        assert!((loss_val.data - 6.5).abs() \u003c 1e-12);\n\n        let grad = mse.grad_wrt_prediction(\u0026pred, \u0026target);\n        // grad = (pred - target) / n = [2.0, 3.0] / 2 = [1.0, 1.5]\n        let expected_grad = vec![1.0, 1.5];\n        assert_eq!(grad.to_vec(), expected_grad);\n    }\n\n    #[test]\n    fn test_mae_loss() {\n        let pred = Tensor1D::\u003cCpuBackend\u003e::new(vec![3.0f32, -1.0]);\n        let target = Tensor1D::\u003cCpuBackend\u003e::new(vec![1.0f32, 2.0]);\n\n        let mae = MAELoss;\n        let loss_val = mae.loss(\u0026pred, \u0026target);\n        // (|3-1| + |-1-2|) / 2 = (2 + 3) / 2 = 2.5\n        assert!((loss_val.data - 2.5).abs() \u003c 1e-12);\n\n        let grad = mae.grad_wrt_prediction(\u0026pred, \u0026target);\n        // sign(pred - target) / n = [1.0, -1.0] / 2 = [0.5, -0.5]\n        let expected_grad = vec![0.5, -0.5];\n        assert_eq!(grad.to_vec(), expected_grad);\n    }\n\n    #[test]\n    fn test_bce_with_logits_loss() {\n        let logits = Tensor1D::\u003cCpuBackend\u003e::new(vec![0.0f32, 2.0, -2.0]); // sigmoid: [0.5, ~0.88, ~0.12]\n        let targets = Tensor1D::\u003cCpuBackend\u003e::new(vec![1.0f32, 1.0, 0.0]);\n\n        let bce = BCEWithLogitsLoss;\n        let loss_val = bce.loss(\u0026logits, \u0026targets);\n\n        // Expected (computed manually or via PyTorch):\n        // For z=0, t=1: -(0 + log(1+1)) = -log(2) â -0.6931 â but formula gives: max(0,0) - 0*1 + log(1+1) = 0 + 0 + log(2) â 0.6931\n        // For z=2, t=1: max(2,0) - 2*1 + log(1+exp(-2)) = 2 - 2 + log(1+0.135) â 0.127\n        // For z=-2, t=0: max(-2,0)=0 - (-2)*0 + log(1+exp(-2)) = 0 + 0 + log(1.135) â 0.127\n        // Mean â (0.6931 + 0.127 + 0.127) / 3 â 0.3156\n        let expected_loss = 0.3156;\n        assert!((loss_val.data - expected_loss).abs() \u003c 1e-3);\n\n        let grad = bce.grad_wrt_prediction(\u0026logits, \u0026targets);\n        // grad = (sigmoid(z) - t) / n\n        let sig = vec![\n            0.5,\n            1.0 / (1.0 + (-2.0f64).exp()),\n            1.0 / (1.0 + (2.0f64).exp()),\n        ];\n        let expected_grad: Vec\u003cf64\u003e = sig\n            .iter()\n            .zip(targets.to_vec().iter())\n            .map(|(s, t)| (s - t) / 3.0)\n            .collect();\n        for (g, e) in grad.to_vec().iter().zip(expected_grad.iter()) {\n            assert!((g - e).abs() \u003c 1e-5);\n        }\n    }\n\n    #[test]\n    fn test_bce_numerical_stability() {\n        // Large positive and negative logits\n        let logits = Tensor1D::\u003cCpuBackend\u003e::new(vec![100.0f32, -100.0]);\n        let targets = Tensor1D::\u003cCpuBackend\u003e::new(vec![1.0f32, 0.0]);\n\n        let bce = BCEWithLogitsLoss;\n        let loss_val = bce.loss(\u0026logits, \u0026targets);\n        // Should not overflow or produce NaN\n        assert!(loss_val.data.is_finite());\n\n        let grad = bce.grad_wrt_prediction(\u0026logits, \u0026targets);\n        assert!(grad.to_vec().iter().all(|\u0026x| x.is_finite()));\n    }\n}\n","traces":[{"line":45,"address":[2365408,2365612,2365618],"length":1,"stats":{"Line":1}},{"line":46,"address":[2365437],"length":1,"stats":{"Line":1}},{"line":47,"address":[2365447,2365499],"length":1,"stats":{"Line":2}},{"line":50,"address":[2365386,2365152,2365392],"length":1,"stats":{"Line":1}},{"line":55,"address":[2365212],"length":1,"stats":{"Line":1}},{"line":56,"address":[2365270,2365222],"length":1,"stats":{"Line":2}},{"line":57,"address":[2365359],"length":1,"stats":{"Line":1}},{"line":74,"address":[2364976,2365132,2365138],"length":1,"stats":{"Line":1}},{"line":75,"address":[2365060,2365005],"length":1,"stats":{"Line":2}},{"line":78,"address":[2364963,2364640,2364957],"length":1,"stats":{"Line":1}},{"line":79,"address":[2364711],"length":1,"stats":{"Line":1}},{"line":80,"address":[2364726],"length":1,"stats":{"Line":1}},{"line":81,"address":[2364826,2364775],"length":1,"stats":{"Line":2}},{"line":82,"address":[2364915],"length":1,"stats":{"Line":1}},{"line":100,"address":[2366920,2365936,2366926],"length":1,"stats":{"Line":1}},{"line":103,"address":[2365990],"length":1,"stats":{"Line":1}},{"line":104,"address":[],"length":0,"stats":{"Line":0}},{"line":106,"address":[2366098,2366154],"length":1,"stats":{"Line":2}},{"line":108,"address":[2366318,2366262],"length":1,"stats":{"Line":2}},{"line":111,"address":[2366641],"length":1,"stats":{"Line":1}},{"line":112,"address":[2366789],"length":1,"stats":{"Line":1}},{"line":113,"address":[2366804],"length":1,"stats":{"Line":1}},{"line":116,"address":[2365908,2365632,2365914],"length":1,"stats":{"Line":1}},{"line":122,"address":[2365681],"length":1,"stats":{"Line":1}},{"line":123,"address":[2365835,2365763],"length":1,"stats":{"Line":2}}],"covered":24,"coverable":25},{"path":["/","home","victor","ML","machinelearne-rs","lib","src","model","linear.rs"],"content":"//! Linear models for regression and classification.\n//!\n//! This module implements a type-safe linear model with compile-time state tracking:\n//! - [`LinearRegression`] = `LinearModel\u003cUnfitted\u003e` â used during training.\n//! - [`LinearModel\u003cFitted\u003e`] â inference-only, serializable predictor.\n//!\n//! The design follows [ADR-0001: separate-trainer-losses](https://github.com/vzaguskin/machinelearne-rs/issues/1):\n//! \u003e *\"Fitted model is free from training hyperparameters.\"*\n//!\n//! Supports L1/L2 regularization via loss functions and works with any backend implementing [`Backend`].\n//!\npub use crate::backend::scalar::{Scalar, ScalarOps};\npub use crate::backend::tensor1d::Tensor1D;\npub use crate::backend::tensor2d::Tensor2D;\npub use crate::backend::Backend;\nuse crate::loss::TensorLike;\npub use crate::model::{Fitted, InferenceModel, ParamOps, TrainableModel, Unfitted};\nuse std::marker::PhantomData;\n\n/// Trainable parameters of a linear model: weights and bias.\n///\n/// Used internally by both [`TrainableModel`] and [`InferenceModel`] implementations.\n/// Implements [`ParamOps`] to support optimizer updates (e.g., SGD).\n#[derive(Clone)]\npub struct LinearParams\u003cB: Backend\u003e\nwhere\n    Tensor1D\u003cB\u003e: Clone,\n    Scalar\u003cB\u003e: Clone,\n{\n    pub weights: Tensor1D\u003cB\u003e,\n    pub bias: Scalar\u003cB\u003e,\n}\n\n/// Serializable representation of linear model parameters.\n///\n/// Converts internal backend-specific tensors into plain `Vec\u003cf32\u003e` for storage.\n/// Used by [`InferenceModel::save_to_file`] and [`InferenceModel::load_from_file`].\n///\n/// â ï¸ Currently uses `f32` for compactness\n#[cfg(feature = \"serde\")]\n#[derive(serde::Serialize, serde::Deserialize)]\npub struct SerializableLinearParams {\n    pub weights: Vec\u003cf32\u003e,\n    pub bias: f32,\n}\n\nimpl\u003cB: Backend\u003e From\u003c\u0026LinearParams\u003cB\u003e\u003e for SerializableLinearParams {\n    fn from(params: \u0026LinearParams\u003cB\u003e) -\u003e Self {\n        // ÐÑÐµÐ´Ð¿Ð¾Ð»Ð°Ð³Ð°ÐµÐ¼, ÑÑÐ¾ ÑÑÐ¸ Ð¼ÐµÑÐ¾Ð´Ñ ÑÑÑÐµÑÑÐ²ÑÑÑ Ð² Ð²Ð°ÑÐµÐ¼ Ð±ÑÐºÐµÐ½Ð´Ðµ\n        let weights = params\n            .weights\n            .to_vec()\n            .into_iter()\n            .map(|x| x as f32)\n            .collect(); // Vec\u003cf32\u003e\n        let bias = params.bias.data.to_f64() as f32; // f32\n        Self { weights, bias }\n    }\n}\n\nimpl\u003cB: Backend\u003e TryFrom\u003cSerializableLinearParams\u003e for LinearParams\u003cB\u003e {\n    type Error = Box\u003cdyn std::error::Error\u003e;\n\n    fn try_from(value: SerializableLinearParams) -\u003e Result\u003cSelf, Self::Error\u003e {\n        let weights = Tensor1D::\u003cB\u003e::new(value.weights);\n        let bias = Scalar::\u003cB\u003e::new(value.bias as f64);\n        Ok(Self { weights, bias })\n    }\n}\n\nimpl\u003cB\u003e ParamOps\u003cB\u003e for LinearParams\u003cB\u003e\nwhere\n    B: Backend,\n{\n    fn add(\u0026self, other: \u0026Self) -\u003e Self {\n        let w = self.weights.add(\u0026other.weights);\n        let b = self.bias + other.bias;\n        Self {\n            weights: w,\n            bias: b,\n        }\n    }\n    fn scale(\u0026self, scalar: Scalar\u003cB\u003e) -\u003e Self {\n        let w = self.weights.scale(\u0026scalar);\n        let b = self.bias * scalar;\n        Self {\n            weights: w,\n            bias: b,\n        }\n    }\n}\n\n/// A linear model with state encoded at the type level.\n///\n/// - When `S = Unfitted`: implements [`TrainableModel`] â used during training.\n/// - When `S = Fitted`: implements [`InferenceModel`] â used for prediction and serialization.\n///\n/// This enforces, at compile time, that you cannot call `predict()` on an untrained model.\npub struct LinearModel\u003cB: Backend, S\u003e {\n    params: LinearParams\u003cB\u003e,\n    _state: std::marker::PhantomData\u003cS\u003e,\n}\n\nimpl\u003cB: Backend\u003e LinearModel\u003cB, Fitted\u003e {\n    /// Creates a new fitted linear model from trained parameters.\n    ///\n    /// Typically called internally by [`TrainableModel::into_fitted`].\n    /// Useful for manual model construction or loading from external sources.\n    pub fn new(params: LinearParams\u003cB\u003e) -\u003e Self {\n        Self {\n            params,\n            _state: std::marker::PhantomData::\u003cFitted\u003e,\n        }\n    }\n}\n\n/// Implements inference for a trained linear model: `y = w^T x + b`.\n///\n/// - Single-sample input: [`Tensor1D\u003cB\u003e`] â output: [`Scalar\u003cB\u003e`]\n/// - Batch input: [`Tensor2D\u003cB\u003e`] â output: [`Tensor1D\u003cB\u003e`]\n///\n/// Serialization uses [`SerializableLinearParams`] (see `save_to_file`/`load_from_file`).\nimpl\u003cB: Backend\u003e InferenceModel\u003cB\u003e for LinearModel\u003cB, Fitted\u003e {\n    type InputSingle = Tensor1D\u003cB\u003e;\n    type InputBatch = Tensor2D\u003cB\u003e;\n    type OutputSingle = Scalar\u003cB\u003e;\n    type OutputBatch = Tensor1D\u003cB\u003e;\n    type ParamsRepr = SerializableLinearParams;\n\n    /// Predict on a single sample (feature vector).\n    fn predict(\u0026self, input: \u0026Self::InputSingle) -\u003e Self::OutputSingle {\n        self.params.weights.dot(input) + self.params.bias\n    }\n\n    fn predict_batch(\u0026self, input: \u0026Self::InputBatch) -\u003e Self::OutputBatch {\n        input\n            .dot(\u0026self.params.weights)\n            .add_scalar(\u0026self.params.bias)\n    }\n\n    fn extract_params(\u0026self) -\u003e Self::ParamsRepr {\n        (\u0026self.params).into()\n    }\n\n    fn from_params(params: Self::ParamsRepr) -\u003e Result\u003cSelf, Box\u003cdyn std::error::Error\u003e\u003e\n    where\n        Self: Sized,\n    {\n        let internal_params = LinearParams::\u003cB\u003e::try_from(params)?;\n        Ok(Self::new(internal_params))\n    }\n}\n\n/// Implements training interface for linear regression.\n///\n/// Forward pass: `X @ w + b`  \n/// Backward pass: computes gradients âw = X^T Â· grad, âb = sum(grad)\n///\n/// After training, convert to inference model via [`Self::into_fitted`].\nimpl\u003cB: Backend\u003e TrainableModel\u003cB\u003e for LinearModel\u003cB, Unfitted\u003e {\n    type Params = LinearParams\u003cB\u003e;\n    type Gradients = LinearParams\u003cB\u003e;\n    type Prediction = Tensor1D\u003cB\u003e;\n    type Input = Tensor2D\u003cB\u003e;\n    type Output = LinearModel\u003cB, Fitted\u003e;\n\n    fn forward(\u0026self, x: \u0026Self::Input) -\u003e Self::Prediction {\n        x.dot(\u0026self.params.weights).add_scalar(\u0026self.params.bias)\n    }\n\n    fn params(\u0026self) -\u003e \u0026Self::Params {\n        \u0026self.params\n    }\n\n    fn update_params(\u0026mut self, params: \u0026Self::Params) {\n        self.params = params.clone();\n    }\n\n    fn into_fitted(self) -\u003e LinearModel\u003cB, Fitted\u003e {\n        LinearModel::\u003cB, Fitted\u003e::new(self.params)\n    }\n\n    fn backward(\u0026self, x: \u0026Self::Input, grad_output: \u0026Self::Prediction) -\u003e Self::Gradients {\n        let grad_weights = x.tdot(grad_output);\n        let grad_bias = grad_output.sum();\n        LinearParams {\n            weights: grad_weights,\n            bias: grad_bias,\n        }\n    }\n}\n\n/// Alias for an **unfitted** linear regression model.\n///\n/// Equivalent to `LinearModel\u003cUnfitted\u003e`. Use this type when constructing\n/// a model for training with [`Trainer`].\npub type LinearRegression\u003cB\u003e = LinearModel\u003cB, Unfitted\u003e;\n\nimpl\u003cB: Backend\u003e LinearRegression\u003cB\u003e {\n    // Creates a new linear regression model with zero-initialized weights.\n    ///\n    /// # Parameters\n    /// - `n_features`: number of input features (dimensionality of `x`).\n    pub fn new(n_features: usize) -\u003e Self {\n        let params = LinearParams {\n            weights: Tensor1D::\u003cB\u003e::zeros(n_features),\n            bias: Scalar::\u003cB\u003e::new(0.),\n        };\n        Self {\n            params,\n            _state: PhantomData,\n        }\n    }\n\n    /// Constructs a model from explicit parameters (e.g., for testing or warm start).\n    pub fn from_params(params: LinearParams\u003cB\u003e) -\u003e Self {\n        Self {\n            params,\n            _state: PhantomData,\n        }\n    }\n}\n\n// Convenient alias for CPU-based linear regression.\n///\n/// Example:\n/// ```rust\n/// use machinelearne_rs::model::linear::LinearRegressor;\n/// let model = LinearRegressor::new(10); // 10 features\n/// ```\npub type LinearRegressor = LinearRegression\u003ccrate::backend::CpuBackend\u003e;\n\n#[cfg(test)]\nmod tests {\n    use super::*;\n    use crate::backend::CpuBackend;\n\n    // === ParamOps Tests ===\n\n    #[test]\n    fn test_param_ops_add() {\n        let p1 = LinearParams {\n            weights: Tensor1D::\u003cCpuBackend\u003e::new(vec![1.0, 2.0]),\n            bias: Scalar::\u003cCpuBackend\u003e::new(0.5),\n        };\n        let p2 = LinearParams {\n            weights: Tensor1D::\u003cCpuBackend\u003e::new(vec![0.5, 1.0]),\n            bias: Scalar::\u003cCpuBackend\u003e::new(0.5),\n        };\n\n        let result = p1.add(\u0026p2);\n\n        assert_eq!(result.weights.to_vec(), vec![1.5, 3.0]);\n        assert_eq!(result.bias.data.to_f64(), 1.0);\n    }\n\n    #[test]\n    fn test_param_ops_add_negative() {\n        let p1 = LinearParams {\n            weights: Tensor1D::\u003cCpuBackend\u003e::new(vec![1.0]),\n            bias: Scalar::\u003cCpuBackend\u003e::new(1.0),\n        };\n        let p2 = LinearParams {\n            weights: Tensor1D::\u003cCpuBackend\u003e::new(vec![-0.5]),\n            bias: Scalar::\u003cCpuBackend\u003e::new(-2.0),\n        };\n\n        let result = p1.add(\u0026p2);\n\n        assert_eq!(result.weights.to_vec(), vec![0.5]);\n        assert_eq!(result.bias.data.to_f64(), -1.0);\n    }\n\n    #[test]\n    fn test_param_ops_scale() {\n        let p = LinearParams {\n            weights: Tensor1D::\u003cCpuBackend\u003e::new(vec![2.0, 4.0]),\n            bias: Scalar::\u003cCpuBackend\u003e::new(1.0),\n        };\n\n        let result = p.scale(Scalar::\u003cCpuBackend\u003e::new(0.5));\n\n        assert_eq!(result.weights.to_vec(), vec![1.0, 2.0]);\n        assert_eq!(result.bias.data.to_f64(), 0.5);\n    }\n\n    #[test]\n    fn test_param_ops_scale_negative() {\n        let p = LinearParams {\n            weights: Tensor1D::\u003cCpuBackend\u003e::new(vec![2.0]),\n            bias: Scalar::\u003cCpuBackend\u003e::new(1.0),\n        };\n\n        let result = p.scale(Scalar::\u003cCpuBackend\u003e::new(-1.0));\n\n        assert_eq!(result.weights.to_vec(), vec![-2.0]);\n        assert_eq!(result.bias.data.to_f64(), -1.0);\n    }\n\n    #[test]\n    fn test_param_ops_scale_zero() {\n        let p = LinearParams {\n            weights: Tensor1D::\u003cCpuBackend\u003e::new(vec![5.0, 10.0]),\n            bias: Scalar::\u003cCpuBackend\u003e::new(100.0),\n        };\n\n        let result = p.scale(Scalar::\u003cCpuBackend\u003e::new(0.0));\n\n        assert_eq!(result.weights.to_vec(), vec![0.0, 0.0]);\n        assert_eq!(result.bias.data.to_f64(), 0.0);\n    }\n\n    // === LinearRegression (Unfitted) Tests ===\n\n    #[test]\n    fn test_linear_regression_new_zero_initialized() {\n        let model = LinearRegression::\u003cCpuBackend\u003e::new(3);\n\n        let params = model.params();\n        assert_eq!(params.weights.to_vec(), vec![0.0, 0.0, 0.0]);\n        assert_eq!(params.bias.data.to_f64(), 0.0);\n    }\n\n    #[test]\n    fn test_linear_regression_new_single_feature() {\n        let model = LinearRegression::\u003cCpuBackend\u003e::new(1);\n\n        assert_eq!(model.params().weights.to_vec(), vec![0.0]);\n        assert_eq!(model.params().bias.data.to_f64(), 0.0);\n    }\n\n    #[test]\n    fn test_linear_regression_new_large_features() {\n        let n_features = 1000;\n        let model = LinearRegression::\u003cCpuBackend\u003e::new(n_features);\n\n        assert_eq!(model.params().weights.to_vec().len(), n_features);\n        assert!(model.params().weights.to_vec().iter().all(|\u0026x| x == 0.0));\n    }\n\n    #[test]\n    fn test_linear_regression_from_params() {\n        let params = LinearParams {\n            weights: Tensor1D::\u003cCpuBackend\u003e::new(vec![1.0, 2.0, 3.0]),\n            bias: Scalar::\u003cCpuBackend\u003e::new(0.5),\n        };\n\n        let model = LinearRegression::\u003cCpuBackend\u003e::from_params(params.clone());\n\n        assert_eq!(model.params().weights.to_vec(), params.weights.to_vec());\n        assert_eq!(model.params().bias.data.to_f64(), params.bias.data.to_f64());\n    }\n\n    #[test]\n    fn test_linear_regression_update_params() {\n        let mut model = LinearRegression::\u003cCpuBackend\u003e::new(2);\n\n        let new_params = LinearParams {\n            weights: Tensor1D::\u003cCpuBackend\u003e::new(vec![1.0, 2.0]),\n            bias: Scalar::\u003cCpuBackend\u003e::new(0.5),\n        };\n\n        model.update_params(\u0026new_params);\n\n        assert_eq!(model.params().weights.to_vec(), vec![1.0, 2.0]);\n        assert_eq!(model.params().bias.data.to_f64(), 0.5);\n    }\n\n    #[test]\n    fn test_linear_regression_forward_with_zero_params() {\n        let model = LinearRegression::\u003cCpuBackend\u003e::new(2);\n\n        let x = Tensor2D::\u003cCpuBackend\u003e::new(vec![1.0, 2.0, 3.0, 4.0], 2, 2);\n        let pred = model.forward(\u0026x);\n\n        // With zero weights and bias: X @ w + b = 0 + 0 = 0\n        assert_eq!(pred.to_vec(), vec![0.0, 0.0]);\n    }\n\n    #[test]\n    fn test_linear_regression_forward_correctness() {\n        // Create model with known weights\n        let params = LinearParams {\n            weights: Tensor1D::\u003cCpuBackend\u003e::new(vec![2.0, 3.0]),\n            bias: Scalar::\u003cCpuBackend\u003e::new(1.0),\n        };\n        let model = LinearRegression::\u003cCpuBackend\u003e::from_params(params);\n\n        // Input: [[1, 0], [0, 1]]\n        // Expected: [[2*1 + 3*0 + 1], [2*0 + 3*1 + 1]] = [3, 4]\n        let x = Tensor2D::\u003cCpuBackend\u003e::new(vec![1.0, 0.0, 0.0, 1.0], 2, 2);\n        let pred = model.forward(\u0026x);\n\n        assert_eq!(pred.to_vec(), vec![3.0, 4.0]);\n    }\n\n    #[test]\n    fn test_linear_regression_forward_batch() {\n        let params = LinearParams {\n            weights: Tensor1D::\u003cCpuBackend\u003e::new(vec![1.0, 2.0]),\n            bias: Scalar::\u003cCpuBackend\u003e::new(3.0),\n        };\n        let model = LinearRegression::\u003cCpuBackend\u003e::from_params(params);\n\n        // Input: [[1, 1], [2, 2]]\n        // Expected: [[1*1 + 2*1 + 3], [1*2 + 2*2 + 3]] = [6, 9]\n        let x = Tensor2D::\u003cCpuBackend\u003e::new(vec![1.0, 1.0, 2.0, 2.0], 2, 2);\n        let pred = model.forward(\u0026x);\n\n        assert_eq!(pred.to_vec(), vec![6.0, 9.0]);\n    }\n\n    #[test]\n    fn test_linear_regression_backward_single_sample() {\n        let model = LinearRegression::\u003cCpuBackend\u003e::new(1);\n        let x = Tensor2D::\u003cCpuBackend\u003e::new(vec![2.0], 1, 1);\n        let grad_output = Tensor1D::\u003cCpuBackend\u003e::new(vec![0.5]);\n\n        let grads = model.backward(\u0026x, \u0026grad_output);\n\n        // grad_w = X^T @ grad = [2] * [0.5] = 1.0\n        // grad_b = sum(grad) = 0.5\n        assert_eq!(grads.weights.to_vec(), vec![1.0]);\n        assert_eq!(grads.bias.data.to_f64(), 0.5);\n    }\n\n    #[test]\n    fn test_linear_regression_backward_batch() {\n        let model = LinearRegression::\u003cCpuBackend\u003e::new(2);\n        // Input: [[1, 2], [3, 4]]\n        let x = Tensor2D::\u003cCpuBackend\u003e::new(vec![1.0, 2.0, 3.0, 4.0], 2, 2);\n        // grad_output: [0.5, 0.25]\n        let grad_output = Tensor1D::\u003cCpuBackend\u003e::new(vec![0.5, 0.25]);\n\n        let grads = model.backward(\u0026x, \u0026grad_output);\n\n        // grad_w = X^T @ grad = [[1, 3], [2, 4]] @ [0.5, 0.25]^T\n        //       = [1*0.5 + 3*0.25, 2*0.5 + 4*0.25]\n        //       = [0.5 + 0.75, 1.0 + 1.0] = [1.25, 2.0]\n        // grad_b = sum([0.5, 0.25]) = 0.75\n        assert!((grads.weights.to_vec()[0] - 1.25).abs() \u003c 1e-10);\n        assert!((grads.weights.to_vec()[1] - 2.0).abs() \u003c 1e-10);\n        assert!((grads.bias.data.to_f64() - 0.75).abs() \u003c 1e-10);\n    }\n\n    #[test]\n    fn test_linear_regression_backward_negative_gradients() {\n        let model = LinearRegression::\u003cCpuBackend\u003e::new(1);\n        let x = Tensor2D::\u003cCpuBackend\u003e::new(vec![1.0], 1, 1);\n        let grad_output = Tensor1D::\u003cCpuBackend\u003e::new(vec![-0.5]);\n\n        let grads = model.backward(\u0026x, \u0026grad_output);\n\n        assert_eq!(grads.weights.to_vec(), vec![-0.5]);\n        assert_eq!(grads.bias.data.to_f64(), -0.5);\n    }\n\n    #[test]\n    fn test_linear_regression_backward_zero_gradients() {\n        let model = LinearRegression::\u003cCpuBackend\u003e::new(2);\n        let x = Tensor2D::\u003cCpuBackend\u003e::new(vec![1.0, 2.0, 3.0, 4.0], 2, 2);\n        let grad_output = Tensor1D::\u003cCpuBackend\u003e::new(vec![0.0, 0.0]);\n\n        let grads = model.backward(\u0026x, \u0026grad_output);\n\n        assert_eq!(grads.weights.to_vec(), vec![0.0, 0.0]);\n        assert_eq!(grads.bias.data.to_f64(), 0.0);\n    }\n\n    // === Fitted Model Tests ===\n\n    #[test]\n    fn test_linear_model_fitted_new() {\n        let params = LinearParams {\n            weights: Tensor1D::\u003cCpuBackend\u003e::new(vec![1.0, 2.0]),\n            bias: Scalar::\u003cCpuBackend\u003e::new(0.5),\n        };\n\n        let model = LinearModel::\u003cCpuBackend, Fitted\u003e::new(params);\n\n        assert_eq!(model.params.weights.to_vec(), vec![1.0, 2.0]);\n        assert_eq!(model.params.bias.data.to_f64(), 0.5);\n    }\n\n    #[test]\n    fn test_linear_model_predict_single_sample() {\n        let params = LinearParams {\n            weights: Tensor1D::\u003cCpuBackend\u003e::new(vec![2.0, 3.0]),\n            bias: Scalar::\u003cCpuBackend\u003e::new(1.0),\n        };\n        let model = LinearModel::\u003cCpuBackend, Fitted\u003e::new(params);\n\n        let input = Tensor1D::\u003cCpuBackend\u003e::new(vec![1.0, 2.0]);\n        let pred = model.predict(\u0026input);\n\n        // y = 2*1 + 3*2 + 1 = 2 + 6 + 1 = 9\n        assert_eq!(pred.data.to_f64(), 9.0);\n    }\n\n    #[test]\n    fn test_linear_model_predict_batch() {\n        let params = LinearParams {\n            weights: Tensor1D::\u003cCpuBackend\u003e::new(vec![1.0, 2.0]),\n            bias: Scalar::\u003cCpuBackend\u003e::new(3.0),\n        };\n        let model = LinearModel::\u003cCpuBackend, Fitted\u003e::new(params);\n\n        let batch = Tensor2D::\u003cCpuBackend\u003e::new(vec![1.0, 1.0, 2.0, 2.0], 2, 2);\n        let pred = model.predict_batch(\u0026batch);\n\n        // y1 = 1*1 + 2*1 + 3 = 6\n        // y2 = 1*2 + 2*2 + 3 = 9\n        assert_eq!(pred.to_vec(), vec![6.0, 9.0]);\n    }\n\n    #[test]\n    fn test_linear_model_predict_single_feature() {\n        let params = LinearParams {\n            weights: Tensor1D::\u003cCpuBackend\u003e::new(vec![5.0]),\n            bias: Scalar::\u003cCpuBackend\u003e::new(0.0),\n        };\n        let model = LinearModel::\u003cCpuBackend, Fitted\u003e::new(params);\n\n        let input = Tensor1D::\u003cCpuBackend\u003e::new(vec![2.0]);\n        let pred = model.predict(\u0026input);\n\n        assert_eq!(pred.data.to_f64(), 10.0);\n    }\n\n    #[test]\n    fn test_linear_model_predict_zero_bias() {\n        let params = LinearParams {\n            weights: Tensor1D::\u003cCpuBackend\u003e::new(vec![1.0, 1.0]),\n            bias: Scalar::\u003cCpuBackend\u003e::new(0.0),\n        };\n        let model = LinearModel::\u003cCpuBackend, Fitted\u003e::new(params);\n\n        let input = Tensor1D::\u003cCpuBackend\u003e::new(vec![3.0, 4.0]);\n        let pred = model.predict(\u0026input);\n\n        assert_eq!(pred.data.to_f64(), 7.0);\n    }\n\n    #[test]\n    fn test_linear_model_predict_negative_weights() {\n        let params = LinearParams {\n            weights: Tensor1D::\u003cCpuBackend\u003e::new(vec![-1.0, -2.0]),\n            bias: Scalar::\u003cCpuBackend\u003e::new(5.0),\n        };\n        let model = LinearModel::\u003cCpuBackend, Fitted\u003e::new(params);\n\n        let input = Tensor1D::\u003cCpuBackend\u003e::new(vec![1.0, 1.0]);\n        let pred = model.predict(\u0026input);\n\n        // y = -1*1 + -2*1 + 5 = 2\n        assert_eq!(pred.data.to_f64(), 2.0);\n    }\n\n    #[test]\n    fn test_linear_model_extract_params() {\n        let params = LinearParams {\n            weights: Tensor1D::\u003cCpuBackend\u003e::new(vec![1.5, 2.5, 3.5]),\n            bias: Scalar::\u003cCpuBackend\u003e::new(0.25),\n        };\n        let model = LinearModel::\u003cCpuBackend, Fitted\u003e::new(params);\n\n        let extracted = model.extract_params();\n\n        assert_eq!(extracted.weights.len(), 3);\n        assert!((extracted.weights[0] - 1.5).abs() \u003c 1e-6);\n        assert!((extracted.weights[1] - 2.5).abs() \u003c 1e-6);\n        assert!((extracted.weights[2] - 3.5).abs() \u003c 1e-6);\n        assert!((extracted.bias - 0.25).abs() \u003c 1e-6);\n    }\n\n    #[test]\n    fn test_linear_model_from_params() -\u003e Result\u003c(), Box\u003cdyn std::error::Error\u003e\u003e {\n        let serial = SerializableLinearParams {\n            weights: vec![1.0, 2.0, 3.0],\n            bias: 0.5,\n        };\n\n        let model = LinearModel::\u003cCpuBackend, Fitted\u003e::from_params(serial)?;\n\n        assert_eq!(model.params.weights.to_vec(), vec![1.0, 2.0, 3.0]);\n        assert_eq!(model.params.bias.data.to_f64(), 0.5);\n\n        Ok(())\n    }\n\n    #[test]\n    fn test_linear_model_into_fitted() {\n        let params = LinearParams {\n            weights: Tensor1D::\u003cCpuBackend\u003e::new(vec![1.0]),\n            bias: Scalar::\u003cCpuBackend\u003e::new(0.5),\n        };\n        let unfitted_model = LinearRegression::\u003cCpuBackend\u003e::from_params(params);\n\n        let fitted_model: LinearModel\u003cCpuBackend, Fitted\u003e = unfitted_model.into_fitted();\n\n        assert_eq!(fitted_model.params.weights.to_vec(), vec![1.0]);\n        assert_eq!(fitted_model.params.bias.data.to_f64(), 0.5);\n    }\n\n    #[test]\n    fn test_linear_model_predict_does_not_mutate_input() {\n        let params = LinearParams {\n            weights: Tensor1D::\u003cCpuBackend\u003e::new(vec![1.0, 1.0]),\n            bias: Scalar::\u003cCpuBackend\u003e::new(0.0),\n        };\n        let model = LinearModel::\u003cCpuBackend, Fitted\u003e::new(params);\n\n        let input = Tensor1D::\u003cCpuBackend\u003e::new(vec![1.0, 2.0]);\n        let original = input.clone();\n\n        let _ = model.predict(\u0026input);\n\n        // Input should be unchanged\n        assert_eq!(input.to_vec(), original.to_vec());\n    }\n\n    // === Serialization Tests ===\n\n    #[test]\n    fn test_linear_model_save_load() -\u003e Result\u003c(), Box\u003cdyn std::error::Error\u003e\u003e {\n        let weights = vec![1.0, 2.0, 3.0];\n        let bias = 0.5;\n        let serial = SerializableLinearParams { weights, bias };\n        let params = LinearParams::\u003cCpuBackend\u003e::try_from(serial)?;\n        let model = LinearModel::\u003cCpuBackend, Fitted\u003e::new(params);\n\n        // Save\n        let tmp = tempfile::tempdir()?;\n        let path = tmp.path().join(\"model.bin\");\n        model.save_to_file(\u0026path)?;\n\n        // Load\n        let loaded = LinearModel::\u003cCpuBackend, Fitted\u003e::load_from_file(\u0026path)?;\n\n        // Compare\n        let orig_repr = model.extract_params();\n        let loaded_repr = loaded.extract_params();\n        assert_eq!(orig_repr.weights, loaded_repr.weights);\n        assert_eq!(orig_repr.bias, loaded_repr.bias);\n\n        Ok(())\n    }\n\n    #[test]\n    fn test_serialization_params_roundtrip() -\u003e Result\u003c(), Box\u003cdyn std::error::Error\u003e\u003e {\n        let original = LinearParams {\n            weights: Tensor1D::\u003cCpuBackend\u003e::new(vec![0.1, 0.2, 0.3]),\n            bias: Scalar::\u003cCpuBackend\u003e::new(0.05),\n        };\n\n        // Convert to serializable\n        let serial: SerializableLinearParams = (\u0026original).into();\n\n        // Convert back\n        let restored = LinearParams::\u003cCpuBackend\u003e::try_from(serial)?;\n\n        // Verify with tolerance due to f32/f64 conversion precision loss\n        for (orig, rest) in original\n            .weights\n            .to_vec()\n            .iter()\n            .zip(restored.weights.to_vec().iter())\n        {\n            assert!((orig - rest).abs() \u003c 1e-6);\n        }\n        assert!((original.bias.data.to_f64() - restored.bias.data.to_f64()).abs() \u003c 1e-6);\n\n        Ok(())\n    }\n}\n","traces":[{"line":48,"address":[3119904,3120189,3120183],"length":1,"stats":{"Line":1}},{"line":50,"address":[],"length":0,"stats":{"Line":0}},{"line":51,"address":[],"length":0,"stats":{"Line":0}},{"line":54,"address":[3120208,3120219,3119987],"length":1,"stats":{"Line":3}},{"line":56,"address":[3120012,3120083],"length":1,"stats":{"Line":2}},{"line":64,"address":[3119224,3118960],"length":1,"stats":{"Line":1}},{"line":65,"address":[3118982],"length":1,"stats":{"Line":1}},{"line":66,"address":[3119019,3119094],"length":1,"stats":{"Line":2}},{"line":67,"address":[3119103],"length":1,"stats":{"Line":1}},{"line":75,"address":[3118528,3118736,3118730],"length":1,"stats":{"Line":1}},{"line":76,"address":[3118570],"length":1,"stats":{"Line":1}},{"line":77,"address":[3118585,3118652],"length":1,"stats":{"Line":4}},{"line":83,"address":[3118940,3118946,3118752],"length":1,"stats":{"Line":1}},{"line":84,"address":[3118791],"length":1,"stats":{"Line":2}},{"line":85,"address":[3118801,3118868],"length":1,"stats":{"Line":3}},{"line":109,"address":[3124144],"length":1,"stats":{"Line":1}},{"line":131,"address":[3119856],"length":1,"stats":{"Line":1}},{"line":132,"address":[3119874],"length":1,"stats":{"Line":1}},{"line":135,"address":[3119779,3119785,3119648],"length":1,"stats":{"Line":1}},{"line":136,"address":[],"length":0,"stats":{"Line":0}},{"line":137,"address":[3119700],"length":1,"stats":{"Line":1}},{"line":138,"address":[3119720],"length":1,"stats":{"Line":1}},{"line":141,"address":[3119808],"length":1,"stats":{"Line":1}},{"line":142,"address":[3119825],"length":1,"stats":{"Line":1}},{"line":145,"address":[3119603,3119628,3119248],"length":1,"stats":{"Line":1}},{"line":149,"address":[3119264,3119386],"length":1,"stats":{"Line":2}},{"line":150,"address":[3119535,3119440],"length":1,"stats":{"Line":2}},{"line":167,"address":[3120611,3120480,3120617],"length":1,"stats":{"Line":2}},{"line":168,"address":[3120532],"length":1,"stats":{"Line":2}},{"line":171,"address":[3120464],"length":1,"stats":{"Line":1}},{"line":172,"address":[],"length":0,"stats":{"Line":0}},{"line":175,"address":[3120382,3120304],"length":1,"stats":{"Line":1}},{"line":176,"address":[3120327,3120412,3120347],"length":1,"stats":{"Line":2}},{"line":179,"address":[3120224],"length":1,"stats":{"Line":1}},{"line":180,"address":[3120235],"length":1,"stats":{"Line":1}},{"line":183,"address":[3120640,3120846,3120852],"length":1,"stats":{"Line":4}},{"line":184,"address":[3120701],"length":1,"stats":{"Line":4}},{"line":185,"address":[3120768,3120711],"length":1,"stats":{"Line":8}},{"line":204,"address":[3124409,3124240,3124403],"length":1,"stats":{"Line":4}},{"line":206,"address":[3124263],"length":1,"stats":{"Line":4}},{"line":207,"address":[3124271],"length":1,"stats":{"Line":1}},{"line":216,"address":[3124192],"length":1,"stats":{"Line":1}}],"covered":38,"coverable":42},{"path":["/","home","victor","ML","machinelearne-rs","lib","src","model","mod.rs"],"content":"//! Core model abstractions: training vs inference separation.\n//!\n//! This module defines the two central traits:\n//! - [`TrainableModel`]: used during training; owns mutable parameters and computes gradients.\n//! - [`InferenceModel`]: lightweight, serializable, stateless predictor for production use.\n//!\n//! This design ensures that a fitted model carries **only** what is needed for prediction,\n//! with no optimizer state, loss functions, or training hyperparameters.\n//!\n//! See [`linear::LinearRegressor`] for a concrete example.\n\npub mod state;\npub use state::{Fitted, Unfitted};\n\npub mod linear;\npub use crate::backend::scalar::{Scalar, ScalarOps};\npub use crate::backend::Backend;\nuse crate::serialization::SerializableParams;\n\n/// A model that can be trained: it computes forward passes, gradients, and updates its parameters.\n///\n/// This trait is used **only during training**. After training, it is converted into an\n/// [`InferenceModel`] via [`Self::into_fitted`], which strips away all training-related state.\n///\n/// # Type Parameters\n/// - `B`: The backend (e.g., `CpuBackend`) used for computation.\n/// - `Input`: Input data type (e.g., `Tensor1D\u003cB\u003e`).\n/// - `Prediction`: Output of the forward pass (e.g., scalar for regression).\n/// - `Params`: Internal trainable parameters (e.g., weights + bias).\n/// - `Gradients`: Gradient structure matching `Params`.\n/// - `Output`: The corresponding [`InferenceModel`] type.\n///\n/// # Safety \u0026 Invariants\n/// - `backward` must be called **after** `forward` with the same input.\n/// - `update_params` must preserve the shape/structure of parameters.\n///\n/// # Example\n/// See [`linear::LinearRegressor`] for a full implementation.\npub trait TrainableModel\u003cB: Backend\u003e {\n    type Input;\n    type Prediction;\n    type Params;\n    type Gradients;\n    type Output;\n\n    fn forward(\u0026self, input: \u0026Self::Input) -\u003e Self::Prediction;\n    fn backward(\u0026self, input: \u0026Self::Input, grad_output: \u0026Self::Prediction) -\u003e Self::Gradients;\n    fn params(\u0026self) -\u003e \u0026Self::Params;\n    fn update_params(\u0026mut self, new_params: \u0026Self::Params);\n\n    fn into_fitted(self) -\u003e Self::Output;\n}\n/// Operations required to update model parameters during optimization.\n///\n/// Optimizers like SGD rely on these operations to compute weight updates:\n/// ```text\n/// w_new = w_old + (-lr) * grad\n/// ```\n///\n/// Implementations must be **element-wise** and preserve parameter structure.\n///\npub trait ParamOps\u003cB: Backend\u003e: Clone {\n    fn add(\u0026self, other: \u0026Self) -\u003e Self;\n    fn scale(\u0026self, scalar: Scalar\u003cB\u003e) -\u003e Self;\n}\n/// A lightweight, serializable model for inference only.\n///\n/// This trait represents the **final product** of training: it contains no optimizer state,\n/// loss functions, or batch metadata. It is designed for:\n/// - Fast prediction (`predict`, `predict_batch`)\n/// - Serialization (`save_to_file`, `load_from_file`)\n/// - Deployment in production environments\n///\n/// # Type Parameters\n/// - `InputSingle` / `OutputSingle`: types for single-sample prediction.\n/// - `InputBatch` / `OutputBatch`: types for batched prediction (optional optimization).\n/// - `ParamsRepr`: a serializable representation of internal parameters (e.g., struct with `Vec\u003cf64\u003e`).\n///\n/// # Guarantees\n/// - `extract_params()` + `from_params()` is a round-trip.\n/// - `save_to_file` / `load_from_file` are compatible across platforms (uses `bincode` internally).\n///\n/// # Example\n/// ```rust\n/// use machinelearne_rs::{CpuBackend,\n///                         Tensor1D,\n///                         model::linear::LinearModel,\n///                         model::linear::LinearParams,\n///                         model::linear::Fitted,\n///                         model::InferenceModel,\n///                         backend::Scalar};\n/// let weights = Tensor1D::\u003cCpuBackend\u003e::new(vec![3.0f32, 4.0]);\n/// let bias = Scalar::\u003cCpuBackend\u003e::new(1.0);\n/// let params = LinearParams { weights, bias };\n/// let model = LinearModel::\u003cCpuBackend, Fitted\u003e::from_params((\u0026params).into()).unwrap();\n/// let input = Tensor1D::\u003cCpuBackend\u003e::new(vec![3.0, 4.0]);\n/// let pred = model.predict(\u0026input); // â 1*3 + 2*4 + 0.5 = 11.5\n/// ```\npub trait InferenceModel\u003cB: Backend\u003e {\n    type InputSingle;\n    type OutputSingle;\n    type InputBatch;\n    type OutputBatch;\n    type ParamsRepr: SerializableParams;\n    fn predict(\u0026self, input: \u0026Self::InputSingle) -\u003e Self::OutputSingle;\n    fn predict_batch(\u0026self, input: \u0026Self::InputBatch) -\u003e Self::OutputBatch;\n    fn extract_params(\u0026self) -\u003e Self::ParamsRepr;\n    fn from_params(params: Self::ParamsRepr) -\u003e Result\u003cSelf, Box\u003cdyn std::error::Error\u003e\u003e\n    where\n        Self: Sized;\n\n    fn save_to_file\u003cP: AsRef\u003cstd::path::Path\u003e\u003e(\u0026self, path: P) -\u003e std::io::Result\u003c()\u003e {\n        let bytes = self\n            .extract_params()\n            .to_bytes()\n            .map_err(std::io::Error::other)?;\n        std::fs::write(path, bytes)\n    }\n\n    fn load_from_file\u003cP: AsRef\u003cstd::path::Path\u003e\u003e(\n        path: P,\n    ) -\u003e Result\u003cSelf, Box\u003cdyn std::error::Error\u003e\u003e\n    where\n        Self: Sized,\n    {\n        let bytes = std::fs::read(path)?;\n        let params = Self::ParamsRepr::from_bytes(\u0026bytes)\n            .map_err(|e| -\u003e Box\u003cdyn std::error::Error\u003e { Box::new(e) })?;\n        Self::from_params(params)\n    }\n}\n","traces":[{"line":112,"address":[3121410,3121369,3120864],"length":1,"stats":{"Line":1}},{"line":113,"address":[3120898,3121380,3121062,3121116],"length":1,"stats":{"Line":2}},{"line":116,"address":[3121393,3121103,3121006,3121210],"length":1,"stats":{"Line":1}},{"line":117,"address":[3121259],"length":1,"stats":{"Line":1}},{"line":120,"address":[3122059,3121440,3122086],"length":1,"stats":{"Line":1}},{"line":126,"address":[3121465],"length":1,"stats":{"Line":1}},{"line":127,"address":[3121613,3121823,3121696,3121742],"length":1,"stats":{"Line":3}},{"line":128,"address":[2477824,2477833],"length":1,"stats":{"Line":1}},{"line":129,"address":[3121917],"length":1,"stats":{"Line":1}}],"covered":9,"coverable":9},{"path":["/","home","victor","ML","machinelearne-rs","lib","src","model","state.rs"],"content":"/// A marker type indicating that a model is **not yet trained**.\n///\n/// This phantom type is used in generic parameters (e.g., `LinearRegressor\u003cUnfitted\u003e`)\n/// to enforce compile-time guarantees:\n/// - Training methods (like `Trainer::fit`) require an `Unfitted` model.\n/// - Inference methods (`predict`) are **not available** until the model is converted to `Fitted`.\n///\n/// This prevents accidental use of an untrained model for prediction.\npub struct Unfitted;\n\n/// A marker type indicating that a model has been **fully trained**.\n///\n/// After training, a model is converted from `Model\u003cUnfitted\u003e` to `Model\u003cFitted\u003e`,\n/// which implements [`InferenceModel`] and can be serialized or used for prediction.\n///\n/// A `Fitted` model contains **only inference parameters** â no optimizer state,\n/// loss function, or training hyperparameters (per ADR-0001).\npub struct Fitted;\n","traces":[],"covered":0,"coverable":0},{"path":["/","home","victor","ML","machinelearne-rs","lib","src","optimizer","mod.rs"],"content":"use crate::backend::scalar::{Scalar, ScalarOps};\nuse crate::backend::tensor1d::Tensor1D;\nuse crate::backend::Backend;\nuse crate::loss::TensorLike;\nuse crate::model::linear::LinearParams;\n\n/// Trait for gradient-based optimizers.\n///\n/// Optimizers are responsible for updating model parameters based on computed gradients.\n/// The architecture follows separation of concerns principle: training logic (`Trainer`)\n/// is decoupled from parameter update logic. This enables composable design where any\n/// model can be paired with any optimizer while maintaining full type safety without\n/// dynamic dispatch.\n///\n/// # Type Parameters\n/// * `B` â computation backend implementing [`Backend`]\n/// * `P` â model parameters type (e.g., [`LinearParams`])\n///\n/// # Example\n/// ```rust\n/// # use machinelearne_rs::optimizer::{SGD, Optimizer};\n/// # use machinelearne_rs::backend::CpuBackend;\n/// # use machinelearne_rs::model::linear::LinearParams;\n/// # use machinelearne_rs::backend::tensor1d::Tensor1D;\n/// # use machinelearne_rs::backend::scalar::Scalar;\n/// #\n/// # // Mock parameters and gradients for demonstration\n/// # let params = LinearParams {\n/// #     weights: Tensor1D::\u003cCpuBackend\u003e::new(vec![1.0, 2.0, 3.0]),\n/// #     bias: Scalar::\u003cCpuBackend\u003e::new(0.5),\n/// # };\n/// # let gradients = LinearParams {\n/// #     weights: Tensor1D::\u003cCpuBackend\u003e::new(vec![0.1, -0.2, 0.05]),\n/// #     bias: Scalar::\u003cCpuBackend\u003e::new(-0.01),\n/// # };\n/// let sgd = SGD::\u003cCpuBackend\u003e::new(0.01);\n/// let updated_params = sgd.step(\u0026params, \u0026gradients);\n/// ```\npub trait Optimizer\u003cB: Backend, P\u003e {\n    /// Performs an optimization step using the update rule:\n    /// ```text\n    /// params_new = params - learning_rate * gradients\n    /// ```\n    ///\n    /// # Arguments\n    /// * `params` â current model parameters\n    /// * `gradients` â loss gradients w.r.t. parameters (typically computed via backpropagation)\n    ///\n    /// # Returns\n    /// A new owned instance of updated parameters.\n    ///\n    /// # Note\n    /// This method does not mutate inputs â it returns a new value. This functional\n    /// approach simplifies state management in training loops and enables easier\n    /// composition with immutable data structures.\n    fn step(\u0026self, params: \u0026P, gradients: \u0026P) -\u003e P;\n}\n\n/// Stochastic Gradient Descent (SGD) optimizer.\n///\n/// The simplest first-order optimizer that updates parameters according to:\n/// ```text\n/// Î¸ â Î¸ - Î· Â· âL(Î¸)\n/// ```\n/// where `Î·` is the learning rate and `âL(Î¸)` is the loss gradient.\n///\n/// # Design Notes\n/// * Backend-agnostic via [`Backend`] trait bounds\n/// * Type-safe binding to specific parameter structures through trait implementation\n/// * Stateless by design (no momentum, adaptive learning rates) â specialized variants\n///   like Adam or RMSProp should be implemented as separate optimizers\n/// * Immutable update semantics: returns new parameters instead of mutating in place\n///\n/// # Example\n/// ```rust\n/// use machinelearne_rs::optimizer::SGD;\n/// use machinelearne_rs::backend::CpuBackend;\n///\n/// // Create SGD optimizer with learning rate 0.01\n/// let optimizer = SGD::\u003cCpuBackend\u003e::new(0.01);\n/// ```\n#[derive(Clone)]\npub struct SGD\u003cB: Backend\u003e {\n    /// Learning rate (Î·). Stored as a backend scalar to enable type-safe\n    /// arithmetic operations with tensors.\n    lr: Scalar\u003cB\u003e,\n}\n\nimpl\u003cB: Backend\u003e SGD\u003cB\u003e {\n    /// Creates a new SGD optimizer with the specified learning rate.\n    ///\n    /// # Arguments\n    /// * `lr` â learning rate (positive value, typically in range 1e-4 .. 1e-1)\n    ///\n    /// # Panics\n    /// Does not panic directly, but backend implementations may validate `lr`\n    /// during scalar construction (e.g., rejecting NaN or negative values).\n    pub fn new(lr: f64) -\u003e Self {\n        Self {\n            lr: Scalar::\u003cB\u003e::new(lr),\n        }\n    }\n\n    /// Returns the current learning rate.\n    pub fn learning_rate(\u0026self) -\u003e f64 {\n        self.lr.data.to_f64()\n    }\n}\n\nimpl\u003cB: Backend\u003e Optimizer\u003cB, LinearParams\u003cB\u003e\u003e for SGD\u003cB\u003e\nwhere\n    Tensor1D\u003cB\u003e: Clone,\n    Scalar\u003cB\u003e: Clone,\n{\n    fn step(\u0026self, params: \u0026LinearParams\u003cB\u003e, grads: \u0026LinearParams\u003cB\u003e) -\u003e LinearParams\u003cB\u003e {\n        // weights_new = weights - lr * grad_weights\n        // Using (-lr) enables single scaling operation instead of scale + subtract\n        let neg_lr = Scalar::\u003cB\u003e::new(0.0) - self.lr;\n        let scaled_grad = grads.weights.scale(\u0026neg_lr);\n        let weights_update = params.weights.add(\u0026scaled_grad);\n\n        // bias_new = bias - lr * grad_bias\n        // Using explicit backend methods for consistency with tensor operations\n        let scaled_bias_grad = grads.bias * self.lr;\n        let bias_update = params.bias - scaled_bias_grad;\n\n        LinearParams {\n            weights: weights_update,\n            bias: bias_update,\n        }\n    }\n}\n\n#[cfg(test)]\nmod tests {\n    use super::*;\n    use crate::backend::CpuBackend;\n\n    #[test]\n    fn test_sgd_new_initialization() {\n        let lr = 0.01;\n        let sgd = SGD::\u003cCpuBackend\u003e::new(lr);\n\n        assert_eq!(sgd.learning_rate(), lr);\n    }\n\n    #[test]\n    fn test_sgd_learning_rate_accessor() {\n        let sgd = SGD::\u003cCpuBackend\u003e::new(0.001);\n        assert_eq!(sgd.learning_rate(), 0.001);\n\n        let sgd_large = SGD::\u003cCpuBackend\u003e::new(1.0);\n        assert_eq!(sgd_large.learning_rate(), 1.0);\n    }\n\n    #[test]\n    fn test_sgd_step_correctness_weights() {\n        // params_new = params_old - lr * grads\n        let lr = 0.1;\n        let params = LinearParams {\n            weights: Tensor1D::\u003cCpuBackend\u003e::new(vec![2.0, 3.0]),\n            bias: Scalar::\u003cCpuBackend\u003e::new(1.0),\n        };\n        let grads = LinearParams {\n            weights: Tensor1D::\u003cCpuBackend\u003e::new(vec![1.0, -1.0]),\n            bias: Scalar::\u003cCpuBackend\u003e::new(0.5),\n        };\n\n        let sgd = SGD::\u003cCpuBackend\u003e::new(lr);\n        let updated = sgd.step(\u0026params, \u0026grads);\n\n        // weights: [2.0 - 0.1*1.0, 3.0 - 0.1*(-1.0)] = [1.9, 3.1]\n        assert_eq!(updated.weights.to_vec(), vec![1.9, 3.1]);\n    }\n\n    #[test]\n    fn test_sgd_step_correctness_bias() {\n        let lr = 0.05;\n        let params = LinearParams {\n            weights: Tensor1D::\u003cCpuBackend\u003e::new(vec![1.0]),\n            bias: Scalar::\u003cCpuBackend\u003e::new(2.0),\n        };\n        let grads = LinearParams {\n            weights: Tensor1D::\u003cCpuBackend\u003e::new(vec![0.5]),\n            bias: Scalar::\u003cCpuBackend\u003e::new(-1.0),\n        };\n\n        let sgd = SGD::\u003cCpuBackend\u003e::new(lr);\n        let updated = sgd.step(\u0026params, \u0026grads);\n\n        // bias: 2.0 - 0.05*(-1.0) = 2.0 + 0.05 = 2.05\n        assert_eq!(updated.bias.data.to_f64(), 2.05);\n    }\n\n    #[test]\n    fn test_sgd_step_single_parameter() {\n        let sgd = SGD::\u003cCpuBackend\u003e::new(0.1);\n        let params = LinearParams {\n            weights: Tensor1D::\u003cCpuBackend\u003e::new(vec![5.0]),\n            bias: Scalar::\u003cCpuBackend\u003e::new(0.0),\n        };\n        let grads = LinearParams {\n            weights: Tensor1D::\u003cCpuBackend\u003e::new(vec![2.0]),\n            bias: Scalar::\u003cCpuBackend\u003e::new(0.1),\n        };\n\n        let updated = sgd.step(\u0026params, \u0026grads);\n\n        // weight: 5.0 - 0.1*2.0 = 4.8\n        // bias: 0.0 - 0.1*0.1 = -0.01\n        // Use approximate comparison for floating point\n        assert!((updated.weights.to_vec()[0] - 4.8).abs() \u003c 1e-10);\n        assert!((updated.bias.data.to_f64() - (-0.01)).abs() \u003c 1e-10);\n    }\n\n    #[test]\n    fn test_sgd_step_multiple_parameters() {\n        let sgd = SGD::\u003cCpuBackend\u003e::new(0.01);\n        let params = LinearParams {\n            weights: Tensor1D::\u003cCpuBackend\u003e::new(vec![1.0, 2.0, 3.0, 4.0]),\n            bias: Scalar::\u003cCpuBackend\u003e::new(0.5),\n        };\n        let grads = LinearParams {\n            weights: Tensor1D::\u003cCpuBackend\u003e::new(vec![0.5, -0.25, 0.1, -0.2]),\n            bias: Scalar::\u003cCpuBackend\u003e::new(0.01),\n        };\n\n        let updated = sgd.step(\u0026params, \u0026grads);\n\n        // weights: [1.0-0.01*0.5, 2.0-0.01*(-0.25), 3.0-0.01*0.1, 4.0-0.01*(-0.2)]\n        //         = [0.995, 2.0025, 2.999, 4.002]\n        let expected_weights = vec![0.995, 2.0025, 2.999, 4.002];\n        assert!(updated\n            .weights\n            .to_vec()\n            .iter()\n            .zip(expected_weights.iter())\n            .all(|(a, b)| (a - b).abs() \u003c 1e-10));\n\n        // bias: 0.5 - 0.01*0.01 = 0.4999\n        assert!((updated.bias.data.to_f64() - 0.4999).abs() \u003c 1e-5);\n    }\n\n    #[test]\n    fn test_sgd_step_zero_gradients() {\n        // When gradients are zero, parameters should not change\n        let sgd = SGD::\u003cCpuBackend\u003e::new(0.1);\n        let params = LinearParams {\n            weights: Tensor1D::\u003cCpuBackend\u003e::new(vec![1.0, 2.0]),\n            bias: Scalar::\u003cCpuBackend\u003e::new(0.5),\n        };\n        let zero_grads = LinearParams {\n            weights: Tensor1D::\u003cCpuBackend\u003e::new(vec![0.0, 0.0]),\n            bias: Scalar::\u003cCpuBackend\u003e::new(0.0),\n        };\n\n        let updated = sgd.step(\u0026params, \u0026zero_grads);\n\n        assert_eq!(updated.weights.to_vec(), vec![1.0, 2.0]);\n        assert_eq!(updated.bias.data.to_f64(), 0.5);\n    }\n\n    #[test]\n    fn test_sgd_step_zero_learning_rate() {\n        // Zero learning rate should not change parameters\n        let sgd = SGD::\u003cCpuBackend\u003e::new(0.0);\n        let params = LinearParams {\n            weights: Tensor1D::\u003cCpuBackend\u003e::new(vec![1.0, 2.0]),\n            bias: Scalar::\u003cCpuBackend\u003e::new(0.5),\n        };\n        let grads = LinearParams {\n            weights: Tensor1D::\u003cCpuBackend\u003e::new(vec![1.0, 1.0]),\n            bias: Scalar::\u003cCpuBackend\u003e::new(1.0),\n        };\n\n        let updated = sgd.step(\u0026params, \u0026grads);\n\n        // Parameters should remain unchanged\n        assert_eq!(updated.weights.to_vec(), vec![1.0, 2.0]);\n        assert_eq!(updated.bias.data.to_f64(), 0.5);\n    }\n\n    #[test]\n    fn test_sgd_step_negative_learning_rate() {\n        // Negative learning rate should move in opposite direction\n        let sgd = SGD::\u003cCpuBackend\u003e::new(-0.1);\n        let params = LinearParams {\n            weights: Tensor1D::\u003cCpuBackend\u003e::new(vec![1.0]),\n            bias: Scalar::\u003cCpuBackend\u003e::new(0.0),\n        };\n        let grads = LinearParams {\n            weights: Tensor1D::\u003cCpuBackend\u003e::new(vec![1.0]),\n            bias: Scalar::\u003cCpuBackend\u003e::new(0.5),\n        };\n\n        let updated = sgd.step(\u0026params, \u0026grads);\n\n        // With negative LR: params - (-0.1)*grad = params + 0.1*grad\n        // weight: 1.0 + 0.1*1.0 = 1.1\n        // bias: 0.0 + 0.1*0.5 = 0.05\n        assert_eq!(updated.weights.to_vec(), vec![1.1]);\n        assert_eq!(updated.bias.data.to_f64(), 0.05);\n    }\n\n    #[test]\n    fn test_sgd_step_very_large_learning_rate() {\n        let sgd = SGD::\u003cCpuBackend\u003e::new(100.0);\n        let params = LinearParams {\n            weights: Tensor1D::\u003cCpuBackend\u003e::new(vec![1.0]),\n            bias: Scalar::\u003cCpuBackend\u003e::new(0.0),\n        };\n        let grads = LinearParams {\n            weights: Tensor1D::\u003cCpuBackend\u003e::new(vec![0.01]),\n            bias: Scalar::\u003cCpuBackend\u003e::new(0.01),\n        };\n\n        let updated = sgd.step(\u0026params, \u0026grads);\n\n        // weight: 1.0 - 100.0*0.01 = 0.0\n        // bias: 0.0 - 100.0*0.01 = -1.0\n        assert!((updated.weights.to_vec()[0] - 0.0).abs() \u003c 1e-6);\n        assert!((updated.bias.data.to_f64() - (-1.0)).abs() \u003c 1e-6);\n    }\n\n    #[test]\n    fn test_sgd_step_very_large_gradients() {\n        let sgd = SGD::\u003cCpuBackend\u003e::new(0.01);\n        let params = LinearParams {\n            weights: Tensor1D::\u003cCpuBackend\u003e::new(vec![0.0]),\n            bias: Scalar::\u003cCpuBackend\u003e::new(0.0),\n        };\n        let grads = LinearParams {\n            weights: Tensor1D::\u003cCpuBackend\u003e::new(vec![1000.0]),\n            bias: Scalar::\u003cCpuBackend\u003e::new(1000.0),\n        };\n\n        let updated = sgd.step(\u0026params, \u0026grads);\n\n        // weight: 0.0 - 0.01*1000.0 = -10.0\n        // bias: 0.0 - 0.01*1000.0 = -10.0\n        assert_eq!(updated.weights.to_vec(), vec![-10.0]);\n        assert_eq!(updated.bias.data.to_f64(), -10.0);\n    }\n\n    #[test]\n    fn test_sgd_step_negative_gradients() {\n        let sgd = SGD::\u003cCpuBackend\u003e::new(0.1);\n        let params = LinearParams {\n            weights: Tensor1D::\u003cCpuBackend\u003e::new(vec![1.0]),\n            bias: Scalar::\u003cCpuBackend\u003e::new(0.0),\n        };\n        let grads = LinearParams {\n            weights: Tensor1D::\u003cCpuBackend\u003e::new(vec![-1.0]),\n            bias: Scalar::\u003cCpuBackend\u003e::new(-0.5),\n        };\n\n        let updated = sgd.step(\u0026params, \u0026grads);\n\n        // weight: 1.0 - 0.1*(-1.0) = 1.1\n        // bias: 0.0 - 0.1*(-0.5) = 0.05\n        assert_eq!(updated.weights.to_vec(), vec![1.1]);\n        assert_eq!(updated.bias.data.to_f64(), 0.05);\n    }\n\n    #[test]\n    fn test_sgd_step_fractional_values() {\n        let sgd = SGD::\u003cCpuBackend\u003e::new(0.125);\n        let params = LinearParams {\n            weights: Tensor1D::\u003cCpuBackend\u003e::new(vec![0.5]),\n            bias: Scalar::\u003cCpuBackend\u003e::new(0.25),\n        };\n        let grads = LinearParams {\n            weights: Tensor1D::\u003cCpuBackend\u003e::new(vec![0.8]),\n            bias: Scalar::\u003cCpuBackend\u003e::new(0.4),\n        };\n\n        let updated = sgd.step(\u0026params, \u0026grads);\n\n        // weight: 0.5 - 0.125*0.8 = 0.5 - 0.1 = 0.4\n        // bias: 0.25 - 0.125*0.4 = 0.25 - 0.05 = 0.2\n        // Use approximate comparison for floating point\n        assert!((updated.weights.to_vec()[0] - 0.4).abs() \u003c 1e-6);\n        assert!((updated.bias.data.to_f64() - 0.2).abs() \u003c 1e-6);\n    }\n\n    #[test]\n    fn test_sgd_step_does_not_mutate_inputs() {\n        // Verify that step() does not mutate the original params or grads\n        let sgd = SGD::\u003cCpuBackend\u003e::new(0.1);\n        let original_params = LinearParams {\n            weights: Tensor1D::\u003cCpuBackend\u003e::new(vec![1.0, 2.0]),\n            bias: Scalar::\u003cCpuBackend\u003e::new(0.5),\n        };\n        let original_grads = LinearParams {\n            weights: Tensor1D::\u003cCpuBackend\u003e::new(vec![0.5, 0.3]),\n            bias: Scalar::\u003cCpuBackend\u003e::new(0.1),\n        };\n\n        // Clone to compare after step\n        let params_copy = original_params.clone();\n        let grads_copy = original_grads.clone();\n\n        let _ = sgd.step(\u0026original_params, \u0026original_grads);\n\n        // Original should be unchanged\n        assert_eq!(\n            original_params.weights.to_vec(),\n            params_copy.weights.to_vec()\n        );\n        assert_eq!(\n            original_params.bias.data.to_f64(),\n            params_copy.bias.data.to_f64()\n        );\n        assert_eq!(original_grads.weights.to_vec(), grads_copy.weights.to_vec());\n        assert_eq!(\n            original_grads.bias.data.to_f64(),\n            grads_copy.bias.data.to_f64()\n        );\n    }\n\n    #[test]\n    fn test_sgd_clone() {\n        let sgd = SGD::\u003cCpuBackend\u003e::new(0.01);\n        let sgd_clone = sgd.clone();\n\n        assert_eq!(sgd.learning_rate(), sgd_clone.learning_rate());\n\n        // Both should produce same updates\n        let params = LinearParams {\n            weights: Tensor1D::\u003cCpuBackend\u003e::new(vec![1.0]),\n            bias: Scalar::\u003cCpuBackend\u003e::new(0.0),\n        };\n        let grads = LinearParams {\n            weights: Tensor1D::\u003cCpuBackend\u003e::new(vec![0.5]),\n            bias: Scalar::\u003cCpuBackend\u003e::new(0.2),\n        };\n\n        let updated1 = sgd.step(\u0026params, \u0026grads);\n        let updated2 = sgd_clone.step(\u0026params, \u0026grads);\n\n        assert_eq!(updated1.weights.to_vec(), updated2.weights.to_vec());\n        assert_eq!(updated1.bias.data.to_f64(), updated2.bias.data.to_f64());\n    }\n\n    #[test]\n    fn test_sgd_very_small_learning_rate() {\n        let sgd = SGD::\u003cCpuBackend\u003e::new(1e-10);\n        let params = LinearParams {\n            weights: Tensor1D::\u003cCpuBackend\u003e::new(vec![1.0]),\n            bias: Scalar::\u003cCpuBackend\u003e::new(0.5),\n        };\n        let grads = LinearParams {\n            weights: Tensor1D::\u003cCpuBackend\u003e::new(vec![1000.0]),\n            bias: Scalar::\u003cCpuBackend\u003e::new(1000.0),\n        };\n\n        let updated = sgd.step(\u0026params, \u0026grads);\n\n        // With very small LR, parameters should barely change\n        // weight: 1.0 - 1e-10 * 1000.0 = 0.9999999\n        // bias: 0.5 - 1e-10 * 1000.0 = 0.4999999\n        assert!((updated.weights.to_vec()[0] - 0.9999999).abs() \u003c 1e-6);\n        assert!((updated.bias.data.to_f64() - 0.4999999).abs() \u003c 1e-6);\n    }\n}\n","traces":[{"line":98,"address":[3085632],"length":1,"stats":{"Line":1}},{"line":100,"address":[3085638],"length":1,"stats":{"Line":1}},{"line":105,"address":[3085616],"length":1,"stats":{"Line":2}},{"line":106,"address":[3085621],"length":1,"stats":{"Line":1}},{"line":115,"address":[3085200,3085592,3085586],"length":1,"stats":{"Line":3}},{"line":118,"address":[3085259],"length":1,"stats":{"Line":3}},{"line":119,"address":[3085304],"length":1,"stats":{"Line":3}},{"line":120,"address":[3085324],"length":1,"stats":{"Line":2}},{"line":124,"address":[3085445,3085378],"length":1,"stats":{"Line":6}},{"line":125,"address":[3085454],"length":1,"stats":{"Line":4}}],"covered":10,"coverable":10},{"path":["/","home","victor","ML","machinelearne-rs","lib","src","pipeline.rs"],"content":"// src/pipeline.rs\npub trait TrainablePipeline {\n    type Model;\n    type Loss;\n    type Optimizer;\n    type Backend;\n\n    fn fit(\n        trainer: \u0026Trainer\u003cSelf::Model, Self::Loss, Self::Optimizer\u003e,\n        model: Self::Model,\n        x: \u0026\u003cSelf::Backend as Backend\u003e::Tensor2D,\n        y: \u0026\u003cSelf::Backend as Backend\u003e::Tensor1D,\n    ) -\u003e \u003cSelf::Model as Model\u003e::Fitted;\n}\n","traces":[],"covered":0,"coverable":0},{"path":["/","home","victor","ML","machinelearne-rs","lib","src","preprocessing","column_transformer","column_transformer.rs"],"content":"//! ColumnTransformer implementation.\n//!\n//! Applies different transformers to different column subsets and concatenates results.\n\nuse crate::backend::{Backend, Tensor2D};\nuse crate::preprocessing::encoding::{\n    FittedOneHotEncoder, FittedOrdinalEncoder, OneHotEncoder, OneHotEncoderParams, OrdinalEncoder,\n    OrdinalEncoderParams,\n};\nuse crate::preprocessing::error::PreprocessingError;\nuse crate::preprocessing::imputation::{FittedSimpleImputer, SimpleImputer, SimpleImputerParams};\nuse crate::preprocessing::pipeline::pipeline::{FittedPipeline, Pipeline, PipelineStepEnum};\nuse crate::preprocessing::scaling::{\n    FittedMaxAbsScaler, FittedMinMaxScaler, FittedNormalizer, FittedRobustScaler,\n    FittedStandardScaler, MaxAbsScaler, MaxAbsScalerParams, MinMaxScaler, MinMaxScalerParams,\n    Normalizer, NormalizerParams, RobustScaler, RobustScalerParams, StandardScaler,\n    StandardScalerParams,\n};\nuse crate::preprocessing::traits::{FittedTransformer, Transformer};\nuse crate::serialization::SerializableParams;\nuse serde::{Deserialize, Serialize};\nuse std::marker::PhantomData;\nuse std::ops::Range;\n\n/// Specifies which columns a transformer should be applied to.\n#[derive(Clone, Debug)]\npub enum ColumnSpec {\n    /// Apply to specific column indices.\n    Indices(Vec\u003cusize\u003e),\n    /// Apply to a range of columns.\n    Range(Range\u003cusize\u003e),\n    /// Apply to all columns.\n    All,\n}\n\nimpl ColumnSpec {\n    /// Resolve the column spec to actual column indices.\n    fn resolve(\u0026self, n_features: usize) -\u003e Vec\u003cusize\u003e {\n        match self {\n            ColumnSpec::Indices(indices) =\u003e indices.clone(),\n            ColumnSpec::Range(range) =\u003e range.clone().collect(),\n            ColumnSpec::All =\u003e (0..n_features).collect(),\n        }\n    }\n}\n\n/// Enum of unfitted transformers that can be used in a ColumnTransformer.\n#[derive(Clone)]\npub enum ColumnTransformerStep\u003cB: Backend\u003e {\n    StandardScaler(StandardScaler\u003cB\u003e),\n    MinMaxScaler(MinMaxScaler\u003cB\u003e),\n    RobustScaler(RobustScaler\u003cB\u003e),\n    MaxAbsScaler(MaxAbsScaler\u003cB\u003e),\n    Normalizer(Normalizer\u003cB\u003e),\n    SimpleImputer(SimpleImputer\u003cB\u003e),\n    OneHotEncoder(OneHotEncoder\u003cB\u003e),\n    OrdinalEncoder(OrdinalEncoder\u003cB\u003e),\n    Pipeline(Pipeline\u003cB\u003e),\n}\n\n/// Enum of fitted transformers for ColumnTransformer.\n#[derive(Clone)]\npub enum FittedColumnTransformerStep\u003cB: Backend\u003e {\n    StandardScaler(FittedStandardScaler\u003cB\u003e),\n    MinMaxScaler(FittedMinMaxScaler\u003cB\u003e),\n    RobustScaler(FittedRobustScaler\u003cB\u003e),\n    MaxAbsScaler(FittedMaxAbsScaler\u003cB\u003e),\n    Normalizer(FittedNormalizer\u003cB\u003e),\n    SimpleImputer(FittedSimpleImputer\u003cB\u003e),\n    OneHotEncoder(FittedOneHotEncoder\u003cB\u003e),\n    OrdinalEncoder(FittedOrdinalEncoder\u003cB\u003e),\n    Pipeline(FittedPipeline\u003cB\u003e),\n}\n\nimpl\u003cB: Backend\u003e FittedColumnTransformerStep\u003cB\u003e {\n    /// Transform the data.\n    fn transform(\u0026self, data: \u0026Tensor2D\u003cB\u003e) -\u003e Result\u003cTensor2D\u003cB\u003e, PreprocessingError\u003e {\n        match self {\n            FittedColumnTransformerStep::StandardScaler(t) =\u003e t.transform(data),\n            FittedColumnTransformerStep::MinMaxScaler(t) =\u003e t.transform(data),\n            FittedColumnTransformerStep::RobustScaler(t) =\u003e t.transform(data),\n            FittedColumnTransformerStep::MaxAbsScaler(t) =\u003e t.transform(data),\n            FittedColumnTransformerStep::Normalizer(t) =\u003e t.transform(data),\n            FittedColumnTransformerStep::SimpleImputer(t) =\u003e t.transform(data),\n            FittedColumnTransformerStep::OneHotEncoder(t) =\u003e t.transform(data),\n            FittedColumnTransformerStep::OrdinalEncoder(t) =\u003e t.transform(data),\n            FittedColumnTransformerStep::Pipeline(t) =\u003e t.transform(data),\n        }\n    }\n\n    /// Get the step name.\n    fn step_name(\u0026self) -\u003e \u0026'static str {\n        match self {\n            FittedColumnTransformerStep::StandardScaler(_) =\u003e \"StandardScaler\",\n            FittedColumnTransformerStep::MinMaxScaler(_) =\u003e \"MinMaxScaler\",\n            FittedColumnTransformerStep::RobustScaler(_) =\u003e \"RobustScaler\",\n            FittedColumnTransformerStep::MaxAbsScaler(_) =\u003e \"MaxAbsScaler\",\n            FittedColumnTransformerStep::Normalizer(_) =\u003e \"Normalizer\",\n            FittedColumnTransformerStep::SimpleImputer(_) =\u003e \"SimpleImputer\",\n            FittedColumnTransformerStep::OneHotEncoder(_) =\u003e \"OneHotEncoder\",\n            FittedColumnTransformerStep::OrdinalEncoder(_) =\u003e \"OrdinalEncoder\",\n            FittedColumnTransformerStep::Pipeline(_) =\u003e \"Pipeline\",\n        }\n    }\n\n    /// Get the number of output features.\n    fn n_features_out(\u0026self) -\u003e usize {\n        match self {\n            FittedColumnTransformerStep::StandardScaler(t) =\u003e t.n_features_in(),\n            FittedColumnTransformerStep::MinMaxScaler(t) =\u003e t.n_features_in(),\n            FittedColumnTransformerStep::RobustScaler(t) =\u003e t.n_features_in(),\n            FittedColumnTransformerStep::MaxAbsScaler(t) =\u003e t.n_features_in(),\n            FittedColumnTransformerStep::Normalizer(t) =\u003e t.n_features_in(),\n            FittedColumnTransformerStep::SimpleImputer(t) =\u003e t.n_features_in(),\n            FittedColumnTransformerStep::OneHotEncoder(t) =\u003e t.n_features_out(),\n            FittedColumnTransformerStep::OrdinalEncoder(t) =\u003e t.n_features_in(),\n            FittedColumnTransformerStep::Pipeline(t) =\u003e t.n_features_in(),\n        }\n    }\n}\n\n/// Fit a column transformer step from an unfitted step.\nfn fit_step\u003cB: Backend\u003e(\n    step: \u0026ColumnTransformerStep\u003cB\u003e,\n    data: \u0026Tensor2D\u003cB\u003e,\n) -\u003e Result\u003cFittedColumnTransformerStep\u003cB\u003e, PreprocessingError\u003e {\n    match step {\n        ColumnTransformerStep::StandardScaler(t) =\u003e {\n            t.fit(data).map(FittedColumnTransformerStep::StandardScaler)\n        }\n        ColumnTransformerStep::MinMaxScaler(t) =\u003e {\n            t.fit(data).map(FittedColumnTransformerStep::MinMaxScaler)\n        }\n        ColumnTransformerStep::RobustScaler(t) =\u003e {\n            t.fit(data).map(FittedColumnTransformerStep::RobustScaler)\n        }\n        ColumnTransformerStep::MaxAbsScaler(t) =\u003e {\n            t.fit(data).map(FittedColumnTransformerStep::MaxAbsScaler)\n        }\n        ColumnTransformerStep::Normalizer(t) =\u003e {\n            t.fit(data).map(FittedColumnTransformerStep::Normalizer)\n        }\n        ColumnTransformerStep::SimpleImputer(t) =\u003e {\n            t.fit(data).map(FittedColumnTransformerStep::SimpleImputer)\n        }\n        ColumnTransformerStep::OneHotEncoder(t) =\u003e {\n            t.fit(data).map(FittedColumnTransformerStep::OneHotEncoder)\n        }\n        ColumnTransformerStep::OrdinalEncoder(t) =\u003e {\n            t.fit(data).map(FittedColumnTransformerStep::OrdinalEncoder)\n        }\n        ColumnTransformerStep::Pipeline(t) =\u003e {\n            t.fit(data).map(FittedColumnTransformerStep::Pipeline)\n        }\n    }\n}\n\n/// ColumnTransformer applies different transformers to different columns.\n///\n/// This is useful when you have heterogeneous data and want to apply\n/// different preprocessing to different feature subsets (e.g., scale\n/// numerical features, one-hot encode categorical features).\n///\n/// # Example\n/// ```ignore\n/// use machinelearne_rs::preprocessing::{\n///     ColumnTransformer, ColumnSpec, StandardScaler, OneHotEncoder, Transformer\n/// };\n/// use machinelearne_rs::backend::CpuBackend;\n///\n/// // Columns: [age, income, city_code]\n/// // Scale numerical cols [0, 1], one-hot encode col [2]\n/// let ct = ColumnTransformer::\u003cCpuBackend\u003e::new()\n///     .add(StandardScaler::new(), ColumnSpec::Indices(vec![0, 1]))\n///     .add(OneHotEncoder::new(), ColumnSpec::Indices(vec![2]));\n///\n/// let fitted = ct.fit(\u0026data)?;\n/// let transformed = fitted.transform(\u0026data)?;\n/// ```\n#[derive(Clone)]\npub struct ColumnTransformer\u003cB: Backend\u003e {\n    steps: Vec\u003c(ColumnSpec, ColumnTransformerStep\u003cB\u003e)\u003e,\n    _backend: PhantomData\u003cB\u003e,\n}\n\nimpl\u003cB: Backend\u003e Default for ColumnTransformer\u003cB\u003e {\n    fn default() -\u003e Self {\n        Self::new()\n    }\n}\n\nimpl\u003cB: Backend\u003e ColumnTransformer\u003cB\u003e {\n    /// Create a new empty ColumnTransformer.\n    pub fn new() -\u003e Self {\n        Self {\n            steps: Vec::new(),\n            _backend: PhantomData,\n        }\n    }\n\n    /// Add a StandardScaler for specified columns.\n    pub fn add_standard_scaler(mut self, scaler: StandardScaler\u003cB\u003e, spec: ColumnSpec) -\u003e Self {\n        self.steps\n            .push((spec, ColumnTransformerStep::StandardScaler(scaler)));\n        self\n    }\n\n    /// Add a MinMaxScaler for specified columns.\n    pub fn add_minmax_scaler(mut self, scaler: MinMaxScaler\u003cB\u003e, spec: ColumnSpec) -\u003e Self {\n        self.steps\n            .push((spec, ColumnTransformerStep::MinMaxScaler(scaler)));\n        self\n    }\n\n    /// Add a RobustScaler for specified columns.\n    pub fn add_robust_scaler(mut self, scaler: RobustScaler\u003cB\u003e, spec: ColumnSpec) -\u003e Self {\n        self.steps\n            .push((spec, ColumnTransformerStep::RobustScaler(scaler)));\n        self\n    }\n\n    /// Add a MaxAbsScaler for specified columns.\n    pub fn add_maxabs_scaler(mut self, scaler: MaxAbsScaler\u003cB\u003e, spec: ColumnSpec) -\u003e Self {\n        self.steps\n            .push((spec, ColumnTransformerStep::MaxAbsScaler(scaler)));\n        self\n    }\n\n    /// Add a Normalizer for specified columns.\n    pub fn add_normalizer(mut self, normalizer: Normalizer\u003cB\u003e, spec: ColumnSpec) -\u003e Self {\n        self.steps\n            .push((spec, ColumnTransformerStep::Normalizer(normalizer)));\n        self\n    }\n\n    /// Add a SimpleImputer for specified columns.\n    pub fn add_simple_imputer(mut self, imputer: SimpleImputer\u003cB\u003e, spec: ColumnSpec) -\u003e Self {\n        self.steps\n            .push((spec, ColumnTransformerStep::SimpleImputer(imputer)));\n        self\n    }\n\n    /// Add a OneHotEncoder for specified columns.\n    pub fn add_one_hot_encoder(mut self, encoder: OneHotEncoder\u003cB\u003e, spec: ColumnSpec) -\u003e Self {\n        self.steps\n            .push((spec, ColumnTransformerStep::OneHotEncoder(encoder)));\n        self\n    }\n\n    /// Add an OrdinalEncoder for specified columns.\n    pub fn add_ordinal_encoder(mut self, encoder: OrdinalEncoder\u003cB\u003e, spec: ColumnSpec) -\u003e Self {\n        self.steps\n            .push((spec, ColumnTransformerStep::OrdinalEncoder(encoder)));\n        self\n    }\n\n    /// Add a Pipeline for specified columns.\n    pub fn add_pipeline(mut self, pipeline: Pipeline\u003cB\u003e, spec: ColumnSpec) -\u003e Self {\n        self.steps\n            .push((spec, ColumnTransformerStep::Pipeline(pipeline)));\n        self\n    }\n\n    /// Add a generic step.\n    pub fn add(mut self, step: ColumnTransformerStep\u003cB\u003e, spec: ColumnSpec) -\u003e Self {\n        self.steps.push((spec, step));\n        self\n    }\n\n    /// Get the number of transformer steps.\n    pub fn len(\u0026self) -\u003e usize {\n        self.steps.len()\n    }\n\n    /// Check if empty.\n    pub fn is_empty(\u0026self) -\u003e bool {\n        self.steps.is_empty()\n    }\n}\n\n/// Serializable parameters for fitted column transformer step.\n#[derive(Clone, Serialize, Deserialize)]\npub struct StepParams {\n    /// Column indices this step was applied to.\n    pub columns: Vec\u003cusize\u003e,\n    /// Step type name.\n    pub step_type: String,\n    /// Serialized step parameters.\n    pub params: Vec\u003cu8\u003e,\n}\n\n/// Serializable parameters for a fitted ColumnTransformer.\n#[derive(Clone, Serialize, Deserialize)]\npub struct ColumnTransformerParams {\n    /// Number of input features.\n    pub n_features_in: usize,\n    /// Number of output features.\n    pub n_features_out: usize,\n    /// Step parameters.\n    pub steps: Vec\u003cStepParams\u003e,\n}\n\n/// Fitted ColumnTransformer ready for inference.\n#[derive(Clone)]\npub struct FittedColumnTransformer\u003cB: Backend\u003e {\n    /// Fitted steps with their column indices.\n    fitted_steps: Vec\u003c(Vec\u003cusize\u003e, FittedColumnTransformerStep\u003cB\u003e)\u003e,\n    /// Number of input features.\n    n_features_in: usize,\n    /// Number of output features.\n    n_features_out: usize,\n    _backend: PhantomData\u003cB\u003e,\n}\n\nimpl\u003cB: Backend\u003e FittedColumnTransformer\u003cB\u003e {\n    /// Get the number of input features.\n    pub fn n_features_in(\u0026self) -\u003e usize {\n        self.n_features_in\n    }\n\n    /// Get the number of output features.\n    pub fn n_features_out(\u0026self) -\u003e usize {\n        self.n_features_out\n    }\n\n    /// Get step names.\n    pub fn step_names(\u0026self) -\u003e Vec\u003c(\u0026'static str, \u0026[usize])\u003e {\n        self.fitted_steps\n            .iter()\n            .map(|(cols, step)| (step.step_name(), cols.as_slice()))\n            .collect()\n    }\n}\n\nimpl\u003cB: Backend\u003e Transformer\u003cB\u003e for ColumnTransformer\u003cB\u003e {\n    type Input = Tensor2D\u003cB\u003e;\n    type Output = Tensor2D\u003cB\u003e;\n    type Params = ColumnTransformerParams;\n    type Fitted = FittedColumnTransformer\u003cB\u003e;\n\n    fn fit(\u0026self, data: \u0026Self::Input) -\u003e Result\u003cSelf::Fitted, PreprocessingError\u003e {\n        let (rows, cols) = data.shape();\n\n        if rows == 0 {\n            return Err(PreprocessingError::EmptyData(\n                \"Cannot fit ColumnTransformer on empty data\".to_string(),\n            ));\n        }\n\n        if self.steps.is_empty() {\n            return Err(PreprocessingError::InvalidParameter(\n                \"Cannot fit empty ColumnTransformer\".to_string(),\n            ));\n        }\n\n        let mut fitted_steps = Vec::with_capacity(self.steps.len());\n        let mut n_features_out = 0;\n\n        for (spec, step) in \u0026self.steps {\n            let columns = spec.resolve(cols);\n\n            // Validate columns\n            for \u0026col in \u0026columns {\n                if col \u003e= cols {\n                    return Err(PreprocessingError::InvalidParameter(format!(\n                        \"Column index {} out of bounds (max {})\",\n                        col,\n                        cols - 1\n                    )));\n                }\n            }\n\n            // Extract columns\n            let col_data = extract_columns(data, \u0026columns)?;\n\n            // Fit transformer\n            let fitted = fit_step(step, \u0026col_data)?;\n\n            n_features_out += fitted.n_features_out();\n            fitted_steps.push((columns, fitted));\n        }\n\n        Ok(FittedColumnTransformer {\n            fitted_steps,\n            n_features_in: cols,\n            n_features_out,\n            _backend: PhantomData,\n        })\n    }\n\n    fn fit_transform(\u0026self, data: \u0026Self::Input) -\u003e Result\u003cSelf::Output, PreprocessingError\u003e {\n        let fitted = self.fit(data)?;\n        fitted.transform(data)\n    }\n}\n\nimpl\u003cB: Backend\u003e FittedTransformer\u003cB\u003e for FittedColumnTransformer\u003cB\u003e {\n    type Input = Tensor2D\u003cB\u003e;\n    type Output = Tensor2D\u003cB\u003e;\n    type Params = ColumnTransformerParams;\n\n    fn transform(\u0026self, data: \u0026Self::Input) -\u003e Result\u003cSelf::Output, PreprocessingError\u003e {\n        let (rows, cols) = data.shape();\n\n        if cols != self.n_features_in {\n            return Err(PreprocessingError::FeatureMismatch {\n                expected_features: self.n_features_in,\n                got_features: cols,\n            });\n        }\n\n        if rows == 0 {\n            return Ok(Tensor2D::zeros(0, self.n_features_out));\n        }\n\n        // Transform each step and collect outputs\n        let mut transformed_outputs = Vec::with_capacity(self.fitted_steps.len());\n\n        for (columns, step) in \u0026self.fitted_steps {\n            let col_data = extract_columns(data, columns)?;\n            let transformed = step.transform(\u0026col_data)?;\n            transformed_outputs.push(transformed);\n        }\n\n        // Concatenate all outputs horizontally\n        hcat_tensors(\u0026transformed_outputs)\n    }\n\n    fn inverse_transform(\u0026self, _data: \u0026Self::Output) -\u003e Result\u003cSelf::Input, PreprocessingError\u003e {\n        Err(PreprocessingError::InvalidParameter(\n            \"ColumnTransformer does not support inverse_transform\".to_string(),\n        ))\n    }\n\n    fn extract_params(\u0026self) -\u003e Self::Params {\n        let steps = self\n            .fitted_steps\n            .iter()\n            .map(|(columns, step)| {\n                let (step_type, params) = match step {\n                    FittedColumnTransformerStep::StandardScaler(t) =\u003e (\n                        \"StandardScaler\".to_string(),\n                        t.extract_params().to_bytes().unwrap(),\n                    ),\n                    FittedColumnTransformerStep::MinMaxScaler(t) =\u003e (\n                        \"MinMaxScaler\".to_string(),\n                        t.extract_params().to_bytes().unwrap(),\n                    ),\n                    FittedColumnTransformerStep::RobustScaler(t) =\u003e (\n                        \"RobustScaler\".to_string(),\n                        t.extract_params().to_bytes().unwrap(),\n                    ),\n                    FittedColumnTransformerStep::MaxAbsScaler(t) =\u003e (\n                        \"MaxAbsScaler\".to_string(),\n                        t.extract_params().to_bytes().unwrap(),\n                    ),\n                    FittedColumnTransformerStep::Normalizer(t) =\u003e (\n                        \"Normalizer\".to_string(),\n                        t.extract_params().to_bytes().unwrap(),\n                    ),\n                    FittedColumnTransformerStep::SimpleImputer(t) =\u003e (\n                        \"SimpleImputer\".to_string(),\n                        t.extract_params().to_bytes().unwrap(),\n                    ),\n                    FittedColumnTransformerStep::OneHotEncoder(t) =\u003e (\n                        \"OneHotEncoder\".to_string(),\n                        t.extract_params().to_bytes().unwrap(),\n                    ),\n                    FittedColumnTransformerStep::OrdinalEncoder(t) =\u003e (\n                        \"OrdinalEncoder\".to_string(),\n                        t.extract_params().to_bytes().unwrap(),\n                    ),\n                    FittedColumnTransformerStep::Pipeline(t) =\u003e {\n                        // Pipeline needs special handling\n                        let mut step_params = Vec::new();\n                        for s in t.steps() {\n                            let (name, bytes) = match s {\n                                PipelineStepEnum::StandardScaler(st) =\u003e {\n                                    (\"StandardScaler\", st.extract_params().to_bytes().unwrap())\n                                }\n                                PipelineStepEnum::MinMaxScaler(st) =\u003e {\n                                    (\"MinMaxScaler\", st.extract_params().to_bytes().unwrap())\n                                }\n                                PipelineStepEnum::RobustScaler(st) =\u003e {\n                                    (\"RobustScaler\", st.extract_params().to_bytes().unwrap())\n                                }\n                                PipelineStepEnum::MaxAbsScaler(st) =\u003e {\n                                    (\"MaxAbsScaler\", st.extract_params().to_bytes().unwrap())\n                                }\n                                PipelineStepEnum::Normalizer(st) =\u003e {\n                                    (\"Normalizer\", st.extract_params().to_bytes().unwrap())\n                                }\n                                PipelineStepEnum::SimpleImputer(st) =\u003e {\n                                    (\"SimpleImputer\", st.extract_params().to_bytes().unwrap())\n                                }\n                                PipelineStepEnum::OneHotEncoder(st) =\u003e {\n                                    (\"OneHotEncoder\", st.extract_params().to_bytes().unwrap())\n                                }\n                                PipelineStepEnum::OrdinalEncoder(st) =\u003e {\n                                    (\"OrdinalEncoder\", st.extract_params().to_bytes().unwrap())\n                                }\n                            };\n                            step_params.push((name.to_string(), bytes));\n                        }\n                        (\n                            \"Pipeline\".to_string(),\n                            bincode::serialize(\u0026step_params).unwrap(),\n                        )\n                    }\n                };\n                StepParams {\n                    columns: columns.clone(),\n                    step_type,\n                    params,\n                }\n            })\n            .collect();\n\n        ColumnTransformerParams {\n            n_features_in: self.n_features_in,\n            n_features_out: self.n_features_out,\n            steps,\n        }\n    }\n\n    fn from_params(params: Self::Params) -\u003e Result\u003cSelf, PreprocessingError\u003e {\n        let mut fitted_steps = Vec::with_capacity(params.steps.len());\n\n        for step_params in params.steps {\n            let step = match step_params.step_type.as_str() {\n                \"StandardScaler\" =\u003e {\n                    let p: StandardScalerParams = bincode::deserialize(\u0026step_params.params)\n                        .map_err(|e| PreprocessingError::SerializationError(e.to_string()))?;\n                    FittedColumnTransformerStep::StandardScaler(FittedStandardScaler::from_params(\n                        p,\n                    )?)\n                }\n                \"MinMaxScaler\" =\u003e {\n                    let p: MinMaxScalerParams = bincode::deserialize(\u0026step_params.params)\n                        .map_err(|e| PreprocessingError::SerializationError(e.to_string()))?;\n                    FittedColumnTransformerStep::MinMaxScaler(FittedMinMaxScaler::from_params(p)?)\n                }\n                \"RobustScaler\" =\u003e {\n                    let p: RobustScalerParams = bincode::deserialize(\u0026step_params.params)\n                        .map_err(|e| PreprocessingError::SerializationError(e.to_string()))?;\n                    FittedColumnTransformerStep::RobustScaler(FittedRobustScaler::from_params(p)?)\n                }\n                \"MaxAbsScaler\" =\u003e {\n                    let p: MaxAbsScalerParams = bincode::deserialize(\u0026step_params.params)\n                        .map_err(|e| PreprocessingError::SerializationError(e.to_string()))?;\n                    FittedColumnTransformerStep::MaxAbsScaler(FittedMaxAbsScaler::from_params(p)?)\n                }\n                \"Normalizer\" =\u003e {\n                    let p: NormalizerParams = bincode::deserialize(\u0026step_params.params)\n                        .map_err(|e| PreprocessingError::SerializationError(e.to_string()))?;\n                    FittedColumnTransformerStep::Normalizer(FittedNormalizer::from_params(p)?)\n                }\n                \"SimpleImputer\" =\u003e {\n                    let p: SimpleImputerParams = bincode::deserialize(\u0026step_params.params)\n                        .map_err(|e| PreprocessingError::SerializationError(e.to_string()))?;\n                    FittedColumnTransformerStep::SimpleImputer(FittedSimpleImputer::from_params(p)?)\n                }\n                \"OneHotEncoder\" =\u003e {\n                    let p: OneHotEncoderParams = bincode::deserialize(\u0026step_params.params)\n                        .map_err(|e| PreprocessingError::SerializationError(e.to_string()))?;\n                    FittedColumnTransformerStep::OneHotEncoder(FittedOneHotEncoder::from_params(p)?)\n                }\n                \"OrdinalEncoder\" =\u003e {\n                    let p: OrdinalEncoderParams = bincode::deserialize(\u0026step_params.params)\n                        .map_err(|e| PreprocessingError::SerializationError(e.to_string()))?;\n                    FittedColumnTransformerStep::OrdinalEncoder(FittedOrdinalEncoder::from_params(\n                        p,\n                    )?)\n                }\n                \"Pipeline\" =\u003e {\n                    let inner_steps: Vec\u003c(String, Vec\u003cu8\u003e)\u003e =\n                        bincode::deserialize(\u0026step_params.params)\n                            .map_err(|e| PreprocessingError::SerializationError(e.to_string()))?;\n                    let mut steps = Vec::new();\n                    for (name, bytes) in inner_steps {\n                        let step = match name.as_str() {\n                            \"StandardScaler\" =\u003e {\n                                let p: StandardScalerParams = bincode::deserialize(\u0026bytes)\n                                    .map_err(|e| {\n                                        PreprocessingError::SerializationError(e.to_string())\n                                    })?;\n                                PipelineStepEnum::StandardScaler(FittedStandardScaler::from_params(\n                                    p,\n                                )?)\n                            }\n                            \"MinMaxScaler\" =\u003e {\n                                let p: MinMaxScalerParams =\n                                    bincode::deserialize(\u0026bytes).map_err(|e| {\n                                        PreprocessingError::SerializationError(e.to_string())\n                                    })?;\n                                PipelineStepEnum::MinMaxScaler(FittedMinMaxScaler::from_params(p)?)\n                            }\n                            \"RobustScaler\" =\u003e {\n                                let p: RobustScalerParams =\n                                    bincode::deserialize(\u0026bytes).map_err(|e| {\n                                        PreprocessingError::SerializationError(e.to_string())\n                                    })?;\n                                PipelineStepEnum::RobustScaler(FittedRobustScaler::from_params(p)?)\n                            }\n                            \"MaxAbsScaler\" =\u003e {\n                                let p: MaxAbsScalerParams =\n                                    bincode::deserialize(\u0026bytes).map_err(|e| {\n                                        PreprocessingError::SerializationError(e.to_string())\n                                    })?;\n                                PipelineStepEnum::MaxAbsScaler(FittedMaxAbsScaler::from_params(p)?)\n                            }\n                            \"Normalizer\" =\u003e {\n                                let p: NormalizerParams =\n                                    bincode::deserialize(\u0026bytes).map_err(|e| {\n                                        PreprocessingError::SerializationError(e.to_string())\n                                    })?;\n                                PipelineStepEnum::Normalizer(FittedNormalizer::from_params(p)?)\n                            }\n                            \"SimpleImputer\" =\u003e {\n                                let p: SimpleImputerParams =\n                                    bincode::deserialize(\u0026bytes).map_err(|e| {\n                                        PreprocessingError::SerializationError(e.to_string())\n                                    })?;\n                                PipelineStepEnum::SimpleImputer(FittedSimpleImputer::from_params(\n                                    p,\n                                )?)\n                            }\n                            \"OneHotEncoder\" =\u003e {\n                                let p: OneHotEncoderParams =\n                                    bincode::deserialize(\u0026bytes).map_err(|e| {\n                                        PreprocessingError::SerializationError(e.to_string())\n                                    })?;\n                                PipelineStepEnum::OneHotEncoder(FittedOneHotEncoder::from_params(\n                                    p,\n                                )?)\n                            }\n                            \"OrdinalEncoder\" =\u003e {\n                                let p: OrdinalEncoderParams = bincode::deserialize(\u0026bytes)\n                                    .map_err(|e| {\n                                        PreprocessingError::SerializationError(e.to_string())\n                                    })?;\n                                PipelineStepEnum::OrdinalEncoder(FittedOrdinalEncoder::from_params(\n                                    p,\n                                )?)\n                            }\n                            _ =\u003e {\n                                return Err(PreprocessingError::SerializationError(format!(\n                                    \"Unknown step type: {}\",\n                                    name\n                                )))\n                            }\n                        };\n                        steps.push(step);\n                    }\n                    // Get n_features from the first step\n                    let n_features = steps.first().map(|s| s.n_features_in()).unwrap_or(0);\n                    FittedColumnTransformerStep::Pipeline(FittedPipeline::from_steps(\n                        steps, n_features,\n                    ))\n                }\n                _ =\u003e {\n                    return Err(PreprocessingError::SerializationError(format!(\n                        \"Unknown step type: {}\",\n                        step_params.step_type\n                    )))\n                }\n            };\n            fitted_steps.push((step_params.columns, step));\n        }\n\n        Ok(FittedColumnTransformer {\n            fitted_steps,\n            n_features_in: params.n_features_in,\n            n_features_out: params.n_features_out,\n            _backend: PhantomData,\n        })\n    }\n\n    fn n_features_in(\u0026self) -\u003e usize {\n        self.n_features_in\n    }\n}\n\n/// Extract specified columns from a 2D tensor.\nfn extract_columns\u003cB: Backend\u003e(\n    data: \u0026Tensor2D\u003cB\u003e,\n    columns: \u0026[usize],\n) -\u003e Result\u003cTensor2D\u003cB\u003e, PreprocessingError\u003e {\n    if columns.is_empty() {\n        let (rows, _) = data.shape();\n        return Ok(Tensor2D::zeros(rows, 0));\n    }\n\n    // Use the backend method to select columns\n    let inner = B::select_columns_2d(\u0026data.data, columns);\n    Ok(Tensor2D {\n        data: inner,\n        backend: PhantomData,\n    })\n}\n\n/// Horizontally concatenate tensors.\nfn hcat_tensors\u003cB: Backend\u003e(tensors: \u0026[Tensor2D\u003cB\u003e]) -\u003e Result\u003cTensor2D\u003cB\u003e, PreprocessingError\u003e {\n    if tensors.is_empty() {\n        return Err(PreprocessingError::InvalidParameter(\n            \"Cannot concatenate empty slice of tensors\".to_string(),\n        ));\n    }\n\n    let inner_tensors: Vec\u003c_\u003e = tensors.iter().map(|t| t.data.clone()).collect();\n    let result = B::hcat_2d(\u0026inner_tensors)?;\n    Ok(Tensor2D {\n        data: result,\n        backend: PhantomData,\n    })\n}\n\n#[cfg(test)]\nmod tests {\n    use super::*;\n    use crate::backend::CpuBackend;\n\n    #[test]\n    fn test_column_transformer_basic() {\n        // [[1, 10], [2, 20], [3, 30]]\n        let data = Tensor2D::\u003cCpuBackend\u003e::new(vec![1.0f32, 10.0, 2.0, 20.0, 3.0, 30.0], 3, 2);\n\n        let ct = ColumnTransformer::\u003cCpuBackend\u003e::new()\n            .add_standard_scaler(StandardScaler::new(), ColumnSpec::Indices(vec![0]))\n            .add_standard_scaler(StandardScaler::new(), ColumnSpec::Indices(vec![1]));\n\n        let fitted = ct.fit(\u0026data).unwrap();\n        assert_eq!(fitted.n_features_in(), 2);\n        assert_eq!(fitted.n_features_out(), 2);\n\n        let transformed = fitted.transform(\u0026data).unwrap();\n        assert_eq!(transformed.shape(), (3, 2));\n    }\n\n    #[test]\n    fn test_column_transformer_one_hot() {\n        // [[0], [1], [0]]\n        let data = Tensor2D::\u003cCpuBackend\u003e::new(vec![0.0f32, 1.0, 0.0], 3, 1);\n\n        let ct = ColumnTransformer::\u003cCpuBackend\u003e::new()\n            .add_one_hot_encoder(OneHotEncoder::new(), ColumnSpec::All);\n\n        let fitted = ct.fit(\u0026data).unwrap();\n        assert_eq!(fitted.n_features_out(), 2); // 2 categories\n\n        let transformed = fitted.transform(\u0026data).unwrap();\n        assert_eq!(transformed.shape(), (3, 2));\n    }\n\n    #[test]\n    fn test_column_transformer_mixed() {\n        // [[1, 10, 0], [2, 20, 1], [3, 30, 0]] - cols 0,1 numeric, col 2 categorical\n        let data = Tensor2D::\u003cCpuBackend\u003e::new(\n            vec![1.0f32, 10.0, 0.0, 2.0, 20.0, 1.0, 3.0, 30.0, 0.0],\n            3,\n            3,\n        );\n\n        let ct = ColumnTransformer::\u003cCpuBackend\u003e::new()\n            .add_standard_scaler(StandardScaler::new(), ColumnSpec::Range(0..2))\n            .add_one_hot_encoder(OneHotEncoder::new(), ColumnSpec::Indices(vec![2]));\n\n        let fitted = ct.fit(\u0026data).unwrap();\n        // StandardScaler outputs 2 cols, OneHotEncoder outputs 2 cols = 4 total\n        assert_eq!(fitted.n_features_out(), 4);\n\n        let transformed = fitted.transform(\u0026data).unwrap();\n        assert_eq!(transformed.shape(), (3, 4));\n    }\n\n    #[test]\n    fn test_column_transformer_with_pipeline() {\n        let data = Tensor2D::\u003cCpuBackend\u003e::new(vec![1.0f32, 10.0, 2.0, 20.0, 3.0, 30.0], 3, 2);\n\n        let pipeline = Pipeline::\u003cCpuBackend\u003e::new()\n            .add_standard_scaler(StandardScaler::new())\n            .add_minmax_scaler(MinMaxScaler::new().with_range(0.0, 1.0));\n\n        let ct = ColumnTransformer::\u003cCpuBackend\u003e::new().add_pipeline(pipeline, ColumnSpec::All);\n\n        let fitted = ct.fit(\u0026data).unwrap();\n        let transformed = fitted.transform(\u0026data).unwrap();\n\n        // Check values are in [0, 1] range after MinMaxScaler\n        let vals = transformed.ravel().to_vec();\n        for \u0026v in \u0026vals {\n            assert!(v \u003e= -1e-6 \u0026\u0026 v \u003c= 1.0 + 1e-6);\n        }\n    }\n\n    #[test]\n    fn test_column_transformer_serialization() {\n        let data = Tensor2D::\u003cCpuBackend\u003e::new(\n            vec![1.0f32, 10.0, 0.0, 2.0, 20.0, 1.0, 3.0, 30.0, 0.0],\n            3,\n            3,\n        );\n\n        let ct = ColumnTransformer::\u003cCpuBackend\u003e::new()\n            .add_standard_scaler(StandardScaler::new(), ColumnSpec::Indices(vec![0, 1]))\n            .add_one_hot_encoder(OneHotEncoder::new(), ColumnSpec::Indices(vec![2]));\n\n        let fitted = ct.fit(\u0026data).unwrap();\n\n        let temp_file = std::env::temp_dir().join(\"test_column_transformer.bin\");\n        fitted.save_to_file(\u0026temp_file).unwrap();\n\n        let loaded = FittedColumnTransformer::\u003cCpuBackend\u003e::load_from_file(\u0026temp_file).unwrap();\n\n        assert_eq!(loaded.n_features_in(), fitted.n_features_in());\n        assert_eq!(loaded.n_features_out(), fitted.n_features_out());\n\n        // Compare transform results\n        let t1 = fitted.transform(\u0026data).unwrap();\n        let t2 = loaded.transform(\u0026data).unwrap();\n\n        let v1 = t1.ravel().to_vec();\n        let v2 = t2.ravel().to_vec();\n        for (a, b) in v1.iter().zip(v2.iter()) {\n            assert!((a - b).abs() \u003c 1e-6);\n        }\n\n        std::fs::remove_file(temp_file).ok();\n    }\n\n    #[test]\n    fn test_column_transformer_feature_mismatch() {\n        let train = Tensor2D::\u003cCpuBackend\u003e::new(vec![1.0f32, 10.0, 2.0, 20.0], 2, 2);\n        let test = Tensor2D::\u003cCpuBackend\u003e::new(vec![1.0f32, 10.0, 2.0], 1, 3);\n\n        let ct = ColumnTransformer::\u003cCpuBackend\u003e::new()\n            .add_standard_scaler(StandardScaler::new(), ColumnSpec::All);\n\n        let fitted = ct.fit(\u0026train).unwrap();\n\n        let result = fitted.transform(\u0026test);\n        assert!(matches!(\n            result,\n            Err(PreprocessingError::FeatureMismatch { .. })\n        ));\n    }\n\n    #[test]\n    fn test_column_transformer_column_out_of_bounds() {\n        let data = Tensor2D::\u003cCpuBackend\u003e::new(vec![1.0f32, 10.0], 1, 2);\n\n        let ct = ColumnTransformer::\u003cCpuBackend\u003e::new()\n            .add_standard_scaler(StandardScaler::new(), ColumnSpec::Indices(vec![5]));\n\n        let result = ct.fit(\u0026data);\n        assert!(matches!(\n            result,\n            Err(PreprocessingError::InvalidParameter(_))\n        ));\n    }\n}\n","traces":[{"line":38,"address":[2519104],"length":1,"stats":{"Line":6}},{"line":39,"address":[2519137],"length":1,"stats":{"Line":6}},{"line":40,"address":[2519199],"length":1,"stats":{"Line":3}},{"line":41,"address":[2519223],"length":1,"stats":{"Line":1}},{"line":42,"address":[2519261],"length":1,"stats":{"Line":2}},{"line":77,"address":[3373728],"length":1,"stats":{"Line":1}},{"line":78,"address":[3373765],"length":1,"stats":{"Line":2}},{"line":79,"address":[3373839],"length":1,"stats":{"Line":1}},{"line":80,"address":[3373873],"length":1,"stats":{"Line":0}},{"line":81,"address":[3373903],"length":1,"stats":{"Line":0}},{"line":82,"address":[3373937],"length":1,"stats":{"Line":0}},{"line":83,"address":[3373971],"length":1,"stats":{"Line":0}},{"line":84,"address":[3374002],"length":1,"stats":{"Line":0}},{"line":85,"address":[3374033],"length":1,"stats":{"Line":1}},{"line":86,"address":[3374064],"length":1,"stats":{"Line":0}},{"line":87,"address":[3374095],"length":1,"stats":{"Line":1}},{"line":92,"address":[],"length":0,"stats":{"Line":0}},{"line":93,"address":[],"length":0,"stats":{"Line":0}},{"line":94,"address":[],"length":0,"stats":{"Line":0}},{"line":95,"address":[],"length":0,"stats":{"Line":0}},{"line":96,"address":[],"length":0,"stats":{"Line":0}},{"line":97,"address":[],"length":0,"stats":{"Line":0}},{"line":98,"address":[],"length":0,"stats":{"Line":0}},{"line":99,"address":[],"length":0,"stats":{"Line":0}},{"line":100,"address":[],"length":0,"stats":{"Line":0}},{"line":101,"address":[],"length":0,"stats":{"Line":0}},{"line":102,"address":[],"length":0,"stats":{"Line":0}},{"line":107,"address":[3373408],"length":1,"stats":{"Line":1}},{"line":108,"address":[3373421],"length":1,"stats":{"Line":1}},{"line":109,"address":[3373484],"length":1,"stats":{"Line":2}},{"line":110,"address":[3373512],"length":1,"stats":{"Line":0}},{"line":111,"address":[3373536],"length":1,"stats":{"Line":0}},{"line":112,"address":[3373564],"length":1,"stats":{"Line":0}},{"line":113,"address":[3373589],"length":1,"stats":{"Line":0}},{"line":114,"address":[3373614],"length":1,"stats":{"Line":0}},{"line":115,"address":[3373639],"length":1,"stats":{"Line":1}},{"line":116,"address":[3373664],"length":1,"stats":{"Line":0}},{"line":117,"address":[3373689],"length":1,"stats":{"Line":1}},{"line":123,"address":[3374128],"length":1,"stats":{"Line":6}},{"line":127,"address":[3374170],"length":1,"stats":{"Line":6}},{"line":128,"address":[3374210],"length":1,"stats":{"Line":4}},{"line":129,"address":[3374222],"length":1,"stats":{"Line":4}},{"line":131,"address":[3374261],"length":1,"stats":{"Line":0}},{"line":132,"address":[3374273],"length":1,"stats":{"Line":0}},{"line":134,"address":[3374312],"length":1,"stats":{"Line":0}},{"line":135,"address":[3374324],"length":1,"stats":{"Line":0}},{"line":137,"address":[3374369],"length":1,"stats":{"Line":0}},{"line":138,"address":[3374381],"length":1,"stats":{"Line":0}},{"line":140,"address":[3374426],"length":1,"stats":{"Line":0}},{"line":141,"address":[3374438],"length":1,"stats":{"Line":0}},{"line":143,"address":[3374483],"length":1,"stats":{"Line":0}},{"line":144,"address":[3374495],"length":1,"stats":{"Line":0}},{"line":146,"address":[3374540],"length":1,"stats":{"Line":1}},{"line":147,"address":[3374552],"length":1,"stats":{"Line":1}},{"line":149,"address":[3374594],"length":1,"stats":{"Line":0}},{"line":150,"address":[3374606],"length":1,"stats":{"Line":0}},{"line":152,"address":[3374648],"length":1,"stats":{"Line":1}},{"line":153,"address":[3374660],"length":1,"stats":{"Line":1}},{"line":187,"address":[],"length":0,"stats":{"Line":0}},{"line":188,"address":[],"length":0,"stats":{"Line":0}},{"line":194,"address":[3372800],"length":1,"stats":{"Line":5}},{"line":196,"address":[3372813],"length":1,"stats":{"Line":6}},{"line":202,"address":[3372778,3372576],"length":1,"stats":{"Line":5}},{"line":203,"address":[],"length":0,"stats":{"Line":0}},{"line":204,"address":[3372629],"length":1,"stats":{"Line":5}},{"line":205,"address":[3372748],"length":1,"stats":{"Line":5}},{"line":209,"address":[],"length":0,"stats":{"Line":0}},{"line":210,"address":[],"length":0,"stats":{"Line":0}},{"line":211,"address":[],"length":0,"stats":{"Line":0}},{"line":212,"address":[],"length":0,"stats":{"Line":0}},{"line":216,"address":[],"length":0,"stats":{"Line":0}},{"line":217,"address":[],"length":0,"stats":{"Line":0}},{"line":218,"address":[],"length":0,"stats":{"Line":0}},{"line":219,"address":[],"length":0,"stats":{"Line":0}},{"line":223,"address":[],"length":0,"stats":{"Line":0}},{"line":224,"address":[],"length":0,"stats":{"Line":0}},{"line":225,"address":[],"length":0,"stats":{"Line":0}},{"line":226,"address":[],"length":0,"stats":{"Line":0}},{"line":230,"address":[],"length":0,"stats":{"Line":0}},{"line":231,"address":[],"length":0,"stats":{"Line":0}},{"line":232,"address":[],"length":0,"stats":{"Line":0}},{"line":233,"address":[],"length":0,"stats":{"Line":0}},{"line":237,"address":[],"length":0,"stats":{"Line":0}},{"line":238,"address":[],"length":0,"stats":{"Line":0}},{"line":239,"address":[],"length":0,"stats":{"Line":0}},{"line":240,"address":[],"length":0,"stats":{"Line":0}},{"line":244,"address":[3372553,3372368],"length":1,"stats":{"Line":1}},{"line":245,"address":[],"length":0,"stats":{"Line":0}},{"line":246,"address":[3372409],"length":1,"stats":{"Line":1}},{"line":247,"address":[3372523],"length":1,"stats":{"Line":1}},{"line":251,"address":[],"length":0,"stats":{"Line":0}},{"line":252,"address":[],"length":0,"stats":{"Line":0}},{"line":253,"address":[],"length":0,"stats":{"Line":0}},{"line":254,"address":[],"length":0,"stats":{"Line":0}},{"line":258,"address":[3372344,3372160],"length":1,"stats":{"Line":1}},{"line":259,"address":[],"length":0,"stats":{"Line":0}},{"line":260,"address":[3372190],"length":1,"stats":{"Line":1}},{"line":261,"address":[3372314],"length":1,"stats":{"Line":1}},{"line":265,"address":[],"length":0,"stats":{"Line":0}},{"line":266,"address":[],"length":0,"stats":{"Line":0}},{"line":267,"address":[],"length":0,"stats":{"Line":0}},{"line":271,"address":[],"length":0,"stats":{"Line":0}},{"line":272,"address":[],"length":0,"stats":{"Line":0}},{"line":276,"address":[],"length":0,"stats":{"Line":0}},{"line":277,"address":[],"length":0,"stats":{"Line":0}},{"line":317,"address":[3373376],"length":1,"stats":{"Line":1}},{"line":318,"address":[3373381],"length":1,"stats":{"Line":1}},{"line":322,"address":[3373392],"length":1,"stats":{"Line":1}},{"line":323,"address":[3373397],"length":1,"stats":{"Line":1}},{"line":327,"address":[],"length":0,"stats":{"Line":0}},{"line":328,"address":[],"length":0,"stats":{"Line":0}},{"line":330,"address":[],"length":0,"stats":{"Line":0}},{"line":341,"address":[3378206,3376000,3378653],"length":1,"stats":{"Line":1}},{"line":342,"address":[3376051],"length":1,"stats":{"Line":6}},{"line":344,"address":[3376096],"length":1,"stats":{"Line":6}},{"line":345,"address":[3376133],"length":1,"stats":{"Line":0}},{"line":346,"address":[3376102],"length":1,"stats":{"Line":0}},{"line":350,"address":[3376224],"length":1,"stats":{"Line":6}},{"line":351,"address":[3376349],"length":1,"stats":{"Line":0}},{"line":352,"address":[3376318],"length":1,"stats":{"Line":0}},{"line":356,"address":[3376253],"length":1,"stats":{"Line":6}},{"line":357,"address":[3376286],"length":1,"stats":{"Line":6}},{"line":359,"address":[3376488,3378183,3376298],"length":1,"stats":{"Line":13}},{"line":360,"address":[3376813,3376625],"length":1,"stats":{"Line":12}},{"line":363,"address":[3376897,3376829],"length":1,"stats":{"Line":12}},{"line":364,"address":[3377008],"length":1,"stats":{"Line":7}},{"line":365,"address":[3378368,3378304],"length":1,"stats":{"Line":2}},{"line":366,"address":[],"length":0,"stats":{"Line":0}},{"line":367,"address":[],"length":0,"stats":{"Line":0}},{"line":368,"address":[3378267,3378347],"length":1,"stats":{"Line":1}},{"line":374,"address":[3378257,3377024],"length":1,"stats":{"Line":6}},{"line":377,"address":[3377401,3377338],"length":1,"stats":{"Line":7}},{"line":379,"address":[3377731,3378132,3377789],"length":1,"stats":{"Line":2}},{"line":380,"address":[3377830],"length":1,"stats":{"Line":1}},{"line":383,"address":[3376704],"length":1,"stats":{"Line":1}},{"line":384,"address":[3376648],"length":1,"stats":{"Line":1}},{"line":385,"address":[],"length":0,"stats":{"Line":0}},{"line":386,"address":[3376696],"length":1,"stats":{"Line":1}},{"line":387,"address":[],"length":0,"stats":{"Line":0}},{"line":391,"address":[],"length":0,"stats":{"Line":0}},{"line":392,"address":[],"length":0,"stats":{"Line":0}},{"line":393,"address":[],"length":0,"stats":{"Line":0}},{"line":402,"address":[3404670,3404719,3403344],"length":1,"stats":{"Line":1}},{"line":403,"address":[3403395],"length":1,"stats":{"Line":1}},{"line":405,"address":[3403439],"length":1,"stats":{"Line":1}},{"line":406,"address":[3403480],"length":1,"stats":{"Line":1}},{"line":407,"address":[3403476],"length":1,"stats":{"Line":1}},{"line":408,"address":[],"length":0,"stats":{"Line":0}},{"line":412,"address":[3403450],"length":1,"stats":{"Line":1}},{"line":413,"address":[3403536],"length":1,"stats":{"Line":0}},{"line":417,"address":[3403609],"length":1,"stats":{"Line":1}},{"line":419,"address":[3403642,3403722],"length":1,"stats":{"Line":2}},{"line":420,"address":[3403952,3403843,3404717],"length":1,"stats":{"Line":3}},{"line":421,"address":[3404277,3404214],"length":1,"stats":{"Line":2}},{"line":422,"address":[3404503],"length":1,"stats":{"Line":1}},{"line":426,"address":[3403872],"length":1,"stats":{"Line":1}},{"line":429,"address":[],"length":0,"stats":{"Line":0}},{"line":430,"address":[],"length":0,"stats":{"Line":0}},{"line":431,"address":[],"length":0,"stats":{"Line":0}},{"line":435,"address":[3397408],"length":1,"stats":{"Line":1}},{"line":436,"address":[3397438],"length":1,"stats":{"Line":1}},{"line":437,"address":[],"length":0,"stats":{"Line":0}},{"line":439,"address":[3397552,3398764,3397460,3397599,3398743],"length":1,"stats":{"Line":3}},{"line":440,"address":[3398654,3397638],"length":1,"stats":{"Line":2}},{"line":441,"address":[3398537,3397712],"length":1,"stats":{"Line":2}},{"line":442,"address":[3397732],"length":1,"stats":{"Line":1}},{"line":443,"address":[3398456,3397777,3398507],"length":1,"stats":{"Line":3}},{"line":445,"address":[3398915,3397795],"length":1,"stats":{"Line":0}},{"line":446,"address":[3397803],"length":1,"stats":{"Line":0}},{"line":447,"address":[3398885,3398834,3397848],"length":1,"stats":{"Line":0}},{"line":449,"address":[3399188,3397866],"length":1,"stats":{"Line":0}},{"line":450,"address":[3397886],"length":1,"stats":{"Line":0}},{"line":451,"address":[3399158,3397931,3399107],"length":1,"stats":{"Line":0}},{"line":453,"address":[3397949,3399461],"length":1,"stats":{"Line":0}},{"line":454,"address":[3397969],"length":1,"stats":{"Line":0}},{"line":455,"address":[3399431,3399380,3398014],"length":1,"stats":{"Line":0}},{"line":457,"address":[3399714,3398032],"length":1,"stats":{"Line":0}},{"line":458,"address":[3398049],"length":1,"stats":{"Line":0}},{"line":459,"address":[3399646,3398083],"length":1,"stats":{"Line":0}},{"line":461,"address":[3398110,3399956],"length":1,"stats":{"Line":0}},{"line":462,"address":[3398127],"length":1,"stats":{"Line":0}},{"line":463,"address":[3399875,3399926,3398169],"length":1,"stats":{"Line":0}},{"line":465,"address":[3400229,3398187],"length":1,"stats":{"Line":2}},{"line":466,"address":[3398204],"length":1,"stats":{"Line":1}},{"line":467,"address":[3400148,3400199,3398246],"length":1,"stats":{"Line":3}},{"line":469,"address":[3400502,3398264],"length":1,"stats":{"Line":0}},{"line":470,"address":[3398281],"length":1,"stats":{"Line":0}},{"line":471,"address":[3400472,3400421,3398323],"length":1,"stats":{"Line":0}},{"line":473,"address":[3398341],"length":1,"stats":{"Line":0}},{"line":475,"address":[3398366],"length":1,"stats":{"Line":0}},{"line":476,"address":[3400684,3398376,3403302],"length":1,"stats":{"Line":0}},{"line":477,"address":[3401848,3400812],"length":1,"stats":{"Line":0}},{"line":478,"address":[3401409],"length":1,"stats":{"Line":0}},{"line":479,"address":[3401744,3401697,3401429],"length":1,"stats":{"Line":0}},{"line":481,"address":[3401444],"length":1,"stats":{"Line":0}},{"line":482,"address":[3402005,3401460,3401958],"length":1,"stats":{"Line":0}},{"line":484,"address":[3401475],"length":1,"stats":{"Line":0}},{"line":485,"address":[3402128,3401495,3402175],"length":1,"stats":{"Line":0}},{"line":487,"address":[3401510],"length":1,"stats":{"Line":0}},{"line":488,"address":[3402298,3401530,3402345],"length":1,"stats":{"Line":0}},{"line":490,"address":[3401545],"length":1,"stats":{"Line":0}},{"line":491,"address":[3401557,3402461],"length":1,"stats":{"Line":0}},{"line":493,"address":[3401581],"length":1,"stats":{"Line":0}},{"line":494,"address":[3401601,3402625,3402672],"length":1,"stats":{"Line":0}},{"line":496,"address":[3401616],"length":1,"stats":{"Line":0}},{"line":497,"address":[3402795,3402842,3401636],"length":1,"stats":{"Line":0}},{"line":499,"address":[3401651],"length":1,"stats":{"Line":0}},{"line":500,"address":[3403012,3401671,3402965],"length":1,"stats":{"Line":0}},{"line":503,"address":[3401920,3403160],"length":1,"stats":{"Line":0}},{"line":506,"address":[3400869],"length":1,"stats":{"Line":0}},{"line":507,"address":[3400916,3400967],"length":1,"stats":{"Line":0}},{"line":511,"address":[3401267],"length":1,"stats":{"Line":1}},{"line":512,"address":[3398718],"length":1,"stats":{"Line":1}},{"line":513,"address":[3401171],"length":1,"stats":{"Line":1}},{"line":514,"address":[3401219],"length":1,"stats":{"Line":1}},{"line":520,"address":[3397492],"length":1,"stats":{"Line":1}},{"line":521,"address":[3397496],"length":1,"stats":{"Line":1}},{"line":526,"address":[3394896,3378672,3383868],"length":1,"stats":{"Line":1}},{"line":527,"address":[3378759,3378975],"length":1,"stats":{"Line":2}},{"line":529,"address":[3379226,3379099,3379005,3394746],"length":1,"stats":{"Line":4}},{"line":530,"address":[3379652,3379351],"length":1,"stats":{"Line":2}},{"line":531,"address":[3379674],"length":1,"stats":{"Line":1}},{"line":532,"address":[3394031,3393985,3379755,3394131],"length":1,"stats":{"Line":3}},{"line":533,"address":[3394083,3394008,3396428,3396400],"length":1,"stats":{"Line":1}},{"line":534,"address":[3394444,3394544,3394380],"length":1,"stats":{"Line":2}},{"line":535,"address":[3394292],"length":1,"stats":{"Line":1}},{"line":538,"address":[3379729,3379800],"length":1,"stats":{"Line":2}},{"line":539,"address":[3393221,3393175,3393321,3379848],"length":1,"stats":{"Line":0}},{"line":540,"address":[3395216,3393273,3393198,3395244],"length":1,"stats":{"Line":0}},{"line":541,"address":[3393730,3393546],"length":1,"stats":{"Line":0}},{"line":543,"address":[3379822,3379893],"length":1,"stats":{"Line":2}},{"line":544,"address":[3379941,3392437,3392537,3392391],"length":1,"stats":{"Line":0}},{"line":545,"address":[3392414,3392489,3395072,3395100],"length":1,"stats":{"Line":0}},{"line":546,"address":[3392730,3392898],"length":1,"stats":{"Line":0}},{"line":548,"address":[3379915,3379986],"length":1,"stats":{"Line":2}},{"line":549,"address":[3391801,3391655,3391701,3380034],"length":1,"stats":{"Line":0}},{"line":550,"address":[3396976,3391753,3391678,3397004],"length":1,"stats":{"Line":0}},{"line":551,"address":[3391962,3392114],"length":1,"stats":{"Line":0}},{"line":553,"address":[3380008,3380079],"length":1,"stats":{"Line":2}},{"line":554,"address":[3391626,3391383,3391343,3391243,3391197,3380127],"length":1,"stats":{"Line":0}},{"line":555,"address":[3397292,3391220,3391295,3397264],"length":1,"stats":{"Line":0}},{"line":556,"address":[3391408,3391376,3391621],"length":1,"stats":{"Line":0}},{"line":558,"address":[3380172,3380101],"length":1,"stats":{"Line":2}},{"line":559,"address":[3390674,3390597,3390551,3380220],"length":1,"stats":{"Line":0}},{"line":560,"address":[3395648,3395676,3390574,3390626],"length":1,"stats":{"Line":0}},{"line":561,"address":[3390939,3390803],"length":1,"stats":{"Line":0}},{"line":563,"address":[3380194,3380265],"length":1,"stats":{"Line":2}},{"line":564,"address":[3380313,3389913,3389767,3389813],"length":1,"stats":{"Line":3}},{"line":565,"address":[3389865,3389790,3394956,3394928],"length":1,"stats":{"Line":1}},{"line":566,"address":[3390106,3390274],"length":1,"stats":{"Line":2}},{"line":568,"address":[3380358,3380287],"length":1,"stats":{"Line":0}},{"line":569,"address":[3389031,3380406,3389177,3389077],"length":1,"stats":{"Line":0}},{"line":570,"address":[3389054,3396832,3389129,3396860],"length":1,"stats":{"Line":0}},{"line":571,"address":[3389426,3389490,3389590],"length":1,"stats":{"Line":0}},{"line":572,"address":[3389338],"length":1,"stats":{"Line":0}},{"line":575,"address":[3380380,3380451],"length":1,"stats":{"Line":0}},{"line":576,"address":[3380798,3389005,3380480,3380898,3380752],"length":1,"stats":{"Line":0}},{"line":577,"address":[],"length":0,"stats":{"Line":0}},{"line":578,"address":[3380850,3380775,3395968,3395996],"length":1,"stats":{"Line":0}},{"line":579,"address":[3381003],"length":1,"stats":{"Line":0}},{"line":580,"address":[3381287,3381051,3381160],"length":1,"stats":{"Line":0}},{"line":581,"address":[3382099,3381396],"length":1,"stats":{"Line":0}},{"line":582,"address":[3382121],"length":1,"stats":{"Line":0}},{"line":583,"address":[3388145,3382202,3388245,3388099],"length":1,"stats":{"Line":0}},{"line":584,"address":[3396383,3396256,3396389,3388122],"length":1,"stats":{"Line":0}},{"line":585,"address":[3396284,3396323],"length":1,"stats":{"Line":0}},{"line":587,"address":[3388494,3388558,3388658],"length":1,"stats":{"Line":0}},{"line":588,"address":[3388406],"length":1,"stats":{"Line":0}},{"line":591,"address":[3382176,3382247],"length":1,"stats":{"Line":0}},{"line":592,"address":[3395637,3382295,3395631,3387289,3395504,3387435],"length":1,"stats":{"Line":0}},{"line":593,"address":[],"length":0,"stats":{"Line":0}},{"line":594,"address":[3395571,3395532],"length":1,"stats":{"Line":0}},{"line":596,"address":[3387844,3387660],"length":1,"stats":{"Line":0}},{"line":598,"address":[3382340,3382269],"length":1,"stats":{"Line":0}},{"line":599,"address":[3386651,3382388,3386505,3397120,3397253,3397247],"length":1,"stats":{"Line":0}},{"line":600,"address":[],"length":0,"stats":{"Line":0}},{"line":601,"address":[3397148,3397187],"length":1,"stats":{"Line":0}},{"line":603,"address":[3386844,3387012],"length":1,"stats":{"Line":0}},{"line":605,"address":[3382362,3382427],"length":1,"stats":{"Line":0}},{"line":606,"address":[3395957,3385921,3382472,3385775,3395951,3395824],"length":1,"stats":{"Line":0}},{"line":607,"address":[],"length":0,"stats":{"Line":0}},{"line":608,"address":[3395852,3395891],"length":1,"stats":{"Line":0}},{"line":610,"address":[3386234,3386082],"length":1,"stats":{"Line":0}},{"line":612,"address":[3382508,3382449],"length":1,"stats":{"Line":0}},{"line":613,"address":[3396671,3396544,3385509,3385323,3385469,3396677,3382553,3385752],"length":1,"stats":{"Line":0}},{"line":614,"address":[],"length":0,"stats":{"Line":0}},{"line":615,"address":[3396572,3396611],"length":1,"stats":{"Line":0}},{"line":617,"address":[3385747,3385534,3385502],"length":1,"stats":{"Line":0}},{"line":619,"address":[3382589,3382530],"length":1,"stats":{"Line":0}},{"line":620,"address":[3395360,3395487,3395493,3384683,3384806,3382634],"length":1,"stats":{"Line":0}},{"line":621,"address":[],"length":0,"stats":{"Line":0}},{"line":622,"address":[3395427,3395388],"length":1,"stats":{"Line":0}},{"line":624,"address":[3385007,3385071,3385148],"length":1,"stats":{"Line":0}},{"line":625,"address":[3384935],"length":1,"stats":{"Line":0}},{"line":628,"address":[3382611,3382670],"length":1,"stats":{"Line":0}},{"line":629,"address":[3396821,3396688,3384051,3382715,3383905,3396815],"length":1,"stats":{"Line":0}},{"line":630,"address":[],"length":0,"stats":{"Line":0}},{"line":631,"address":[3396716,3396755],"length":1,"stats":{"Line":0}},{"line":633,"address":[3384348,3384512,3384412],"length":1,"stats":{"Line":0}},{"line":634,"address":[3384244],"length":1,"stats":{"Line":0}},{"line":637,"address":[3382751,3382692],"length":1,"stats":{"Line":0}},{"line":638,"address":[3383088,3383042,3383188,3382780],"length":1,"stats":{"Line":0}},{"line":639,"address":[3396245,3396112,3396239,3383065],"length":1,"stats":{"Line":0}},{"line":640,"address":[3396179,3396140],"length":1,"stats":{"Line":0}},{"line":642,"address":[3383501,3383601,3383437],"length":1,"stats":{"Line":0}},{"line":643,"address":[3383349],"length":1,"stats":{"Line":0}},{"line":646,"address":[],"length":0,"stats":{"Line":0}},{"line":647,"address":[3382773,3382811],"length":1,"stats":{"Line":0}},{"line":648,"address":[],"length":0,"stats":{"Line":0}},{"line":649,"address":[],"length":0,"stats":{"Line":0}},{"line":653,"address":[3383708],"length":1,"stats":{"Line":0}},{"line":656,"address":[3381445,3395801,3395792],"length":1,"stats":{"Line":0}},{"line":657,"address":[3381637],"length":1,"stats":{"Line":0}},{"line":658,"address":[3381581],"length":1,"stats":{"Line":0}},{"line":661,"address":[],"length":0,"stats":{"Line":0}},{"line":662,"address":[3380457,3380518],"length":1,"stats":{"Line":0}},{"line":663,"address":[],"length":0,"stats":{"Line":0}},{"line":664,"address":[],"length":0,"stats":{"Line":0}},{"line":668,"address":[3381742],"length":1,"stats":{"Line":1}},{"line":671,"address":[3379472],"length":1,"stats":{"Line":1}},{"line":672,"address":[3379416],"length":1,"stats":{"Line":1}},{"line":673,"address":[3379464],"length":1,"stats":{"Line":1}},{"line":674,"address":[3379468],"length":1,"stats":{"Line":1}},{"line":675,"address":[],"length":0,"stats":{"Line":0}},{"line":679,"address":[],"length":0,"stats":{"Line":0}},{"line":680,"address":[],"length":0,"stats":{"Line":0}},{"line":685,"address":[3366672],"length":1,"stats":{"Line":6}},{"line":689,"address":[3366744],"length":1,"stats":{"Line":6}},{"line":690,"address":[3366853],"length":1,"stats":{"Line":0}},{"line":691,"address":[3366869],"length":1,"stats":{"Line":0}},{"line":695,"address":[3366768],"length":1,"stats":{"Line":6}},{"line":696,"address":[3366778],"length":1,"stats":{"Line":5}},{"line":697,"address":[],"length":0,"stats":{"Line":0}},{"line":698,"address":[],"length":0,"stats":{"Line":0}},{"line":703,"address":[3366578,3366000,3366584],"length":1,"stats":{"Line":1}},{"line":704,"address":[3366059],"length":1,"stats":{"Line":1}},{"line":705,"address":[3366168],"length":1,"stats":{"Line":0}},{"line":706,"address":[3366140],"length":1,"stats":{"Line":0}},{"line":710,"address":[3366608,3366643,3366078],"length":1,"stats":{"Line":3}},{"line":711,"address":[3366294,3366120],"length":1,"stats":{"Line":2}},{"line":712,"address":[3366466],"length":1,"stats":{"Line":1}},{"line":713,"address":[],"length":0,"stats":{"Line":0}},{"line":714,"address":[],"length":0,"stats":{"Line":0}}],"covered":117,"coverable":343},{"path":["/","home","victor","ML","machinelearne-rs","lib","src","preprocessing","column_transformer","mod.rs"],"content":"//! ColumnTransformer for applying different transformers to different columns.\n//!\n//! This module provides the `ColumnTransformer` which allows applying different\n//! preprocessing steps to different subsets of columns in a dataset.\n\n#[allow(clippy::module_inception)]\nmod column_transformer;\n\npub use column_transformer::{\n    ColumnSpec, ColumnTransformer, ColumnTransformerParams, FittedColumnTransformer,\n};\n","traces":[],"covered":0,"coverable":0},{"path":["/","home","victor","ML","machinelearne-rs","lib","src","preprocessing","encoding","label.rs"],"content":"//! Label encoding for 1D target labels.\n//!\n//! Maps target labels to integer indices (0, 1, 2, ...).\n\nuse crate::backend::{Backend, Tensor1D};\nuse crate::preprocessing::error::PreprocessingError;\nuse crate::serialization::SerializableParams;\nuse serde::{Deserialize, Serialize};\nuse std::collections::HashMap;\nuse std::marker::PhantomData;\n\n/// Label encoder for 1D target labels.\n///\n/// Converts target labels to integer indices, suitable for classification\n/// targets. Unlike OrdinalEncoder which works on 2D data, LabelEncoder\n/// works on 1D arrays (a single target column).\n///\n/// # Example\n/// ```ignore\n/// use machinelearne_rs::preprocessing::LabelEncoder;\n/// use machinelearne_rs::backend::CpuBackend;\n///\n/// // Target labels: [2, 0, 1, 0]\n/// let labels = Tensor1D::new(vec![2.0, 0.0, 1.0, 0.0]);\n///\n/// let encoder = LabelEncoder::\u003cCpuBackend\u003e::new();\n/// let fitted = encoder.fit(\u0026labels)?;\n///\n/// // Encoded: [2, 0, 1, 0] (ordinal mapping)\n/// let encoded = fitted.transform(\u0026labels)?;\n/// ```\n#[derive(Clone, Debug)]\npub struct LabelEncoder\u003cB: Backend\u003e {\n    _backend: PhantomData\u003cB\u003e,\n}\n\nimpl\u003cB: Backend\u003e LabelEncoder\u003cB\u003e {\n    /// Create a new LabelEncoder.\n    pub fn new() -\u003e Self {\n        Self {\n            _backend: PhantomData,\n        }\n    }\n\n    /// Fit the encoder to the labels and return the fitted encoder.\n    pub fn fit(\u0026self, labels: \u0026Tensor1D\u003cB\u003e) -\u003e Result\u003cFittedLabelEncoder\u003cB\u003e, PreprocessingError\u003e {\n        let n = labels.len();\n\n        if n == 0 {\n            return Err(PreprocessingError::EmptyData(\n                \"Cannot fit LabelEncoder on empty data\".to_string(),\n            ));\n        }\n\n        let label_vec = labels.to_vec();\n\n        // Find unique classes\n        let mut classes_set: std::collections::BTreeSet\u003ci32\u003e = std::collections::BTreeSet::new();\n        for \u0026val in \u0026label_vec {\n            if !val.is_finite() {\n                return Err(PreprocessingError::InvalidParameter(format!(\n                    \"LabelEncoder expects finite values, got {}\",\n                    val\n                )));\n            }\n            classes_set.insert(val.round() as i32);\n        }\n\n        let classes_: Vec\u003cf32\u003e = classes_set.into_iter().map(|x| x as f32).collect();\n        let n_classes = classes_.len();\n\n        // Create mapping from class to index\n        let mut class_to_idx: HashMap\u003ci32, usize\u003e = HashMap::new();\n        for (idx, \u0026class) in classes_.iter().enumerate() {\n            class_to_idx.insert(class as i32, idx);\n        }\n\n        Ok(FittedLabelEncoder {\n            classes_,\n            class_to_idx,\n            n_classes,\n            _backend: PhantomData,\n        })\n    }\n\n    /// Fit and transform in one step.\n    pub fn fit_transform(\u0026self, labels: \u0026Tensor1D\u003cB\u003e) -\u003e Result\u003cTensor1D\u003cB\u003e, PreprocessingError\u003e {\n        let fitted = self.fit(labels)?;\n        fitted.transform(labels)\n    }\n}\n\nimpl\u003cB: Backend\u003e Default for LabelEncoder\u003cB\u003e {\n    fn default() -\u003e Self {\n        Self::new()\n    }\n}\n\n/// Serializable parameters for a fitted LabelEncoder.\n#[derive(Clone, Serialize, Deserialize)]\npub struct LabelEncoderParams {\n    /// Unique classes in sorted order.\n    pub classes_: Vec\u003cf32\u003e,\n    /// Number of unique classes.\n    pub n_classes: usize,\n}\n\n// Note: We rely on the blanket impl of SerializableParams for Serialize + Deserialize\n\n/// Fitted LabelEncoder ready for inference.\n#[derive(Clone)]\npub struct FittedLabelEncoder\u003cB: Backend\u003e {\n    /// Unique classes in sorted order.\n    classes_: Vec\u003cf32\u003e,\n    /// Mapping from class value to index.\n    class_to_idx: HashMap\u003ci32, usize\u003e,\n    /// Number of classes.\n    n_classes: usize,\n    _backend: PhantomData\u003cB\u003e,\n}\n\nimpl\u003cB: Backend\u003e FittedLabelEncoder\u003cB\u003e {\n    /// Get the unique classes.\n    pub fn classes(\u0026self) -\u003e \u0026[f32] {\n        \u0026self.classes_\n    }\n\n    /// Get the number of classes.\n    pub fn n_classes(\u0026self) -\u003e usize {\n        self.n_classes\n    }\n\n    /// Transform labels to encoded indices.\n    pub fn transform(\u0026self, labels: \u0026Tensor1D\u003cB\u003e) -\u003e Result\u003cTensor1D\u003cB\u003e, PreprocessingError\u003e {\n        let n = labels.len();\n        if n == 0 {\n            return Ok(Tensor1D::zeros(0));\n        }\n\n        let label_vec = labels.to_vec();\n        let mut result = Vec::with_capacity(n);\n\n        for \u0026val in \u0026label_vec {\n            let key = val.round() as i32;\n            match self.class_to_idx.get(\u0026key) {\n                Some(\u0026idx) =\u003e result.push(idx as f64),\n                None =\u003e {\n                    return Err(PreprocessingError::InvalidParameter(format!(\n                        \"Unknown label value: {}\",\n                        val\n                    )));\n                }\n            }\n        }\n\n        Ok(Tensor1D::new(result.iter().map(|\u0026x| x as f32).collect()))\n    }\n\n    /// Inverse transform encoded indices back to original labels.\n    pub fn inverse_transform(\n        \u0026self,\n        indices: \u0026Tensor1D\u003cB\u003e,\n    ) -\u003e Result\u003cTensor1D\u003cB\u003e, PreprocessingError\u003e {\n        let n = indices.len();\n        if n == 0 {\n            return Ok(Tensor1D::zeros(0));\n        }\n\n        let idx_vec = indices.to_vec();\n        let mut result = Vec::with_capacity(n);\n\n        for \u0026idx in \u0026idx_vec {\n            let idx_usize = idx.round() as usize;\n            if idx_usize \u003e= self.n_classes {\n                return Err(PreprocessingError::InvalidParameter(format!(\n                    \"Index {} out of bounds (max {})\",\n                    idx_usize,\n                    self.n_classes - 1\n                )));\n            }\n            result.push(self.classes_[idx_usize] as f64);\n        }\n\n        Ok(Tensor1D::new(result.iter().map(|\u0026x| x as f32).collect()))\n    }\n\n    /// Extract parameters for serialization.\n    pub fn extract_params(\u0026self) -\u003e LabelEncoderParams {\n        LabelEncoderParams {\n            classes_: self.classes_.clone(),\n            n_classes: self.n_classes,\n        }\n    }\n\n    /// Reconstruct from parameters.\n    pub fn from_params(params: LabelEncoderParams) -\u003e Result\u003cSelf, PreprocessingError\u003e {\n        let mut class_to_idx = HashMap::new();\n        for (idx, \u0026class) in params.classes_.iter().enumerate() {\n            class_to_idx.insert(class as i32, idx);\n        }\n\n        Ok(FittedLabelEncoder {\n            classes_: params.classes_,\n            class_to_idx,\n            n_classes: params.n_classes,\n            _backend: PhantomData,\n        })\n    }\n\n    /// Save to file.\n    pub fn save_to_file\u003cP: AsRef\u003cstd::path::Path\u003e\u003e(\u0026self, path: P) -\u003e std::io::Result\u003c()\u003e {\n        let params = self.extract_params();\n        let bytes = params.to_bytes().map_err(std::io::Error::other)?;\n        std::fs::write(path, bytes)\n    }\n\n    /// Load from file.\n    pub fn load_from_file\u003cP: AsRef\u003cstd::path::Path\u003e\u003e(path: P) -\u003e Result\u003cSelf, PreprocessingError\u003e {\n        let bytes = std::fs::read(path)?;\n        let params = LabelEncoderParams::from_bytes(\u0026bytes)\n            .map_err(|e| PreprocessingError::SerializationError(e.to_string()))?;\n        Self::from_params(params)\n    }\n}\n\n#[cfg(test)]\nmod tests {\n    use super::*;\n    use crate::backend::CpuBackend;\n\n    #[test]\n    fn test_label_encoder_basic() {\n        let labels = Tensor1D::\u003cCpuBackend\u003e::new(vec![2.0f32, 0.0, 1.0, 0.0]);\n\n        let encoder = LabelEncoder::\u003cCpuBackend\u003e::new();\n        let fitted = encoder.fit(\u0026labels).unwrap();\n\n        assert_eq!(fitted.n_classes(), 3);\n        assert_eq!(fitted.classes(), \u0026[0.0f32, 1.0, 2.0]);\n\n        let encoded = fitted.transform(\u0026labels).unwrap();\n        let vals = encoded.to_vec();\n\n        // Classes: [0, 1, 2] -\u003e indices: 0-\u003e0, 1-\u003e1, 2-\u003e2\n        assert!((vals[0] - 2.0).abs() \u003c 1e-6); // 2 -\u003e idx 2\n        assert!((vals[1] - 0.0).abs() \u003c 1e-6); // 0 -\u003e idx 0\n        assert!((vals[2] - 1.0).abs() \u003c 1e-6); // 1 -\u003e idx 1\n        assert!((vals[3] - 0.0).abs() \u003c 1e-6); // 0 -\u003e idx 0\n    }\n\n    #[test]\n    fn test_label_encoder_non_contiguous() {\n        let labels = Tensor1D::\u003cCpuBackend\u003e::new(vec![10.0f32, 5.0, 0.0]);\n\n        let encoder = LabelEncoder::\u003cCpuBackend\u003e::new();\n        let fitted = encoder.fit(\u0026labels).unwrap();\n\n        assert_eq!(fitted.classes(), \u0026[0.0f32, 5.0, 10.0]);\n\n        let encoded = fitted.transform(\u0026labels).unwrap();\n        let vals = encoded.to_vec();\n\n        assert!((vals[0] - 2.0).abs() \u003c 1e-6); // 10 -\u003e idx 2\n        assert!((vals[1] - 1.0).abs() \u003c 1e-6); // 5 -\u003e idx 1\n        assert!((vals[2] - 0.0).abs() \u003c 1e-6); // 0 -\u003e idx 0\n    }\n\n    #[test]\n    fn test_label_encoder_inverse() {\n        let labels = Tensor1D::\u003cCpuBackend\u003e::new(vec![2.0f32, 0.0, 1.0, 0.0]);\n\n        let encoder = LabelEncoder::\u003cCpuBackend\u003e::new();\n        let fitted = encoder.fit(\u0026labels).unwrap();\n\n        let encoded = fitted.transform(\u0026labels).unwrap();\n        let recovered = fitted.inverse_transform(\u0026encoded).unwrap();\n\n        let orig = labels.to_vec();\n        let rec = recovered.to_vec();\n\n        for (o, r) in orig.iter().zip(rec.iter()) {\n            assert!((o - r).abs() \u003c 1e-6);\n        }\n    }\n\n    #[test]\n    fn test_label_encoder_unknown_error() {\n        let train = Tensor1D::\u003cCpuBackend\u003e::new(vec![0.0f32, 1.0]);\n        let test = Tensor1D::\u003cCpuBackend\u003e::new(vec![2.0f32]);\n\n        let encoder = LabelEncoder::\u003cCpuBackend\u003e::new();\n        let fitted = encoder.fit(\u0026train).unwrap();\n\n        let result = fitted.transform(\u0026test);\n        assert!(result.is_err());\n    }\n\n    #[test]\n    fn test_label_encoder_serialization() {\n        let labels = Tensor1D::\u003cCpuBackend\u003e::new(vec![2.0f32, 0.0, 1.0, 0.0]);\n\n        let encoder = LabelEncoder::\u003cCpuBackend\u003e::new();\n        let fitted = encoder.fit(\u0026labels).unwrap();\n\n        let temp_file = std::env::temp_dir().join(\"test_label.bin\");\n        fitted.save_to_file(\u0026temp_file).unwrap();\n\n        let loaded = FittedLabelEncoder::\u003cCpuBackend\u003e::load_from_file(\u0026temp_file).unwrap();\n\n        assert_eq!(loaded.n_classes(), fitted.n_classes());\n        assert_eq!(loaded.classes(), fitted.classes());\n\n        let e1 = fitted.transform(\u0026labels).unwrap();\n        let e2 = loaded.transform(\u0026labels).unwrap();\n\n        let v1 = e1.to_vec();\n        let v2 = e2.to_vec();\n        for (a, b) in v1.iter().zip(v2.iter()) {\n            assert!((a - b).abs() \u003c 1e-6);\n        }\n\n        std::fs::remove_file(temp_file).ok();\n    }\n}\n","traces":[{"line":39,"address":[],"length":0,"stats":{"Line":0}},{"line":46,"address":[2753829,2752160,2753434],"length":1,"stats":{"Line":1}},{"line":47,"address":[2752206],"length":1,"stats":{"Line":2}},{"line":49,"address":[2752227],"length":1,"stats":{"Line":3}},{"line":50,"address":[2752268],"length":1,"stats":{"Line":0}},{"line":51,"address":[2752237],"length":1,"stats":{"Line":0}},{"line":55,"address":[2752383],"length":1,"stats":{"Line":3}},{"line":58,"address":[2752396],"length":1,"stats":{"Line":3}},{"line":59,"address":[2752456,2752537],"length":1,"stats":{"Line":6}},{"line":60,"address":[2753444,2752642],"length":1,"stats":{"Line":6}},{"line":61,"address":[2753501,2753450],"length":1,"stats":{"Line":0}},{"line":62,"address":[],"length":0,"stats":{"Line":0}},{"line":63,"address":[],"length":0,"stats":{"Line":0}},{"line":66,"address":[2753477,2753759],"length":1,"stats":{"Line":6}},{"line":69,"address":[2752665,2753865,2753856],"length":1,"stats":{"Line":9}},{"line":70,"address":[2752782,2752839],"length":1,"stats":{"Line":4}},{"line":73,"address":[2752855],"length":1,"stats":{"Line":1}},{"line":74,"address":[2752870,2752934],"length":1,"stats":{"Line":4}},{"line":75,"address":[2753166,2753429],"length":1,"stats":{"Line":4}},{"line":78,"address":[2753289],"length":1,"stats":{"Line":3}},{"line":79,"address":[2753215],"length":1,"stats":{"Line":3}},{"line":80,"address":[2753263],"length":1,"stats":{"Line":1}},{"line":81,"address":[],"length":0,"stats":{"Line":0}},{"line":82,"address":[],"length":0,"stats":{"Line":0}},{"line":87,"address":[],"length":0,"stats":{"Line":0}},{"line":88,"address":[],"length":0,"stats":{"Line":0}},{"line":89,"address":[],"length":0,"stats":{"Line":0}},{"line":94,"address":[],"length":0,"stats":{"Line":0}},{"line":95,"address":[],"length":0,"stats":{"Line":0}},{"line":124,"address":[2757312],"length":1,"stats":{"Line":1}},{"line":125,"address":[2757317],"length":1,"stats":{"Line":1}},{"line":129,"address":[2757328],"length":1,"stats":{"Line":1}},{"line":130,"address":[2757333],"length":1,"stats":{"Line":1}},{"line":134,"address":[2757344,2758470,2758464],"length":1,"stats":{"Line":1}},{"line":135,"address":[2757395],"length":1,"stats":{"Line":2}},{"line":136,"address":[2757416],"length":1,"stats":{"Line":4}},{"line":137,"address":[2757422],"length":1,"stats":{"Line":0}},{"line":140,"address":[2757508],"length":1,"stats":{"Line":4}},{"line":141,"address":[2757521],"length":1,"stats":{"Line":4}},{"line":143,"address":[2757604,2757668],"length":1,"stats":{"Line":8}},{"line":144,"address":[2758019,2757773],"length":1,"stats":{"Line":8}},{"line":145,"address":[2758058],"length":1,"stats":{"Line":4}},{"line":146,"address":[2758126,2758459],"length":1,"stats":{"Line":6}},{"line":147,"address":[],"length":0,"stats":{"Line":0}},{"line":148,"address":[2758209],"length":1,"stats":{"Line":1}},{"line":149,"address":[],"length":0,"stats":{"Line":0}},{"line":150,"address":[],"length":0,"stats":{"Line":0}},{"line":156,"address":[2757806,2758496,2758506],"length":1,"stats":{"Line":9}},{"line":160,"address":[2756032,2757246,2757252],"length":1,"stats":{"Line":1}},{"line":164,"address":[2756086],"length":1,"stats":{"Line":1}},{"line":165,"address":[2756107],"length":1,"stats":{"Line":1}},{"line":166,"address":[2756113],"length":1,"stats":{"Line":0}},{"line":169,"address":[2756199],"length":1,"stats":{"Line":1}},{"line":170,"address":[2756212],"length":1,"stats":{"Line":1}},{"line":172,"address":[2756298,2756362],"length":1,"stats":{"Line":2}},{"line":173,"address":[2756704,2756467],"length":1,"stats":{"Line":2}},{"line":174,"address":[2756781],"length":1,"stats":{"Line":1}},{"line":175,"address":[2756917,2756980],"length":1,"stats":{"Line":0}},{"line":176,"address":[],"length":0,"stats":{"Line":0}},{"line":177,"address":[],"length":0,"stats":{"Line":0}},{"line":178,"address":[2756832,2756960,2756901],"length":1,"stats":{"Line":0}},{"line":181,"address":[2756800,2756861],"length":1,"stats":{"Line":2}},{"line":184,"address":[2757290,2756491,2757280],"length":1,"stats":{"Line":3}},{"line":188,"address":[2755104],"length":1,"stats":{"Line":1}},{"line":190,"address":[2755127],"length":1,"stats":{"Line":1}},{"line":191,"address":[2755158],"length":1,"stats":{"Line":1}},{"line":196,"address":[2753888,2754512],"length":1,"stats":{"Line":1}},{"line":197,"address":[2753915],"length":1,"stats":{"Line":1}},{"line":198,"address":[2754025,2753964],"length":1,"stats":{"Line":2}},{"line":199,"address":[2754257,2754507],"length":1,"stats":{"Line":2}},{"line":202,"address":[2754380],"length":1,"stats":{"Line":1}},{"line":203,"address":[2754308],"length":1,"stats":{"Line":1}},{"line":204,"address":[2754343],"length":1,"stats":{"Line":1}},{"line":205,"address":[2754371],"length":1,"stats":{"Line":1}},{"line":206,"address":[],"length":0,"stats":{"Line":0}},{"line":211,"address":[2755043,2755081,2754544],"length":1,"stats":{"Line":1}},{"line":212,"address":[2754578],"length":1,"stats":{"Line":1}},{"line":213,"address":[2754722,2754668,2755054],"length":1,"stats":{"Line":2}},{"line":214,"address":[2754884,2754986],"length":1,"stats":{"Line":2}},{"line":218,"address":[2755200,2755843,2755870],"length":1,"stats":{"Line":1}},{"line":219,"address":[2755225],"length":1,"stats":{"Line":1}},{"line":220,"address":[2755373,2755502,2755599,2755456],"length":1,"stats":{"Line":3}},{"line":221,"address":[2755888,2755916,2755479,2755551],"length":1,"stats":{"Line":1}},{"line":222,"address":[2755701],"length":1,"stats":{"Line":1}}],"covered":61,"coverable":84},{"path":["/","home","victor","ML","machinelearne-rs","lib","src","preprocessing","encoding","mod.rs"],"content":"//! Categorical feature encoding transformers.\n//!\n//! This module provides encoders for converting categorical features to numerical\n//! representations that can be used by machine learning models.\n//!\n//! # Available Encoders\n//!\n//! ## OneHotEncoder\n//! Converts categorical integer values to one-hot (dummy) encoding.\n//!\n//! ```ignore\n//! // Input: [[0], [1], [2]]  (3 samples, 1 categorical feature)\n//! // Output: [[1,0,0], [0,1,0], [0,0,1]]  (3 samples, 3 binary features)\n//! ```\n//!\n//! ## OrdinalEncoder\n//! Maps categorical values to integer ordinals (0, 1, 2, ...).\n//!\n//! ## LabelEncoder\n//! Encodes 1D target labels to integers (for classification targets).\n//!\n//! # Design Notes\n//!\n//! All encoders work with `f32` tensor representations, where categorical values\n//! are pre-mapped to integers by the user. This design:\n//! - Maintains consistency with the tensor-based API\n//! - Avoids string handling in the core library\n//! - Allows users to handle their own category-to-integer mapping\n\nmod label;\nmod one_hot;\nmod ordinal;\n\npub use label::{FittedLabelEncoder, LabelEncoder, LabelEncoderParams};\npub use one_hot::{FittedOneHotEncoder, OneHotEncoder, OneHotEncoderParams, OneHotOutput};\npub use ordinal::{FittedOrdinalEncoder, OrdinalEncoder, OrdinalEncoderParams};\n\n/// Strategy for handling unknown categories during transform.\n#[derive(Clone, Copy, Debug, Default, PartialEq, serde::Serialize, serde::Deserialize)]\npub enum HandleUnknown {\n    /// Raise an error when unknown categories are encountered.\n    #[default]\n    Error,\n    /// Ignore unknown categories (output zeros for one-hot, NaN for ordinal).\n    Ignore,\n}\n","traces":[],"covered":0,"coverable":0},{"path":["/","home","victor","ML","machinelearne-rs","lib","src","preprocessing","encoding","one_hot.rs"],"content":"//! One-hot encoding for categorical features.\n//!\n//! Transforms categorical integer values to one-hot (dummy) encoded vectors.\n\nuse crate::backend::{Backend, Tensor2D};\nuse crate::preprocessing::encoding::HandleUnknown;\nuse crate::preprocessing::error::PreprocessingError;\nuse crate::preprocessing::traits::{FittedTransformer, Transformer};\nuse serde::{Deserialize, Serialize};\nuse std::collections::HashSet;\nuse std::marker::PhantomData;\n\n/// One-hot encoder for categorical features.\n///\n/// Converts integer categories to one-hot encoded vectors. Each input column\n/// is treated as a categorical feature, and the encoder learns the unique\n/// values (categories) present in each column during fitting.\n///\n/// # Example\n/// ```ignore\n/// use machinelearne_rs::preprocessing::{OneHotEncoder, Transformer};\n/// use machinelearne_rs::backend::CpuBackend;\n///\n/// // Input: 3 samples with 1 categorical feature each\n/// // Categories: 0, 1, 2 (e.g., \"red\", \"green\", \"blue\")\n/// let data = Tensor2D::new(vec![0.0, 1.0, 2.0], 3, 1);\n///\n/// let encoder = OneHotEncoder::\u003cCpuBackend\u003e::new();\n/// let fitted = encoder.fit(\u0026data)?;\n///\n/// // Output: 3x3 one-hot matrix\n/// let encoded = fitted.transform(\u0026data)?;\n/// // [[1, 0, 0],\n/// //  [0, 1, 0],\n/// //  [0, 0, 1]]\n/// ```\n#[derive(Clone, Debug)]\npub struct OneHotEncoder\u003cB: Backend\u003e {\n    /// How to handle unknown categories during transform.\n    handle_unknown: HandleUnknown,\n    _backend: PhantomData\u003cB\u003e,\n}\n\nimpl\u003cB: Backend\u003e OneHotEncoder\u003cB\u003e {\n    /// Create a new OneHotEncoder with default settings.\n    pub fn new() -\u003e Self {\n        Self {\n            handle_unknown: HandleUnknown::default(),\n            _backend: PhantomData,\n        }\n    }\n\n    /// Set the strategy for handling unknown categories.\n    pub fn with_handle_unknown(mut self, strategy: HandleUnknown) -\u003e Self {\n        self.handle_unknown = strategy;\n        self\n    }\n}\n\nimpl\u003cB: Backend\u003e Default for OneHotEncoder\u003cB\u003e {\n    fn default() -\u003e Self {\n        Self::new()\n    }\n}\n\n/// Output configuration for one-hot encoding.\n#[derive(Clone, Debug, Default)]\npub struct OneHotOutput;\n\n/// Serializable parameters for a fitted OneHotEncoder.\n#[derive(Clone, Serialize, Deserialize)]\npub struct OneHotEncoderParams {\n    /// Categories (unique values) for each input column.\n    pub categories_: Vec\u003cVec\u003cf32\u003e\u003e,\n    /// Number of categories per column.\n    pub n_values_: Vec\u003cusize\u003e,\n    /// Total number of output features.\n    pub n_features_out: usize,\n    /// Number of input features.\n    pub n_features_in: usize,\n    /// Handle unknown strategy.\n    pub handle_unknown: HandleUnknown,\n}\n\n// Note: We rely on the blanket impl of SerializableParams for Serialize + Deserialize\n\n/// Fitted OneHotEncoder ready for inference.\n#[derive(Clone)]\npub struct FittedOneHotEncoder\u003cB: Backend\u003e {\n    /// Categories (unique sorted values) for each input column.\n    categories_: Vec\u003cVec\u003cf32\u003e\u003e,\n    /// Number of categories per column.\n    n_values_: Vec\u003cusize\u003e,\n    /// Total number of output features (sum of n_values_).\n    n_features_out: usize,\n    /// Number of input features.\n    n_features_in: usize,\n    /// Handle unknown strategy.\n    handle_unknown: HandleUnknown,\n    _backend: PhantomData\u003cB\u003e,\n}\n\nimpl\u003cB: Backend\u003e FittedOneHotEncoder\u003cB\u003e {\n    /// Get the categories learned for each feature.\n    pub fn categories(\u0026self) -\u003e \u0026[Vec\u003cf32\u003e] {\n        \u0026self.categories_\n    }\n\n    /// Get the number of output features.\n    pub fn n_features_out(\u0026self) -\u003e usize {\n        self.n_features_out\n    }\n\n    /// Get the number of categories per input feature.\n    pub fn n_values(\u0026self) -\u003e \u0026[usize] {\n        \u0026self.n_values_\n    }\n}\n\nimpl\u003cB: Backend\u003e Transformer\u003cB\u003e for OneHotEncoder\u003cB\u003e {\n    type Input = Tensor2D\u003cB\u003e;\n    type Output = Tensor2D\u003cB\u003e;\n    type Params = OneHotEncoderParams;\n    type Fitted = FittedOneHotEncoder\u003cB\u003e;\n\n    fn fit(\u0026self, data: \u0026Self::Input) -\u003e Result\u003cSelf::Fitted, PreprocessingError\u003e {\n        let (rows, cols) = data.shape();\n\n        if rows == 0 {\n            return Err(PreprocessingError::EmptyData(\n                \"Cannot fit OneHotEncoder on empty data\".to_string(),\n            ));\n        }\n\n        let data_vec = data.ravel().to_vec();\n\n        // Find unique categories for each column\n        let mut categories_: Vec\u003cVec\u003cf32\u003e\u003e = Vec::with_capacity(cols);\n        let mut n_values_: Vec\u003cusize\u003e = Vec::with_capacity(cols);\n\n        for col in 0..cols {\n            let mut col_cats: HashSet\u003ci32\u003e = HashSet::new();\n            for row in 0..rows {\n                let val = data_vec[row * cols + col];\n                if !val.is_finite() || val \u003c 0.0 || val.fract() != 0.0 {\n                    return Err(PreprocessingError::InvalidParameter(format!(\n                        \"OneHotEncoder expects non-negative integer values, got {} at ({}, {})\",\n                        val, row, col\n                    )));\n                }\n                col_cats.insert(val as i32);\n            }\n\n            // Sort categories\n            let mut sorted_cats: Vec\u003cf32\u003e = col_cats.into_iter().map(|x| x as f32).collect();\n            sorted_cats.sort_by(|a, b| a.partial_cmp(b).unwrap());\n\n            n_values_.push(sorted_cats.len());\n            categories_.push(sorted_cats);\n        }\n\n        let n_features_out: usize = n_values_.iter().sum();\n\n        Ok(FittedOneHotEncoder {\n            categories_,\n            n_values_,\n            n_features_out,\n            n_features_in: cols,\n            handle_unknown: self.handle_unknown,\n            _backend: PhantomData,\n        })\n    }\n\n    fn fit_transform(\u0026self, data: \u0026Self::Input) -\u003e Result\u003cSelf::Output, PreprocessingError\u003e {\n        let fitted = self.fit(data)?;\n        fitted.transform(data)\n    }\n}\n\nimpl\u003cB: Backend\u003e FittedTransformer\u003cB\u003e for FittedOneHotEncoder\u003cB\u003e {\n    type Input = Tensor2D\u003cB\u003e;\n    type Output = Tensor2D\u003cB\u003e;\n    type Params = OneHotEncoderParams;\n\n    fn transform(\u0026self, data: \u0026Self::Input) -\u003e Result\u003cSelf::Output, PreprocessingError\u003e {\n        let (rows, cols) = data.shape();\n\n        if cols != self.n_features_in {\n            return Err(PreprocessingError::FeatureMismatch {\n                expected_features: self.n_features_in,\n                got_features: cols,\n            });\n        }\n\n        if rows == 0 {\n            return Ok(Tensor2D::zeros(0, self.n_features_out));\n        }\n\n        let data_vec = data.ravel().to_vec();\n        let mut result = vec![0.0f64; rows * self.n_features_out];\n\n        for row in 0..rows {\n            let mut out_col_offset = 0;\n            for col in 0..cols {\n                let val = data_vec[row * cols + col];\n                let cats = \u0026self.categories_[col];\n\n                // Find category index\n                let cat_idx = cats.iter().position(|\u0026c| (c - val as f32).abs() \u003c 1e-6);\n\n                match cat_idx {\n                    Some(idx) =\u003e {\n                        result[row * self.n_features_out + out_col_offset + idx] = 1.0;\n                    }\n                    None =\u003e {\n                        if self.handle_unknown == HandleUnknown::Error {\n                            return Err(PreprocessingError::InvalidParameter(format!(\n                                \"Unknown category {} in column {}\",\n                                val, col\n                            )));\n                        }\n                        // With Ignore, leave as zeros\n                    }\n                }\n\n                out_col_offset += self.n_values_[col];\n            }\n        }\n\n        Ok(Tensor2D::new(\n            result.iter().map(|\u0026x| x as f32).collect(),\n            rows,\n            self.n_features_out,\n        ))\n    }\n\n    fn inverse_transform(\u0026self, data: \u0026Self::Output) -\u003e Result\u003cSelf::Input, PreprocessingError\u003e {\n        let (rows, out_cols) = data.shape();\n\n        if out_cols != self.n_features_out {\n            return Err(PreprocessingError::FeatureMismatch {\n                expected_features: self.n_features_out,\n                got_features: out_cols,\n            });\n        }\n\n        let data_vec = data.ravel().to_vec();\n        let mut result = vec![0.0f64; rows * self.n_features_in];\n\n        for row in 0..rows {\n            let mut in_col_offset = 0;\n            for col in 0..self.n_features_in {\n                let n_cats = self.n_values_[col];\n                let cats = \u0026self.categories_[col];\n\n                // Find the index of the 1 in this column's one-hot section\n                let mut found = false;\n                for (i, \u0026cat) in cats.iter().enumerate() {\n                    let val = data_vec[row * out_cols + in_col_offset + i];\n                    if val \u003e 0.5 {\n                        result[row * self.n_features_in + col] = cat as f64;\n                        found = true;\n                        break;\n                    }\n                }\n\n                if !found {\n                    // No category found - this shouldn't happen for valid one-hot data\n                    result[row * self.n_features_in + col] = f64::NAN;\n                }\n\n                in_col_offset += n_cats;\n            }\n        }\n\n        Ok(Tensor2D::new(\n            result.iter().map(|\u0026x| x as f32).collect(),\n            rows,\n            self.n_features_in,\n        ))\n    }\n\n    fn extract_params(\u0026self) -\u003e Self::Params {\n        OneHotEncoderParams {\n            categories_: self.categories_.clone(),\n            n_values_: self.n_values_.clone(),\n            n_features_out: self.n_features_out,\n            n_features_in: self.n_features_in,\n            handle_unknown: self.handle_unknown,\n        }\n    }\n\n    fn from_params(params: Self::Params) -\u003e Result\u003cSelf, PreprocessingError\u003e {\n        Ok(FittedOneHotEncoder {\n            categories_: params.categories_,\n            n_values_: params.n_values_,\n            n_features_out: params.n_features_out,\n            n_features_in: params.n_features_in,\n            handle_unknown: params.handle_unknown,\n            _backend: PhantomData,\n        })\n    }\n\n    fn n_features_in(\u0026self) -\u003e usize {\n        self.n_features_in\n    }\n}\n\n#[cfg(test)]\nmod tests {\n    use super::*;\n    use crate::backend::CpuBackend;\n\n    #[test]\n    fn test_one_hot_encoder_single_column() {\n        // Input: [[0], [1], [2]]\n        let data = Tensor2D::\u003cCpuBackend\u003e::new(vec![0.0f32, 1.0, 2.0], 3, 1);\n\n        let encoder = OneHotEncoder::\u003cCpuBackend\u003e::new();\n        let fitted = encoder.fit(\u0026data).unwrap();\n\n        assert_eq!(fitted.n_features_in(), 1);\n        assert_eq!(fitted.n_features_out(), 3);\n        assert_eq!(fitted.categories()[0], vec![0.0f32, 1.0, 2.0]);\n\n        let transformed = fitted.transform(\u0026data).unwrap();\n        let vals = transformed.ravel().to_vec();\n\n        // Expected: [[1,0,0], [0,1,0], [0,0,1]]\n        assert!((vals[0] - 1.0).abs() \u003c 1e-6);\n        assert!((vals[1] - 0.0).abs() \u003c 1e-6);\n        assert!((vals[2] - 0.0).abs() \u003c 1e-6);\n        assert!((vals[3] - 0.0).abs() \u003c 1e-6);\n        assert!((vals[4] - 1.0).abs() \u003c 1e-6);\n        assert!((vals[5] - 0.0).abs() \u003c 1e-6);\n    }\n\n    #[test]\n    fn test_one_hot_encoder_missing_category() {\n        // Input: [[0], [2]] - category 1 is missing\n        let data = Tensor2D::\u003cCpuBackend\u003e::new(vec![0.0f32, 2.0], 2, 1);\n\n        let encoder = OneHotEncoder::\u003cCpuBackend\u003e::new();\n        let fitted = encoder.fit(\u0026data).unwrap();\n\n        // Should only have 2 categories\n        assert_eq!(fitted.n_features_out(), 2);\n        assert_eq!(fitted.categories()[0], vec![0.0f32, 2.0]);\n\n        let transformed = fitted.transform(\u0026data).unwrap();\n        // [[1,0], [0,1]]\n        let vals = transformed.ravel().to_vec();\n        assert!((vals[0] - 1.0).abs() \u003c 1e-6);\n        assert!((vals[1] - 0.0).abs() \u003c 1e-6);\n        assert!((vals[2] - 0.0).abs() \u003c 1e-6);\n        assert!((vals[3] - 1.0).abs() \u003c 1e-6);\n    }\n\n    #[test]\n    fn test_one_hot_encoder_multiple_columns() {\n        // Input: [[0, 1], [1, 0]]\n        // Column 0 has categories [0, 1]\n        // Column 1 has categories [0, 1]\n        let data = Tensor2D::\u003cCpuBackend\u003e::new(vec![0.0f32, 1.0, 1.0, 0.0], 2, 2);\n\n        let encoder = OneHotEncoder::\u003cCpuBackend\u003e::new();\n        let fitted = encoder.fit(\u0026data).unwrap();\n\n        assert_eq!(fitted.n_features_out(), 4); // 2 + 2\n\n        let transformed = fitted.transform(\u0026data).unwrap();\n        // Expected: [[1,0, 0,1], [0,1, 1,0]]\n        let vals = transformed.ravel().to_vec();\n        assert!((vals[0] - 1.0).abs() \u003c 1e-6); // row 0, col 0 -\u003e [1,0]\n        assert!((vals[1] - 0.0).abs() \u003c 1e-6);\n        assert!((vals[2] - 0.0).abs() \u003c 1e-6); // row 0, col 1 -\u003e [0,1]\n        assert!((vals[3] - 1.0).abs() \u003c 1e-6);\n    }\n\n    #[test]\n    fn test_one_hot_encoder_unknown_error() {\n        let train = Tensor2D::\u003cCpuBackend\u003e::new(vec![0.0f32, 1.0], 2, 1);\n        let test = Tensor2D::\u003cCpuBackend\u003e::new(vec![2.0f32], 1, 1); // unknown category\n\n        let encoder = OneHotEncoder::\u003cCpuBackend\u003e::new();\n        let fitted = encoder.fit(\u0026train).unwrap();\n\n        let result = fitted.transform(\u0026test);\n        assert!(result.is_err());\n    }\n\n    #[test]\n    fn test_one_hot_encoder_unknown_ignore() {\n        let train = Tensor2D::\u003cCpuBackend\u003e::new(vec![0.0f32, 1.0], 2, 1);\n        let test = Tensor2D::\u003cCpuBackend\u003e::new(vec![2.0f32], 1, 1); // unknown category\n\n        let encoder = OneHotEncoder::\u003cCpuBackend\u003e::new().with_handle_unknown(HandleUnknown::Ignore);\n        let fitted = encoder.fit(\u0026train).unwrap();\n\n        let transformed = fitted.transform(\u0026test).unwrap();\n        // Should output [0, 0] for unknown\n        let vals = transformed.ravel().to_vec();\n        assert!((vals[0] - 0.0).abs() \u003c 1e-6);\n        assert!((vals[1] - 0.0).abs() \u003c 1e-6);\n    }\n\n    #[test]\n    fn test_one_hot_encoder_inverse() {\n        let data = Tensor2D::\u003cCpuBackend\u003e::new(vec![0.0f32, 1.0, 2.0], 3, 1);\n\n        let encoder = OneHotEncoder::\u003cCpuBackend\u003e::new();\n        let fitted = encoder.fit(\u0026data).unwrap();\n\n        let transformed = fitted.transform(\u0026data).unwrap();\n        let recovered = fitted.inverse_transform(\u0026transformed).unwrap();\n\n        let orig = data.ravel().to_vec();\n        let rec = recovered.ravel().to_vec();\n\n        for (o, r) in orig.iter().zip(rec.iter()) {\n            assert!((o - r).abs() \u003c 1e-6);\n        }\n    }\n\n    #[test]\n    fn test_one_hot_encoder_serialization() {\n        let data = Tensor2D::\u003cCpuBackend\u003e::new(vec![0.0f32, 1.0, 2.0], 3, 1);\n\n        let encoder = OneHotEncoder::\u003cCpuBackend\u003e::new();\n        let fitted = encoder.fit(\u0026data).unwrap();\n\n        let temp_file = std::env::temp_dir().join(\"test_onehot.bin\");\n        fitted.save_to_file(\u0026temp_file).unwrap();\n\n        let loaded = FittedOneHotEncoder::\u003cCpuBackend\u003e::load_from_file(\u0026temp_file).unwrap();\n\n        assert_eq!(loaded.n_features_in(), fitted.n_features_in());\n        assert_eq!(loaded.n_features_out(), fitted.n_features_out());\n        assert_eq!(loaded.categories(), fitted.categories());\n\n        std::fs::remove_file(temp_file).ok();\n    }\n}\n","traces":[{"line":46,"address":[3474544],"length":1,"stats":{"Line":1}},{"line":48,"address":[3474545],"length":1,"stats":{"Line":1}},{"line":54,"address":[3474496],"length":1,"stats":{"Line":1}},{"line":55,"address":[3474518],"length":1,"stats":{"Line":1}},{"line":56,"address":[3474524],"length":1,"stats":{"Line":1}},{"line":61,"address":[],"length":0,"stats":{"Line":0}},{"line":62,"address":[],"length":0,"stats":{"Line":0}},{"line":105,"address":[3474560],"length":1,"stats":{"Line":1}},{"line":106,"address":[3474565],"length":1,"stats":{"Line":1}},{"line":110,"address":[3474576],"length":1,"stats":{"Line":2}},{"line":111,"address":[3474581],"length":1,"stats":{"Line":2}},{"line":115,"address":[],"length":0,"stats":{"Line":0}},{"line":116,"address":[],"length":0,"stats":{"Line":0}},{"line":126,"address":[3461616,3464105,3463374],"length":1,"stats":{"Line":1}},{"line":127,"address":[3461682],"length":1,"stats":{"Line":1}},{"line":129,"address":[3461735],"length":1,"stats":{"Line":1}},{"line":130,"address":[3461779],"length":1,"stats":{"Line":0}},{"line":131,"address":[3461745],"length":1,"stats":{"Line":0}},{"line":135,"address":[3461966,3461914],"length":1,"stats":{"Line":1}},{"line":138,"address":[3462061],"length":1,"stats":{"Line":1}},{"line":139,"address":[3462091],"length":1,"stats":{"Line":1}},{"line":141,"address":[3462162,3462250,3463351],"length":1,"stats":{"Line":9}},{"line":142,"address":[3462371],"length":1,"stats":{"Line":3}},{"line":143,"address":[3462775,3462859],"length":1,"stats":{"Line":6}},{"line":144,"address":[3462968,3463385],"length":1,"stats":{"Line":6}},{"line":145,"address":[3463601,3463496],"length":1,"stats":{"Line":9}},{"line":146,"address":[3463723,3463534],"length":1,"stats":{"Line":0}},{"line":147,"address":[],"length":0,"stats":{"Line":0}},{"line":148,"address":[],"length":0,"stats":{"Line":0}},{"line":151,"address":[3463654],"length":1,"stats":{"Line":5}},{"line":155,"address":[3462998,3464217,3464208],"length":1,"stats":{"Line":15}},{"line":156,"address":[3464171,3464128,3463207,3463139],"length":1,"stats":{"Line":20}},{"line":158,"address":[3463222],"length":1,"stats":{"Line":5}},{"line":159,"address":[3463265],"length":1,"stats":{"Line":5}},{"line":162,"address":[3462381],"length":1,"stats":{"Line":5}},{"line":164,"address":[3462610],"length":1,"stats":{"Line":1}},{"line":165,"address":[3462512],"length":1,"stats":{"Line":4}},{"line":166,"address":[3462560],"length":1,"stats":{"Line":1}},{"line":167,"address":[],"length":0,"stats":{"Line":0}},{"line":168,"address":[],"length":0,"stats":{"Line":0}},{"line":169,"address":[3462608],"length":1,"stats":{"Line":5}},{"line":170,"address":[],"length":0,"stats":{"Line":0}},{"line":174,"address":[],"length":0,"stats":{"Line":0}},{"line":175,"address":[],"length":0,"stats":{"Line":0}},{"line":176,"address":[],"length":0,"stats":{"Line":0}},{"line":185,"address":[3466896,3469186,3469180],"length":1,"stats":{"Line":2}},{"line":186,"address":[3466962],"length":1,"stats":{"Line":2}},{"line":188,"address":[3467007],"length":1,"stats":{"Line":3}},{"line":189,"address":[3467060],"length":1,"stats":{"Line":0}},{"line":190,"address":[3467056],"length":1,"stats":{"Line":0}},{"line":191,"address":[],"length":0,"stats":{"Line":0}},{"line":195,"address":[3467021],"length":1,"stats":{"Line":4}},{"line":196,"address":[3467125],"length":1,"stats":{"Line":0}},{"line":199,"address":[3467223,3467275],"length":1,"stats":{"Line":1}},{"line":200,"address":[3467378],"length":1,"stats":{"Line":4}},{"line":202,"address":[3467456,3467543],"length":1,"stats":{"Line":5}},{"line":203,"address":[3467672],"length":1,"stats":{"Line":1}},{"line":204,"address":[3468007,3469154,3467684],"length":1,"stats":{"Line":6}},{"line":205,"address":[3468140],"length":1,"stats":{"Line":4}},{"line":206,"address":[3468293],"length":1,"stats":{"Line":1}},{"line":209,"address":[3468333,3469200,3469213],"length":1,"stats":{"Line":9}},{"line":211,"address":[3468462],"length":1,"stats":{"Line":1}},{"line":212,"address":[3468492],"length":1,"stats":{"Line":3}},{"line":213,"address":[3468513,3468943],"length":1,"stats":{"Line":4}},{"line":215,"address":[],"length":0,"stats":{"Line":0}},{"line":216,"address":[3468544],"length":1,"stats":{"Line":1}},{"line":217,"address":[3468626],"length":1,"stats":{"Line":1}},{"line":218,"address":[],"length":0,"stats":{"Line":0}},{"line":219,"address":[],"length":0,"stats":{"Line":0}},{"line":226,"address":[3469159,3469116,3468584],"length":1,"stats":{"Line":4}},{"line":230,"address":[3467894],"length":1,"stats":{"Line":4}},{"line":231,"address":[3467726,3469290,3469280],"length":1,"stats":{"Line":7}},{"line":232,"address":[],"length":0,"stats":{"Line":0}},{"line":233,"address":[3467874],"length":1,"stats":{"Line":1}},{"line":237,"address":[3466845,3464656,3466839],"length":1,"stats":{"Line":1}},{"line":238,"address":[3464722],"length":1,"stats":{"Line":1}},{"line":240,"address":[3464767],"length":1,"stats":{"Line":1}},{"line":241,"address":[3464853],"length":1,"stats":{"Line":0}},{"line":242,"address":[3464849],"length":1,"stats":{"Line":0}},{"line":243,"address":[],"length":0,"stats":{"Line":0}},{"line":247,"address":[3464918,3464797],"length":1,"stats":{"Line":1}},{"line":248,"address":[3465021],"length":1,"stats":{"Line":1}},{"line":250,"address":[3465099,3465186],"length":1,"stats":{"Line":2}},{"line":251,"address":[3465315],"length":1,"stats":{"Line":1}},{"line":252,"address":[3465327,3465665,3466813],"length":1,"stats":{"Line":3}},{"line":253,"address":[3465798],"length":1,"stats":{"Line":1}},{"line":254,"address":[3465867],"length":1,"stats":{"Line":1}},{"line":257,"address":[3465905],"length":1,"stats":{"Line":1}},{"line":258,"address":[3465913],"length":1,"stats":{"Line":1}},{"line":259,"address":[3466217,3466256],"length":1,"stats":{"Line":2}},{"line":260,"address":[3466420],"length":1,"stats":{"Line":1}},{"line":261,"address":[3466454],"length":1,"stats":{"Line":1}},{"line":262,"address":[3466592],"length":1,"stats":{"Line":1}},{"line":263,"address":[],"length":0,"stats":{"Line":0}},{"line":267,"address":[3466232,3466795],"length":1,"stats":{"Line":1}},{"line":269,"address":[3466621,3466688],"length":1,"stats":{"Line":0}},{"line":272,"address":[3466645,3466805,3466818],"length":1,"stats":{"Line":2}},{"line":276,"address":[3465541],"length":1,"stats":{"Line":1}},{"line":277,"address":[3466864,3465373,3466874],"length":1,"stats":{"Line":3}},{"line":278,"address":[],"length":0,"stats":{"Line":0}},{"line":279,"address":[3465521],"length":1,"stats":{"Line":1}},{"line":283,"address":[3464627,3464432,3464633],"length":1,"stats":{"Line":1}},{"line":285,"address":[3464456],"length":1,"stats":{"Line":1}},{"line":286,"address":[3464478],"length":1,"stats":{"Line":1}},{"line":287,"address":[3464544],"length":1,"stats":{"Line":1}},{"line":288,"address":[3464548],"length":1,"stats":{"Line":1}},{"line":289,"address":[3464552],"length":1,"stats":{"Line":1}},{"line":293,"address":[3464224],"length":1,"stats":{"Line":1}},{"line":294,"address":[3464306],"length":1,"stats":{"Line":1}},{"line":295,"address":[3464239],"length":1,"stats":{"Line":1}},{"line":296,"address":[3464265],"length":1,"stats":{"Line":1}},{"line":297,"address":[3464295],"length":1,"stats":{"Line":1}},{"line":298,"address":[3464299],"length":1,"stats":{"Line":1}},{"line":299,"address":[3464303],"length":1,"stats":{"Line":1}},{"line":300,"address":[],"length":0,"stats":{"Line":0}},{"line":304,"address":[3464416],"length":1,"stats":{"Line":1}},{"line":305,"address":[3464421],"length":1,"stats":{"Line":1}}],"covered":87,"coverable":117},{"path":["/","home","victor","ML","machinelearne-rs","lib","src","preprocessing","encoding","ordinal.rs"],"content":"//! Ordinal encoding for categorical features.\n//!\n//! Maps categorical values to integer ordinals (0, 1, 2, ...).\n\nuse crate::backend::{Backend, Tensor2D};\nuse crate::preprocessing::encoding::HandleUnknown;\nuse crate::preprocessing::error::PreprocessingError;\nuse crate::preprocessing::traits::{FittedTransformer, Transformer};\nuse serde::{Deserialize, Serialize};\nuse std::collections::HashMap;\nuse std::marker::PhantomData;\n\n/// Ordinal encoder for categorical features.\n///\n/// Maps each unique category to an integer ordinal (0, 1, 2, ...).\n/// The mapping is learned from the training data, with categories\n/// sorted in ascending order.\n///\n/// # Example\n/// ```ignore\n/// use machinelearne_rs::preprocessing::{OrdinalEncoder, Transformer};\n/// use machinelearne_rs::backend::CpuBackend;\n///\n/// // Input: [[0], [2], [1]]  (categories: 0, 1, 2)\n/// let data = Tensor2D::new(vec![0.0, 2.0, 1.0], 3, 1);\n///\n/// let encoder = OrdinalEncoder::\u003cCpuBackend\u003e::new();\n/// let fitted = encoder.fit(\u0026data)?;\n///\n/// // Output: [[0], [2], [1]]  (ordinal mapping)\n/// let encoded = fitted.transform(\u0026data)?;\n/// ```\n#[derive(Clone, Debug)]\npub struct OrdinalEncoder\u003cB: Backend\u003e {\n    /// How to handle unknown categories during transform.\n    handle_unknown: HandleUnknown,\n    _backend: PhantomData\u003cB\u003e,\n}\n\nimpl\u003cB: Backend\u003e OrdinalEncoder\u003cB\u003e {\n    /// Create a new OrdinalEncoder with default settings.\n    pub fn new() -\u003e Self {\n        Self {\n            handle_unknown: HandleUnknown::default(),\n            _backend: PhantomData,\n        }\n    }\n\n    /// Set the strategy for handling unknown categories.\n    pub fn with_handle_unknown(mut self, strategy: HandleUnknown) -\u003e Self {\n        self.handle_unknown = strategy;\n        self\n    }\n}\n\nimpl\u003cB: Backend\u003e Default for OrdinalEncoder\u003cB\u003e {\n    fn default() -\u003e Self {\n        Self::new()\n    }\n}\n\n/// Serializable parameters for a fitted OrdinalEncoder.\n#[derive(Clone, Serialize, Deserialize)]\npub struct OrdinalEncoderParams {\n    /// Categories (unique sorted values) for each input column.\n    pub categories_: Vec\u003cVec\u003cf32\u003e\u003e,\n    /// Mapping from category value to ordinal for each column.\n    /// Stored as (category, ordinal) pairs for serialization.\n    pub mappings_: Vec\u003cVec\u003c(f32, usize)\u003e\u003e,\n    /// Number of input features.\n    pub n_features_in: usize,\n    /// Handle unknown strategy.\n    pub handle_unknown: HandleUnknown,\n}\n\n// Note: We rely on the blanket impl of SerializableParams for Serialize + Deserialize\n\n/// Fitted OrdinalEncoder ready for inference.\n#[derive(Clone)]\npub struct FittedOrdinalEncoder\u003cB: Backend\u003e {\n    /// Categories (unique sorted values) for each input column.\n    categories_: Vec\u003cVec\u003cf32\u003e\u003e,\n    /// Mapping from category value to ordinal index for each column.\n    mappings_: Vec\u003cHashMap\u003ci32, usize\u003e\u003e,\n    /// Number of input features.\n    n_features_in: usize,\n    /// Handle unknown strategy.\n    handle_unknown: HandleUnknown,\n    _backend: PhantomData\u003cB\u003e,\n}\n\nimpl\u003cB: Backend\u003e FittedOrdinalEncoder\u003cB\u003e {\n    /// Get the categories learned for each feature.\n    pub fn categories(\u0026self) -\u003e \u0026[Vec\u003cf32\u003e] {\n        \u0026self.categories_\n    }\n\n    /// Get the mapping (category -\u003e ordinal) for a specific feature.\n    pub fn mapping(\u0026self, feature_idx: usize) -\u003e Option\u003c\u0026HashMap\u003ci32, usize\u003e\u003e {\n        self.mappings_.get(feature_idx)\n    }\n}\n\nimpl\u003cB: Backend\u003e Transformer\u003cB\u003e for OrdinalEncoder\u003cB\u003e {\n    type Input = Tensor2D\u003cB\u003e;\n    type Output = Tensor2D\u003cB\u003e;\n    type Params = OrdinalEncoderParams;\n    type Fitted = FittedOrdinalEncoder\u003cB\u003e;\n\n    fn fit(\u0026self, data: \u0026Self::Input) -\u003e Result\u003cSelf::Fitted, PreprocessingError\u003e {\n        let (rows, cols) = data.shape();\n\n        if rows == 0 {\n            return Err(PreprocessingError::EmptyData(\n                \"Cannot fit OrdinalEncoder on empty data\".to_string(),\n            ));\n        }\n\n        let data_vec = data.ravel().to_vec();\n\n        // Find unique categories for each column\n        let mut categories_: Vec\u003cVec\u003cf32\u003e\u003e = Vec::with_capacity(cols);\n        let mut mappings_: Vec\u003cHashMap\u003ci32, usize\u003e\u003e = Vec::with_capacity(cols);\n\n        for col in 0..cols {\n            let mut col_cats: std::collections::HashSet\u003ci32\u003e = std::collections::HashSet::new();\n            for row in 0..rows {\n                let val = data_vec[row * cols + col];\n                if !val.is_finite() {\n                    return Err(PreprocessingError::InvalidParameter(format!(\n                        \"OrdinalEncoder expects finite values, got {} at ({}, {})\",\n                        val, row, col\n                    )));\n                }\n                // For ordinal encoding, we allow non-integer values too\n                // (they'll be rounded to i32 for the mapping key)\n                col_cats.insert(val.round() as i32);\n            }\n\n            // Sort categories\n            let mut sorted_cats: Vec\u003ci32\u003e = col_cats.into_iter().collect();\n            sorted_cats.sort();\n\n            // Create mapping\n            let mut mapping = HashMap::new();\n            for (idx, \u0026cat) in sorted_cats.iter().enumerate() {\n                mapping.insert(cat, idx);\n            }\n\n            categories_.push(sorted_cats.into_iter().map(|x| x as f32).collect());\n            mappings_.push(mapping);\n        }\n\n        Ok(FittedOrdinalEncoder {\n            categories_,\n            mappings_,\n            n_features_in: cols,\n            handle_unknown: self.handle_unknown,\n            _backend: PhantomData,\n        })\n    }\n\n    fn fit_transform(\u0026self, data: \u0026Self::Input) -\u003e Result\u003cSelf::Output, PreprocessingError\u003e {\n        let fitted = self.fit(data)?;\n        fitted.transform(data)\n    }\n}\n\nimpl\u003cB: Backend\u003e FittedTransformer\u003cB\u003e for FittedOrdinalEncoder\u003cB\u003e {\n    type Input = Tensor2D\u003cB\u003e;\n    type Output = Tensor2D\u003cB\u003e;\n    type Params = OrdinalEncoderParams;\n\n    fn transform(\u0026self, data: \u0026Self::Input) -\u003e Result\u003cSelf::Output, PreprocessingError\u003e {\n        let (rows, cols) = data.shape();\n\n        if cols != self.n_features_in {\n            return Err(PreprocessingError::FeatureMismatch {\n                expected_features: self.n_features_in,\n                got_features: cols,\n            });\n        }\n\n        if rows == 0 {\n            return Ok(Tensor2D::zeros(0, cols));\n        }\n\n        let data_vec = data.ravel().to_vec();\n        let mut result = vec![0.0f64; rows * cols];\n\n        for row in 0..rows {\n            for col in 0..cols {\n                let val = data_vec[row * cols + col];\n                let key = val.round() as i32;\n\n                match self.mappings_[col].get(\u0026key) {\n                    Some(\u0026ordinal) =\u003e {\n                        result[row * cols + col] = ordinal as f64;\n                    }\n                    None =\u003e {\n                        if self.handle_unknown == HandleUnknown::Error {\n                            return Err(PreprocessingError::InvalidParameter(format!(\n                                \"Unknown category {} in column {}\",\n                                val, col\n                            )));\n                        }\n                        // With Ignore, output NaN\n                        result[row * cols + col] = f64::NAN;\n                    }\n                }\n            }\n        }\n\n        Ok(Tensor2D::new(\n            result.iter().map(|\u0026x| x as f32).collect(),\n            rows,\n            cols,\n        ))\n    }\n\n    fn inverse_transform(\u0026self, data: \u0026Self::Output) -\u003e Result\u003cSelf::Input, PreprocessingError\u003e {\n        let (rows, cols) = data.shape();\n\n        if cols != self.n_features_in {\n            return Err(PreprocessingError::FeatureMismatch {\n                expected_features: self.n_features_in,\n                got_features: cols,\n            });\n        }\n\n        let data_vec = data.ravel().to_vec();\n        let mut result = vec![0.0f64; rows * cols];\n\n        for row in 0..rows {\n            for col in 0..cols {\n                let ordinal = data_vec[row * cols + col] as usize;\n                if ordinal \u003c self.categories_[col].len() {\n                    result[row * cols + col] = self.categories_[col][ordinal] as f64;\n                } else {\n                    result[row * cols + col] = f64::NAN;\n                }\n            }\n        }\n\n        Ok(Tensor2D::new(\n            result.iter().map(|\u0026x| x as f32).collect(),\n            rows,\n            cols,\n        ))\n    }\n\n    fn extract_params(\u0026self) -\u003e Self::Params {\n        // Convert HashMaps to Vec\u003c(f32, usize)\u003e for serialization\n        let mappings_: Vec\u003cVec\u003c(f32, usize)\u003e\u003e = self\n            .mappings_\n            .iter()\n            .map(|m| {\n                let mut pairs: Vec\u003c(f32, usize)\u003e = m.iter().map(|(\u0026k, \u0026v)| (k as f32, v)).collect();\n                pairs.sort_by_key(|\u0026(_, v)| v);\n                pairs\n            })\n            .collect();\n\n        OrdinalEncoderParams {\n            categories_: self.categories_.clone(),\n            mappings_,\n            n_features_in: self.n_features_in,\n            handle_unknown: self.handle_unknown,\n        }\n    }\n\n    fn from_params(params: Self::Params) -\u003e Result\u003cSelf, PreprocessingError\u003e {\n        // Convert Vec\u003c(f32, usize)\u003e back to HashMap\n        let mappings_: Vec\u003cHashMap\u003ci32, usize\u003e\u003e = params\n            .mappings_\n            .iter()\n            .map(|pairs| pairs.iter().map(|\u0026(k, v)| (k.round() as i32, v)).collect())\n            .collect();\n\n        Ok(FittedOrdinalEncoder {\n            categories_: params.categories_,\n            mappings_,\n            n_features_in: params.n_features_in,\n            handle_unknown: params.handle_unknown,\n            _backend: PhantomData,\n        })\n    }\n\n    fn n_features_in(\u0026self) -\u003e usize {\n        self.n_features_in\n    }\n}\n\n#[cfg(test)]\nmod tests {\n    use super::*;\n    use crate::backend::CpuBackend;\n\n    #[test]\n    fn test_ordinal_encoder_basic() {\n        // Input: [[0], [2], [1]]\n        let data = Tensor2D::\u003cCpuBackend\u003e::new(vec![0.0f32, 2.0, 1.0], 3, 1);\n\n        let encoder = OrdinalEncoder::\u003cCpuBackend\u003e::new();\n        let fitted = encoder.fit(\u0026data).unwrap();\n\n        assert_eq!(fitted.n_features_in(), 1);\n        assert_eq!(fitted.categories()[0], vec![0.0f32, 1.0, 2.0]);\n\n        let transformed = fitted.transform(\u0026data).unwrap();\n        let vals = transformed.ravel().to_vec();\n\n        // Categories sorted: 0-\u003e0, 1-\u003e1, 2-\u003e2\n        assert!((vals[0] - 0.0).abs() \u003c 1e-6); // 0 -\u003e 0\n        assert!((vals[1] - 2.0).abs() \u003c 1e-6); // 2 -\u003e 2\n        assert!((vals[2] - 1.0).abs() \u003c 1e-6); // 1 -\u003e 1\n    }\n\n    #[test]\n    fn test_ordinal_encoder_non_contiguous() {\n        // Input: [[0], [5], [10]]\n        let data = Tensor2D::\u003cCpuBackend\u003e::new(vec![0.0f32, 5.0, 10.0], 3, 1);\n\n        let encoder = OrdinalEncoder::\u003cCpuBackend\u003e::new();\n        let fitted = encoder.fit(\u0026data).unwrap();\n\n        assert_eq!(fitted.categories()[0], vec![0.0f32, 5.0, 10.0]);\n\n        let transformed = fitted.transform(\u0026data).unwrap();\n        let vals = transformed.ravel().to_vec();\n\n        // Ordinals: 0-\u003e0, 5-\u003e1, 10-\u003e2\n        assert!((vals[0] - 0.0).abs() \u003c 1e-6);\n        assert!((vals[1] - 1.0).abs() \u003c 1e-6);\n        assert!((vals[2] - 2.0).abs() \u003c 1e-6);\n    }\n\n    #[test]\n    fn test_ordinal_encoder_inverse() {\n        let data = Tensor2D::\u003cCpuBackend\u003e::new(vec![0.0f32, 5.0, 10.0], 3, 1);\n\n        let encoder = OrdinalEncoder::\u003cCpuBackend\u003e::new();\n        let fitted = encoder.fit(\u0026data).unwrap();\n\n        let transformed = fitted.transform(\u0026data).unwrap();\n        let recovered = fitted.inverse_transform(\u0026transformed).unwrap();\n\n        let orig = data.ravel().to_vec();\n        let rec = recovered.ravel().to_vec();\n\n        for (o, r) in orig.iter().zip(rec.iter()) {\n            assert!((o - r).abs() \u003c 1e-6);\n        }\n    }\n\n    #[test]\n    fn test_ordinal_encoder_unknown_error() {\n        let train = Tensor2D::\u003cCpuBackend\u003e::new(vec![0.0f32, 1.0], 2, 1);\n        let test = Tensor2D::\u003cCpuBackend\u003e::new(vec![2.0f32], 1, 1);\n\n        let encoder = OrdinalEncoder::\u003cCpuBackend\u003e::new();\n        let fitted = encoder.fit(\u0026train).unwrap();\n\n        let result = fitted.transform(\u0026test);\n        assert!(result.is_err());\n    }\n\n    #[test]\n    fn test_ordinal_encoder_unknown_ignore() {\n        let train = Tensor2D::\u003cCpuBackend\u003e::new(vec![0.0f32, 1.0], 2, 1);\n        let test = Tensor2D::\u003cCpuBackend\u003e::new(vec![2.0f32], 1, 1);\n\n        let encoder =\n            OrdinalEncoder::\u003cCpuBackend\u003e::new().with_handle_unknown(HandleUnknown::Ignore);\n        let fitted = encoder.fit(\u0026train).unwrap();\n\n        let transformed = fitted.transform(\u0026test).unwrap();\n        let vals = transformed.ravel().to_vec();\n        assert!(vals[0].is_nan());\n    }\n\n    #[test]\n    fn test_ordinal_encoder_serialization() {\n        let data = Tensor2D::\u003cCpuBackend\u003e::new(vec![0.0f32, 5.0, 10.0], 3, 1);\n\n        let encoder = OrdinalEncoder::\u003cCpuBackend\u003e::new();\n        let fitted = encoder.fit(\u0026data).unwrap();\n\n        let temp_file = std::env::temp_dir().join(\"test_ordinal.bin\");\n        fitted.save_to_file(\u0026temp_file).unwrap();\n\n        let loaded = FittedOrdinalEncoder::\u003cCpuBackend\u003e::load_from_file(\u0026temp_file).unwrap();\n\n        assert_eq!(loaded.n_features_in(), fitted.n_features_in());\n        assert_eq!(loaded.categories(), fitted.categories());\n\n        // Verify transform gives same result\n        let t1 = fitted.transform(\u0026data).unwrap();\n        let t2 = loaded.transform(\u0026data).unwrap();\n        let v1 = t1.ravel().to_vec();\n        let v2 = t2.ravel().to_vec();\n        for (a, b) in v1.iter().zip(v2.iter()) {\n            assert!((a - b).abs() \u003c 1e-6);\n        }\n\n        std::fs::remove_file(temp_file).ok();\n    }\n}\n","traces":[{"line":42,"address":[2004144],"length":1,"stats":{"Line":1}},{"line":44,"address":[2004145],"length":1,"stats":{"Line":1}},{"line":50,"address":[2004096],"length":1,"stats":{"Line":1}},{"line":51,"address":[2004118],"length":1,"stats":{"Line":1}},{"line":52,"address":[2004124],"length":1,"stats":{"Line":1}},{"line":57,"address":[],"length":0,"stats":{"Line":0}},{"line":58,"address":[],"length":0,"stats":{"Line":0}},{"line":94,"address":[2004160],"length":1,"stats":{"Line":1}},{"line":95,"address":[2004165],"length":1,"stats":{"Line":1}},{"line":99,"address":[],"length":0,"stats":{"Line":0}},{"line":100,"address":[],"length":0,"stats":{"Line":0}},{"line":110,"address":[1993451,1991360,1994165],"length":1,"stats":{"Line":1}},{"line":111,"address":[1991426],"length":1,"stats":{"Line":1}},{"line":113,"address":[1991487],"length":1,"stats":{"Line":1}},{"line":114,"address":[1991531],"length":1,"stats":{"Line":0}},{"line":115,"address":[1991497],"length":1,"stats":{"Line":0}},{"line":119,"address":[1991718,1991666],"length":1,"stats":{"Line":1}},{"line":122,"address":[1991813],"length":1,"stats":{"Line":2}},{"line":123,"address":[1991843],"length":1,"stats":{"Line":2}},{"line":125,"address":[1991994,1993423,1991910],"length":1,"stats":{"Line":6}},{"line":126,"address":[1992115],"length":1,"stats":{"Line":2}},{"line":127,"address":[1992500,1992404],"length":1,"stats":{"Line":4}},{"line":128,"address":[1992609,1993480],"length":1,"stats":{"Line":4}},{"line":129,"address":[1993591],"length":1,"stats":{"Line":2}},{"line":130,"address":[1993629,1993721],"length":1,"stats":{"Line":0}},{"line":131,"address":[],"length":0,"stats":{"Line":0}},{"line":132,"address":[],"length":0,"stats":{"Line":0}},{"line":137,"address":[1993696,1994095],"length":1,"stats":{"Line":4}},{"line":141,"address":[1992639],"length":1,"stats":{"Line":2}},{"line":142,"address":[1992825,1992757],"length":1,"stats":{"Line":4}},{"line":145,"address":[1992840],"length":1,"stats":{"Line":2}},{"line":146,"address":[1992928,1992847],"length":1,"stats":{"Line":4}},{"line":147,"address":[1993428,1993164],"length":1,"stats":{"Line":5}},{"line":150,"address":[1994201,1994192,1993174],"length":1,"stats":{"Line":7}},{"line":151,"address":[1993313],"length":1,"stats":{"Line":3}},{"line":154,"address":[1992247],"length":1,"stats":{"Line":2}},{"line":155,"address":[1992149],"length":1,"stats":{"Line":3}},{"line":156,"address":[1992197],"length":1,"stats":{"Line":2}},{"line":157,"address":[],"length":0,"stats":{"Line":0}},{"line":158,"address":[1992245],"length":1,"stats":{"Line":3}},{"line":159,"address":[],"length":0,"stats":{"Line":0}},{"line":163,"address":[],"length":0,"stats":{"Line":0}},{"line":164,"address":[],"length":0,"stats":{"Line":0}},{"line":165,"address":[],"length":0,"stats":{"Line":0}},{"line":174,"address":[1999486,1999492,1997216],"length":1,"stats":{"Line":2}},{"line":175,"address":[1997282],"length":1,"stats":{"Line":2}},{"line":177,"address":[1997327],"length":1,"stats":{"Line":2}},{"line":178,"address":[1997380],"length":1,"stats":{"Line":0}},{"line":179,"address":[1997376],"length":1,"stats":{"Line":0}},{"line":180,"address":[],"length":0,"stats":{"Line":0}},{"line":184,"address":[1997341],"length":1,"stats":{"Line":2}},{"line":185,"address":[1997445],"length":1,"stats":{"Line":0}},{"line":188,"address":[1997539,1997591],"length":1,"stats":{"Line":3}},{"line":189,"address":[1997694],"length":1,"stats":{"Line":2}},{"line":191,"address":[1997771,1997858],"length":1,"stats":{"Line":5}},{"line":192,"address":[1998306,1997987],"length":1,"stats":{"Line":5}},{"line":193,"address":[1998433],"length":1,"stats":{"Line":3}},{"line":194,"address":[1998566],"length":1,"stats":{"Line":2}},{"line":196,"address":[1998649],"length":1,"stats":{"Line":3}},{"line":197,"address":[1998765],"length":1,"stats":{"Line":2}},{"line":198,"address":[1999331,1998789],"length":1,"stats":{"Line":3}},{"line":200,"address":[1999052],"length":1,"stats":{"Line":1}},{"line":201,"address":[1998819],"length":1,"stats":{"Line":2}},{"line":202,"address":[1998890,1999057],"length":1,"stats":{"Line":2}},{"line":203,"address":[],"length":0,"stats":{"Line":0}},{"line":204,"address":[],"length":0,"stats":{"Line":0}},{"line":208,"address":[1998867,1998938],"length":1,"stats":{"Line":2}},{"line":214,"address":[1998193],"length":1,"stats":{"Line":2}},{"line":215,"address":[1999530,1998029,1999520],"length":1,"stats":{"Line":4}},{"line":216,"address":[],"length":0,"stats":{"Line":0}},{"line":217,"address":[],"length":0,"stats":{"Line":0}},{"line":221,"address":[1997170,1995376,1997164],"length":1,"stats":{"Line":1}},{"line":222,"address":[1995442],"length":1,"stats":{"Line":1}},{"line":224,"address":[1995487],"length":1,"stats":{"Line":1}},{"line":225,"address":[1995573],"length":1,"stats":{"Line":0}},{"line":226,"address":[1995569],"length":1,"stats":{"Line":0}},{"line":227,"address":[],"length":0,"stats":{"Line":0}},{"line":231,"address":[1995638,1995517],"length":1,"stats":{"Line":1}},{"line":232,"address":[1995741],"length":1,"stats":{"Line":1}},{"line":234,"address":[1995905,1995818],"length":1,"stats":{"Line":2}},{"line":235,"address":[1996034,1996364],"length":1,"stats":{"Line":2}},{"line":236,"address":[1996502],"length":1,"stats":{"Line":1}},{"line":237,"address":[1997159,1996729,1996962],"length":1,"stats":{"Line":2}},{"line":238,"address":[1996977,1996824],"length":1,"stats":{"Line":2}},{"line":240,"address":[1996796,1996853],"length":1,"stats":{"Line":0}},{"line":245,"address":[1996240],"length":1,"stats":{"Line":1}},{"line":246,"address":[1997194,1997184,1996076],"length":1,"stats":{"Line":3}},{"line":247,"address":[],"length":0,"stats":{"Line":0}},{"line":248,"address":[],"length":0,"stats":{"Line":0}},{"line":252,"address":[1995039,1995033,1994784],"length":1,"stats":{"Line":1}},{"line":254,"address":[1994815],"length":1,"stats":{"Line":1}},{"line":255,"address":[],"length":0,"stats":{"Line":0}},{"line":257,"address":[1994841,1995275,1995056,1995281],"length":1,"stats":{"Line":2}},{"line":258,"address":[1995113,1995328,1995343],"length":1,"stats":{"Line":3}},{"line":259,"address":[1995306,1995224,1995296,1995163],"length":1,"stats":{"Line":4}},{"line":260,"address":[1995241],"length":1,"stats":{"Line":1}},{"line":265,"address":[1994864],"length":1,"stats":{"Line":1}},{"line":267,"address":[1994958],"length":1,"stats":{"Line":1}},{"line":268,"address":[1994962],"length":1,"stats":{"Line":1}},{"line":272,"address":[1994208,1994556],"length":1,"stats":{"Line":1}},{"line":274,"address":[1994238],"length":1,"stats":{"Line":1}},{"line":275,"address":[],"length":0,"stats":{"Line":0}},{"line":277,"address":[1994611,1994686,1994576,1994333,1994672],"length":1,"stats":{"Line":5}},{"line":280,"address":[1994422],"length":1,"stats":{"Line":1}},{"line":281,"address":[1994380],"length":1,"stats":{"Line":1}},{"line":282,"address":[],"length":0,"stats":{"Line":0}},{"line":283,"address":[1994415],"length":1,"stats":{"Line":1}},{"line":284,"address":[1994419],"length":1,"stats":{"Line":1}},{"line":285,"address":[],"length":0,"stats":{"Line":0}},{"line":289,"address":[1994768],"length":1,"stats":{"Line":1}},{"line":290,"address":[1994773],"length":1,"stats":{"Line":1}}],"covered":79,"coverable":111},{"path":["/","home","victor","ML","machinelearne-rs","lib","src","preprocessing","error.rs"],"content":"//! Error types for preprocessing operations.\n\nuse std::fmt;\n\n/// Error type for preprocessing operations.\n#[derive(Debug)]\npub enum PreprocessingError {\n    /// Shape mismatch between expected and actual tensor dimensions.\n    InvalidShape { expected: String, got: String },\n    /// Numerical computation error (overflow, underflow, etc.).\n    NumericalError(String),\n    /// Data contains missing values (NaN) when not expected.\n    MissingValues(String),\n    /// Invalid hyperparameter value.\n    InvalidParameter(String),\n    /// Serialization or deserialization error.\n    SerializationError(String),\n    /// I/O error during file operations.\n    IoError(String),\n    /// Empty data provided where non-empty was required.\n    EmptyData(String),\n    /// Feature dimension mismatch.\n    FeatureMismatch {\n        expected_features: usize,\n        got_features: usize,\n    },\n}\n\nimpl fmt::Display for PreprocessingError {\n    fn fmt(\u0026self, f: \u0026mut fmt::Formatter\u003c'_\u003e) -\u003e fmt::Result {\n        match self {\n            PreprocessingError::InvalidShape { expected, got } =\u003e {\n                write!(f, \"Invalid shape: expected {}, got {}\", expected, got)\n            }\n            PreprocessingError::NumericalError(msg) =\u003e {\n                write!(f, \"Numerical error: {}\", msg)\n            }\n            PreprocessingError::MissingValues(msg) =\u003e {\n                write!(f, \"Missing values: {}\", msg)\n            }\n            PreprocessingError::InvalidParameter(msg) =\u003e {\n                write!(f, \"Invalid parameter: {}\", msg)\n            }\n            PreprocessingError::SerializationError(msg) =\u003e {\n                write!(f, \"Serialization error: {}\", msg)\n            }\n            PreprocessingError::IoError(msg) =\u003e {\n                write!(f, \"I/O error: {}\", msg)\n            }\n            PreprocessingError::EmptyData(msg) =\u003e {\n                write!(f, \"Empty data: {}\", msg)\n            }\n            PreprocessingError::FeatureMismatch {\n                expected_features,\n                got_features,\n            } =\u003e {\n                write!(\n                    f,\n                    \"Feature mismatch: expected {} features, got {}\",\n                    expected_features, got_features\n                )\n            }\n        }\n    }\n}\n\nimpl std::error::Error for PreprocessingError {}\n\nimpl From\u003cstd::io::Error\u003e for PreprocessingError {\n    fn from(err: std::io::Error) -\u003e Self {\n        PreprocessingError::IoError(err.to_string())\n    }\n}\n\nimpl From\u003cbincode::Error\u003e for PreprocessingError {\n    fn from(err: bincode::Error) -\u003e Self {\n        PreprocessingError::SerializationError(err.to_string())\n    }\n}\n","traces":[{"line":30,"address":[3364608],"length":1,"stats":{"Line":0}},{"line":31,"address":[3364641],"length":1,"stats":{"Line":0}},{"line":32,"address":[3364700],"length":1,"stats":{"Line":0}},{"line":33,"address":[3364714],"length":1,"stats":{"Line":0}},{"line":35,"address":[3364887],"length":1,"stats":{"Line":0}},{"line":36,"address":[3364899],"length":1,"stats":{"Line":0}},{"line":38,"address":[3365016],"length":1,"stats":{"Line":0}},{"line":39,"address":[3365028],"length":1,"stats":{"Line":0}},{"line":41,"address":[3365145],"length":1,"stats":{"Line":0}},{"line":42,"address":[3365157],"length":1,"stats":{"Line":0}},{"line":44,"address":[3365274],"length":1,"stats":{"Line":0}},{"line":45,"address":[3365286],"length":1,"stats":{"Line":0}},{"line":47,"address":[3365403],"length":1,"stats":{"Line":0}},{"line":48,"address":[3365415],"length":1,"stats":{"Line":0}},{"line":50,"address":[3365532],"length":1,"stats":{"Line":0}},{"line":51,"address":[3365544],"length":1,"stats":{"Line":0}},{"line":53,"address":[3365661],"length":1,"stats":{"Line":0}},{"line":57,"address":[3365688],"length":1,"stats":{"Line":0}},{"line":70,"address":[3363686,3363692,3363552],"length":1,"stats":{"Line":0}},{"line":71,"address":[3363625,3363581],"length":1,"stats":{"Line":0}},{"line":76,"address":[3363712,3363841,3363847],"length":1,"stats":{"Line":0}},{"line":77,"address":[3363741,3363781],"length":1,"stats":{"Line":0}}],"covered":0,"coverable":22},{"path":["/","home","victor","ML","machinelearne-rs","lib","src","preprocessing","feature_engineering","mod.rs"],"content":"//! Feature engineering transformers.\n//!\n//! This module provides transformers for generating new features from existing data.\n\nmod polynomial;\n\npub use polynomial::{FittedPolynomialFeatures, PolynomialFeatures, PolynomialFeaturesParams};\n","traces":[],"covered":0,"coverable":0},{"path":["/","home","victor","ML","machinelearne-rs","lib","src","preprocessing","feature_engineering","polynomial.rs"],"content":"//! Polynomial feature generation.\n//!\n//! Generates polynomial and interaction features from input data.\n\nuse crate::backend::{Backend, Tensor2D};\nuse crate::preprocessing::error::PreprocessingError;\nuse crate::preprocessing::traits::{FittedTransformer, Transformer};\nuse serde::{Deserialize, Serialize};\nuse std::marker::PhantomData;\n\n/// PolynomialFeatures transformer for generating polynomial and interaction features.\n///\n/// Generates a new feature matrix consisting of all polynomial combinations\n/// of the features with degree less than or equal to the specified degree.\n///\n/// For example, if an input sample is two dimensional and of the form [a, b],\n/// the degree-2 polynomial features are [1, a, b, a^2, ab, b^2].\n///\n/// # Example\n/// ```ignore\n/// use machinelearne_rs::preprocessing::{PolynomialFeatures, Transformer};\n/// use machinelearne_rs::backend::CpuBackend;\n///\n/// // Input: [[1, 2], [3, 4]]\n/// let data = Tensor2D::new(vec![1.0, 2.0, 3.0, 4.0], 2, 2);\n///\n/// let poly = PolynomialFeatures::\u003cCpuBackend\u003e::new().with_degree(2);\n/// let fitted = poly.fit(\u0026data)?;\n///\n/// // Output: [[1, 1, 2, 1, 2, 4], [1, 3, 4, 9, 12, 16]]\n/// let transformed = fitted.transform(\u0026data)?;\n/// ```\n#[derive(Clone, Debug)]\npub struct PolynomialFeatures\u003cB: Backend\u003e {\n    /// Maximum degree of polynomial features.\n    degree: usize,\n    /// If True, include a bias column (feature of all 1s).\n    include_bias: bool,\n    /// If True, only produce interaction features (products of distinct features).\n    interaction_only: bool,\n    _backend: PhantomData\u003cB\u003e,\n}\n\nimpl\u003cB: Backend\u003e Default for PolynomialFeatures\u003cB\u003e {\n    fn default() -\u003e Self {\n        Self::new()\n    }\n}\n\nimpl\u003cB: Backend\u003e PolynomialFeatures\u003cB\u003e {\n    /// Create a new PolynomialFeatures with default settings (degree=2).\n    pub fn new() -\u003e Self {\n        Self {\n            degree: 2,\n            include_bias: true,\n            interaction_only: false,\n            _backend: PhantomData,\n        }\n    }\n\n    /// Set the maximum degree of polynomial features.\n    pub fn with_degree(mut self, degree: usize) -\u003e Self {\n        self.degree = degree;\n        self\n    }\n\n    /// Set whether to include a bias column (all 1s).\n    pub fn with_include_bias(mut self, include_bias: bool) -\u003e Self {\n        self.include_bias = include_bias;\n        self\n    }\n\n    /// Set whether to only produce interaction features.\n    pub fn with_interaction_only(mut self, interaction_only: bool) -\u003e Self {\n        self.interaction_only = interaction_only;\n        self\n    }\n}\n\n/// Serializable parameters for fitted PolynomialFeatures.\n#[derive(Clone, Serialize, Deserialize)]\npub struct PolynomialFeaturesParams {\n    /// Maximum degree of polynomial features.\n    pub degree: usize,\n    /// If True, include a bias column.\n    pub include_bias: bool,\n    /// If True, only produce interaction features.\n    pub interaction_only: bool,\n    /// Number of input features.\n    pub n_features_in: usize,\n    /// Number of output features.\n    pub n_features_out: usize,\n    /// List of (degree, feature indices) combinations for output features.\n    /// For example, [(0, [])] for bias, [(1, [0])] for first input feature,\n    /// [(2, [0, 0])] for square of first feature, [(2, [0, 1])] for interaction.\n    pub output_combinations: Vec\u003c(usize, Vec\u003cusize\u003e)\u003e,\n}\n\n/// Fitted PolynomialFeatures ready for inference.\n#[derive(Clone)]\npub struct FittedPolynomialFeatures\u003cB: Backend\u003e {\n    /// Maximum degree of polynomial features.\n    degree: usize,\n    /// If True, include a bias column.\n    include_bias: bool,\n    /// If True, only produce interaction features.\n    interaction_only: bool,\n    /// Number of input features.\n    n_features_in: usize,\n    /// Number of output features.\n    n_features_out: usize,\n    /// Output feature combinations (degree, indices).\n    output_combinations: Vec\u003c(usize, Vec\u003cusize\u003e)\u003e,\n    _backend: PhantomData\u003cB\u003e,\n}\n\nimpl\u003cB: Backend\u003e FittedPolynomialFeatures\u003cB\u003e {\n    /// Get the number of output features.\n    pub fn n_features_out(\u0026self) -\u003e usize {\n        self.n_features_out\n    }\n\n    /// Get the output feature combinations.\n    pub fn output_combinations(\u0026self) -\u003e \u0026[(usize, Vec\u003cusize\u003e)] {\n        \u0026self.output_combinations\n    }\n}\n\nimpl\u003cB: Backend\u003e Transformer\u003cB\u003e for PolynomialFeatures\u003cB\u003e {\n    type Input = Tensor2D\u003cB\u003e;\n    type Output = Tensor2D\u003cB\u003e;\n    type Params = PolynomialFeaturesParams;\n    type Fitted = FittedPolynomialFeatures\u003cB\u003e;\n\n    fn fit(\u0026self, data: \u0026Self::Input) -\u003e Result\u003cSelf::Fitted, PreprocessingError\u003e {\n        let (rows, cols) = data.shape();\n\n        if rows == 0 {\n            return Err(PreprocessingError::EmptyData(\n                \"Cannot fit PolynomialFeatures on empty data\".to_string(),\n            ));\n        }\n\n        if cols == 0 {\n            return Err(PreprocessingError::InvalidParameter(\n                \"Cannot fit PolynomialFeatures on data with no features\".to_string(),\n            ));\n        }\n\n        // Generate output combinations\n        let output_combinations = generate_polynomial_combinations(\n            cols,\n            self.degree,\n            self.include_bias,\n            self.interaction_only,\n        );\n\n        let n_features_out = output_combinations.len();\n\n        Ok(FittedPolynomialFeatures {\n            degree: self.degree,\n            include_bias: self.include_bias,\n            interaction_only: self.interaction_only,\n            n_features_in: cols,\n            n_features_out,\n            output_combinations,\n            _backend: PhantomData,\n        })\n    }\n\n    fn fit_transform(\u0026self, data: \u0026Self::Input) -\u003e Result\u003cSelf::Output, PreprocessingError\u003e {\n        let fitted = self.fit(data)?;\n        fitted.transform(data)\n    }\n}\n\nimpl\u003cB: Backend\u003e FittedTransformer\u003cB\u003e for FittedPolynomialFeatures\u003cB\u003e {\n    type Input = Tensor2D\u003cB\u003e;\n    type Output = Tensor2D\u003cB\u003e;\n    type Params = PolynomialFeaturesParams;\n\n    fn transform(\u0026self, data: \u0026Self::Input) -\u003e Result\u003cSelf::Output, PreprocessingError\u003e {\n        let (rows, cols) = data.shape();\n\n        if cols != self.n_features_in {\n            return Err(PreprocessingError::FeatureMismatch {\n                expected_features: self.n_features_in,\n                got_features: cols,\n            });\n        }\n\n        if rows == 0 {\n            return Ok(Tensor2D::zeros(0, self.n_features_out));\n        }\n\n        let data_vec = data.ravel().to_vec();\n        let mut result = Vec::with_capacity(rows * self.n_features_out);\n\n        for row in 0..rows {\n            let row_start = row * cols;\n            let row_data: Vec\u003cf64\u003e = (0..cols).map(|c| data_vec[row_start + c]).collect();\n\n            for (_degree, indices) in \u0026self.output_combinations {\n                let mut val = 1.0f64;\n                for \u0026idx in indices {\n                    val *= row_data[idx];\n                }\n                result.push(val);\n            }\n        }\n\n        Ok(Tensor2D::new(\n            result.iter().map(|\u0026x| x as f32).collect(),\n            rows,\n            self.n_features_out,\n        ))\n    }\n\n    fn inverse_transform(\u0026self, _data: \u0026Self::Output) -\u003e Result\u003cSelf::Input, PreprocessingError\u003e {\n        Err(PreprocessingError::InvalidParameter(\n            \"PolynomialFeatures does not support inverse_transform\".to_string(),\n        ))\n    }\n\n    fn extract_params(\u0026self) -\u003e Self::Params {\n        PolynomialFeaturesParams {\n            degree: self.degree,\n            include_bias: self.include_bias,\n            interaction_only: self.interaction_only,\n            n_features_in: self.n_features_in,\n            n_features_out: self.n_features_out,\n            output_combinations: self.output_combinations.clone(),\n        }\n    }\n\n    fn from_params(params: Self::Params) -\u003e Result\u003cSelf, PreprocessingError\u003e {\n        Ok(FittedPolynomialFeatures {\n            degree: params.degree,\n            include_bias: params.include_bias,\n            interaction_only: params.interaction_only,\n            n_features_in: params.n_features_in,\n            n_features_out: params.n_features_out,\n            output_combinations: params.output_combinations,\n            _backend: PhantomData,\n        })\n    }\n\n    fn n_features_in(\u0026self) -\u003e usize {\n        self.n_features_in\n    }\n}\n\n/// Generate all polynomial combinations up to given degree.\nfn generate_polynomial_combinations(\n    n_features: usize,\n    degree: usize,\n    include_bias: bool,\n    interaction_only: bool,\n) -\u003e Vec\u003c(usize, Vec\u003cusize\u003e)\u003e {\n    let mut combinations = Vec::new();\n\n    // Bias term (degree 0)\n    if include_bias {\n        combinations.push((0, Vec::new()));\n    }\n\n    // Generate combinations for degrees 1 to max_degree\n    for d in 1..=degree {\n        generate_degree_combinations(\n            n_features,\n            d,\n            d,\n            interaction_only,\n            \u0026mut Vec::new(),\n            \u0026mut combinations,\n        );\n    }\n\n    combinations\n}\n\n/// Recursively generate combinations for a specific degree.\nfn generate_degree_combinations(\n    n_features: usize,\n    target_degree: usize,\n    remaining_degree: usize,\n    interaction_only: bool,\n    current: \u0026mut Vec\u003cusize\u003e,\n    result: \u0026mut Vec\u003c(usize, Vec\u003cusize\u003e)\u003e,\n) {\n    if remaining_degree == 0 {\n        // Check interaction_only constraint\n        if interaction_only {\n            // All indices must be distinct for interaction_only\n            let mut sorted = current.clone();\n            sorted.sort();\n            sorted.dedup();\n            if sorted.len() != current.len() {\n                return; // Has duplicates, skip\n            }\n        }\n        result.push((target_degree, current.clone()));\n        return;\n    }\n\n    // Determine starting index to avoid duplicates and maintain ordering\n    let start = if current.is_empty() {\n        0\n    } else {\n        *current.last().unwrap()\n    };\n\n    for i in start..n_features {\n        current.push(i);\n        generate_degree_combinations(\n            n_features,\n            target_degree,\n            remaining_degree - 1,\n            interaction_only,\n            current,\n            result,\n        );\n        current.pop();\n    }\n}\n\n#[cfg(test)]\nmod tests {\n    use super::*;\n    use crate::backend::CpuBackend;\n\n    #[test]\n    fn test_polynomial_features_basic() {\n        // Input: [[1, 2]] -\u003e [1, a, b, a^2, ab, b^2]\n        let data = Tensor2D::\u003cCpuBackend\u003e::new(vec![1.0f32, 2.0], 1, 2);\n\n        let poly = PolynomialFeatures::\u003cCpuBackend\u003e::new()\n            .with_degree(2)\n            .with_include_bias(true);\n        let fitted = poly.fit(\u0026data).unwrap();\n\n        assert_eq!(fitted.n_features_in(), 2);\n        assert_eq!(fitted.n_features_out(), 6); // bias + 2 linear + 3 quadratic\n\n        let transformed = fitted.transform(\u0026data).unwrap();\n        let vals = transformed.ravel().to_vec();\n\n        // [1, 1, 2, 1, 2, 4]\n        assert!((vals[0] - 1.0).abs() \u003c 1e-6); // bias\n        assert!((vals[1] - 1.0).abs() \u003c 1e-6); // a\n        assert!((vals[2] - 2.0).abs() \u003c 1e-6); // b\n        assert!((vals[3] - 1.0).abs() \u003c 1e-6); // a^2\n        assert!((vals[4] - 2.0).abs() \u003c 1e-6); // ab\n        assert!((vals[5] - 4.0).abs() \u003c 1e-6); // b^2\n    }\n\n    #[test]\n    fn test_polynomial_features_no_bias() {\n        // Input: [[1, 2]] -\u003e [a, b, a^2, ab, b^2]\n        let data = Tensor2D::\u003cCpuBackend\u003e::new(vec![1.0f32, 2.0], 1, 2);\n\n        let poly = PolynomialFeatures::\u003cCpuBackend\u003e::new()\n            .with_degree(2)\n            .with_include_bias(false);\n        let fitted = poly.fit(\u0026data).unwrap();\n\n        assert_eq!(fitted.n_features_out(), 5); // 2 linear + 3 quadratic\n\n        let transformed = fitted.transform(\u0026data).unwrap();\n        let vals = transformed.ravel().to_vec();\n\n        // [1, 2, 1, 2, 4]\n        assert!((vals[0] - 1.0).abs() \u003c 1e-6); // a\n        assert!((vals[1] - 2.0).abs() \u003c 1e-6); // b\n        assert!((vals[2] - 1.0).abs() \u003c 1e-6); // a^2\n        assert!((vals[3] - 2.0).abs() \u003c 1e-6); // ab\n        assert!((vals[4] - 4.0).abs() \u003c 1e-6); // b^2\n    }\n\n    #[test]\n    fn test_polynomial_features_interaction_only() {\n        // Input: [[1, 2, 3]] -\u003e [a, b, c, ab, ac, bc]\n        let data = Tensor2D::\u003cCpuBackend\u003e::new(vec![1.0f32, 2.0, 3.0], 1, 3);\n\n        let poly = PolynomialFeatures::\u003cCpuBackend\u003e::new()\n            .with_degree(2)\n            .with_include_bias(false)\n            .with_interaction_only(true);\n        let fitted = poly.fit(\u0026data).unwrap();\n\n        // 3 linear + 3 interaction (no a^2, b^2, c^2)\n        assert_eq!(fitted.n_features_out(), 6);\n\n        let transformed = fitted.transform(\u0026data).unwrap();\n        let vals = transformed.ravel().to_vec();\n\n        // [1, 2, 3, 2, 3, 6]\n        assert!((vals[0] - 1.0).abs() \u003c 1e-6); // a\n        assert!((vals[1] - 2.0).abs() \u003c 1e-6); // b\n        assert!((vals[2] - 3.0).abs() \u003c 1e-6); // c\n        assert!((vals[3] - 2.0).abs() \u003c 1e-6); // ab\n        assert!((vals[4] - 3.0).abs() \u003c 1e-6); // ac\n        assert!((vals[5] - 6.0).abs() \u003c 1e-6); // bc\n    }\n\n    #[test]\n    fn test_polynomial_features_degree_3() {\n        // Input: [[1, 2]]\n        let data = Tensor2D::\u003cCpuBackend\u003e::new(vec![1.0f32, 2.0], 1, 2);\n\n        let poly = PolynomialFeatures::\u003cCpuBackend\u003e::new()\n            .with_degree(3)\n            .with_include_bias(true);\n        let fitted = poly.fit(\u0026data).unwrap();\n\n        // bias + 2 linear + 3 quadratic + 4 cubic = 10\n        assert_eq!(fitted.n_features_out(), 10);\n\n        let transformed = fitted.transform(\u0026data).unwrap();\n        let vals = transformed.ravel().to_vec();\n\n        // [1, 1, 2, 1, 2, 4, 1, 2, 4, 8]\n        // degree 0: 1\n        // degree 1: a, b\n        // degree 2: a^2, ab, b^2\n        // degree 3: a^3, a^2*b, a*b^2, b^3\n        assert!((vals[0] - 1.0).abs() \u003c 1e-6); // bias\n        assert!((vals[6] - 1.0).abs() \u003c 1e-6); // a^3\n        assert!((vals[9] - 8.0).abs() \u003c 1e-6); // b^3\n    }\n\n    #[test]\n    fn test_polynomial_features_multiple_rows() {\n        let data = Tensor2D::\u003cCpuBackend\u003e::new(vec![1.0f32, 2.0, 3.0, 4.0], 2, 2);\n\n        let poly = PolynomialFeatures::\u003cCpuBackend\u003e::new()\n            .with_degree(2)\n            .with_include_bias(true);\n        let fitted = poly.fit(\u0026data).unwrap();\n\n        let transformed = fitted.transform(\u0026data).unwrap();\n        assert_eq!(transformed.shape(), (2, 6));\n\n        let vals = transformed.ravel().to_vec();\n\n        // Row 0: [1, 1, 2, 1, 2, 4]\n        assert!((vals[0] - 1.0).abs() \u003c 1e-6);\n        assert!((vals[4] - 2.0).abs() \u003c 1e-6);\n        assert!((vals[5] - 4.0).abs() \u003c 1e-6);\n\n        // Row 1: [1, 3, 4, 9, 12, 16]\n        assert!((vals[6] - 1.0).abs() \u003c 1e-6);\n        assert!((vals[10] - 12.0).abs() \u003c 1e-6);\n        assert!((vals[11] - 16.0).abs() \u003c 1e-6);\n    }\n\n    #[test]\n    fn test_polynomial_features_serialization() {\n        let data = Tensor2D::\u003cCpuBackend\u003e::new(vec![1.0f32, 2.0], 1, 2);\n\n        let poly = PolynomialFeatures::\u003cCpuBackend\u003e::new()\n            .with_degree(2)\n            .with_include_bias(true);\n        let fitted = poly.fit(\u0026data).unwrap();\n\n        let temp_file = std::env::temp_dir().join(\"test_poly.bin\");\n        fitted.save_to_file(\u0026temp_file).unwrap();\n\n        let loaded = FittedPolynomialFeatures::\u003cCpuBackend\u003e::load_from_file(\u0026temp_file).unwrap();\n\n        assert_eq!(loaded.n_features_in(), fitted.n_features_in());\n        assert_eq!(loaded.n_features_out(), fitted.n_features_out());\n\n        let t1 = fitted.transform(\u0026data).unwrap();\n        let t2 = loaded.transform(\u0026data).unwrap();\n\n        let v1 = t1.ravel().to_vec();\n        let v2 = t2.ravel().to_vec();\n        for (a, b) in v1.iter().zip(v2.iter()) {\n            assert!((a - b).abs() \u003c 1e-6);\n        }\n\n        std::fs::remove_file(temp_file).ok();\n    }\n}\n","traces":[{"line":45,"address":[],"length":0,"stats":{"Line":0}},{"line":46,"address":[],"length":0,"stats":{"Line":0}},{"line":52,"address":[2817040],"length":1,"stats":{"Line":1}},{"line":62,"address":[2816912],"length":1,"stats":{"Line":1}},{"line":63,"address":[2816920],"length":1,"stats":{"Line":1}},{"line":64,"address":[2816923],"length":1,"stats":{"Line":1}},{"line":68,"address":[2816944],"length":1,"stats":{"Line":1}},{"line":69,"address":[2816958],"length":1,"stats":{"Line":1}},{"line":70,"address":[2816964],"length":1,"stats":{"Line":2}},{"line":74,"address":[2816992],"length":1,"stats":{"Line":1}},{"line":75,"address":[2817006],"length":1,"stats":{"Line":1}},{"line":76,"address":[2817012],"length":1,"stats":{"Line":1}},{"line":119,"address":[2817072],"length":1,"stats":{"Line":1}},{"line":120,"address":[2817077],"length":1,"stats":{"Line":2}},{"line":124,"address":[],"length":0,"stats":{"Line":0}},{"line":125,"address":[],"length":0,"stats":{"Line":0}},{"line":135,"address":[2819112,2818448,2819118],"length":1,"stats":{"Line":1}},{"line":136,"address":[2818499],"length":1,"stats":{"Line":1}},{"line":138,"address":[2818525],"length":1,"stats":{"Line":2}},{"line":139,"address":[2818559],"length":1,"stats":{"Line":0}},{"line":140,"address":[2818531],"length":1,"stats":{"Line":0}},{"line":144,"address":[2818648],"length":1,"stats":{"Line":1}},{"line":145,"address":[2818703],"length":1,"stats":{"Line":0}},{"line":146,"address":[2818672],"length":1,"stats":{"Line":0}},{"line":152,"address":[],"length":0,"stats":{"Line":0}},{"line":153,"address":[2818824],"length":1,"stats":{"Line":3}},{"line":154,"address":[2818827],"length":1,"stats":{"Line":1}},{"line":155,"address":[2818831],"length":1,"stats":{"Line":3}},{"line":158,"address":[2818929,2818857],"length":1,"stats":{"Line":5}},{"line":160,"address":[2818995],"length":1,"stats":{"Line":3}},{"line":161,"address":[2818937],"length":1,"stats":{"Line":2}},{"line":162,"address":[2818940],"length":1,"stats":{"Line":3}},{"line":163,"address":[2818944],"length":1,"stats":{"Line":2}},{"line":164,"address":[],"length":0,"stats":{"Line":0}},{"line":165,"address":[],"length":0,"stats":{"Line":0}},{"line":166,"address":[2818947],"length":1,"stats":{"Line":3}},{"line":167,"address":[],"length":0,"stats":{"Line":0}},{"line":171,"address":[],"length":0,"stats":{"Line":0}},{"line":172,"address":[],"length":0,"stats":{"Line":0}},{"line":173,"address":[],"length":0,"stats":{"Line":0}},{"line":182,"address":[2821056,2821050,2819488],"length":1,"stats":{"Line":1}},{"line":183,"address":[2819554],"length":1,"stats":{"Line":2}},{"line":185,"address":[2819599],"length":1,"stats":{"Line":2}},{"line":186,"address":[2819652],"length":1,"stats":{"Line":0}},{"line":187,"address":[2819648],"length":1,"stats":{"Line":0}},{"line":188,"address":[],"length":0,"stats":{"Line":0}},{"line":192,"address":[2819613],"length":1,"stats":{"Line":3}},{"line":193,"address":[2819717],"length":1,"stats":{"Line":0}},{"line":196,"address":[2819815,2819867],"length":1,"stats":{"Line":2}},{"line":197,"address":[2819970],"length":1,"stats":{"Line":3}},{"line":199,"address":[2820045,2820132],"length":1,"stats":{"Line":5}},{"line":200,"address":[2820556,2820513,2820241],"length":1,"stats":{"Line":5}},{"line":201,"address":[2820549,2821104,2820593,2821118],"length":1,"stats":{"Line":10}},{"line":203,"address":[2820608,2820675],"length":1,"stats":{"Line":5}},{"line":204,"address":[2820801],"length":1,"stats":{"Line":2}},{"line":205,"address":[2821045,2820854,2820809],"length":1,"stats":{"Line":8}},{"line":206,"address":[2821023,2820957],"length":1,"stats":{"Line":3}},{"line":208,"address":[2820983],"length":1,"stats":{"Line":2}},{"line":212,"address":[2820403],"length":1,"stats":{"Line":1}},{"line":213,"address":[2820271,2821072,2821082],"length":1,"stats":{"Line":7}},{"line":214,"address":[],"length":0,"stats":{"Line":0}},{"line":215,"address":[2820383],"length":1,"stats":{"Line":3}},{"line":219,"address":[],"length":0,"stats":{"Line":0}},{"line":220,"address":[],"length":0,"stats":{"Line":0}},{"line":221,"address":[],"length":0,"stats":{"Line":0}},{"line":225,"address":[2819312],"length":1,"stats":{"Line":1}},{"line":227,"address":[2819331],"length":1,"stats":{"Line":1}},{"line":228,"address":[2819340],"length":1,"stats":{"Line":1}},{"line":229,"address":[2819347],"length":1,"stats":{"Line":1}},{"line":230,"address":[2819354],"length":1,"stats":{"Line":1}},{"line":231,"address":[2819363],"length":1,"stats":{"Line":1}},{"line":232,"address":[2819372],"length":1,"stats":{"Line":1}},{"line":236,"address":[2819136],"length":1,"stats":{"Line":1}},{"line":237,"address":[2819196],"length":1,"stats":{"Line":1}},{"line":238,"address":[2819150],"length":1,"stats":{"Line":1}},{"line":239,"address":[2819154],"length":1,"stats":{"Line":1}},{"line":240,"address":[2819158],"length":1,"stats":{"Line":1}},{"line":241,"address":[2819162],"length":1,"stats":{"Line":1}},{"line":242,"address":[2819166],"length":1,"stats":{"Line":1}},{"line":243,"address":[2819170],"length":1,"stats":{"Line":1}},{"line":244,"address":[],"length":0,"stats":{"Line":0}},{"line":248,"address":[2819296],"length":1,"stats":{"Line":1}},{"line":249,"address":[2819301],"length":1,"stats":{"Line":1}},{"line":254,"address":[3139295,3138736,3139301],"length":1,"stats":{"Line":2}},{"line":260,"address":[3138809],"length":1,"stats":{"Line":3}},{"line":263,"address":[3138823],"length":1,"stats":{"Line":2}},{"line":264,"address":[3138908,3138864],"length":1,"stats":{"Line":3}},{"line":268,"address":[3138988,3138832],"length":1,"stats":{"Line":2}},{"line":274,"address":[3139136],"length":1,"stats":{"Line":3}},{"line":279,"address":[3139153],"length":1,"stats":{"Line":3}},{"line":283,"address":[3138447,3137984,3138441],"length":1,"stats":{"Line":3}},{"line":291,"address":[3138071],"length":1,"stats":{"Line":3}},{"line":293,"address":[3138081],"length":1,"stats":{"Line":3}},{"line":295,"address":[3138235],"length":1,"stats":{"Line":1}},{"line":296,"address":[3138262,3138330],"length":1,"stats":{"Line":2}},{"line":297,"address":[3138342],"length":1,"stats":{"Line":1}},{"line":298,"address":[3138349],"length":1,"stats":{"Line":1}},{"line":302,"address":[3138119],"length":1,"stats":{"Line":3}},{"line":307,"address":[3138096,3138508],"length":1,"stats":{"Line":5}},{"line":308,"address":[3138510],"length":1,"stats":{"Line":3}},{"line":310,"address":[3138465],"length":1,"stats":{"Line":3}},{"line":313,"address":[3138557,3138527],"length":1,"stats":{"Line":6}},{"line":314,"address":[3138626],"length":1,"stats":{"Line":3}},{"line":318,"address":[3138644,3138717],"length":1,"stats":{"Line":3}},{"line":323,"address":[3138707],"length":1,"stats":{"Line":2}}],"covered":81,"coverable":105},{"path":["/","home","victor","ML","machinelearne-rs","lib","src","preprocessing","imputation","mod.rs"],"content":"//! Imputation transformers for handling missing values.\n//!\n//! This module provides transformers for imputing (filling in) missing values\n//! in datasets.\n//!\n//! # Available Transformers\n//!\n//! | Transformer | Description |\n//! |-------------|-------------|\n//! | [`SimpleImputer`] | Impute with mean, median, most_frequent, or constant |\n//!\n//! # Example\n//!\n//! ```ignore\n//! use machinelearne_rs::preprocessing::imputation::SimpleImputer;\n//! use machinelearne_rs::preprocessing::{Transformer, ImputeStrategy};\n//! use machinelearne_rs::backend::CpuBackend;\n//!\n//! let imputer = SimpleImputer::\u003cCpuBackend\u003e::new(ImputeStrategy::Mean);\n//! let fitted = imputer.fit(\u0026data)?;\n//! let imputed = fitted.transform(\u0026new_data)?;\n//! ```\n\npub mod simple;\n\npub use simple::{FittedSimpleImputer, ImputeStrategy, SimpleImputer, SimpleImputerParams};\n","traces":[],"covered":0,"coverable":0},{"path":["/","home","victor","ML","machinelearne-rs","lib","src","preprocessing","imputation","simple.rs"],"content":"//! Simple Imputer.\n//!\n//! Imputation transformer for completing missing values.\n//! Supports mean, median, most_frequent, and constant strategies.\n//!\n//! Note: This implementation treats NaN as missing values.\n//!\n//! # Example\n//! ```ignore\n//! use machinelearne_rs::preprocessing::{Transformer, SimpleImputer, ImputeStrategy};\n//! use machinelearne_rs::backend::CpuBackend;\n//!\n//! let imputer = SimpleImputer::\u003cCpuBackend\u003e::new(ImputeStrategy::Mean);\n//! let fitted = imputer.fit(\u0026data)?;\n//! let imputed = fitted.transform(\u0026data)?;\n//! ```\n\nuse crate::backend::{Backend, Tensor1D, Tensor2D};\nuse crate::preprocessing::error::PreprocessingError;\nuse crate::preprocessing::traits::{FittedTransformer, Transformer};\nuse serde::{Deserialize, Serialize};\nuse std::marker::PhantomData;\n\n/// Strategy for imputing missing values.\n#[derive(Clone, Debug, Default, Serialize, Deserialize)]\npub enum ImputeStrategy {\n    /// Replace missing values with the mean of each column.\n    #[default]\n    Mean,\n    /// Replace missing values with the median of each column.\n    Median,\n    /// Replace missing values with the most frequent value of each column.\n    MostFrequent,\n    /// Replace missing values with a constant value.\n    Constant(f64),\n}\n\n/// Serializable parameters for a fitted SimpleImputer.\n#[derive(Clone, Serialize, Deserialize)]\npub struct SimpleImputerParams {\n    /// Strategy used for imputation.\n    pub strategy: ImputeStrategy,\n    /// Statistics (fill values) for each feature.\n    pub statistics_: Vec\u003cf64\u003e,\n    /// Number of features seen during fit.\n    pub n_features: usize,\n}\n\n/// SimpleImputer transformer (unfitted).\n///\n/// Imputation transformer for completing missing values.\n#[derive(Clone)]\npub struct SimpleImputer\u003cB: Backend\u003e {\n    strategy: ImputeStrategy,\n    _backend: PhantomData\u003cB\u003e,\n}\n\nimpl\u003cB: Backend\u003e Default for SimpleImputer\u003cB\u003e {\n    fn default() -\u003e Self {\n        Self::new(ImputeStrategy::default())\n    }\n}\n\nimpl\u003cB: Backend\u003e SimpleImputer\u003cB\u003e {\n    /// Create a new SimpleImputer with the specified strategy.\n    pub fn new(strategy: ImputeStrategy) -\u003e Self {\n        Self {\n            strategy,\n            _backend: PhantomData,\n        }\n    }\n}\n\n/// Compute statistics for imputation, ignoring NaN values.\nfn compute_statistics(\n    data: \u0026[f64],\n    rows: usize,\n    cols: usize,\n    strategy: \u0026ImputeStrategy,\n) -\u003e Vec\u003cf64\u003e {\n    let mut stats = vec![0.0; cols];\n\n    for col in 0..cols {\n        // Collect non-NaN values for this column\n        let column_values: Vec\u003cf64\u003e = (0..rows)\n            .map(|row| data[row * cols + col])\n            .filter(|\u0026v| !v.is_nan())\n            .collect();\n\n        stats[col] = if column_values.is_empty() {\n            0.0 // Default to 0 if all values are missing\n        } else {\n            match strategy {\n                ImputeStrategy::Mean =\u003e {\n                    column_values.iter().sum::\u003cf64\u003e() / column_values.len() as f64\n                }\n                ImputeStrategy::Median =\u003e {\n                    let mut sorted = column_values.clone();\n                    sorted.sort_by(|a, b| a.partial_cmp(b).unwrap_or(std::cmp::Ordering::Equal));\n                    let n = sorted.len();\n                    if n.is_multiple_of(2) {\n                        (sorted[n / 2 - 1] + sorted[n / 2]) / 2.0\n                    } else {\n                        sorted[n / 2]\n                    }\n                }\n                ImputeStrategy::MostFrequent =\u003e {\n                    // Find the most common value\n                    let mut counts = std::collections::HashMap::new();\n                    for \u0026v in \u0026column_values {\n                        *counts.entry(v.to_bits()).or_insert(0) += 1;\n                    }\n                    counts\n                        .into_iter()\n                        .max_by_key(|\u0026(_, count)| count)\n                        .map(|(bits, _)| f64::from_bits(bits))\n                        .unwrap_or(0.0)\n                }\n                ImputeStrategy::Constant(val) =\u003e *val,\n            }\n        };\n    }\n\n    stats\n}\n\nimpl\u003cB: Backend\u003e Transformer\u003cB\u003e for SimpleImputer\u003cB\u003e {\n    type Input = Tensor2D\u003cB\u003e;\n    type Output = Tensor2D\u003cB\u003e;\n    type Params = SimpleImputerParams;\n    type Fitted = FittedSimpleImputer\u003cB\u003e;\n\n    fn fit(\u0026self, data: \u0026Self::Input) -\u003e Result\u003cSelf::Fitted, PreprocessingError\u003e {\n        let (rows, cols) = data.shape();\n\n        if rows == 0 {\n            return Err(PreprocessingError::EmptyData(\n                \"Cannot fit SimpleImputer on empty data\".to_string(),\n            ));\n        }\n\n        let flat_data = B::to_vec_1d(\u0026B::ravel_2d(\u0026data.data));\n        let statistics_ = compute_statistics(\u0026flat_data, rows, cols, \u0026self.strategy);\n\n        let statistics_tensor = Tensor1D {\n            data: B::from_vec_1d(statistics_.iter().map(|\u0026x| x as f32).collect()),\n            backend: PhantomData::\u003cB\u003e,\n        };\n\n        Ok(FittedSimpleImputer {\n            strategy: self.strategy.clone(),\n            statistics_: statistics_tensor,\n            n_features: cols,\n            _backend: PhantomData,\n        })\n    }\n\n    fn fit_transform(\u0026self, data: \u0026Self::Input) -\u003e Result\u003cSelf::Output, PreprocessingError\u003e {\n        let fitted = self.fit(data)?;\n        fitted.transform(data)\n    }\n}\n\n/// Fitted SimpleImputer ready for inference.\n#[derive(Clone)]\npub struct FittedSimpleImputer\u003cB: Backend\u003e {\n    strategy: ImputeStrategy,\n    statistics_: Tensor1D\u003cB\u003e,\n    n_features: usize,\n    _backend: PhantomData\u003cB\u003e,\n}\n\nimpl\u003cB: Backend\u003e FittedSimpleImputer\u003cB\u003e {\n    /// Get the imputation statistics (fill values) for each feature.\n    pub fn statistics(\u0026self) -\u003e \u0026Tensor1D\u003cB\u003e {\n        \u0026self.statistics_\n    }\n}\n\nimpl\u003cB: Backend\u003e FittedTransformer\u003cB\u003e for FittedSimpleImputer\u003cB\u003e {\n    type Input = Tensor2D\u003cB\u003e;\n    type Output = Tensor2D\u003cB\u003e;\n    type Params = SimpleImputerParams;\n\n    fn transform(\u0026self, data: \u0026Self::Input) -\u003e Result\u003cSelf::Output, PreprocessingError\u003e {\n        let (rows, cols) = data.shape();\n\n        if cols != self.n_features {\n            return Err(PreprocessingError::FeatureMismatch {\n                expected_features: self.n_features,\n                got_features: cols,\n            });\n        }\n\n        let flat_data = B::to_vec_1d(\u0026B::ravel_2d(\u0026data.data));\n        let stats = self.statistics_.to_vec();\n\n        // Replace NaN values with statistics\n        let mut result = Vec::with_capacity(rows * cols);\n        for row in 0..rows {\n            for col in 0..cols {\n                let val = flat_data[row * cols + col];\n                result.push(if val.is_nan() { stats[col] } else { val });\n            }\n        }\n\n        Ok(Tensor2D {\n            data: B::from_vec_2d(result.iter().map(|\u0026x| x as f32).collect(), rows, cols),\n            backend: PhantomData::\u003cB\u003e,\n        })\n    }\n\n    fn inverse_transform(\u0026self, _data: \u0026Self::Output) -\u003e Result\u003cSelf::Input, PreprocessingError\u003e {\n        Err(PreprocessingError::InvalidParameter(\n            \"SimpleImputer does not support inverse_transform (missing value information is lost)\"\n                .to_string(),\n        ))\n    }\n\n    fn extract_params(\u0026self) -\u003e Self::Params {\n        SimpleImputerParams {\n            strategy: self.strategy.clone(),\n            statistics_: self.statistics_.to_vec(),\n            n_features: self.n_features,\n        }\n    }\n\n    fn from_params(params: Self::Params) -\u003e Result\u003cSelf, PreprocessingError\u003e {\n        let statistics_ = Tensor1D {\n            data: B::from_vec_1d(params.statistics_.iter().map(|\u0026x| x as f32).collect()),\n            backend: PhantomData::\u003cB\u003e,\n        };\n\n        Ok(Self {\n            strategy: params.strategy,\n            statistics_,\n            n_features: params.n_features,\n            _backend: PhantomData,\n        })\n    }\n\n    fn n_features_in(\u0026self) -\u003e usize {\n        self.n_features\n    }\n\n    fn save_to_file\u003cP: AsRef\u003cstd::path::Path\u003e\u003e(\u0026self, path: P) -\u003e std::io::Result\u003c()\u003e {\n        let params = self.extract_params();\n        let bytes = bincode::serialize(\u0026params).map_err(std::io::Error::other)?;\n        std::fs::write(path, bytes)\n    }\n\n    fn load_from_file\u003cP: AsRef\u003cstd::path::Path\u003e\u003e(path: P) -\u003e Result\u003cSelf, PreprocessingError\u003e\n    where\n        Self: Sized,\n    {\n        let bytes = std::fs::read(path)?;\n        let params: SimpleImputerParams = bincode::deserialize(\u0026bytes)\n            .map_err(|e| PreprocessingError::SerializationError(e.to_string()))?;\n        Self::from_params(params)\n    }\n}\n\n#[cfg(test)]\nmod tests {\n    use super::*;\n    use crate::backend::CpuBackend;\n\n    fn create_test_data_with_missing() -\u003e Tensor2D\u003cCpuBackend\u003e {\n        // [[1, NaN], [3, 4], [5, 6]]\n        Tensor2D::new(vec![1.0f32, f32::NAN, 3.0, 4.0, 5.0, 6.0], 3, 2)\n    }\n\n    #[test]\n    fn test_simple_imputer_mean() {\n        let data = create_test_data_with_missing();\n        let imputer = SimpleImputer::\u003cCpuBackend\u003e::new(ImputeStrategy::Mean);\n        let fitted = imputer.fit(\u0026data).unwrap();\n\n        let stats = fitted.statistics().to_vec();\n        // Column 0: mean of [1, 3, 5] = 3\n        // Column 1: mean of [4, 6] = 5 (NaN excluded)\n        assert!((stats[0] - 3.0).abs() \u003c 1e-6);\n        assert!((stats[1] - 5.0).abs() \u003c 1e-6);\n\n        let imputed = fitted.transform(\u0026data).unwrap();\n        let values = imputed.ravel().to_vec();\n\n        // Column 0 unchanged: [1, 3, 5]\n        assert!((values[0] - 1.0).abs() \u003c 1e-6);\n        assert!((values[2] - 3.0).abs() \u003c 1e-6);\n        assert!((values[4] - 5.0).abs() \u003c 1e-6);\n\n        // Column 1: [5, 4, 6] (NaN replaced with 5)\n        assert!((values[1] - 5.0).abs() \u003c 1e-6);\n        assert!((values[3] - 4.0).abs() \u003c 1e-6);\n        assert!((values[5] - 6.0).abs() \u003c 1e-6);\n    }\n\n    #[test]\n    fn test_simple_imputer_median() {\n        let data = create_test_data_with_missing();\n        let imputer = SimpleImputer::\u003cCpuBackend\u003e::new(ImputeStrategy::Median);\n        let fitted = imputer.fit(\u0026data).unwrap();\n\n        let stats = fitted.statistics().to_vec();\n        // Column 0: median of [1, 3, 5] = 3\n        // Column 1: median of [4, 6] = 5\n        assert!((stats[0] - 3.0).abs() \u003c 1e-6);\n        assert!((stats[1] - 5.0).abs() \u003c 1e-6);\n    }\n\n    #[test]\n    fn test_simple_imputer_constant() {\n        let data = create_test_data_with_missing();\n        let imputer = SimpleImputer::\u003cCpuBackend\u003e::new(ImputeStrategy::Constant(-1.0));\n        let fitted = imputer.fit(\u0026data).unwrap();\n\n        let stats = fitted.statistics().to_vec();\n        assert!((stats[0] - (-1.0)).abs() \u003c 1e-6);\n        assert!((stats[1] - (-1.0)).abs() \u003c 1e-6);\n\n        let imputed = fitted.transform(\u0026data).unwrap();\n        let values = imputed.ravel().to_vec();\n\n        // NaN replaced with -1\n        assert!((values[1] - (-1.0)).abs() \u003c 1e-6);\n    }\n\n    #[test]\n    fn test_simple_imputer_inverse_not_supported() {\n        let data = create_test_data_with_missing();\n        let imputer = SimpleImputer::\u003cCpuBackend\u003e::new(ImputeStrategy::Mean);\n        let fitted = imputer.fit(\u0026data).unwrap();\n\n        let imputed = fitted.transform(\u0026data).unwrap();\n        let result = fitted.inverse_transform(\u0026imputed);\n\n        assert!(matches!(\n            result,\n            Err(PreprocessingError::InvalidParameter(_))\n        ));\n    }\n\n    #[test]\n    fn test_simple_imputer_serialization() {\n        let data = create_test_data_with_missing();\n        let imputer = SimpleImputer::\u003cCpuBackend\u003e::new(ImputeStrategy::Mean);\n        let fitted = imputer.fit(\u0026data).unwrap();\n\n        let params = fitted.extract_params();\n        let restored = FittedSimpleImputer::\u003cCpuBackend\u003e::from_params(params).unwrap();\n\n        let imputed1 = fitted.transform(\u0026data).unwrap();\n        let imputed2 = restored.transform(\u0026data).unwrap();\n\n        let i1 = imputed1.ravel().to_vec();\n        let i2 = imputed2.ravel().to_vec();\n\n        for (a, b) in i1.iter().zip(i2.iter()) {\n            assert!((a - b).abs() \u003c 1e-6);\n        }\n    }\n\n    #[test]\n    fn test_simple_imputer_feature_mismatch() {\n        let data = create_test_data_with_missing(); // 2 features\n        let imputer = SimpleImputer::\u003cCpuBackend\u003e::new(ImputeStrategy::Mean);\n        let fitted = imputer.fit(\u0026data).unwrap();\n\n        let wrong_data = Tensor2D::\u003cCpuBackend\u003e::new(vec![1.0f32, 2.0, 3.0], 1, 3); // 3 features\n        let result = fitted.transform(\u0026wrong_data);\n\n        assert!(matches!(\n            result,\n            Err(PreprocessingError::FeatureMismatch {\n                expected_features: 2,\n                got_features: 3\n            })\n        ));\n    }\n}\n","traces":[{"line":59,"address":[],"length":0,"stats":{"Line":0}},{"line":60,"address":[],"length":0,"stats":{"Line":0}},{"line":66,"address":[2411888],"length":1,"stats":{"Line":1}},{"line":75,"address":[2471392,2471947,2469984],"length":1,"stats":{"Line":3}},{"line":81,"address":[2470079],"length":1,"stats":{"Line":3}},{"line":83,"address":[2470118,2470213],"length":1,"stats":{"Line":6}},{"line":86,"address":[2408144,2408166],"length":1,"stats":{"Line":9}},{"line":87,"address":[2408333,2408320],"length":1,"stats":{"Line":9}},{"line":90,"address":[2470538,2470660,2470954,2470599,2471916],"length":1,"stats":{"Line":11}},{"line":91,"address":[2470648],"length":1,"stats":{"Line":0}},{"line":93,"address":[2470613],"length":1,"stats":{"Line":3}},{"line":95,"address":[2470807,2470675],"length":1,"stats":{"Line":4}},{"line":98,"address":[2470698],"length":1,"stats":{"Line":1}},{"line":99,"address":[2471003,2471082],"length":1,"stats":{"Line":4}},{"line":100,"address":[2471097],"length":1,"stats":{"Line":1}},{"line":101,"address":[2471127,2471387],"length":1,"stats":{"Line":2}},{"line":102,"address":[2471228,2471150],"length":1,"stats":{"Line":2}},{"line":104,"address":[2471157,2471148],"length":1,"stats":{"Line":2}},{"line":109,"address":[2470739],"length":1,"stats":{"Line":0}},{"line":110,"address":[2471864,2471482,2471398],"length":1,"stats":{"Line":0}},{"line":111,"address":[2471869,2471806,2471587],"length":1,"stats":{"Line":0}},{"line":113,"address":[2471763,2471602],"length":1,"stats":{"Line":0}},{"line":114,"address":[2471674],"length":1,"stats":{"Line":0}},{"line":115,"address":[2471697],"length":1,"stats":{"Line":0}},{"line":116,"address":[2471712],"length":1,"stats":{"Line":0}},{"line":117,"address":[2471744],"length":1,"stats":{"Line":0}},{"line":119,"address":[2470757],"length":1,"stats":{"Line":1}},{"line":124,"address":[2470441],"length":1,"stats":{"Line":2}},{"line":133,"address":[2404864,2405768,2405774],"length":1,"stats":{"Line":1}},{"line":134,"address":[2404915],"length":1,"stats":{"Line":1}},{"line":136,"address":[2404952],"length":1,"stats":{"Line":2}},{"line":137,"address":[2404993],"length":1,"stats":{"Line":0}},{"line":138,"address":[2404962],"length":1,"stats":{"Line":0}},{"line":142,"address":[2405159,2405113],"length":1,"stats":{"Line":3}},{"line":143,"address":[2405254],"length":1,"stats":{"Line":3}},{"line":146,"address":[2405792,2405802,2405392,2405325],"length":1,"stats":{"Line":10}},{"line":150,"address":[2405635],"length":1,"stats":{"Line":3}},{"line":151,"address":[2405521],"length":1,"stats":{"Line":3}},{"line":152,"address":[2405603],"length":1,"stats":{"Line":2}},{"line":153,"address":[],"length":0,"stats":{"Line":0}},{"line":154,"address":[],"length":0,"stats":{"Line":0}},{"line":158,"address":[],"length":0,"stats":{"Line":0}},{"line":159,"address":[],"length":0,"stats":{"Line":0}},{"line":160,"address":[],"length":0,"stats":{"Line":0}},{"line":175,"address":[2411904],"length":1,"stats":{"Line":1}},{"line":176,"address":[2411912],"length":1,"stats":{"Line":1}},{"line":185,"address":[2406528,2408001,2407995],"length":1,"stats":{"Line":2}},{"line":186,"address":[2406594],"length":1,"stats":{"Line":1}},{"line":188,"address":[2406639],"length":1,"stats":{"Line":3}},{"line":189,"address":[2406725],"length":1,"stats":{"Line":1}},{"line":190,"address":[2406721],"length":1,"stats":{"Line":1}},{"line":191,"address":[],"length":0,"stats":{"Line":0}},{"line":195,"address":[2406790,2406669],"length":1,"stats":{"Line":1}},{"line":196,"address":[2406885],"length":1,"stats":{"Line":2}},{"line":199,"address":[2406920],"length":1,"stats":{"Line":1}},{"line":200,"address":[2407118,2407034],"length":1,"stats":{"Line":3}},{"line":201,"address":[2407604,2407247],"length":1,"stats":{"Line":3}},{"line":202,"address":[2407730],"length":1,"stats":{"Line":1}},{"line":203,"address":[2407869],"length":1,"stats":{"Line":1}},{"line":207,"address":[2407432],"length":1,"stats":{"Line":1}},{"line":208,"address":[2408026,2407289,2408016],"length":1,"stats":{"Line":5}},{"line":209,"address":[],"length":0,"stats":{"Line":0}},{"line":213,"address":[2406400],"length":1,"stats":{"Line":1}},{"line":214,"address":[2406450],"length":1,"stats":{"Line":1}},{"line":215,"address":[],"length":0,"stats":{"Line":0}},{"line":216,"address":[2406423],"length":1,"stats":{"Line":1}},{"line":220,"address":[2406256],"length":1,"stats":{"Line":1}},{"line":222,"address":[2406286],"length":1,"stats":{"Line":1}},{"line":223,"address":[2406306],"length":1,"stats":{"Line":1}},{"line":224,"address":[2406345],"length":1,"stats":{"Line":1}},{"line":228,"address":[2406184,2405824],"length":1,"stats":{"Line":1}},{"line":230,"address":[2405922,2405854,2406218,2406208],"length":1,"stats":{"Line":4}},{"line":234,"address":[2406063],"length":1,"stats":{"Line":1}},{"line":235,"address":[2406051],"length":1,"stats":{"Line":1}},{"line":236,"address":[],"length":0,"stats":{"Line":0}},{"line":237,"address":[2406059],"length":1,"stats":{"Line":1}},{"line":238,"address":[],"length":0,"stats":{"Line":0}},{"line":242,"address":[2406240],"length":1,"stats":{"Line":0}},{"line":243,"address":[2406245],"length":1,"stats":{"Line":0}},{"line":246,"address":[],"length":0,"stats":{"Line":0}},{"line":247,"address":[],"length":0,"stats":{"Line":0}},{"line":248,"address":[],"length":0,"stats":{"Line":0}},{"line":249,"address":[],"length":0,"stats":{"Line":0}},{"line":256,"address":[],"length":0,"stats":{"Line":0}},{"line":257,"address":[],"length":0,"stats":{"Line":0}},{"line":258,"address":[],"length":0,"stats":{"Line":0}},{"line":259,"address":[],"length":0,"stats":{"Line":0}}],"covered":54,"coverable":87},{"path":["/","home","victor","ML","machinelearne-rs","lib","src","preprocessing","mod.rs"],"content":"//! Data preprocessing transformers for machine learning pipelines.\n//!\n//! This module provides a comprehensive set of data preprocessing transformers\n//! following the same type-state pattern as models in this library.\n//!\n//! # Design Philosophy\n//!\n//! - **Type Safety**: Transformers use phantom types to distinguish fitted/unfitted states\n//! - **Backend Agnostic**: All transformers work with any `Backend` implementation\n//! - **Serializable**: Fitted transformers can be saved and loaded\n//! - **sklearn-compatible**: API familiar to users of scikit-learn\n//!\n//! # Core Traits\n//!\n//! - [`Transformer`]: Unfitted transformer with hyperparameters\n//! - [`FittedTransformer`]: Fitted transformer ready for inference\n//!\n//! # Available Transformers\n//!\n//! ## Scaling\n//! - [`StandardScaler`]: Z-score normalization\n//! - [`MinMaxScaler`]: Scale to [0, 1] or custom range\n//! - [`RobustScaler`]: Use median and IQR (robust to outliers)\n//! - [`MaxAbsScaler`]: Scale by maximum absolute value\n//! - [`Normalizer`]: Scale individual samples to unit norm\n//!\n//! ## Imputation\n//! - [`SimpleImputer`]: Fill missing values with mean, median, most_frequent, or constant\n//!\n//! ## Encoding\n//! - [`OneHotEncoder`]: One-hot (dummy) encoding for categorical features\n//! - [`OrdinalEncoder`]: Ordinal (integer) encoding for categorical features\n//! - [`LabelEncoder`]: Label encoding for 1D classification targets\n//!\n//! ## Feature Engineering\n//! - [`PolynomialFeatures`]: Generate polynomial and interaction features\n//!\n//! ## Pipeline\n//! - [`Pipeline`]: Chain multiple transformers together\n//!\n//! ## Column Transformer\n//! - [`ColumnTransformer`]: Apply different transformers to different columns\n//!\n//! # Example\n//!\n//! ```ignore\n//! use machinelearne_rs::preprocessing::{Transformer, StandardScaler};\n//! use machinelearne_rs::backend::CpuBackend;\n//!\n//! // Create and fit a scaler\n//! let scaler = StandardScaler::\u003cCpuBackend\u003e::new()\n//!     .with_mean(true)\n//!     .with_std(true);\n//!\n//! let fitted = scaler.fit(\u0026training_data)?;\n//!\n//! // Transform training data\n//! let scaled_train = fitted.transform(\u0026training_data)?;\n//!\n//! // Save for later use\n//! fitted.save_to_file(\"scaler.bin\")?;\n//!\n//! // Later, load and transform new data\n//! let loaded = FittedStandardScaler::load_from_file(\"scaler.bin\")?;\n//! let scaled_test = loaded.transform(\u0026test_data)?;\n//! ```\n\npub mod column_transformer;\npub mod encoding;\npub mod error;\npub mod feature_engineering;\npub mod imputation;\npub mod pipeline;\npub mod predictive_pipeline;\npub mod scaling;\npub mod traits;\n\n// Re-export main types\npub use column_transformer::{\n    ColumnSpec, ColumnTransformer, ColumnTransformerParams, FittedColumnTransformer,\n};\npub use encoding::{\n    FittedLabelEncoder, FittedOneHotEncoder, FittedOrdinalEncoder, HandleUnknown, LabelEncoder,\n    LabelEncoderParams, OneHotEncoder, OneHotEncoderParams, OneHotOutput, OrdinalEncoder,\n    OrdinalEncoderParams,\n};\npub use error::PreprocessingError;\npub use feature_engineering::{\n    FittedPolynomialFeatures, PolynomialFeatures, PolynomialFeaturesParams,\n};\npub use imputation::{FittedSimpleImputer, ImputeStrategy, SimpleImputer, SimpleImputerParams};\npub use pipeline::{FittedPipeline, Pipeline, PipelineParams, PipelineStep, PipelineStepEnum};\npub use predictive_pipeline::{PredictivePipeline, PredictivePipelineParams};\npub use scaling::{\n    FittedMaxAbsScaler, FittedMinMaxScaler, FittedNormalizer, FittedRobustScaler,\n    FittedStandardScaler, MaxAbsScaler, MaxAbsScalerParams, MinMaxScaler, MinMaxScalerConfig,\n    MinMaxScalerParams, NormType, Normalizer, NormalizerParams, RobustScaler, RobustScalerConfig,\n    RobustScalerParams, StandardScaler, StandardScalerConfig, StandardScalerParams,\n};\npub use traits::{FittedTransformer, Transformer};\n","traces":[],"covered":0,"coverable":0},{"path":["/","home","victor","ML","machinelearne-rs","lib","src","preprocessing","pipeline","mod.rs"],"content":"//! Pipeline utilities for chaining transformers.\n//!\n//! This module provides tools for combining multiple transformers into\n//! a single pipeline that can be fitted and used for inference.\n//!\n//! # Available Components\n//!\n//! | Component | Description |\n//! |-----------|-------------|\n//! | [`Pipeline`] | Chain transformers sequentially |\n//!\n//! # Example\n//!\n//! ```ignore\n//! use machinelearne_rs::preprocessing::pipeline::Pipeline;\n//! use machinelearne_rs::preprocessing::{StandardScaler, MinMaxScaler, Transformer};\n//! use machinelearne_rs::backend::CpuBackend;\n//!\n//! let pipeline = Pipeline::\u003cCpuBackend\u003e::new()\n//!     .add_standard_scaler(StandardScaler::new())\n//!     .add_minmax_scaler(MinMaxScaler::new());\n//!\n//! let fitted = pipeline.fit(\u0026data)?;\n//! let transformed = fitted.transform(\u0026new_data)?;\n//! ```\n\n#[allow(clippy::module_inception)]\npub mod pipeline;\n\npub use pipeline::{FittedPipeline, Pipeline, PipelineParams, PipelineStep, PipelineStepEnum};\n","traces":[],"covered":0,"coverable":0},{"path":["/","home","victor","ML","machinelearne-rs","lib","src","preprocessing","pipeline","pipeline.rs"],"content":"//! Pipeline for chaining transformers.\n//!\n//! A Pipeline allows chaining multiple transformers together, where the output\n//! of one transformer becomes the input to the next.\n//!\n//! # Example\n//! ```ignore\n//! use machinelearne_rs::preprocessing::{\n//!     Pipeline, StandardScaler, MinMaxScaler, Transformer\n//! };\n//! use machinelearne_rs::backend::CpuBackend;\n//!\n//! let pipeline = Pipeline::\u003cCpuBackend\u003e::new()\n//!     .add(StandardScaler::new())\n//!     .add(MinMaxScaler::new().with_range(0.0, 1.0));\n//!\n//! let fitted = pipeline.fit(\u0026data)?;\n//! let transformed = fitted.transform(\u0026data)?;\n//! ```\n\nuse crate::backend::{Backend, Tensor2D};\nuse crate::preprocessing::encoding::{\n    FittedOneHotEncoder, FittedOrdinalEncoder, OneHotEncoder, OneHotEncoderParams, OrdinalEncoder,\n    OrdinalEncoderParams,\n};\nuse crate::preprocessing::error::PreprocessingError;\nuse crate::preprocessing::imputation::{FittedSimpleImputer, SimpleImputer, SimpleImputerParams};\nuse crate::preprocessing::scaling::{\n    FittedMaxAbsScaler, FittedMinMaxScaler, FittedNormalizer, FittedRobustScaler,\n    FittedStandardScaler, MaxAbsScaler, MaxAbsScalerParams, MinMaxScaler, MinMaxScalerParams,\n    Normalizer, NormalizerParams, RobustScaler, RobustScalerParams, StandardScaler,\n    StandardScalerParams,\n};\nuse crate::preprocessing::traits::{FittedTransformer, Transformer};\nuse serde::{Deserialize, Serialize};\nuse std::marker::PhantomData;\n\n/// A trait for fitted transformers that can be part of a pipeline.\n///\n/// This trait is automatically implemented for any fitted transformer\n/// that works with Tensor2D input/output.\npub trait PipelineStep\u003cB: Backend\u003e: Clone {\n    /// Transform the data.\n    fn transform_step(\u0026self, data: \u0026Tensor2D\u003cB\u003e) -\u003e Result\u003cTensor2D\u003cB\u003e, PreprocessingError\u003e;\n    /// Inverse transform the data (if supported).\n    fn inverse_transform_step(\u0026self, data: \u0026Tensor2D\u003cB\u003e)\n        -\u003e Result\u003cTensor2D\u003cB\u003e, PreprocessingError\u003e;\n    /// Get the step name for debugging.\n    fn step_name(\u0026self) -\u003e \u0026'static str;\n}\n\n/// Serializable representation of a fitted pipeline.\n#[derive(Clone, Serialize, Deserialize)]\npub struct PipelineParams {\n    /// Number of steps in the pipeline.\n    pub n_steps: usize,\n    /// Number of features seen during fit.\n    pub n_features: usize,\n}\n\n/// A step in the pipeline that can be serialized.\n#[derive(Clone)]\npub enum PipelineStepEnum\u003cB: Backend\u003e {\n    /// StandardScaler step.\n    StandardScaler(FittedStandardScaler\u003cB\u003e),\n    /// MinMaxScaler step.\n    MinMaxScaler(FittedMinMaxScaler\u003cB\u003e),\n    /// RobustScaler step.\n    RobustScaler(FittedRobustScaler\u003cB\u003e),\n    /// MaxAbsScaler step.\n    MaxAbsScaler(FittedMaxAbsScaler\u003cB\u003e),\n    /// Normalizer step.\n    Normalizer(FittedNormalizer\u003cB\u003e),\n    /// SimpleImputer step.\n    SimpleImputer(FittedSimpleImputer\u003cB\u003e),\n    /// OneHotEncoder step.\n    OneHotEncoder(FittedOneHotEncoder\u003cB\u003e),\n    /// OrdinalEncoder step.\n    OrdinalEncoder(FittedOrdinalEncoder\u003cB\u003e),\n}\n\nimpl\u003cB: Backend\u003e PipelineStep\u003cB\u003e for PipelineStepEnum\u003cB\u003e {\n    fn transform_step(\u0026self, data: \u0026Tensor2D\u003cB\u003e) -\u003e Result\u003cTensor2D\u003cB\u003e, PreprocessingError\u003e {\n        match self {\n            PipelineStepEnum::StandardScaler(t) =\u003e t.transform(data),\n            PipelineStepEnum::MinMaxScaler(t) =\u003e t.transform(data),\n            PipelineStepEnum::RobustScaler(t) =\u003e t.transform(data),\n            PipelineStepEnum::MaxAbsScaler(t) =\u003e t.transform(data),\n            PipelineStepEnum::Normalizer(t) =\u003e t.transform(data),\n            PipelineStepEnum::SimpleImputer(t) =\u003e t.transform(data),\n            PipelineStepEnum::OneHotEncoder(t) =\u003e t.transform(data),\n            PipelineStepEnum::OrdinalEncoder(t) =\u003e t.transform(data),\n        }\n    }\n\n    fn inverse_transform_step(\n        \u0026self,\n        data: \u0026Tensor2D\u003cB\u003e,\n    ) -\u003e Result\u003cTensor2D\u003cB\u003e, PreprocessingError\u003e {\n        match self {\n            PipelineStepEnum::StandardScaler(t) =\u003e t.inverse_transform(data),\n            PipelineStepEnum::MinMaxScaler(t) =\u003e t.inverse_transform(data),\n            PipelineStepEnum::RobustScaler(t) =\u003e t.inverse_transform(data),\n            PipelineStepEnum::MaxAbsScaler(t) =\u003e t.inverse_transform(data),\n            PipelineStepEnum::Normalizer(t) =\u003e t.inverse_transform(data),\n            PipelineStepEnum::SimpleImputer(t) =\u003e t.inverse_transform(data),\n            PipelineStepEnum::OneHotEncoder(t) =\u003e t.inverse_transform(data),\n            PipelineStepEnum::OrdinalEncoder(t) =\u003e t.inverse_transform(data),\n        }\n    }\n\n    fn step_name(\u0026self) -\u003e \u0026'static str {\n        match self {\n            PipelineStepEnum::StandardScaler(_) =\u003e \"StandardScaler\",\n            PipelineStepEnum::MinMaxScaler(_) =\u003e \"MinMaxScaler\",\n            PipelineStepEnum::RobustScaler(_) =\u003e \"RobustScaler\",\n            PipelineStepEnum::MaxAbsScaler(_) =\u003e \"MaxAbsScaler\",\n            PipelineStepEnum::Normalizer(_) =\u003e \"Normalizer\",\n            PipelineStepEnum::SimpleImputer(_) =\u003e \"SimpleImputer\",\n            PipelineStepEnum::OneHotEncoder(_) =\u003e \"OneHotEncoder\",\n            PipelineStepEnum::OrdinalEncoder(_) =\u003e \"OrdinalEncoder\",\n        }\n    }\n}\n\nimpl\u003cB: Backend\u003e PipelineStepEnum\u003cB\u003e {\n    /// Get the number of input features for this step.\n    pub fn n_features_in(\u0026self) -\u003e usize {\n        use crate::preprocessing::traits::FittedTransformer;\n        match self {\n            PipelineStepEnum::StandardScaler(t) =\u003e t.n_features_in(),\n            PipelineStepEnum::MinMaxScaler(t) =\u003e t.n_features_in(),\n            PipelineStepEnum::RobustScaler(t) =\u003e t.n_features_in(),\n            PipelineStepEnum::MaxAbsScaler(t) =\u003e t.n_features_in(),\n            PipelineStepEnum::Normalizer(t) =\u003e t.n_features_in(),\n            PipelineStepEnum::SimpleImputer(t) =\u003e t.n_features_in(),\n            PipelineStepEnum::OneHotEncoder(t) =\u003e t.n_features_in(),\n            PipelineStepEnum::OrdinalEncoder(t) =\u003e t.n_features_in(),\n        }\n    }\n}\n\n/// Builder for a fitted step (used during pipeline construction).\ntrait FittedStepBuilder\u003cB: Backend\u003e: Clone {\n    type Fitted: PipelineStep\u003cB\u003e;\n    fn fit(\u0026self, data: \u0026Tensor2D\u003cB\u003e) -\u003e Result\u003cSelf::Fitted, PreprocessingError\u003e;\n}\n\n/// A step in the unfitted pipeline.\n#[derive(Clone)]\npub enum UnfittedStepEnum\u003cB: Backend\u003e {\n    StandardScaler(StandardScaler\u003cB\u003e),\n    MinMaxScaler(MinMaxScaler\u003cB\u003e),\n    RobustScaler(RobustScaler\u003cB\u003e),\n    MaxAbsScaler(MaxAbsScaler\u003cB\u003e),\n    Normalizer(Normalizer\u003cB\u003e),\n    SimpleImputer(SimpleImputer\u003cB\u003e),\n    OneHotEncoder(OneHotEncoder\u003cB\u003e),\n    OrdinalEncoder(OrdinalEncoder\u003cB\u003e),\n}\n\nimpl\u003cB: Backend\u003e FittedStepBuilder\u003cB\u003e for UnfittedStepEnum\u003cB\u003e {\n    type Fitted = PipelineStepEnum\u003cB\u003e;\n\n    fn fit(\u0026self, data: \u0026Tensor2D\u003cB\u003e) -\u003e Result\u003cSelf::Fitted, PreprocessingError\u003e {\n        match self {\n            UnfittedStepEnum::StandardScaler(t) =\u003e {\n                t.fit(data).map(PipelineStepEnum::StandardScaler)\n            }\n            UnfittedStepEnum::MinMaxScaler(t) =\u003e t.fit(data).map(PipelineStepEnum::MinMaxScaler),\n            UnfittedStepEnum::RobustScaler(t) =\u003e t.fit(data).map(PipelineStepEnum::RobustScaler),\n            UnfittedStepEnum::MaxAbsScaler(t) =\u003e t.fit(data).map(PipelineStepEnum::MaxAbsScaler),\n            UnfittedStepEnum::Normalizer(t) =\u003e t.fit(data).map(PipelineStepEnum::Normalizer),\n            UnfittedStepEnum::SimpleImputer(t) =\u003e t.fit(data).map(PipelineStepEnum::SimpleImputer),\n            UnfittedStepEnum::OneHotEncoder(t) =\u003e t.fit(data).map(PipelineStepEnum::OneHotEncoder),\n            UnfittedStepEnum::OrdinalEncoder(t) =\u003e {\n                t.fit(data).map(PipelineStepEnum::OrdinalEncoder)\n            }\n        }\n    }\n}\n\n/// Pipeline transformer (unfitted).\n///\n/// Chains multiple transformers together.\n#[derive(Clone)]\npub struct Pipeline\u003cB: Backend\u003e {\n    steps: Vec\u003cUnfittedStepEnum\u003cB\u003e\u003e,\n    _backend: PhantomData\u003cB\u003e,\n}\n\nimpl\u003cB: Backend\u003e Default for Pipeline\u003cB\u003e {\n    fn default() -\u003e Self {\n        Self::new()\n    }\n}\n\nimpl\u003cB: Backend\u003e Pipeline\u003cB\u003e {\n    /// Create a new empty pipeline.\n    pub fn new() -\u003e Self {\n        Self {\n            steps: Vec::new(),\n            _backend: PhantomData,\n        }\n    }\n\n    /// Add a StandardScaler to the pipeline.\n    pub fn add_standard_scaler(mut self, scaler: StandardScaler\u003cB\u003e) -\u003e Self {\n        self.steps.push(UnfittedStepEnum::StandardScaler(scaler));\n        self\n    }\n\n    /// Add a MinMaxScaler to the pipeline.\n    pub fn add_minmax_scaler(mut self, scaler: MinMaxScaler\u003cB\u003e) -\u003e Self {\n        self.steps.push(UnfittedStepEnum::MinMaxScaler(scaler));\n        self\n    }\n\n    /// Add a RobustScaler to the pipeline.\n    pub fn add_robust_scaler(mut self, scaler: RobustScaler\u003cB\u003e) -\u003e Self {\n        self.steps.push(UnfittedStepEnum::RobustScaler(scaler));\n        self\n    }\n\n    /// Add a MaxAbsScaler to the pipeline.\n    pub fn add_maxabs_scaler(mut self, scaler: MaxAbsScaler\u003cB\u003e) -\u003e Self {\n        self.steps.push(UnfittedStepEnum::MaxAbsScaler(scaler));\n        self\n    }\n\n    /// Add a Normalizer to the pipeline.\n    pub fn add_normalizer(mut self, normalizer: Normalizer\u003cB\u003e) -\u003e Self {\n        self.steps.push(UnfittedStepEnum::Normalizer(normalizer));\n        self\n    }\n\n    /// Add a SimpleImputer to the pipeline.\n    pub fn add_simple_imputer(mut self, imputer: SimpleImputer\u003cB\u003e) -\u003e Self {\n        self.steps.push(UnfittedStepEnum::SimpleImputer(imputer));\n        self\n    }\n\n    /// Add a OneHotEncoder to the pipeline.\n    pub fn add_one_hot_encoder(mut self, encoder: OneHotEncoder\u003cB\u003e) -\u003e Self {\n        self.steps.push(UnfittedStepEnum::OneHotEncoder(encoder));\n        self\n    }\n\n    /// Add an OrdinalEncoder to the pipeline.\n    pub fn add_ordinal_encoder(mut self, encoder: OrdinalEncoder\u003cB\u003e) -\u003e Self {\n        self.steps.push(UnfittedStepEnum::OrdinalEncoder(encoder));\n        self\n    }\n\n    /// Get the number of steps in the pipeline.\n    pub fn len(\u0026self) -\u003e usize {\n        self.steps.len()\n    }\n\n    /// Check if the pipeline is empty.\n    pub fn is_empty(\u0026self) -\u003e bool {\n        self.steps.is_empty()\n    }\n}\n\nimpl\u003cB: Backend\u003e Transformer\u003cB\u003e for Pipeline\u003cB\u003e {\n    type Input = Tensor2D\u003cB\u003e;\n    type Output = Tensor2D\u003cB\u003e;\n    type Params = PipelineParams;\n    type Fitted = FittedPipeline\u003cB\u003e;\n\n    fn fit(\u0026self, data: \u0026Self::Input) -\u003e Result\u003cSelf::Fitted, PreprocessingError\u003e {\n        if self.steps.is_empty() {\n            return Err(PreprocessingError::InvalidParameter(\n                \"Cannot fit an empty pipeline\".to_string(),\n            ));\n        }\n\n        let (rows, cols) = data.shape();\n        if rows == 0 {\n            return Err(PreprocessingError::EmptyData(\n                \"Cannot fit pipeline on empty data\".to_string(),\n            ));\n        }\n\n        let mut fitted_steps = Vec::with_capacity(self.steps.len());\n        let mut current_data = data.clone();\n\n        for step in \u0026self.steps {\n            let fitted = step.fit(\u0026current_data)?;\n            current_data = fitted.transform_step(\u0026current_data)?;\n            fitted_steps.push(fitted);\n        }\n\n        Ok(FittedPipeline {\n            steps: fitted_steps,\n            n_features: cols,\n            _backend: PhantomData,\n        })\n    }\n\n    fn fit_transform(\u0026self, data: \u0026Self::Input) -\u003e Result\u003cSelf::Output, PreprocessingError\u003e {\n        let fitted = self.fit(data)?;\n        fitted.transform(data)\n    }\n}\n\n/// Fitted Pipeline ready for inference.\n#[derive(Clone)]\npub struct FittedPipeline\u003cB: Backend\u003e {\n    steps: Vec\u003cPipelineStepEnum\u003cB\u003e\u003e,\n    n_features: usize,\n    _backend: PhantomData\u003cB\u003e,\n}\n\nimpl\u003cB: Backend\u003e FittedPipeline\u003cB\u003e {\n    /// Get the number of steps in the pipeline.\n    pub fn len(\u0026self) -\u003e usize {\n        self.steps.len()\n    }\n\n    /// Check if the pipeline is empty.\n    pub fn is_empty(\u0026self) -\u003e bool {\n        self.steps.is_empty()\n    }\n\n    /// Get the names of all steps in the pipeline.\n    pub fn step_names(\u0026self) -\u003e Vec\u003c\u0026'static str\u003e {\n        self.steps.iter().map(|s| s.step_name()).collect()\n    }\n\n    /// Get a reference to the pipeline steps.\n    pub fn steps(\u0026self) -\u003e \u0026[PipelineStepEnum\u003cB\u003e] {\n        \u0026self.steps\n    }\n\n    /// Create a FittedPipeline from steps (for deserialization).\n    pub fn from_steps(steps: Vec\u003cPipelineStepEnum\u003cB\u003e\u003e, n_features: usize) -\u003e Self {\n        Self {\n            steps,\n            n_features,\n            _backend: PhantomData,\n        }\n    }\n}\n\nimpl\u003cB: Backend\u003e FittedTransformer\u003cB\u003e for FittedPipeline\u003cB\u003e {\n    type Input = Tensor2D\u003cB\u003e;\n    type Output = Tensor2D\u003cB\u003e;\n    type Params = PipelineParams;\n\n    fn transform(\u0026self, data: \u0026Self::Input) -\u003e Result\u003cSelf::Output, PreprocessingError\u003e {\n        let (_, cols) = data.shape();\n\n        if cols != self.n_features {\n            return Err(PreprocessingError::FeatureMismatch {\n                expected_features: self.n_features,\n                got_features: cols,\n            });\n        }\n\n        let mut result = data.clone();\n        for step in \u0026self.steps {\n            result = step.transform_step(\u0026result)?;\n        }\n        Ok(result)\n    }\n\n    fn inverse_transform(\u0026self, data: \u0026Self::Output) -\u003e Result\u003cSelf::Input, PreprocessingError\u003e {\n        let (_, cols) = data.shape();\n\n        if cols != self.n_features {\n            return Err(PreprocessingError::FeatureMismatch {\n                expected_features: self.n_features,\n                got_features: cols,\n            });\n        }\n\n        let mut result = data.clone();\n        // Apply inverse transforms in reverse order\n        for step in self.steps.iter().rev() {\n            result = step.inverse_transform_step(\u0026result)?;\n        }\n        Ok(result)\n    }\n\n    fn extract_params(\u0026self) -\u003e Self::Params {\n        PipelineParams {\n            n_steps: self.steps.len(),\n            n_features: self.n_features,\n        }\n    }\n\n    fn from_params(_params: Self::Params) -\u003e Result\u003cSelf, PreprocessingError\u003e {\n        Err(PreprocessingError::InvalidParameter(\n            \"Pipeline does not support from_params - use save_to_file/load_from_file instead\"\n                .to_string(),\n        ))\n    }\n\n    fn n_features_in(\u0026self) -\u003e usize {\n        self.n_features\n    }\n\n    fn save_to_file\u003cP: AsRef\u003cstd::path::Path\u003e\u003e(\u0026self, path: P) -\u003e std::io::Result\u003c()\u003e {\n        // Save each step's params in sequence\n        let mut step_params = Vec::new();\n        for step in \u0026self.steps {\n            let (name, bytes) = match step {\n                PipelineStepEnum::StandardScaler(t) =\u003e (\n                    \"StandardScaler\",\n                    bincode::serialize(\u0026t.extract_params()).map_err(std::io::Error::other)?,\n                ),\n                PipelineStepEnum::MinMaxScaler(t) =\u003e (\n                    \"MinMaxScaler\",\n                    bincode::serialize(\u0026t.extract_params()).map_err(std::io::Error::other)?,\n                ),\n                PipelineStepEnum::RobustScaler(t) =\u003e (\n                    \"RobustScaler\",\n                    bincode::serialize(\u0026t.extract_params()).map_err(std::io::Error::other)?,\n                ),\n                PipelineStepEnum::MaxAbsScaler(t) =\u003e (\n                    \"MaxAbsScaler\",\n                    bincode::serialize(\u0026t.extract_params()).map_err(std::io::Error::other)?,\n                ),\n                PipelineStepEnum::Normalizer(t) =\u003e (\n                    \"Normalizer\",\n                    bincode::serialize(\u0026t.extract_params()).map_err(std::io::Error::other)?,\n                ),\n                PipelineStepEnum::SimpleImputer(t) =\u003e (\n                    \"SimpleImputer\",\n                    bincode::serialize(\u0026t.extract_params()).map_err(std::io::Error::other)?,\n                ),\n                PipelineStepEnum::OneHotEncoder(t) =\u003e (\n                    \"OneHotEncoder\",\n                    bincode::serialize(\u0026t.extract_params()).map_err(std::io::Error::other)?,\n                ),\n                PipelineStepEnum::OrdinalEncoder(t) =\u003e (\n                    \"OrdinalEncoder\",\n                    bincode::serialize(\u0026t.extract_params()).map_err(std::io::Error::other)?,\n                ),\n            };\n            step_params.push((name.to_string(), bytes));\n        }\n\n        let serialized =\n            bincode::serialize(\u0026(self.n_features, step_params)).map_err(std::io::Error::other)?;\n        std::fs::write(path, serialized)\n    }\n\n    fn load_from_file\u003cP: AsRef\u003cstd::path::Path\u003e\u003e(path: P) -\u003e Result\u003cSelf, PreprocessingError\u003e\n    where\n        Self: Sized,\n    {\n        let bytes = std::fs::read(path)?;\n        let (n_features, step_params): (usize, Vec\u003c(String, Vec\u003cu8\u003e)\u003e) =\n            bincode::deserialize(\u0026bytes)\n                .map_err(|e| PreprocessingError::SerializationError(e.to_string()))?;\n\n        let mut steps = Vec::new();\n        for (name, step_bytes) in step_params {\n            let step = match name.as_str() {\n                \"StandardScaler\" =\u003e {\n                    let params: StandardScalerParams = bincode::deserialize(\u0026step_bytes)\n                        .map_err(|e| PreprocessingError::SerializationError(e.to_string()))?;\n                    PipelineStepEnum::StandardScaler(FittedStandardScaler::from_params(params)?)\n                }\n                \"MinMaxScaler\" =\u003e {\n                    let params: MinMaxScalerParams = bincode::deserialize(\u0026step_bytes)\n                        .map_err(|e| PreprocessingError::SerializationError(e.to_string()))?;\n                    PipelineStepEnum::MinMaxScaler(FittedMinMaxScaler::from_params(params)?)\n                }\n                \"RobustScaler\" =\u003e {\n                    let params: RobustScalerParams = bincode::deserialize(\u0026step_bytes)\n                        .map_err(|e| PreprocessingError::SerializationError(e.to_string()))?;\n                    PipelineStepEnum::RobustScaler(FittedRobustScaler::from_params(params)?)\n                }\n                \"MaxAbsScaler\" =\u003e {\n                    let params: MaxAbsScalerParams = bincode::deserialize(\u0026step_bytes)\n                        .map_err(|e| PreprocessingError::SerializationError(e.to_string()))?;\n                    PipelineStepEnum::MaxAbsScaler(FittedMaxAbsScaler::from_params(params)?)\n                }\n                \"Normalizer\" =\u003e {\n                    let params: NormalizerParams = bincode::deserialize(\u0026step_bytes)\n                        .map_err(|e| PreprocessingError::SerializationError(e.to_string()))?;\n                    PipelineStepEnum::Normalizer(FittedNormalizer::from_params(params)?)\n                }\n                \"SimpleImputer\" =\u003e {\n                    let params: SimpleImputerParams = bincode::deserialize(\u0026step_bytes)\n                        .map_err(|e| PreprocessingError::SerializationError(e.to_string()))?;\n                    PipelineStepEnum::SimpleImputer(FittedSimpleImputer::from_params(params)?)\n                }\n                \"OneHotEncoder\" =\u003e {\n                    let params: OneHotEncoderParams = bincode::deserialize(\u0026step_bytes)\n                        .map_err(|e| PreprocessingError::SerializationError(e.to_string()))?;\n                    PipelineStepEnum::OneHotEncoder(FittedOneHotEncoder::from_params(params)?)\n                }\n                \"OrdinalEncoder\" =\u003e {\n                    let params: OrdinalEncoderParams = bincode::deserialize(\u0026step_bytes)\n                        .map_err(|e| PreprocessingError::SerializationError(e.to_string()))?;\n                    PipelineStepEnum::OrdinalEncoder(FittedOrdinalEncoder::from_params(params)?)\n                }\n                _ =\u003e {\n                    return Err(PreprocessingError::SerializationError(format!(\n                        \"Unknown step type: {}\",\n                        name\n                    )))\n                }\n            };\n            steps.push(step);\n        }\n\n        Ok(Self {\n            steps,\n            n_features,\n            _backend: PhantomData,\n        })\n    }\n}\n\n#[cfg(test)]\nmod tests {\n    use super::*;\n    use crate::backend::CpuBackend;\n    use crate::preprocessing::scaling::NormType;\n\n    fn create_test_data() -\u003e Tensor2D\u003cCpuBackend\u003e {\n        // [[0, 1], [0, 1], [1, 3]]\n        Tensor2D::new(vec![0.0f32, 1.0, 0.0, 1.0, 1.0, 3.0], 3, 2)\n    }\n\n    #[test]\n    fn test_pipeline_single_step() {\n        let data = create_test_data();\n        let pipeline = Pipeline::\u003cCpuBackend\u003e::new().add_standard_scaler(StandardScaler::new());\n\n        let fitted = pipeline.fit(\u0026data).unwrap();\n        let transformed = fitted.transform(\u0026data).unwrap();\n\n        // Should be standardized\n        let values = transformed.ravel().to_vec();\n        assert!(values.iter().all(|\u0026v| v.is_finite()));\n    }\n\n    #[test]\n    fn test_pipeline_multiple_steps() {\n        let data = create_test_data();\n        let pipeline = Pipeline::\u003cCpuBackend\u003e::new()\n            .add_standard_scaler(StandardScaler::new())\n            .add_minmax_scaler(MinMaxScaler::new().with_range(0.0, 1.0));\n\n        let fitted = pipeline.fit(\u0026data).unwrap();\n        let transformed = fitted.transform(\u0026data).unwrap();\n\n        // After StandardScaler + MinMaxScaler, values should be in [0, 1]\n        let values = transformed.ravel().to_vec();\n        assert!(values.iter().all(|\u0026v| v \u003e= -1e-6 \u0026\u0026 v \u003c= 1.0 + 1e-6));\n    }\n\n    #[test]\n    fn test_pipeline_inverse_transform() {\n        let data = create_test_data();\n        let pipeline = Pipeline::\u003cCpuBackend\u003e::new()\n            .add_standard_scaler(StandardScaler::new())\n            .add_minmax_scaler(MinMaxScaler::new().with_range(0.0, 1.0));\n\n        let fitted = pipeline.fit(\u0026data).unwrap();\n        let transformed = fitted.transform(\u0026data).unwrap();\n        let recovered = fitted.inverse_transform(\u0026transformed).unwrap();\n\n        let original = data.ravel().to_vec();\n        let recovered_vec = recovered.ravel().to_vec();\n\n        for (o, r) in original.iter().zip(recovered_vec.iter()) {\n            assert!((o - r).abs() \u003c 1e-5, \"Expected {}, got {}\", o, r);\n        }\n    }\n\n    #[test]\n    fn test_pipeline_with_normalizer() {\n        let data = create_test_data();\n        let pipeline = Pipeline::\u003cCpuBackend\u003e::new().add_normalizer(Normalizer::new(NormType::L2));\n\n        let fitted = pipeline.fit(\u0026data).unwrap();\n        let transformed = fitted.transform(\u0026data).unwrap();\n\n        // Each row should have L2 norm = 1\n        let values = transformed.ravel().to_vec();\n        for row in 0..3 {\n            let row_norm = (values[row * 2].powi(2) + values[row * 2 + 1].powi(2)).sqrt();\n            assert!(\n                (row_norm - 1.0).abs() \u003c 1e-5,\n                \"Row {} L2 norm: {}\",\n                row,\n                row_norm\n            );\n        }\n    }\n\n    #[test]\n    fn test_pipeline_serialization() {\n        let data = create_test_data();\n        let pipeline = Pipeline::\u003cCpuBackend\u003e::new()\n            .add_standard_scaler(StandardScaler::new())\n            .add_minmax_scaler(MinMaxScaler::new().with_range(0.0, 1.0));\n\n        let fitted = pipeline.fit(\u0026data).unwrap();\n\n        // Save to file\n        let temp_file = std::env::temp_dir().join(\"test_pipeline.bin\");\n        fitted.save_to_file(\u0026temp_file).unwrap();\n\n        // Load from file\n        let loaded = FittedPipeline::\u003cCpuBackend\u003e::load_from_file(\u0026temp_file).unwrap();\n\n        // Compare results\n        let transformed1 = fitted.transform(\u0026data).unwrap();\n        let transformed2 = loaded.transform(\u0026data).unwrap();\n\n        let t1 = transformed1.ravel().to_vec();\n        let t2 = transformed2.ravel().to_vec();\n\n        for (a, b) in t1.iter().zip(t2.iter()) {\n            assert!((a - b).abs() \u003c 1e-6);\n        }\n\n        // Clean up\n        std::fs::remove_file(temp_file).ok();\n    }\n\n    #[test]\n    fn test_pipeline_feature_mismatch() {\n        let data = create_test_data(); // 2 features\n        let pipeline = Pipeline::\u003cCpuBackend\u003e::new().add_standard_scaler(StandardScaler::new());\n\n        let fitted = pipeline.fit(\u0026data).unwrap();\n\n        let wrong_data = Tensor2D::\u003cCpuBackend\u003e::new(vec![1.0f32, 2.0, 3.0], 1, 3); // 3 features\n        let result = fitted.transform(\u0026wrong_data);\n\n        assert!(matches!(\n            result,\n            Err(PreprocessingError::FeatureMismatch {\n                expected_features: 2,\n                got_features: 3\n            })\n        ));\n    }\n\n    #[test]\n    fn test_pipeline_empty() {\n        let pipeline = Pipeline::\u003cCpuBackend\u003e::new();\n        let data = create_test_data();\n\n        let result = pipeline.fit(\u0026data);\n        assert!(matches!(\n            result,\n            Err(PreprocessingError::InvalidParameter(_))\n        ));\n    }\n\n    #[test]\n    fn test_pipeline_step_names() {\n        let pipeline = Pipeline::\u003cCpuBackend\u003e::new()\n            .add_standard_scaler(StandardScaler::new())\n            .add_minmax_scaler(MinMaxScaler::new());\n\n        let data = create_test_data();\n        let fitted = pipeline.fit(\u0026data).unwrap();\n\n        let names = fitted.step_names();\n        assert_eq!(names, vec![\"StandardScaler\", \"MinMaxScaler\"]);\n    }\n}\n","traces":[{"line":83,"address":[3188560],"length":1,"stats":{"Line":1}},{"line":84,"address":[3188593],"length":1,"stats":{"Line":1}},{"line":85,"address":[3188666],"length":1,"stats":{"Line":1}},{"line":86,"address":[3188699],"length":1,"stats":{"Line":2}},{"line":87,"address":[3188728],"length":1,"stats":{"Line":0}},{"line":88,"address":[3188761],"length":1,"stats":{"Line":0}},{"line":89,"address":[3188791],"length":1,"stats":{"Line":1}},{"line":90,"address":[3188821],"length":1,"stats":{"Line":0}},{"line":91,"address":[3188851],"length":1,"stats":{"Line":0}},{"line":92,"address":[3188881],"length":1,"stats":{"Line":0}},{"line":96,"address":[3188912],"length":1,"stats":{"Line":1}},{"line":100,"address":[3188945],"length":1,"stats":{"Line":1}},{"line":101,"address":[3189018],"length":1,"stats":{"Line":1}},{"line":102,"address":[3189051],"length":1,"stats":{"Line":1}},{"line":103,"address":[3189080],"length":1,"stats":{"Line":0}},{"line":104,"address":[3189113],"length":1,"stats":{"Line":0}},{"line":105,"address":[3189143],"length":1,"stats":{"Line":0}},{"line":106,"address":[3189173],"length":1,"stats":{"Line":0}},{"line":107,"address":[3189203],"length":1,"stats":{"Line":0}},{"line":108,"address":[3189233],"length":1,"stats":{"Line":0}},{"line":112,"address":[3189264],"length":1,"stats":{"Line":1}},{"line":113,"address":[3189269],"length":1,"stats":{"Line":1}},{"line":114,"address":[3189328],"length":1,"stats":{"Line":1}},{"line":115,"address":[3189354],"length":1,"stats":{"Line":1}},{"line":116,"address":[3189380],"length":1,"stats":{"Line":0}},{"line":117,"address":[3189403],"length":1,"stats":{"Line":0}},{"line":118,"address":[3189426],"length":1,"stats":{"Line":0}},{"line":119,"address":[3189449],"length":1,"stats":{"Line":0}},{"line":120,"address":[3189472],"length":1,"stats":{"Line":0}},{"line":121,"address":[3189495],"length":1,"stats":{"Line":0}},{"line":128,"address":[3188256],"length":1,"stats":{"Line":0}},{"line":130,"address":[3188270],"length":1,"stats":{"Line":0}},{"line":131,"address":[3188334],"length":1,"stats":{"Line":0}},{"line":132,"address":[3188363],"length":1,"stats":{"Line":0}},{"line":133,"address":[3188388],"length":1,"stats":{"Line":0}},{"line":134,"address":[3188417],"length":1,"stats":{"Line":0}},{"line":135,"address":[3188443],"length":1,"stats":{"Line":0}},{"line":136,"address":[3188469],"length":1,"stats":{"Line":0}},{"line":137,"address":[3188495],"length":1,"stats":{"Line":0}},{"line":138,"address":[3188521],"length":1,"stats":{"Line":0}},{"line":165,"address":[3189536],"length":1,"stats":{"Line":1}},{"line":166,"address":[3189579],"length":1,"stats":{"Line":1}},{"line":167,"address":[3189639],"length":1,"stats":{"Line":1}},{"line":168,"address":[3189647],"length":1,"stats":{"Line":1}},{"line":170,"address":[3189687],"length":1,"stats":{"Line":1}},{"line":171,"address":[3189735],"length":1,"stats":{"Line":0}},{"line":172,"address":[3189789],"length":1,"stats":{"Line":0}},{"line":173,"address":[3189843],"length":1,"stats":{"Line":1}},{"line":174,"address":[3189897],"length":1,"stats":{"Line":0}},{"line":175,"address":[3189948],"length":1,"stats":{"Line":0}},{"line":176,"address":[3189999],"length":1,"stats":{"Line":0}},{"line":177,"address":[3190007],"length":1,"stats":{"Line":0}},{"line":193,"address":[],"length":0,"stats":{"Line":0}},{"line":194,"address":[],"length":0,"stats":{"Line":0}},{"line":200,"address":[3187984],"length":1,"stats":{"Line":1}},{"line":202,"address":[3187997],"length":1,"stats":{"Line":1}},{"line":208,"address":[3187966,3187824],"length":1,"stats":{"Line":1}},{"line":209,"address":[3187866],"length":1,"stats":{"Line":1}},{"line":210,"address":[3187939],"length":1,"stats":{"Line":1}},{"line":214,"address":[3187648,3187793],"length":1,"stats":{"Line":1}},{"line":215,"address":[3187687],"length":1,"stats":{"Line":1}},{"line":216,"address":[3187766],"length":1,"stats":{"Line":1}},{"line":220,"address":[],"length":0,"stats":{"Line":0}},{"line":221,"address":[],"length":0,"stats":{"Line":0}},{"line":222,"address":[],"length":0,"stats":{"Line":0}},{"line":226,"address":[],"length":0,"stats":{"Line":0}},{"line":227,"address":[],"length":0,"stats":{"Line":0}},{"line":228,"address":[],"length":0,"stats":{"Line":0}},{"line":232,"address":[3187631,3187504],"length":1,"stats":{"Line":1}},{"line":233,"address":[3187535],"length":1,"stats":{"Line":1}},{"line":234,"address":[3187604],"length":1,"stats":{"Line":1}},{"line":238,"address":[],"length":0,"stats":{"Line":0}},{"line":239,"address":[],"length":0,"stats":{"Line":0}},{"line":240,"address":[],"length":0,"stats":{"Line":0}},{"line":244,"address":[],"length":0,"stats":{"Line":0}},{"line":245,"address":[],"length":0,"stats":{"Line":0}},{"line":246,"address":[],"length":0,"stats":{"Line":0}},{"line":250,"address":[],"length":0,"stats":{"Line":0}},{"line":251,"address":[],"length":0,"stats":{"Line":0}},{"line":252,"address":[],"length":0,"stats":{"Line":0}},{"line":256,"address":[],"length":0,"stats":{"Line":0}},{"line":257,"address":[],"length":0,"stats":{"Line":0}},{"line":261,"address":[],"length":0,"stats":{"Line":0}},{"line":262,"address":[],"length":0,"stats":{"Line":0}},{"line":272,"address":[3171550,3169840,3171584],"length":1,"stats":{"Line":1}},{"line":273,"address":[3169891],"length":1,"stats":{"Line":1}},{"line":274,"address":[3169986],"length":1,"stats":{"Line":1}},{"line":275,"address":[3169958],"length":1,"stats":{"Line":1}},{"line":279,"address":[3169921],"length":1,"stats":{"Line":1}},{"line":280,"address":[3169947],"length":1,"stats":{"Line":1}},{"line":281,"address":[3170088],"length":1,"stats":{"Line":0}},{"line":282,"address":[3170057],"length":1,"stats":{"Line":0}},{"line":286,"address":[3170179],"length":1,"stats":{"Line":1}},{"line":287,"address":[3170228],"length":1,"stats":{"Line":1}},{"line":289,"address":[3170294,3170358,3171489],"length":1,"stats":{"Line":3}},{"line":290,"address":[3171556,3170651,3170470],"length":1,"stats":{"Line":2}},{"line":291,"address":[3170997,3171246,3171061],"length":1,"stats":{"Line":2}},{"line":292,"address":[3171347],"length":1,"stats":{"Line":1}},{"line":295,"address":[3170530],"length":1,"stats":{"Line":1}},{"line":296,"address":[3170490],"length":1,"stats":{"Line":1}},{"line":297,"address":[],"length":0,"stats":{"Line":0}},{"line":298,"address":[],"length":0,"stats":{"Line":0}},{"line":302,"address":[],"length":0,"stats":{"Line":0}},{"line":303,"address":[],"length":0,"stats":{"Line":0}},{"line":304,"address":[],"length":0,"stats":{"Line":0}},{"line":318,"address":[],"length":0,"stats":{"Line":0}},{"line":319,"address":[],"length":0,"stats":{"Line":0}},{"line":323,"address":[],"length":0,"stats":{"Line":0}},{"line":324,"address":[],"length":0,"stats":{"Line":0}},{"line":328,"address":[3188096],"length":1,"stats":{"Line":1}},{"line":329,"address":[3188192,3188217,3188128],"length":1,"stats":{"Line":3}},{"line":333,"address":[3188240],"length":1,"stats":{"Line":0}},{"line":334,"address":[3188245],"length":1,"stats":{"Line":0}},{"line":338,"address":[3188048],"length":1,"stats":{"Line":0}},{"line":352,"address":[3187024,3186304,3187018],"length":1,"stats":{"Line":1}},{"line":353,"address":[3186355],"length":1,"stats":{"Line":1}},{"line":355,"address":[3186378],"length":1,"stats":{"Line":1}},{"line":356,"address":[3186440],"length":1,"stats":{"Line":1}},{"line":357,"address":[3186436],"length":1,"stats":{"Line":1}},{"line":358,"address":[],"length":0,"stats":{"Line":0}},{"line":362,"address":[3186394],"length":1,"stats":{"Line":1}},{"line":363,"address":[3186998,3186532,3186404],"length":1,"stats":{"Line":3}},{"line":364,"address":[3186639,3186743,3186925],"length":1,"stats":{"Line":2}},{"line":366,"address":[3186646],"length":1,"stats":{"Line":1}},{"line":369,"address":[3185456,3186284,3186278],"length":1,"stats":{"Line":1}},{"line":370,"address":[3185507],"length":1,"stats":{"Line":1}},{"line":372,"address":[3185530],"length":1,"stats":{"Line":1}},{"line":373,"address":[3185595],"length":1,"stats":{"Line":0}},{"line":374,"address":[3185591],"length":1,"stats":{"Line":0}},{"line":375,"address":[],"length":0,"stats":{"Line":0}},{"line":379,"address":[3185549],"length":1,"stats":{"Line":1}},{"line":381,"address":[3185559,3186255,3185693],"length":1,"stats":{"Line":3}},{"line":382,"address":[3186176,3185884,3185991],"length":1,"stats":{"Line":2}},{"line":384,"address":[3185891],"length":1,"stats":{"Line":1}},{"line":387,"address":[],"length":0,"stats":{"Line":0}},{"line":389,"address":[],"length":0,"stats":{"Line":0}},{"line":390,"address":[],"length":0,"stats":{"Line":0}},{"line":394,"address":[],"length":0,"stats":{"Line":0}},{"line":395,"address":[],"length":0,"stats":{"Line":0}},{"line":396,"address":[],"length":0,"stats":{"Line":0}},{"line":397,"address":[],"length":0,"stats":{"Line":0}},{"line":401,"address":[3175856],"length":1,"stats":{"Line":1}},{"line":402,"address":[3175861],"length":1,"stats":{"Line":1}},{"line":405,"address":[3172547,3175831,3171616],"length":1,"stats":{"Line":1}},{"line":407,"address":[3171655],"length":1,"stats":{"Line":1}},{"line":408,"address":[3175728,3171838,3171759],"length":1,"stats":{"Line":3}},{"line":409,"address":[3173181,3171939],"length":1,"stats":{"Line":2}},{"line":410,"address":[3173107,3172615],"length":1,"stats":{"Line":2}},{"line":411,"address":[],"length":0,"stats":{"Line":0}},{"line":412,"address":[3172966,3173280,3172903,3172635],"length":1,"stats":{"Line":3}},{"line":414,"address":[3172650,3173523],"length":1,"stats":{"Line":2}},{"line":415,"address":[],"length":0,"stats":{"Line":0}},{"line":416,"address":[3173605,3173385,3172666,3173322],"length":1,"stats":{"Line":3}},{"line":418,"address":[3172681,3173848],"length":1,"stats":{"Line":0}},{"line":419,"address":[],"length":0,"stats":{"Line":0}},{"line":420,"address":[3173930,3173647,3172701,3173710],"length":1,"stats":{"Line":0}},{"line":422,"address":[3172716,3174173],"length":1,"stats":{"Line":0}},{"line":423,"address":[],"length":0,"stats":{"Line":0}},{"line":424,"address":[3174255,3173972,3172736,3174035],"length":1,"stats":{"Line":0}},{"line":426,"address":[3172751,3174501],"length":1,"stats":{"Line":0}},{"line":427,"address":[],"length":0,"stats":{"Line":0}},{"line":428,"address":[3174586,3172763,3174290],"length":1,"stats":{"Line":0}},{"line":430,"address":[3174816,3172787],"length":1,"stats":{"Line":0}},{"line":431,"address":[],"length":0,"stats":{"Line":0}},{"line":432,"address":[3174898,3174678,3174615,3172807],"length":1,"stats":{"Line":0}},{"line":434,"address":[3175141,3172822],"length":1,"stats":{"Line":0}},{"line":435,"address":[],"length":0,"stats":{"Line":0}},{"line":436,"address":[3174940,3175003,3172842,3175223],"length":1,"stats":{"Line":0}},{"line":438,"address":[3175468,3172857],"length":1,"stats":{"Line":0}},{"line":439,"address":[],"length":0,"stats":{"Line":0}},{"line":440,"address":[3172877,3175265,3175328,3175759],"length":1,"stats":{"Line":0}},{"line":443,"address":[3175586,3173253],"length":1,"stats":{"Line":2}},{"line":446,"address":[3172004,3172558],"length":1,"stats":{"Line":1}},{"line":447,"address":[],"length":0,"stats":{"Line":0}},{"line":448,"address":[3172412],"length":1,"stats":{"Line":1}},{"line":451,"address":[3175872,3179014,3184144],"length":1,"stats":{"Line":1}},{"line":455,"address":[3175918],"length":1,"stats":{"Line":1}},{"line":456,"address":[3176464],"length":1,"stats":{"Line":1}},{"line":457,"address":[],"length":0,"stats":{"Line":0}},{"line":458,"address":[3184448,3184476,3176359,3176284],"length":1,"stats":{"Line":1}},{"line":460,"address":[3176536],"length":1,"stats":{"Line":1}},{"line":461,"address":[3176811,3176584,3176684],"length":1,"stats":{"Line":3}},{"line":462,"address":[3177240,3176920],"length":1,"stats":{"Line":2}},{"line":463,"address":[3177262],"length":1,"stats":{"Line":1}},{"line":464,"address":[3183248,3183294,3183394,3177343],"length":1,"stats":{"Line":3}},{"line":465,"address":[3185312,3183346,3185340,3183271],"length":1,"stats":{"Line":1}},{"line":466,"address":[3183555,3183707],"length":1,"stats":{"Line":2}},{"line":468,"address":[3177388,3177317],"length":1,"stats":{"Line":2}},{"line":469,"address":[3182484,3182584,3182438,3177436],"length":1,"stats":{"Line":3}},{"line":470,"address":[3185196,3185168,3182536,3182461],"length":1,"stats":{"Line":1}},{"line":471,"address":[3182993,3182809],"length":1,"stats":{"Line":2}},{"line":473,"address":[3177410,3177481],"length":1,"stats":{"Line":0}},{"line":474,"address":[3181800,3177529,3181654,3181700],"length":1,"stats":{"Line":0}},{"line":475,"address":[3181677,3181752,3184908,3184880],"length":1,"stats":{"Line":0}},{"line":476,"address":[3181993,3182161],"length":1,"stats":{"Line":0}},{"line":478,"address":[3177571,3177503],"length":1,"stats":{"Line":0}},{"line":479,"address":[3177616,3181067,3180921,3180967],"length":1,"stats":{"Line":0}},{"line":480,"address":[3184736,3181019,3184764,3180944],"length":1,"stats":{"Line":0}},{"line":481,"address":[3181228,3181380],"length":1,"stats":{"Line":0}},{"line":483,"address":[3177593,3177652],"length":1,"stats":{"Line":0}},{"line":484,"address":[3180655,3180515,3180469,3180898,3177697,3180615],"length":1,"stats":{"Line":0}},{"line":485,"address":[3180567,3180492,3185052,3185024],"length":1,"stats":{"Line":0}},{"line":486,"address":[3180648,3180680,3180893],"length":1,"stats":{"Line":0}},{"line":488,"address":[3177733,3177674],"length":1,"stats":{"Line":0}},{"line":489,"address":[3177778,3179875,3179829,3179952],"length":1,"stats":{"Line":0}},{"line":490,"address":[3184592,3184620,3179852,3179904],"length":1,"stats":{"Line":0}},{"line":491,"address":[3180217,3180081],"length":1,"stats":{"Line":0}},{"line":493,"address":[3177755,3177814],"length":1,"stats":{"Line":0}},{"line":494,"address":[3179197,3177859,3179051,3179097],"length":1,"stats":{"Line":0}},{"line":495,"address":[3179149,3184188,3184160,3179074],"length":1,"stats":{"Line":0}},{"line":496,"address":[3179558,3179390],"length":1,"stats":{"Line":0}},{"line":498,"address":[3177895,3177836],"length":1,"stats":{"Line":0}},{"line":499,"address":[3178188,3178234,3178334,3177924],"length":1,"stats":{"Line":0}},{"line":500,"address":[3178211,3184332,3178286,3184304],"length":1,"stats":{"Line":0}},{"line":501,"address":[3178647,3178495],"length":1,"stats":{"Line":0}},{"line":503,"address":[],"length":0,"stats":{"Line":0}},{"line":504,"address":[3177917,3177956],"length":1,"stats":{"Line":0}},{"line":505,"address":[],"length":0,"stats":{"Line":0}},{"line":506,"address":[],"length":0,"stats":{"Line":0}},{"line":510,"address":[3178854],"length":1,"stats":{"Line":1}},{"line":513,"address":[3177025],"length":1,"stats":{"Line":1}},{"line":514,"address":[3176977],"length":1,"stats":{"Line":1}},{"line":515,"address":[],"length":0,"stats":{"Line":0}},{"line":516,"address":[],"length":0,"stats":{"Line":0}}],"covered":93,"coverable":224},{"path":["/","home","victor","ML","machinelearne-rs","lib","src","preprocessing","predictive_pipeline.rs"],"content":"//! Predictive pipeline combining preprocessing and model inference.\n//!\n//! This module provides a unified wrapper for a complete ML pipeline that\n//! combines preprocessing transformers with a trained model.\n\nuse crate::backend::{Backend, Tensor1D, Tensor2D};\nuse crate::model::InferenceModel;\nuse crate::preprocessing::column_transformer::FittedColumnTransformer;\nuse crate::preprocessing::error::PreprocessingError;\nuse crate::preprocessing::feature_engineering::FittedPolynomialFeatures;\nuse crate::preprocessing::traits::FittedTransformer;\nuse crate::serialization::SerializableParams;\nuse serde::{Deserialize, Serialize};\nuse std::io;\nuse std::marker::PhantomData;\n\n/// Serializable parameters for the predictive pipeline.\n#[derive(Clone, Serialize, Deserialize)]\npub struct PredictivePipelineParams {\n    /// Preprocessor parameters.\n    pub preprocessor: Vec\u003cu8\u003e,\n    /// Polynomial features parameters (optional).\n    pub poly: Option\u003cVec\u003cu8\u003e\u003e,\n    /// Model parameters.\n    pub model: Vec\u003cu8\u003e,\n    /// Number of input features.\n    pub n_features_in: usize,\n}\n\n/// Predictive pipeline combining preprocessing and model inference.\n///\n/// This provides a unified interface for:\n/// 1. Preprocessing new data using a fitted ColumnTransformer\n/// 2. Optionally generating polynomial features\n/// 3. Making predictions with a trained model\n///\n/// The entire pipeline can be serialized and loaded for deployment.\npub struct PredictivePipeline\u003c\n    B: Backend,\n    M: InferenceModel\u003cB, InputBatch = Tensor2D\u003cB\u003e, OutputBatch = Tensor1D\u003cB\u003e\u003e,\n\u003e {\n    /// Fitted column transformer for preprocessing.\n    preprocessor: FittedColumnTransformer\u003cB\u003e,\n    /// Optional polynomial features transformer.\n    poly: Option\u003cFittedPolynomialFeatures\u003cB\u003e\u003e,\n    /// Trained model for inference.\n    model: M,\n    /// Number of input features expected.\n    n_features_in: usize,\n    _backend: PhantomData\u003cB\u003e,\n}\n\nimpl\u003cB: Backend, M: InferenceModel\u003cB, InputBatch = Tensor2D\u003cB\u003e, OutputBatch = Tensor1D\u003cB\u003e\u003e\u003e\n    PredictivePipeline\u003cB, M\u003e\n{\n    /// Create a new predictive pipeline.\n    pub fn new(\n        preprocessor: FittedColumnTransformer\u003cB\u003e,\n        poly: Option\u003cFittedPolynomialFeatures\u003cB\u003e\u003e,\n        model: M,\n    ) -\u003e Self {\n        let n_features_in = preprocessor.n_features_in();\n        Self {\n            preprocessor,\n            poly,\n            model,\n            n_features_in,\n            _backend: PhantomData,\n        }\n    }\n\n    /// Get the number of input features.\n    pub fn n_features_in(\u0026self) -\u003e usize {\n        self.n_features_in\n    }\n\n    /// Preprocess data (transform using preprocessor and polynomial features).\n    pub fn preprocess(\u0026self, data: \u0026Tensor2D\u003cB\u003e) -\u003e Result\u003cTensor2D\u003cB\u003e, PreprocessingError\u003e {\n        let mut processed = self.preprocessor.transform(data)?;\n\n        if let Some(ref poly) = self.poly {\n            processed = poly.transform(\u0026processed)?;\n        }\n\n        Ok(processed)\n    }\n\n    /// Make predictions on new data.\n    pub fn predict(\u0026self, data: \u0026Tensor2D\u003cB\u003e) -\u003e Result\u003cTensor1D\u003cB\u003e, PreprocessingError\u003e {\n        let (_, cols) = data.shape();\n\n        if cols != self.n_features_in {\n            return Err(PreprocessingError::FeatureMismatch {\n                expected_features: self.n_features_in,\n                got_features: cols,\n            });\n        }\n\n        let processed = self.preprocess(data)?;\n        let predictions = self.model.predict_batch(\u0026processed);\n\n        Ok(predictions)\n    }\n\n    /// Save the entire pipeline to a file.\n    pub fn save_to_file\u003cP: AsRef\u003cstd::path::Path\u003e\u003e(\u0026self, path: P) -\u003e io::Result\u003c()\u003e {\n        let params = self.extract_params();\n        let bytes = params.to_bytes().map_err(io::Error::other)?;\n        std::fs::write(path, bytes)\n    }\n\n    /// Load a pipeline from a file.\n    pub fn load_from_file\u003cP: AsRef\u003cstd::path::Path\u003e\u003e(\n        path: P,\n        model_loader: impl FnOnce(\u0026[u8]) -\u003e Result\u003cM, PreprocessingError\u003e,\n    ) -\u003e Result\u003cSelf, PreprocessingError\u003e {\n        let bytes = std::fs::read(path).map_err(|e| {\n            PreprocessingError::SerializationError(format!(\"Failed to read file: {}\", e))\n        })?;\n        let params = PredictivePipelineParams::from_bytes(\u0026bytes)\n            .map_err(|e| PreprocessingError::SerializationError(e.to_string()))?;\n        Self::from_params(params, model_loader)\n    }\n\n    /// Extract parameters for serialization.\n    pub fn extract_params(\u0026self) -\u003e PredictivePipelineParams {\n        let preprocessor_bytes = self.preprocessor.extract_params().to_bytes().unwrap();\n        let poly_bytes = self\n            .poly\n            .as_ref()\n            .map(|p| p.extract_params().to_bytes().unwrap());\n        let model_bytes = self.model.extract_params().to_bytes().unwrap();\n\n        PredictivePipelineParams {\n            preprocessor: preprocessor_bytes,\n            poly: poly_bytes,\n            model: model_bytes,\n            n_features_in: self.n_features_in,\n        }\n    }\n\n    /// Reconstruct from parameters.\n    pub fn from_params(\n        params: PredictivePipelineParams,\n        model_loader: impl FnOnce(\u0026[u8]) -\u003e Result\u003cM, PreprocessingError\u003e,\n    ) -\u003e Result\u003cSelf, PreprocessingError\u003e {\n        let preprocessor_params =\n            crate::preprocessing::column_transformer::ColumnTransformerParams::from_bytes(\n                \u0026params.preprocessor,\n            )\n            .map_err(|e| PreprocessingError::SerializationError(e.to_string()))?;\n        let preprocessor = FittedColumnTransformer::from_params(preprocessor_params)?;\n\n        let poly = if let Some(poly_bytes) = params.poly {\n            let poly_params =\n                crate::preprocessing::feature_engineering::PolynomialFeaturesParams::from_bytes(\n                    \u0026poly_bytes,\n                )\n                .map_err(|e| PreprocessingError::SerializationError(e.to_string()))?;\n            Some(FittedPolynomialFeatures::from_params(poly_params)?)\n        } else {\n            None\n        };\n\n        let model = model_loader(\u0026params.model)?;\n\n        Ok(Self {\n            preprocessor,\n            poly,\n            model,\n            n_features_in: params.n_features_in,\n            _backend: PhantomData,\n        })\n    }\n}\n\n#[cfg(test)]\nmod tests {\n    use super::*;\n    use crate::backend::CpuBackend;\n    use crate::model::linear::{LinearModel, LinearParams, SerializableLinearParams};\n    use crate::model::state::Fitted;\n    use crate::preprocessing::{\n        ColumnSpec, ColumnTransformer, PolynomialFeatures, StandardScaler, Transformer,\n    };\n\n    // Helper to create a simple fitted model for testing\n    fn create_test_model(n_features: usize) -\u003e LinearModel\u003cCpuBackend, Fitted\u003e {\n        let serial_params = SerializableLinearParams {\n            weights: vec![1.0; n_features],\n            bias: 0.0,\n        };\n        let params = LinearParams::try_from(serial_params).expect(\"Failed to convert params\");\n        \u003cLinearModel\u003cCpuBackend, Fitted\u003e\u003e::new(params)\n    }\n\n    #[test]\n    fn test_predictive_pipeline_basic() {\n        // Create simple data\n        let data = Tensor2D::\u003cCpuBackend\u003e::new(vec![1.0f32, 2.0, 3.0, 4.0], 2, 2);\n\n        // Build preprocessor\n        let ct = ColumnTransformer::\u003cCpuBackend\u003e::new()\n            .add_standard_scaler(StandardScaler::new(), ColumnSpec::All);\n        let fitted_ct = ct.fit(\u0026data).unwrap();\n\n        // Create model\n        let model = create_test_model(2);\n\n        // Build pipeline\n        let pipeline = PredictivePipeline::new(fitted_ct, None, model);\n\n        // Predict\n        let predictions = pipeline.predict(\u0026data).unwrap();\n        assert_eq!(predictions.len(), 2);\n    }\n\n    #[test]\n    fn test_predictive_pipeline_with_polynomial() {\n        // Create simple data\n        let data = Tensor2D::\u003cCpuBackend\u003e::new(vec![1.0f32, 2.0, 3.0, 4.0], 2, 2);\n\n        // Build preprocessor\n        let ct = ColumnTransformer::\u003cCpuBackend\u003e::new()\n            .add_standard_scaler(StandardScaler::new(), ColumnSpec::All);\n        let fitted_ct = ct.fit(\u0026data).unwrap();\n\n        // Transform to get intermediate data for poly fitting\n        let preprocessed = fitted_ct.transform(\u0026data).unwrap();\n\n        // Build polynomial features\n        let poly = PolynomialFeatures::\u003cCpuBackend\u003e::new()\n            .with_degree(2)\n            .with_include_bias(false);\n        let fitted_poly = poly.fit(\u0026preprocessed).unwrap();\n\n        // Create model with correct number of features\n        let n_poly_features = fitted_poly.n_features_out();\n        let model = create_test_model(n_poly_features);\n\n        // Build pipeline\n        let pipeline = PredictivePipeline::new(fitted_ct, Some(fitted_poly), model);\n\n        // Predict\n        let predictions = pipeline.predict(\u0026data).unwrap();\n        assert_eq!(predictions.len(), 2);\n    }\n\n    #[test]\n    fn test_predictive_pipeline_preprocess() {\n        // Create simple data\n        let data = Tensor2D::\u003cCpuBackend\u003e::new(vec![1.0f32, 2.0, 3.0, 4.0], 2, 2);\n\n        // Build preprocessor\n        let ct = ColumnTransformer::\u003cCpuBackend\u003e::new()\n            .add_standard_scaler(StandardScaler::new(), ColumnSpec::All);\n        let fitted_ct = ct.fit(\u0026data).unwrap();\n\n        // Create model\n        let model = create_test_model(2);\n\n        // Build pipeline\n        let pipeline = PredictivePipeline::new(fitted_ct, None, model);\n\n        // Preprocess\n        let processed = pipeline.preprocess(\u0026data).unwrap();\n        assert_eq!(processed.shape(), (2, 2));\n    }\n\n    #[test]\n    fn test_predictive_pipeline_feature_mismatch() {\n        // Create data with 2 features\n        let data = Tensor2D::\u003cCpuBackend\u003e::new(vec![1.0f32, 2.0, 3.0, 4.0], 2, 2);\n\n        // Build preprocessor for 2 features\n        let ct = ColumnTransformer::\u003cCpuBackend\u003e::new()\n            .add_standard_scaler(StandardScaler::new(), ColumnSpec::All);\n        let fitted_ct = ct.fit(\u0026data).unwrap();\n\n        // Create model\n        let model = create_test_model(2);\n\n        // Build pipeline\n        let pipeline = PredictivePipeline::new(fitted_ct, None, model);\n\n        // Try to predict with wrong number of features\n        let bad_data = Tensor2D::\u003cCpuBackend\u003e::new(vec![1.0f32, 2.0, 3.0], 1, 3);\n        let result = pipeline.predict(\u0026bad_data);\n\n        assert!(matches!(\n            result,\n            Err(PreprocessingError::FeatureMismatch { .. })\n        ));\n    }\n\n    #[test]\n    fn test_predictive_pipeline_serialization() {\n        // Create simple data\n        let data = Tensor2D::\u003cCpuBackend\u003e::new(vec![1.0f32, 2.0, 3.0, 4.0], 2, 2);\n\n        // Build preprocessor\n        let ct = ColumnTransformer::\u003cCpuBackend\u003e::new()\n            .add_standard_scaler(StandardScaler::new(), ColumnSpec::All);\n        let fitted_ct = ct.fit(\u0026data).unwrap();\n\n        // Create model\n        let model = create_test_model(2);\n\n        // Build pipeline\n        let pipeline = PredictivePipeline::new(fitted_ct, None, model);\n\n        // Save\n        let temp_file = std::env::temp_dir().join(\"test_predictive_pipeline.bin\");\n        pipeline.save_to_file(\u0026temp_file).unwrap();\n\n        // Load with model loader\n        let loaded =\n            PredictivePipeline::\u003cCpuBackend, LinearModel\u003cCpuBackend, Fitted\u003e\u003e::load_from_file(\n                \u0026temp_file,\n                |bytes| {\n                    let serial_params: SerializableLinearParams = bincode::deserialize(bytes)\n                        .map_err(|e| PreprocessingError::SerializationError(e.to_string()))?;\n                    let params = LinearParams::try_from(serial_params)\n                        .map_err(|e| PreprocessingError::SerializationError(e.to_string()))?;\n                    Ok(\u003cLinearModel\u003cCpuBackend, Fitted\u003e\u003e::new(params))\n                },\n            )\n            .unwrap();\n\n        // Verify\n        assert_eq!(loaded.n_features_in(), pipeline.n_features_in());\n\n        let p1 = pipeline.predict(\u0026data).unwrap();\n        let p2 = loaded.predict(\u0026data).unwrap();\n\n        let v1 = p1.to_vec();\n        let v2 = p2.to_vec();\n        for (a, b) in v1.iter().zip(v2.iter()) {\n            assert!((a - b).abs() \u003c 1e-6);\n        }\n\n        std::fs::remove_file(temp_file).ok();\n    }\n}\n","traces":[{"line":57,"address":[2045911,2045881,2045568],"length":1,"stats":{"Line":2}},{"line":62,"address":[2045665,2045607],"length":1,"stats":{"Line":4}},{"line":73,"address":[2043280],"length":1,"stats":{"Line":1}},{"line":74,"address":[2043285],"length":1,"stats":{"Line":1}},{"line":78,"address":[2038768,2039452,2039458],"length":1,"stats":{"Line":1}},{"line":79,"address":[2038806],"length":1,"stats":{"Line":1}},{"line":81,"address":[2039424,2038973],"length":1,"stats":{"Line":2}},{"line":82,"address":[2039169,2039351,2039036],"length":1,"stats":{"Line":2}},{"line":85,"address":[2039043],"length":1,"stats":{"Line":1}},{"line":89,"address":[2046455,2046461,2045936],"length":1,"stats":{"Line":1}},{"line":90,"address":[2045987],"length":1,"stats":{"Line":1}},{"line":92,"address":[2046010],"length":1,"stats":{"Line":1}},{"line":93,"address":[2046124],"length":1,"stats":{"Line":1}},{"line":94,"address":[2046117],"length":1,"stats":{"Line":1}},{"line":95,"address":[],"length":0,"stats":{"Line":0}},{"line":99,"address":[2046169,2046029],"length":1,"stats":{"Line":2}},{"line":100,"address":[2046320],"length":1,"stats":{"Line":1}},{"line":102,"address":[2046383],"length":1,"stats":{"Line":2}},{"line":106,"address":[2042688,2043252,2043214],"length":1,"stats":{"Line":1}},{"line":107,"address":[2042722],"length":1,"stats":{"Line":1}},{"line":108,"address":[2042869,2043225,2042812],"length":1,"stats":{"Line":2}},{"line":109,"address":[2043052,2043157],"length":1,"stats":{"Line":2}},{"line":113,"address":[2045134,2045100,2044176],"length":1,"stats":{"Line":1}},{"line":117,"address":[2044294,2045296,2044396,2045543,2045537,2044201,2045132],"length":1,"stats":{"Line":2}},{"line":118,"address":[2045318,2045383],"length":1,"stats":{"Line":0}},{"line":120,"address":[2044473,2044602,2044699,2044556],"length":1,"stats":{"Line":3}},{"line":121,"address":[2044579,2044651,2045180,2045152],"length":1,"stats":{"Line":1}},{"line":122,"address":[2044897],"length":1,"stats":{"Line":1}},{"line":126,"address":[2043296,2044003,2043997],"length":1,"stats":{"Line":1}},{"line":127,"address":[2043336],"length":1,"stats":{"Line":1}},{"line":128,"address":[2043477],"length":1,"stats":{"Line":1}},{"line":129,"address":[],"length":0,"stats":{"Line":0}},{"line":131,"address":[2043504,2044016,2044047],"length":1,"stats":{"Line":1}},{"line":132,"address":[2043516,2043588],"length":1,"stats":{"Line":2}},{"line":138,"address":[2043871],"length":1,"stats":{"Line":1}},{"line":143,"address":[2039472,2041967,2041250],"length":1,"stats":{"Line":1}},{"line":147,"address":[2039502],"length":1,"stats":{"Line":1}},{"line":149,"address":[2039558],"length":1,"stats":{"Line":1}},{"line":151,"address":[2039661,2042428,2042400,2039727],"length":1,"stats":{"Line":1}},{"line":152,"address":[2040019,2042023,2039892],"length":1,"stats":{"Line":2}},{"line":154,"address":[2040319,2040202],"length":1,"stats":{"Line":2}},{"line":155,"address":[],"length":0,"stats":{"Line":0}},{"line":157,"address":[2040272],"length":1,"stats":{"Line":0}},{"line":159,"address":[2040481,2040409,2042544,2042572],"length":1,"stats":{"Line":0}},{"line":160,"address":[2040842,2040690],"length":1,"stats":{"Line":0}},{"line":162,"address":[2040301],"length":1,"stats":{"Line":1}},{"line":165,"address":[2041426,2041284,2041164,2042018],"length":1,"stats":{"Line":2}},{"line":167,"address":[2041699],"length":1,"stats":{"Line":1}},{"line":168,"address":[2041583],"length":1,"stats":{"Line":1}},{"line":169,"address":[2041631],"length":1,"stats":{"Line":1}},{"line":170,"address":[],"length":0,"stats":{"Line":0}},{"line":171,"address":[2041695],"length":1,"stats":{"Line":1}},{"line":172,"address":[],"length":0,"stats":{"Line":0}}],"covered":44,"coverable":53},{"path":["/","home","victor","ML","machinelearne-rs","lib","src","preprocessing","scaling","maxabs.rs"],"content":"//! Max-Abs Scaler.\n//!\n//! Scales each feature by its maximum absolute value.\n//! This estimator scales and translates each feature individually such that\n//! the maximal absolute value of each feature in the training set will be 1.0.\n//!\n//! It does not shift/center the data, and thus does not destroy any sparsity.\n//!\n//! # Example\n//! ```ignore\n//! use machinelearne_rs::preprocessing::{Transformer, MaxAbsScaler};\n//! use machinelearne_rs::backend::CpuBackend;\n//!\n//! let scaler = MaxAbsScaler::\u003cCpuBackend\u003e::new();\n//! let fitted = scaler.fit(\u0026data)?;\n//! let scaled = fitted.transform(\u0026data)?;\n//! ```\n\nuse crate::backend::{Backend, Tensor1D, Tensor2D};\nuse crate::preprocessing::error::PreprocessingError;\nuse crate::preprocessing::traits::{FittedTransformer, Transformer};\nuse serde::{Deserialize, Serialize};\nuse std::marker::PhantomData;\n\n/// Serializable parameters for a fitted MaxAbsScaler.\n#[derive(Clone, Serialize, Deserialize)]\npub struct MaxAbsScalerParams {\n    /// Maximum absolute value for each feature.\n    pub max_abs_: Vec\u003cf64\u003e,\n    /// Scale factor for each feature (1.0 / max_abs).\n    pub scale_: Vec\u003cf64\u003e,\n    /// Number of features seen during fit.\n    pub n_features: usize,\n}\n\n/// MaxAbsScaler transformer (unfitted).\n///\n/// Scales each feature by its maximum absolute value.\n#[derive(Clone, Default)]\npub struct MaxAbsScaler\u003cB: Backend\u003e {\n    _backend: PhantomData\u003cB\u003e,\n}\n\nimpl\u003cB: Backend\u003e MaxAbsScaler\u003cB\u003e {\n    /// Create a new MaxAbsScaler.\n    pub fn new() -\u003e Self {\n        Self {\n            _backend: PhantomData,\n        }\n    }\n}\n\nimpl\u003cB: Backend\u003e Transformer\u003cB\u003e for MaxAbsScaler\u003cB\u003e {\n    type Input = Tensor2D\u003cB\u003e;\n    type Output = Tensor2D\u003cB\u003e;\n    type Params = MaxAbsScalerParams;\n    type Fitted = FittedMaxAbsScaler\u003cB\u003e;\n\n    fn fit(\u0026self, data: \u0026Self::Input) -\u003e Result\u003cSelf::Fitted, PreprocessingError\u003e {\n        let (rows, cols) = data.shape();\n\n        if rows == 0 {\n            return Err(PreprocessingError::EmptyData(\n                \"Cannot fit MaxAbsScaler on empty data\".to_string(),\n            ));\n        }\n\n        // Get absolute values of data\n        let abs_data = B::abs_2d(\u0026data.data);\n\n        // Find max absolute value per column\n        let max_abs_raw = B::col_max_2d(\u0026abs_data);\n        let max_abs_vals = B::to_vec_1d(\u0026max_abs_raw);\n\n        // Compute scale: 1.0 / max_abs (handle zero max)\n        let scale_vals: Vec\u003cf64\u003e = max_abs_vals\n            .iter()\n            .map(|\u0026m| if m == 0.0 { 1.0 } else { 1.0 / m })\n            .collect();\n\n        let max_abs_ = Tensor1D {\n            data: B::from_vec_1d(max_abs_vals.iter().map(|\u0026x| x as f32).collect()),\n            backend: PhantomData::\u003cB\u003e,\n        };\n\n        let scale_ = Tensor1D {\n            data: B::from_vec_1d(scale_vals.iter().map(|\u0026x| x as f32).collect()),\n            backend: PhantomData::\u003cB\u003e,\n        };\n\n        Ok(FittedMaxAbsScaler {\n            max_abs_,\n            scale_,\n            n_features: cols,\n            _backend: PhantomData,\n        })\n    }\n\n    fn fit_transform(\u0026self, data: \u0026Self::Input) -\u003e Result\u003cSelf::Output, PreprocessingError\u003e {\n        let fitted = self.fit(data)?;\n        fitted.transform(data)\n    }\n}\n\n/// Fitted MaxAbsScaler ready for inference.\n#[derive(Clone)]\npub struct FittedMaxAbsScaler\u003cB: Backend\u003e {\n    max_abs_: Tensor1D\u003cB\u003e,\n    scale_: Tensor1D\u003cB\u003e,\n    n_features: usize,\n    _backend: PhantomData\u003cB\u003e,\n}\n\nimpl\u003cB: Backend\u003e FittedMaxAbsScaler\u003cB\u003e {\n    /// Get the maximum absolute values for each feature.\n    pub fn max_abs(\u0026self) -\u003e \u0026Tensor1D\u003cB\u003e {\n        \u0026self.max_abs_\n    }\n\n    /// Get the scale factor for each feature.\n    pub fn scale(\u0026self) -\u003e \u0026Tensor1D\u003cB\u003e {\n        \u0026self.scale_\n    }\n}\n\nimpl\u003cB: Backend\u003e FittedTransformer\u003cB\u003e for FittedMaxAbsScaler\u003cB\u003e {\n    type Input = Tensor2D\u003cB\u003e;\n    type Output = Tensor2D\u003cB\u003e;\n    type Params = MaxAbsScalerParams;\n\n    fn transform(\u0026self, data: \u0026Self::Input) -\u003e Result\u003cSelf::Output, PreprocessingError\u003e {\n        let (_, cols) = data.shape();\n\n        if cols != self.n_features {\n            return Err(PreprocessingError::FeatureMismatch {\n                expected_features: self.n_features,\n                got_features: cols,\n            });\n        }\n\n        // X_scaled = X * scale_\n        let result = B::broadcast_mul_1d_to_2d_rows(\u0026data.data, \u0026self.scale_.data);\n\n        Ok(Tensor2D {\n            data: result,\n            backend: PhantomData::\u003cB\u003e,\n        })\n    }\n\n    fn inverse_transform(\u0026self, data: \u0026Self::Output) -\u003e Result\u003cSelf::Input, PreprocessingError\u003e {\n        let (_, cols) = data.shape();\n\n        if cols != self.n_features {\n            return Err(PreprocessingError::FeatureMismatch {\n                expected_features: self.n_features,\n                got_features: cols,\n            });\n        }\n\n        // X = X_scaled * max_abs_\n        let result = B::broadcast_mul_1d_to_2d_rows(\u0026data.data, \u0026self.max_abs_.data);\n\n        Ok(Tensor2D {\n            data: result,\n            backend: PhantomData::\u003cB\u003e,\n        })\n    }\n\n    fn extract_params(\u0026self) -\u003e Self::Params {\n        MaxAbsScalerParams {\n            max_abs_: self.max_abs_.to_vec(),\n            scale_: self.scale_.to_vec(),\n            n_features: self.n_features,\n        }\n    }\n\n    fn from_params(params: Self::Params) -\u003e Result\u003cSelf, PreprocessingError\u003e {\n        let max_abs_ = Tensor1D {\n            data: B::from_vec_1d(params.max_abs_.iter().map(|\u0026x| x as f32).collect()),\n            backend: PhantomData::\u003cB\u003e,\n        };\n        let scale_ = Tensor1D {\n            data: B::from_vec_1d(params.scale_.iter().map(|\u0026x| x as f32).collect()),\n            backend: PhantomData::\u003cB\u003e,\n        };\n\n        Ok(Self {\n            max_abs_,\n            scale_,\n            n_features: params.n_features,\n            _backend: PhantomData,\n        })\n    }\n\n    fn n_features_in(\u0026self) -\u003e usize {\n        self.n_features\n    }\n\n    fn save_to_file\u003cP: AsRef\u003cstd::path::Path\u003e\u003e(\u0026self, path: P) -\u003e std::io::Result\u003c()\u003e {\n        let params = self.extract_params();\n        let bytes = bincode::serialize(\u0026params).map_err(std::io::Error::other)?;\n        std::fs::write(path, bytes)\n    }\n\n    fn load_from_file\u003cP: AsRef\u003cstd::path::Path\u003e\u003e(path: P) -\u003e Result\u003cSelf, PreprocessingError\u003e\n    where\n        Self: Sized,\n    {\n        let bytes = std::fs::read(path)?;\n        let params: MaxAbsScalerParams = bincode::deserialize(\u0026bytes)\n            .map_err(|e| PreprocessingError::SerializationError(e.to_string()))?;\n        Self::from_params(params)\n    }\n}\n\n#[cfg(test)]\nmod tests {\n    use super::*;\n    use crate::backend::CpuBackend;\n\n    fn create_test_data() -\u003e Tensor2D\u003cCpuBackend\u003e {\n        // [[-1, 2], [0, 4], [1, -6]]  -- max abs: [1, 6]\n        Tensor2D::new(vec![-1.0f32, 2.0, 0.0, 4.0, 1.0, -6.0], 3, 2)\n    }\n\n    #[test]\n    fn test_maxabs_scaler_fit() {\n        let data = create_test_data();\n        let scaler = MaxAbsScaler::\u003cCpuBackend\u003e::new();\n        let fitted = scaler.fit(\u0026data).unwrap();\n\n        let max_abs = fitted.max_abs().to_vec();\n        assert!((max_abs[0] - 1.0).abs() \u003c 1e-6);\n        assert!((max_abs[1] - 6.0).abs() \u003c 1e-6);\n\n        let scale = fitted.scale().to_vec();\n        assert!((scale[0] - 1.0).abs() \u003c 1e-6);\n        assert!((scale[1] - 1.0 / 6.0).abs() \u003c 1e-6);\n    }\n\n    #[test]\n    fn test_maxabs_scaler_transform() {\n        let data = create_test_data();\n        let scaler = MaxAbsScaler::\u003cCpuBackend\u003e::new();\n        let fitted = scaler.fit(\u0026data).unwrap();\n\n        let transformed = fitted.transform(\u0026data).unwrap();\n        let values = transformed.ravel().to_vec();\n\n        // First column: [-1, 0, 1] * 1.0 = [-1, 0, 1]\n        assert!((values[0] - (-1.0)).abs() \u003c 1e-6);\n        assert!((values[2] - 0.0).abs() \u003c 1e-6);\n        assert!((values[4] - 1.0).abs() \u003c 1e-6);\n\n        // Second column: [2, 4, -6] * (1/6) = [0.333, 0.667, -1]\n        assert!((values[1] - (2.0 / 6.0)).abs() \u003c 1e-6);\n        assert!((values[3] - (4.0 / 6.0)).abs() \u003c 1e-6);\n        assert!((values[5] - (-1.0)).abs() \u003c 1e-6);\n    }\n\n    #[test]\n    fn test_maxabs_scaler_inverse_transform() {\n        let data = create_test_data();\n        let scaler = MaxAbsScaler::\u003cCpuBackend\u003e::new();\n        let fitted = scaler.fit(\u0026data).unwrap();\n\n        let transformed = fitted.transform(\u0026data).unwrap();\n        let recovered = fitted.inverse_transform(\u0026transformed).unwrap();\n\n        let original = data.ravel().to_vec();\n        let recovered_vec = recovered.ravel().to_vec();\n\n        for (o, r) in original.iter().zip(recovered_vec.iter()) {\n            assert!((o - r).abs() \u003c 1e-6, \"Expected {}, got {}\", o, r);\n        }\n    }\n\n    #[test]\n    fn test_maxabs_scaler_serialization() {\n        let data = create_test_data();\n        let scaler = MaxAbsScaler::\u003cCpuBackend\u003e::new();\n        let fitted = scaler.fit(\u0026data).unwrap();\n\n        let params = fitted.extract_params();\n        let restored = FittedMaxAbsScaler::\u003cCpuBackend\u003e::from_params(params).unwrap();\n\n        let transformed1 = fitted.transform(\u0026data).unwrap();\n        let transformed2 = restored.transform(\u0026data).unwrap();\n\n        let t1 = transformed1.ravel().to_vec();\n        let t2 = transformed2.ravel().to_vec();\n\n        for (i, (a, b)) in t1.iter().zip(t2.iter()).enumerate() {\n            assert!(\n                (a - b).abs() \u003c 1e-6,\n                \"Mismatch at index {}: {} vs {}\",\n                i,\n                a,\n                b\n            );\n        }\n    }\n\n    #[test]\n    fn test_maxabs_scaler_feature_mismatch() {\n        let data = create_test_data(); // 2 features\n        let scaler = MaxAbsScaler::\u003cCpuBackend\u003e::new();\n        let fitted = scaler.fit(\u0026data).unwrap();\n\n        let wrong_data = Tensor2D::\u003cCpuBackend\u003e::new(vec![1.0f32, 2.0, 3.0], 1, 3); // 3 features\n        let result = fitted.transform(\u0026wrong_data);\n\n        assert!(matches!(\n            result,\n            Err(PreprocessingError::FeatureMismatch {\n                expected_features: 2,\n                got_features: 3\n            })\n        ));\n    }\n}\n","traces":[{"line":46,"address":[],"length":0,"stats":{"Line":0}},{"line":59,"address":[3451152,3452341,3452347],"length":1,"stats":{"Line":1}},{"line":60,"address":[3451210],"length":1,"stats":{"Line":2}},{"line":62,"address":[3451239],"length":1,"stats":{"Line":2}},{"line":63,"address":[3451283],"length":1,"stats":{"Line":0}},{"line":64,"address":[3451249],"length":1,"stats":{"Line":0}},{"line":69,"address":[3451418],"length":1,"stats":{"Line":2}},{"line":72,"address":[3451439],"length":1,"stats":{"Line":1}},{"line":73,"address":[3451518],"length":1,"stats":{"Line":2}},{"line":76,"address":[3451570],"length":1,"stats":{"Line":2}},{"line":78,"address":[3452400,3451673,3452410],"length":1,"stats":{"Line":6}},{"line":82,"address":[3451723,3451790,3452506,3452496],"length":1,"stats":{"Line":8}},{"line":87,"address":[3451922,3452368,3451989,3452378],"length":1,"stats":{"Line":8}},{"line":91,"address":[3452159],"length":1,"stats":{"Line":2}},{"line":92,"address":[3452127],"length":1,"stats":{"Line":2}},{"line":93,"address":[],"length":0,"stats":{"Line":0}},{"line":94,"address":[],"length":0,"stats":{"Line":0}},{"line":95,"address":[],"length":0,"stats":{"Line":0}},{"line":99,"address":[],"length":0,"stats":{"Line":0}},{"line":100,"address":[],"length":0,"stats":{"Line":0}},{"line":101,"address":[],"length":0,"stats":{"Line":0}},{"line":116,"address":[3456624],"length":1,"stats":{"Line":1}},{"line":117,"address":[],"length":0,"stats":{"Line":0}},{"line":121,"address":[3456608],"length":1,"stats":{"Line":1}},{"line":122,"address":[3456616],"length":1,"stats":{"Line":1}},{"line":131,"address":[3453728],"length":1,"stats":{"Line":1}},{"line":132,"address":[3453779],"length":1,"stats":{"Line":1}},{"line":134,"address":[3453802],"length":1,"stats":{"Line":1}},{"line":135,"address":[3453921],"length":1,"stats":{"Line":1}},{"line":136,"address":[3453917],"length":1,"stats":{"Line":1}},{"line":137,"address":[],"length":0,"stats":{"Line":0}},{"line":142,"address":[3453818],"length":1,"stats":{"Line":1}},{"line":144,"address":[3453832],"length":1,"stats":{"Line":1}},{"line":145,"address":[],"length":0,"stats":{"Line":0}},{"line":146,"address":[],"length":0,"stats":{"Line":0}},{"line":150,"address":[3453472],"length":1,"stats":{"Line":1}},{"line":151,"address":[3453523],"length":1,"stats":{"Line":1}},{"line":153,"address":[3453546],"length":1,"stats":{"Line":1}},{"line":154,"address":[3453661],"length":1,"stats":{"Line":0}},{"line":155,"address":[3453657],"length":1,"stats":{"Line":0}},{"line":156,"address":[],"length":0,"stats":{"Line":0}},{"line":161,"address":[3453562],"length":1,"stats":{"Line":1}},{"line":163,"address":[3453572],"length":1,"stats":{"Line":1}},{"line":164,"address":[],"length":0,"stats":{"Line":0}},{"line":165,"address":[],"length":0,"stats":{"Line":0}},{"line":169,"address":[3453450,3453280,3453444],"length":1,"stats":{"Line":1}},{"line":171,"address":[3453309],"length":1,"stats":{"Line":1}},{"line":172,"address":[3453319],"length":1,"stats":{"Line":1}},{"line":173,"address":[3453378],"length":1,"stats":{"Line":1}},{"line":177,"address":[3453169,3452528],"length":1,"stats":{"Line":1}},{"line":179,"address":[3453200,3453210,3452558,3452622],"length":1,"stats":{"Line":4}},{"line":183,"address":[3452822,3453232,3453242,3452751],"length":1,"stats":{"Line":4}},{"line":187,"address":[3453024],"length":1,"stats":{"Line":1}},{"line":188,"address":[3452972],"length":1,"stats":{"Line":1}},{"line":189,"address":[],"length":0,"stats":{"Line":0}},{"line":190,"address":[3453020],"length":1,"stats":{"Line":1}},{"line":191,"address":[],"length":0,"stats":{"Line":0}},{"line":195,"address":[3453264],"length":1,"stats":{"Line":0}},{"line":196,"address":[3453269],"length":1,"stats":{"Line":0}},{"line":199,"address":[],"length":0,"stats":{"Line":0}},{"line":200,"address":[],"length":0,"stats":{"Line":0}},{"line":201,"address":[],"length":0,"stats":{"Line":0}},{"line":202,"address":[],"length":0,"stats":{"Line":0}},{"line":209,"address":[],"length":0,"stats":{"Line":0}},{"line":210,"address":[],"length":0,"stats":{"Line":0}},{"line":211,"address":[],"length":0,"stats":{"Line":0}},{"line":212,"address":[],"length":0,"stats":{"Line":0}}],"covered":37,"coverable":67},{"path":["/","home","victor","ML","machinelearne-rs","lib","src","preprocessing","scaling","minmax.rs"],"content":"//! Min-Max Scaler.\n//!\n//! Transforms features by scaling each feature to a given range (default [0, 1]).\n//!\n//! The transformation is given by:\n//! ```text\n//! X_scaled = (X - X_min) / (X_max - X_min) * (max - min) + min\n//! ```\n//!\n//! # Example\n//! ```ignore\n//! use machinelearne_rs::preprocessing::{Transformer, MinMaxScaler};\n//! use machinelearne_rs::backend::CpuBackend;\n//!\n//! let scaler = MinMaxScaler::\u003cCpuBackend\u003e::new()\n//!     .with_range(0.0, 1.0);\n//!\n//! let fitted = scaler.fit(\u0026data)?;\n//! let scaled = fitted.transform(\u0026data)?;\n//! ```\n\nuse crate::backend::{Backend, Tensor1D, Tensor2D};\nuse crate::preprocessing::error::PreprocessingError;\nuse crate::preprocessing::traits::{FittedTransformer, Transformer};\nuse serde::{Deserialize, Serialize};\nuse std::marker::PhantomData;\n\n/// Configuration for MinMaxScaler.\n#[derive(Clone, Serialize, Deserialize)]\npub struct MinMaxScalerConfig {\n    /// Minimum value of the target range.\n    pub min: f64,\n    /// Maximum value of the target range.\n    pub max: f64,\n}\n\nimpl Default for MinMaxScalerConfig {\n    fn default() -\u003e Self {\n        Self { min: 0.0, max: 1.0 }\n    }\n}\n\n/// Serializable parameters for a fitted MinMaxScaler.\n#[derive(Clone, Serialize, Deserialize)]\npub struct MinMaxScalerParams {\n    /// Configuration options.\n    pub config: MinMaxScalerConfig,\n    /// Minimum of each feature.\n    pub min_: Vec\u003cf64\u003e,\n    /// Maximum of each feature.\n    pub max_: Vec\u003cf64\u003e,\n    /// Scale factor for each feature: (max - min) / (feature_max - feature_min).\n    pub scale_: Vec\u003cf64\u003e,\n    /// Number of features seen during fit.\n    pub n_features: usize,\n}\n\n/// MinMaxScaler transformer (unfitted).\n///\n/// Transforms features by scaling each feature to a given range.\n#[derive(Clone)]\npub struct MinMaxScaler\u003cB: Backend\u003e {\n    config: MinMaxScalerConfig,\n    _backend: PhantomData\u003cB\u003e,\n}\n\nimpl\u003cB: Backend\u003e Default for MinMaxScaler\u003cB\u003e {\n    fn default() -\u003e Self {\n        Self::new()\n    }\n}\n\nimpl\u003cB: Backend\u003e MinMaxScaler\u003cB\u003e {\n    /// Create a new MinMaxScaler with default range [0, 1].\n    pub fn new() -\u003e Self {\n        Self {\n            config: MinMaxScalerConfig::default(),\n            _backend: PhantomData,\n        }\n    }\n\n    /// Set the target range for scaling.\n    pub fn with_range(mut self, min: f64, max: f64) -\u003e Self {\n        assert!(max \u003e min, \"max must be greater than min\");\n        self.config.min = min;\n        self.config.max = max;\n        self\n    }\n}\n\nimpl\u003cB: Backend\u003e Transformer\u003cB\u003e for MinMaxScaler\u003cB\u003e {\n    type Input = Tensor2D\u003cB\u003e;\n    type Output = Tensor2D\u003cB\u003e;\n    type Params = MinMaxScalerParams;\n    type Fitted = FittedMinMaxScaler\u003cB\u003e;\n\n    fn fit(\u0026self, data: \u0026Self::Input) -\u003e Result\u003cSelf::Fitted, PreprocessingError\u003e {\n        let (rows, cols) = data.shape();\n\n        if rows == 0 {\n            return Err(PreprocessingError::EmptyData(\n                \"Cannot fit MinMaxScaler on empty data\".to_string(),\n            ));\n        }\n\n        let min_vec = B::col_min_2d(\u0026data.data);\n        let max_vec = B::col_max_2d(\u0026data.data);\n\n        let min_vals = B::to_vec_1d(\u0026min_vec);\n        let max_vals = B::to_vec_1d(\u0026max_vec);\n\n        // Compute scale: (target_max - target_min) / (feature_max - feature_min)\n        let target_range = self.config.max - self.config.min;\n        let scale_vals: Vec\u003cf64\u003e = min_vals\n            .iter()\n            .zip(max_vals.iter())\n            .map(|(\u0026min, \u0026max)| {\n                let range = max - min;\n                if range == 0.0 {\n                    1.0 // Constant feature: scale by 1 to avoid division by zero\n                } else {\n                    target_range / range\n                }\n            })\n            .collect();\n\n        let scale_ = Tensor1D {\n            data: B::from_vec_1d(scale_vals.iter().map(|\u0026x| x as f32).collect()),\n            backend: PhantomData::\u003cB\u003e,\n        };\n\n        let min_ = Tensor1D {\n            data: min_vec,\n            backend: PhantomData::\u003cB\u003e,\n        };\n\n        Ok(FittedMinMaxScaler {\n            config: self.config.clone(),\n            min_,\n            max_: max_vec,\n            scale_,\n            n_features: cols,\n            _backend: PhantomData,\n        })\n    }\n\n    fn fit_transform(\u0026self, data: \u0026Self::Input) -\u003e Result\u003cSelf::Output, PreprocessingError\u003e {\n        let fitted = self.fit(data)?;\n        fitted.transform(data)\n    }\n}\n\n/// Fitted MinMaxScaler ready for inference.\n#[derive(Clone)]\npub struct FittedMinMaxScaler\u003cB: Backend\u003e {\n    config: MinMaxScalerConfig,\n    min_: Tensor1D\u003cB\u003e,\n    max_: B::Tensor1D,\n    scale_: Tensor1D\u003cB\u003e,\n    n_features: usize,\n    _backend: PhantomData\u003cB\u003e,\n}\n\nimpl\u003cB: Backend\u003e FittedMinMaxScaler\u003cB\u003e {\n    /// Get the minimum values for each feature.\n    pub fn min(\u0026self) -\u003e \u0026Tensor1D\u003cB\u003e {\n        \u0026self.min_\n    }\n\n    /// Get the scale factor for each feature.\n    pub fn scale(\u0026self) -\u003e \u0026Tensor1D\u003cB\u003e {\n        \u0026self.scale_\n    }\n\n    /// Get the data range (max - min) for each feature.\n    pub fn data_range(\u0026self) -\u003e Tensor1D\u003cB\u003e {\n        let max_tensor = Tensor1D {\n            data: self.max_.clone(),\n            backend: PhantomData::\u003cB\u003e,\n        };\n        Tensor1D {\n            data: B::sub_1d(\u0026max_tensor.data, \u0026self.min_.data),\n            backend: PhantomData::\u003cB\u003e,\n        }\n    }\n}\n\nimpl\u003cB: Backend\u003e FittedTransformer\u003cB\u003e for FittedMinMaxScaler\u003cB\u003e {\n    type Input = Tensor2D\u003cB\u003e;\n    type Output = Tensor2D\u003cB\u003e;\n    type Params = MinMaxScalerParams;\n\n    fn transform(\u0026self, data: \u0026Self::Input) -\u003e Result\u003cSelf::Output, PreprocessingError\u003e {\n        let (_, cols) = data.shape();\n\n        if cols != self.n_features {\n            return Err(PreprocessingError::FeatureMismatch {\n                expected_features: self.n_features,\n                got_features: cols,\n            });\n        }\n\n        // X_scaled = (X - X_min) * scale_ + target_min\n        // which equals: (X - X_min) / (X_max - X_min) * (target_max - target_min) + target_min\n        let centered = B::broadcast_sub_1d_to_2d_rows(\u0026data.data, \u0026self.min_.data);\n        let scaled = B::broadcast_mul_1d_to_2d_rows(\u0026centered, \u0026self.scale_.data);\n        let result = B::add_scalar_2d(\u0026scaled, \u0026B::scalar_f64(self.config.min));\n\n        Ok(Tensor2D {\n            data: result,\n            backend: PhantomData::\u003cB\u003e,\n        })\n    }\n\n    fn inverse_transform(\u0026self, data: \u0026Self::Output) -\u003e Result\u003cSelf::Input, PreprocessingError\u003e {\n        let (_, cols) = data.shape();\n\n        if cols != self.n_features {\n            return Err(PreprocessingError::FeatureMismatch {\n                expected_features: self.n_features,\n                got_features: cols,\n            });\n        }\n\n        // X = (X_scaled - target_min) / scale_ + X_min\n        let centered = B::add_scalar_2d(\u0026data.data, \u0026B::scalar_f64(-self.config.min));\n        let unscaled = B::broadcast_div_1d_to_2d_rows(\u0026centered, \u0026self.scale_.data);\n        let result = B::broadcast_add_1d_to_2d_rows(\u0026unscaled, \u0026self.min_.data);\n\n        Ok(Tensor2D {\n            data: result,\n            backend: PhantomData::\u003cB\u003e,\n        })\n    }\n\n    fn extract_params(\u0026self) -\u003e Self::Params {\n        MinMaxScalerParams {\n            config: self.config.clone(),\n            min_: self.min_.to_vec(),\n            max_: B::to_vec_1d(\u0026self.max_),\n            scale_: self.scale_.to_vec(),\n            n_features: self.n_features,\n        }\n    }\n\n    fn from_params(params: Self::Params) -\u003e Result\u003cSelf, PreprocessingError\u003e {\n        let min_ = Tensor1D {\n            data: B::from_vec_1d(params.min_.iter().map(|\u0026x| x as f32).collect()),\n            backend: PhantomData::\u003cB\u003e,\n        };\n        let max_ = B::from_vec_1d(params.max_.iter().map(|\u0026x| x as f32).collect());\n        let scale_ = Tensor1D {\n            data: B::from_vec_1d(params.scale_.iter().map(|\u0026x| x as f32).collect()),\n            backend: PhantomData::\u003cB\u003e,\n        };\n\n        Ok(Self {\n            config: params.config,\n            min_,\n            max_,\n            scale_,\n            n_features: params.n_features,\n            _backend: PhantomData,\n        })\n    }\n\n    fn n_features_in(\u0026self) -\u003e usize {\n        self.n_features\n    }\n\n    fn save_to_file\u003cP: AsRef\u003cstd::path::Path\u003e\u003e(\u0026self, path: P) -\u003e std::io::Result\u003c()\u003e {\n        let params = self.extract_params();\n        let bytes = bincode::serialize(\u0026params).map_err(std::io::Error::other)?;\n        std::fs::write(path, bytes)\n    }\n\n    fn load_from_file\u003cP: AsRef\u003cstd::path::Path\u003e\u003e(path: P) -\u003e Result\u003cSelf, PreprocessingError\u003e\n    where\n        Self: Sized,\n    {\n        let bytes = std::fs::read(path)?;\n        let params: MinMaxScalerParams = bincode::deserialize(\u0026bytes)\n            .map_err(|e| PreprocessingError::SerializationError(e.to_string()))?;\n        Self::from_params(params)\n    }\n}\n\n#[cfg(test)]\nmod tests {\n    use super::*;\n    use crate::backend::CpuBackend;\n\n    fn create_test_data() -\u003e Tensor2D\u003cCpuBackend\u003e {\n        // [[0, 1], [0, 1], [1, 3]]\n        Tensor2D::new(vec![0.0f32, 1.0, 0.0, 1.0, 1.0, 3.0], 3, 2)\n    }\n\n    #[test]\n    fn test_minmax_scaler_fit() {\n        let data = create_test_data();\n        let scaler = MinMaxScaler::\u003cCpuBackend\u003e::new();\n        let fitted = scaler.fit(\u0026data).unwrap();\n\n        // Min: [0, 1], Max: [1, 3]\n        let min = fitted.min().to_vec();\n        assert_eq!(min[0], 0.0);\n        assert_eq!(min[1], 1.0);\n\n        // Scale: (1 - 0) / (1 - 0) = 1, (1 - 0) / (3 - 1) = 0.5\n        let scale = fitted.scale().to_vec();\n        assert!((scale[0] - 1.0).abs() \u003c 1e-10);\n        assert!((scale[1] - 0.5).abs() \u003c 1e-10);\n    }\n\n    #[test]\n    fn test_minmax_scaler_transform() {\n        let data = create_test_data();\n        let scaler = MinMaxScaler::\u003cCpuBackend\u003e::new();\n        let fitted = scaler.fit(\u0026data).unwrap();\n\n        let transformed = fitted.transform(\u0026data).unwrap();\n        let values = transformed.ravel().to_vec();\n\n        // First column: [0, 0, 1] -\u003e [0, 0, 1]\n        assert!((values[0] - 0.0).abs() \u003c 1e-6);\n        assert!((values[2] - 0.0).abs() \u003c 1e-6);\n        assert!((values[4] - 1.0).abs() \u003c 1e-6);\n\n        // Second column: [1, 1, 3] -\u003e [0, 0, 1]\n        assert!((values[1] - 0.0).abs() \u003c 1e-6);\n        assert!((values[3] - 0.0).abs() \u003c 1e-6);\n        assert!((values[5] - 1.0).abs() \u003c 1e-6);\n    }\n\n    #[test]\n    fn test_minmax_scaler_inverse_transform() {\n        let data = create_test_data();\n        let scaler = MinMaxScaler::\u003cCpuBackend\u003e::new();\n        let fitted = scaler.fit(\u0026data).unwrap();\n\n        let transformed = fitted.transform(\u0026data).unwrap();\n        let recovered = fitted.inverse_transform(\u0026transformed).unwrap();\n\n        let original = data.ravel().to_vec();\n        let recovered_vec = recovered.ravel().to_vec();\n\n        for (o, r) in original.iter().zip(recovered_vec.iter()) {\n            assert!((o - r).abs() \u003c 1e-6, \"Expected {}, got {}\", o, r);\n        }\n    }\n\n    #[test]\n    fn test_minmax_scaler_custom_range() {\n        let data = create_test_data();\n        let scaler = MinMaxScaler::\u003cCpuBackend\u003e::new().with_range(-1.0, 1.0);\n        let fitted = scaler.fit(\u0026data).unwrap();\n\n        let transformed = fitted.transform(\u0026data).unwrap();\n        let values = transformed.ravel().to_vec();\n\n        // First column: [0, 0, 1] -\u003e [-1, -1, 1]\n        assert!((values[0] - (-1.0)).abs() \u003c 1e-6);\n        assert!((values[2] - (-1.0)).abs() \u003c 1e-6);\n        assert!((values[4] - 1.0).abs() \u003c 1e-6);\n\n        // Second column: [1, 1, 3] -\u003e [-1, -1, 1]\n        assert!((values[1] - (-1.0)).abs() \u003c 1e-6);\n        assert!((values[3] - (-1.0)).abs() \u003c 1e-6);\n        assert!((values[5] - 1.0).abs() \u003c 1e-6);\n    }\n\n    #[test]\n    fn test_minmax_scaler_serialization() {\n        let data = create_test_data();\n        let scaler = MinMaxScaler::\u003cCpuBackend\u003e::new();\n        let fitted = scaler.fit(\u0026data).unwrap();\n\n        let params = fitted.extract_params();\n        let restored = FittedMinMaxScaler::\u003cCpuBackend\u003e::from_params(params).unwrap();\n\n        let transformed1 = fitted.transform(\u0026data).unwrap();\n        let transformed2 = restored.transform(\u0026data).unwrap();\n\n        let t1 = transformed1.ravel().to_vec();\n        let t2 = transformed2.ravel().to_vec();\n\n        for (i, (a, b)) in t1.iter().zip(t2.iter()).enumerate() {\n            assert!(\n                (a - b).abs() \u003c 1e-6,\n                \"Mismatch at index {}: {} vs {}\",\n                i,\n                a,\n                b\n            );\n        }\n    }\n\n    #[test]\n    fn test_minmax_scaler_feature_mismatch() {\n        let data = create_test_data(); // 2 features\n        let scaler = MinMaxScaler::\u003cCpuBackend\u003e::new();\n        let fitted = scaler.fit(\u0026data).unwrap();\n\n        let wrong_data = Tensor2D::\u003cCpuBackend\u003e::new(vec![1.0f32, 2.0, 3.0], 1, 3); // 3 features\n        let result = fitted.transform(\u0026wrong_data);\n\n        assert!(matches!(\n            result,\n            Err(PreprocessingError::FeatureMismatch {\n                expected_features: 2,\n                got_features: 3\n            })\n        ));\n    }\n}\n","traces":[{"line":38,"address":[1949872],"length":1,"stats":{"Line":1}},{"line":68,"address":[],"length":0,"stats":{"Line":0}},{"line":69,"address":[],"length":0,"stats":{"Line":0}},{"line":75,"address":[1851744],"length":1,"stats":{"Line":1}},{"line":77,"address":[1851745],"length":1,"stats":{"Line":1}},{"line":83,"address":[1851616],"length":1,"stats":{"Line":1}},{"line":84,"address":[1851656],"length":1,"stats":{"Line":1}},{"line":85,"address":[1851710],"length":1,"stats":{"Line":1}},{"line":86,"address":[1851716],"length":1,"stats":{"Line":1}},{"line":87,"address":[1851722],"length":1,"stats":{"Line":1}},{"line":97,"address":[1843157,1841584,1843115],"length":1,"stats":{"Line":1}},{"line":98,"address":[1841650],"length":1,"stats":{"Line":1}},{"line":100,"address":[1841695],"length":1,"stats":{"Line":1}},{"line":101,"address":[1841739],"length":1,"stats":{"Line":0}},{"line":102,"address":[1841705],"length":1,"stats":{"Line":0}},{"line":106,"address":[1841858],"length":1,"stats":{"Line":1}},{"line":107,"address":[1841895,1841959],"length":1,"stats":{"Line":3}},{"line":109,"address":[1841983],"length":1,"stats":{"Line":3}},{"line":110,"address":[1842044],"length":1,"stats":{"Line":2}},{"line":113,"address":[1842096],"length":1,"stats":{"Line":3}},{"line":114,"address":[1842126],"length":1,"stats":{"Line":2}},{"line":116,"address":[1842253],"length":1,"stats":{"Line":2}},{"line":117,"address":[1842356,1843232,1843252],"length":1,"stats":{"Line":6}},{"line":118,"address":[1843272],"length":1,"stats":{"Line":3}},{"line":119,"address":[1843329,1843288],"length":1,"stats":{"Line":4}},{"line":120,"address":[1843331],"length":1,"stats":{"Line":0}},{"line":122,"address":[1843312],"length":1,"stats":{"Line":2}},{"line":128,"address":[1843210,1842461,1843200,1842394],"length":1,"stats":{"Line":9}},{"line":137,"address":[1842856],"length":1,"stats":{"Line":3}},{"line":138,"address":[1842665],"length":1,"stats":{"Line":3}},{"line":139,"address":[1842752],"length":1,"stats":{"Line":2}},{"line":140,"address":[1842784],"length":1,"stats":{"Line":3}},{"line":141,"address":[1842824],"length":1,"stats":{"Line":2}},{"line":142,"address":[],"length":0,"stats":{"Line":0}},{"line":143,"address":[],"length":0,"stats":{"Line":0}},{"line":147,"address":[],"length":0,"stats":{"Line":0}},{"line":148,"address":[],"length":0,"stats":{"Line":0}},{"line":149,"address":[],"length":0,"stats":{"Line":0}},{"line":166,"address":[1851760],"length":1,"stats":{"Line":1}},{"line":167,"address":[],"length":0,"stats":{"Line":0}},{"line":171,"address":[1851776],"length":1,"stats":{"Line":1}},{"line":172,"address":[1851784],"length":1,"stats":{"Line":1}},{"line":176,"address":[],"length":0,"stats":{"Line":0}},{"line":178,"address":[],"length":0,"stats":{"Line":0}},{"line":182,"address":[],"length":0,"stats":{"Line":0}},{"line":193,"address":[1845793,1845799,1845312],"length":1,"stats":{"Line":3}},{"line":194,"address":[1845363],"length":1,"stats":{"Line":2}},{"line":196,"address":[1845386],"length":1,"stats":{"Line":3}},{"line":197,"address":[1845465],"length":1,"stats":{"Line":1}},{"line":198,"address":[1845461],"length":1,"stats":{"Line":1}},{"line":199,"address":[],"length":0,"stats":{"Line":0}},{"line":205,"address":[1845412],"length":1,"stats":{"Line":2}},{"line":206,"address":[1845427],"length":1,"stats":{"Line":3}},{"line":207,"address":[1845613,1845552],"length":1,"stats":{"Line":5}},{"line":209,"address":[1845658],"length":1,"stats":{"Line":2}},{"line":210,"address":[],"length":0,"stats":{"Line":0}},{"line":211,"address":[],"length":0,"stats":{"Line":0}},{"line":215,"address":[1844816,1845298,1845292],"length":1,"stats":{"Line":1}},{"line":216,"address":[1844867],"length":1,"stats":{"Line":1}},{"line":218,"address":[1844890],"length":1,"stats":{"Line":1}},{"line":219,"address":[1845005],"length":1,"stats":{"Line":0}},{"line":220,"address":[1845001],"length":1,"stats":{"Line":0}},{"line":221,"address":[],"length":0,"stats":{"Line":0}},{"line":226,"address":[1844901],"length":1,"stats":{"Line":1}},{"line":227,"address":[1844967],"length":1,"stats":{"Line":1}},{"line":228,"address":[1845108],"length":1,"stats":{"Line":1}},{"line":230,"address":[1845157],"length":1,"stats":{"Line":1}},{"line":231,"address":[],"length":0,"stats":{"Line":0}},{"line":232,"address":[],"length":0,"stats":{"Line":0}},{"line":236,"address":[1844480,1844789,1844795],"length":1,"stats":{"Line":1}},{"line":238,"address":[1844514],"length":1,"stats":{"Line":1}},{"line":239,"address":[1844545],"length":1,"stats":{"Line":1}},{"line":240,"address":[1844555],"length":1,"stats":{"Line":1}},{"line":241,"address":[1844610],"length":1,"stats":{"Line":1}},{"line":242,"address":[1844683],"length":1,"stats":{"Line":1}},{"line":246,"address":[1844344,1843360],"length":1,"stats":{"Line":1}},{"line":248,"address":[1844400,1844410,1843481,1843402],"length":1,"stats":{"Line":4}},{"line":251,"address":[1843619,1844368,1843690,1844378],"length":1,"stats":{"Line":4}},{"line":253,"address":[1843790,1844432,1843861,1844442],"length":1,"stats":{"Line":4}},{"line":257,"address":[1844127],"length":1,"stats":{"Line":1}},{"line":258,"address":[1844017],"length":1,"stats":{"Line":1}},{"line":259,"address":[1844027],"length":1,"stats":{"Line":1}},{"line":260,"address":[1844075],"length":1,"stats":{"Line":1}},{"line":261,"address":[],"length":0,"stats":{"Line":0}},{"line":262,"address":[1844123],"length":1,"stats":{"Line":1}},{"line":263,"address":[],"length":0,"stats":{"Line":0}},{"line":267,"address":[1844464],"length":1,"stats":{"Line":0}},{"line":268,"address":[1844469],"length":1,"stats":{"Line":0}},{"line":271,"address":[],"length":0,"stats":{"Line":0}},{"line":272,"address":[],"length":0,"stats":{"Line":0}},{"line":273,"address":[],"length":0,"stats":{"Line":0}},{"line":274,"address":[],"length":0,"stats":{"Line":0}},{"line":281,"address":[],"length":0,"stats":{"Line":0}},{"line":282,"address":[],"length":0,"stats":{"Line":0}},{"line":283,"address":[],"length":0,"stats":{"Line":0}},{"line":284,"address":[],"length":0,"stats":{"Line":0}}],"covered":62,"coverable":96},{"path":["/","home","victor","ML","machinelearne-rs","lib","src","preprocessing","scaling","mod.rs"],"content":"//! Scaling transformers for feature normalization.\n//!\n//! This module provides transformers that scale features to a common range\n//! or distribution, which is essential for many machine learning algorithms.\n//!\n//! # Available Transformers\n//!\n//! | Transformer | Description | Use Case |\n//! |-------------|-------------|----------|\n//! | [`StandardScaler`] | Z-score normalization (mean=0, std=1) | Default choice for most algorithms |\n//! | [`MinMaxScaler`] | Scale to [0, 1] or custom range | When bounded output is needed |\n//! | [`RobustScaler`] | Use median and IQR | Data with outliers |\n//! | [`MaxAbsScaler`] | Scale by max absolute value | Sparse data |\n//! | [`Normalizer`] | Scale individual samples to unit norm | Text classification, clustering |\n//!\n//! # Example\n//!\n//! ```ignore\n//! use machinelearne_rs::preprocessing::scaling::StandardScaler;\n//! use machinelearne_rs::preprocessing::Transformer;\n//! use machinelearne_rs::backend::CpuBackend;\n//!\n//! let scaler = StandardScaler::\u003cCpuBackend\u003e::new();\n//! let fitted = scaler.fit(\u0026data)?;\n//! let scaled = fitted.transform(\u0026new_data)?;\n//! ```\n\npub mod maxabs;\npub mod minmax;\npub mod normalizer;\npub mod robust;\npub mod standard;\n\npub use maxabs::{FittedMaxAbsScaler, MaxAbsScaler, MaxAbsScalerParams};\npub use minmax::{FittedMinMaxScaler, MinMaxScaler, MinMaxScalerConfig, MinMaxScalerParams};\npub use normalizer::{FittedNormalizer, NormType, Normalizer, NormalizerParams};\npub use robust::{FittedRobustScaler, RobustScaler, RobustScalerConfig, RobustScalerParams};\npub use standard::{\n    FittedStandardScaler, StandardScaler, StandardScalerConfig, StandardScalerParams,\n};\n","traces":[],"covered":0,"coverable":0},{"path":["/","home","victor","ML","machinelearne-rs","lib","src","preprocessing","scaling","normalizer.rs"],"content":"//! Normalizer - scales individual samples to unit norm.\n//!\n//! Normalizes samples individually to unit norm. Each sample (row) is\n//! rescaled independently of other samples.\n//!\n//! Supports three norm types:\n//! - L1: Sum of absolute values = 1\n//! - L2 (default): Sum of squares = 1 (Euclidean norm)\n//! - Max: Maximum absolute value = 1\n//!\n//! # Example\n//! ```ignore\n//! use machinelearne_rs::preprocessing::{Transformer, Normalizer, NormType};\n//! use machinelearne_rs::backend::CpuBackend;\n//!\n//! let normalizer = Normalizer::\u003cCpuBackend\u003e::new(NormType::L2);\n//! let normalized = normalizer.fit_transform(\u0026data)?;\n//! ```\n\nuse crate::backend::{Backend, Tensor2D};\nuse crate::preprocessing::error::PreprocessingError;\nuse crate::preprocessing::traits::{FittedTransformer, Transformer};\nuse serde::{Deserialize, Serialize};\nuse std::marker::PhantomData;\n\n/// Type of normalization to apply.\n#[derive(Clone, Copy, Debug, Default, Serialize, Deserialize)]\npub enum NormType {\n    /// L1 norm: sum of absolute values = 1\n    L1,\n    /// L2 norm (Euclidean): sum of squares = 1\n    #[default]\n    L2,\n    /// Max norm: maximum absolute value = 1\n    Max,\n}\n\n/// Serializable parameters for a fitted Normalizer (trivial - just norm type).\n#[derive(Clone, Serialize, Deserialize)]\npub struct NormalizerParams {\n    /// The norm type used.\n    pub norm: NormType,\n    /// Number of features seen during fit.\n    pub n_features: usize,\n}\n\n/// Normalizer transformer.\n///\n/// Scales individual samples to unit norm.\n#[derive(Clone)]\npub struct Normalizer\u003cB: Backend\u003e {\n    norm: NormType,\n    _backend: PhantomData\u003cB\u003e,\n}\n\nimpl\u003cB: Backend\u003e Default for Normalizer\u003cB\u003e {\n    fn default() -\u003e Self {\n        Self::new(NormType::default())\n    }\n}\n\nimpl\u003cB: Backend\u003e Normalizer\u003cB\u003e {\n    /// Create a new Normalizer with the specified norm type.\n    pub fn new(norm: NormType) -\u003e Self {\n        Self {\n            norm,\n            _backend: PhantomData,\n        }\n    }\n}\n\nimpl\u003cB: Backend\u003e Transformer\u003cB\u003e for Normalizer\u003cB\u003e {\n    type Input = Tensor2D\u003cB\u003e;\n    type Output = Tensor2D\u003cB\u003e;\n    type Params = NormalizerParams;\n    type Fitted = FittedNormalizer\u003cB\u003e;\n\n    fn fit(\u0026self, data: \u0026Self::Input) -\u003e Result\u003cSelf::Fitted, PreprocessingError\u003e {\n        let (rows, cols) = data.shape();\n\n        if rows == 0 {\n            return Err(PreprocessingError::EmptyData(\n                \"Cannot fit Normalizer on empty data\".to_string(),\n            ));\n        }\n\n        Ok(FittedNormalizer {\n            norm: self.norm,\n            n_features: cols,\n            _backend: PhantomData,\n        })\n    }\n\n    fn fit_transform(\u0026self, data: \u0026Self::Input) -\u003e Result\u003cSelf::Output, PreprocessingError\u003e {\n        // Normalizer is stateless - just transform directly\n        let (rows, cols) = data.shape();\n\n        if rows == 0 {\n            return Err(PreprocessingError::EmptyData(\n                \"Cannot normalize empty data\".to_string(),\n            ));\n        }\n\n        let flat_data = B::to_vec_1d(\u0026B::ravel_2d(\u0026data.data));\n        let mut result = vec![0.0; rows * cols];\n\n        for row in 0..rows {\n            // Extract row data\n            let row_start = row * cols;\n            let row_data: Vec\u003cf64\u003e = flat_data[row_start..row_start + cols].to_vec();\n\n            // Compute norm\n            let norm = match self.norm {\n                NormType::L1 =\u003e row_data.iter().map(|x| x.abs()).sum::\u003cf64\u003e(),\n                NormType::L2 =\u003e (row_data.iter().map(|x| x * x).sum::\u003cf64\u003e()).sqrt(),\n                NormType::Max =\u003e row_data.iter().map(|x| x.abs()).fold(0.0_f64, f64::max),\n            };\n\n            // Normalize\n            let scale = if norm == 0.0 { 1.0 } else { 1.0 / norm };\n            for col in 0..cols {\n                result[row_start + col] = flat_data[row_start + col] * scale;\n            }\n        }\n\n        Ok(Tensor2D {\n            data: B::from_vec_2d(result.iter().map(|\u0026x| x as f32).collect(), rows, cols),\n            backend: PhantomData::\u003cB\u003e,\n        })\n    }\n}\n\n/// Fitted Normalizer ready for inference.\n#[derive(Clone)]\npub struct FittedNormalizer\u003cB: Backend\u003e {\n    norm: NormType,\n    n_features: usize,\n    _backend: PhantomData\u003cB\u003e,\n}\n\nimpl\u003cB: Backend\u003e FittedNormalizer\u003cB\u003e {\n    /// Get the norm type used.\n    pub fn norm(\u0026self) -\u003e NormType {\n        self.norm\n    }\n}\n\nimpl\u003cB: Backend\u003e FittedTransformer\u003cB\u003e for FittedNormalizer\u003cB\u003e {\n    type Input = Tensor2D\u003cB\u003e;\n    type Output = Tensor2D\u003cB\u003e;\n    type Params = NormalizerParams;\n\n    fn transform(\u0026self, data: \u0026Self::Input) -\u003e Result\u003cSelf::Output, PreprocessingError\u003e {\n        let (rows, cols) = data.shape();\n\n        if cols != self.n_features {\n            return Err(PreprocessingError::FeatureMismatch {\n                expected_features: self.n_features,\n                got_features: cols,\n            });\n        }\n\n        if rows == 0 {\n            return Ok(data.clone());\n        }\n\n        let flat_data = B::to_vec_1d(\u0026B::ravel_2d(\u0026data.data));\n        let mut result = vec![0.0; rows * cols];\n\n        for row in 0..rows {\n            let row_start = row * cols;\n            let row_data: Vec\u003cf64\u003e = flat_data[row_start..row_start + cols].to_vec();\n\n            let norm = match self.norm {\n                NormType::L1 =\u003e row_data.iter().map(|x| x.abs()).sum::\u003cf64\u003e(),\n                NormType::L2 =\u003e (row_data.iter().map(|x| x * x).sum::\u003cf64\u003e()).sqrt(),\n                NormType::Max =\u003e row_data.iter().map(|x| x.abs()).fold(0.0_f64, f64::max),\n            };\n\n            let scale = if norm == 0.0 { 1.0 } else { 1.0 / norm };\n            for col in 0..cols {\n                result[row_start + col] = flat_data[row_start + col] * scale;\n            }\n        }\n\n        Ok(Tensor2D {\n            data: B::from_vec_2d(result.iter().map(|\u0026x| x as f32).collect(), rows, cols),\n            backend: PhantomData::\u003cB\u003e,\n        })\n    }\n\n    fn inverse_transform(\u0026self, _data: \u0026Self::Output) -\u003e Result\u003cSelf::Input, PreprocessingError\u003e {\n        Err(PreprocessingError::InvalidParameter(\n            \"Normalizer does not support inverse_transform (norm information is lost)\".to_string(),\n        ))\n    }\n\n    fn extract_params(\u0026self) -\u003e Self::Params {\n        NormalizerParams {\n            norm: self.norm,\n            n_features: self.n_features,\n        }\n    }\n\n    fn from_params(params: Self::Params) -\u003e Result\u003cSelf, PreprocessingError\u003e {\n        Ok(Self {\n            norm: params.norm,\n            n_features: params.n_features,\n            _backend: PhantomData,\n        })\n    }\n\n    fn n_features_in(\u0026self) -\u003e usize {\n        self.n_features\n    }\n\n    fn save_to_file\u003cP: AsRef\u003cstd::path::Path\u003e\u003e(\u0026self, path: P) -\u003e std::io::Result\u003c()\u003e {\n        let params = self.extract_params();\n        let bytes = bincode::serialize(\u0026params).map_err(std::io::Error::other)?;\n        std::fs::write(path, bytes)\n    }\n\n    fn load_from_file\u003cP: AsRef\u003cstd::path::Path\u003e\u003e(path: P) -\u003e Result\u003cSelf, PreprocessingError\u003e\n    where\n        Self: Sized,\n    {\n        let bytes = std::fs::read(path)?;\n        let params: NormalizerParams = bincode::deserialize(\u0026bytes)\n            .map_err(|e| PreprocessingError::SerializationError(e.to_string()))?;\n        Self::from_params(params)\n    }\n}\n\n#[cfg(test)]\nmod tests {\n    use super::*;\n    use crate::backend::CpuBackend;\n\n    fn create_test_data() -\u003e Tensor2D\u003cCpuBackend\u003e {\n        // [[3, 4], [1, 0], [0, 0]]\n        // Row 0: L2 norm = 5, so normalized = [0.6, 0.8]\n        // Row 1: L2 norm = 1, so normalized = [1, 0]\n        // Row 2: L2 norm = 0, so stays [0, 0]\n        Tensor2D::new(vec![3.0f32, 4.0, 1.0, 0.0, 0.0, 0.0], 3, 2)\n    }\n\n    #[test]\n    fn test_normalizer_l2() {\n        let data = create_test_data();\n        let normalizer = Normalizer::\u003cCpuBackend\u003e::new(NormType::L2);\n        let normalized = normalizer.fit_transform(\u0026data).unwrap();\n        let values = normalized.ravel().to_vec();\n\n        // Row 0: [3, 4] / 5 = [0.6, 0.8]\n        assert!((values[0] - 0.6).abs() \u003c 1e-6);\n        assert!((values[1] - 0.8).abs() \u003c 1e-6);\n\n        // Row 1: [1, 0] / 1 = [1, 0]\n        assert!((values[2] - 1.0).abs() \u003c 1e-6);\n        assert!((values[3] - 0.0).abs() \u003c 1e-6);\n\n        // Row 2: [0, 0] / 1 (zero case) = [0, 0]\n        assert!((values[4] - 0.0).abs() \u003c 1e-6);\n        assert!((values[5] - 0.0).abs() \u003c 1e-6);\n    }\n\n    #[test]\n    fn test_normalizer_l1() {\n        let data = create_test_data();\n        let normalizer = Normalizer::\u003cCpuBackend\u003e::new(NormType::L1);\n        let normalized = normalizer.fit_transform(\u0026data).unwrap();\n        let values = normalized.ravel().to_vec();\n\n        // Row 0: [3, 4] / 7 = [0.428..., 0.571...]\n        assert!((values[0] - 3.0 / 7.0).abs() \u003c 1e-6);\n        assert!((values[1] - 4.0 / 7.0).abs() \u003c 1e-6);\n\n        // Row 1: [1, 0] / 1 = [1, 0]\n        assert!((values[2] - 1.0).abs() \u003c 1e-6);\n        assert!((values[3] - 0.0).abs() \u003c 1e-6);\n    }\n\n    #[test]\n    fn test_normalizer_max() {\n        let data = create_test_data();\n        let normalizer = Normalizer::\u003cCpuBackend\u003e::new(NormType::Max);\n        let normalized = normalizer.fit_transform(\u0026data).unwrap();\n        let values = normalized.ravel().to_vec();\n\n        // Row 0: [3, 4] / 4 = [0.75, 1]\n        assert!((values[0] - 0.75).abs() \u003c 1e-6);\n        assert!((values[1] - 1.0).abs() \u003c 1e-6);\n\n        // Row 1: [1, 0] / 1 = [1, 0]\n        assert!((values[2] - 1.0).abs() \u003c 1e-6);\n        assert!((values[3] - 0.0).abs() \u003c 1e-6);\n    }\n\n    #[test]\n    fn test_normalizer_transform() {\n        let data = create_test_data();\n        let normalizer = Normalizer::\u003cCpuBackend\u003e::new(NormType::L2);\n        let fitted = normalizer.fit(\u0026data).unwrap();\n\n        let normalized = fitted.transform(\u0026data).unwrap();\n        let values = normalized.ravel().to_vec();\n\n        // Same as fit_transform\n        assert!((values[0] - 0.6).abs() \u003c 1e-6);\n        assert!((values[1] - 0.8).abs() \u003c 1e-6);\n    }\n\n    #[test]\n    fn test_normalizer_inverse_not_supported() {\n        let data = create_test_data();\n        let normalizer = Normalizer::\u003cCpuBackend\u003e::new(NormType::L2);\n        let fitted = normalizer.fit(\u0026data).unwrap();\n\n        let normalized = fitted.transform(\u0026data).unwrap();\n        let result = fitted.inverse_transform(\u0026normalized);\n\n        assert!(matches!(\n            result,\n            Err(PreprocessingError::InvalidParameter(_))\n        ));\n    }\n\n    #[test]\n    fn test_normalizer_serialization() {\n        let data = create_test_data();\n        let normalizer = Normalizer::\u003cCpuBackend\u003e::new(NormType::L2);\n        let fitted = normalizer.fit(\u0026data).unwrap();\n\n        let params = fitted.extract_params();\n        let restored = FittedNormalizer::\u003cCpuBackend\u003e::from_params(params).unwrap();\n\n        let normalized1 = fitted.transform(\u0026data).unwrap();\n        let normalized2 = restored.transform(\u0026data).unwrap();\n\n        let n1 = normalized1.ravel().to_vec();\n        let n2 = normalized2.ravel().to_vec();\n\n        for (a, b) in n1.iter().zip(n2.iter()) {\n            assert!((a - b).abs() \u003c 1e-6);\n        }\n    }\n\n    #[test]\n    fn test_normalizer_feature_mismatch() {\n        let data = create_test_data(); // 2 features\n        let normalizer = Normalizer::\u003cCpuBackend\u003e::new(NormType::L2);\n        let fitted = normalizer.fit(\u0026data).unwrap();\n\n        let wrong_data = Tensor2D::\u003cCpuBackend\u003e::new(vec![1.0f32, 2.0, 3.0], 1, 3); // 3 features\n        let result = fitted.transform(\u0026wrong_data);\n\n        assert!(matches!(\n            result,\n            Err(PreprocessingError::FeatureMismatch {\n                expected_features: 2,\n                got_features: 3\n            })\n        ));\n    }\n}\n","traces":[{"line":57,"address":[],"length":0,"stats":{"Line":0}},{"line":58,"address":[],"length":0,"stats":{"Line":0}},{"line":64,"address":[1763280],"length":1,"stats":{"Line":1}},{"line":78,"address":[1760352],"length":1,"stats":{"Line":1}},{"line":79,"address":[1760400],"length":1,"stats":{"Line":1}},{"line":81,"address":[1760426],"length":1,"stats":{"Line":1}},{"line":82,"address":[1760460],"length":1,"stats":{"Line":0}},{"line":83,"address":[1760432],"length":1,"stats":{"Line":0}},{"line":87,"address":[1760539],"length":1,"stats":{"Line":1}},{"line":88,"address":[1760537],"length":1,"stats":{"Line":1}},{"line":89,"address":[],"length":0,"stats":{"Line":0}},{"line":90,"address":[],"length":0,"stats":{"Line":0}},{"line":94,"address":[1760182,1760176,1757920],"length":1,"stats":{"Line":1}},{"line":96,"address":[1757986],"length":1,"stats":{"Line":2}},{"line":98,"address":[1758023],"length":1,"stats":{"Line":2}},{"line":99,"address":[1758063],"length":1,"stats":{"Line":0}},{"line":100,"address":[1758029],"length":1,"stats":{"Line":0}},{"line":104,"address":[1758173,1758225],"length":1,"stats":{"Line":2}},{"line":105,"address":[1758328],"length":1,"stats":{"Line":2}},{"line":107,"address":[1758492,1758405],"length":1,"stats":{"Line":3}},{"line":109,"address":[1758613,1758971,1759000],"length":1,"stats":{"Line":3}},{"line":110,"address":[1758979,1759037],"length":1,"stats":{"Line":3}},{"line":113,"address":[1759135],"length":1,"stats":{"Line":1}},{"line":114,"address":[1760240,1759181,1760254,1759328],"length":1,"stats":{"Line":4}},{"line":115,"address":[1759489,1760286,1760272,1759212],"length":1,"stats":{"Line":4}},{"line":116,"address":[1760222,1760208,1759246,1759647],"length":1,"stats":{"Line":4}},{"line":120,"address":[1759743,1759440],"length":1,"stats":{"Line":3}},{"line":121,"address":[1760171,1759796],"length":1,"stats":{"Line":3}},{"line":122,"address":[1759986,1759945],"length":1,"stats":{"Line":3}},{"line":126,"address":[1758825],"length":1,"stats":{"Line":1}},{"line":127,"address":[1760330,1758646,1760320],"length":1,"stats":{"Line":5}},{"line":128,"address":[],"length":0,"stats":{"Line":0}},{"line":143,"address":[],"length":0,"stats":{"Line":0}},{"line":144,"address":[],"length":0,"stats":{"Line":0}},{"line":153,"address":[1760784,1763102,1763108],"length":1,"stats":{"Line":1}},{"line":154,"address":[1760850],"length":1,"stats":{"Line":1}},{"line":156,"address":[1760895],"length":1,"stats":{"Line":1}},{"line":157,"address":[1760946],"length":1,"stats":{"Line":1}},{"line":158,"address":[1760943],"length":1,"stats":{"Line":1}},{"line":159,"address":[],"length":0,"stats":{"Line":0}},{"line":163,"address":[1760908],"length":1,"stats":{"Line":1}},{"line":164,"address":[1761008],"length":1,"stats":{"Line":0}},{"line":167,"address":[1761098,1761150],"length":1,"stats":{"Line":1}},{"line":168,"address":[1761253],"length":1,"stats":{"Line":1}},{"line":170,"address":[1761417,1761330],"length":1,"stats":{"Line":2}},{"line":171,"address":[1761538,1761925,1761896],"length":1,"stats":{"Line":2}},{"line":172,"address":[1761962,1761904],"length":1,"stats":{"Line":2}},{"line":174,"address":[1762060],"length":1,"stats":{"Line":2}},{"line":175,"address":[1762254,1762107,1763168,1763182],"length":1,"stats":{"Line":0}},{"line":176,"address":[1762138,1763200,1762415,1763214],"length":1,"stats":{"Line":7}},{"line":177,"address":[1762573,1763136,1762172,1763150],"length":1,"stats":{"Line":0}},{"line":180,"address":[1762669,1762366],"length":1,"stats":{"Line":4}},{"line":181,"address":[1763097,1762722],"length":1,"stats":{"Line":4}},{"line":182,"address":[1762912,1762871],"length":1,"stats":{"Line":3}},{"line":186,"address":[1761750],"length":1,"stats":{"Line":4}},{"line":187,"address":[1763258,1763248,1761571],"length":1,"stats":{"Line":7}},{"line":188,"address":[],"length":0,"stats":{"Line":0}},{"line":192,"address":[1760656],"length":1,"stats":{"Line":1}},{"line":193,"address":[1760706],"length":1,"stats":{"Line":1}},{"line":194,"address":[1760679],"length":1,"stats":{"Line":1}},{"line":198,"address":[1760640],"length":1,"stats":{"Line":1}},{"line":200,"address":[1760645],"length":1,"stats":{"Line":1}},{"line":201,"address":[1760648],"length":1,"stats":{"Line":1}},{"line":205,"address":[1760576],"length":1,"stats":{"Line":1}},{"line":206,"address":[1760590],"length":1,"stats":{"Line":1}},{"line":207,"address":[],"length":0,"stats":{"Line":0}},{"line":208,"address":[],"length":0,"stats":{"Line":0}},{"line":209,"address":[],"length":0,"stats":{"Line":0}},{"line":213,"address":[1760624],"length":1,"stats":{"Line":0}},{"line":214,"address":[1760629],"length":1,"stats":{"Line":0}},{"line":217,"address":[],"length":0,"stats":{"Line":0}},{"line":218,"address":[],"length":0,"stats":{"Line":0}},{"line":219,"address":[],"length":0,"stats":{"Line":0}},{"line":220,"address":[],"length":0,"stats":{"Line":0}},{"line":227,"address":[],"length":0,"stats":{"Line":0}},{"line":228,"address":[],"length":0,"stats":{"Line":0}},{"line":229,"address":[],"length":0,"stats":{"Line":0}},{"line":230,"address":[],"length":0,"stats":{"Line":0}}],"covered":49,"coverable":78},{"path":["/","home","victor","ML","machinelearne-rs","lib","src","preprocessing","scaling","robust.rs"],"content":"//! Robust Scaler.\n//!\n//! Scales features using statistics that are robust to outliers.\n//! Uses median and interquartile range (IQR) instead of mean and std.\n//!\n//! The transformation is:\n//! ```text\n//! X_scaled = (X - median) / IQR\n//! ```\n//!\n//! where IQR is the range between the 1st quartile (25%) and 3rd quartile (75%).\n//!\n//! # Example\n//! ```ignore\n//! use machinelearne_rs::preprocessing::{Transformer, RobustScaler};\n//! use machinelearne_rs::backend::CpuBackend;\n//!\n//! let scaler = RobustScaler::\u003cCpuBackend\u003e::new()\n//!     .with_centering(true);\n//!\n//! let fitted = scaler.fit(\u0026data)?;\n//! let scaled = fitted.transform(\u0026data)?;\n//! ```\n\nuse crate::backend::{Backend, Tensor1D, Tensor2D};\nuse crate::preprocessing::error::PreprocessingError;\nuse crate::preprocessing::traits::{FittedTransformer, Transformer};\nuse serde::{Deserialize, Serialize};\nuse std::marker::PhantomData;\n\n/// Configuration for RobustScaler.\n#[derive(Clone, Serialize, Deserialize)]\npub struct RobustScalerConfig {\n    /// If True, center the data by median before scaling.\n    pub with_centering: bool,\n    /// If True, scale the data by IQR.\n    pub with_scaling: bool,\n    /// Quantile range for IQR (default: (25.0, 75.0)).\n    pub quantile_range: (f64, f64),\n}\n\nimpl Default for RobustScalerConfig {\n    fn default() -\u003e Self {\n        Self {\n            with_centering: true,\n            with_scaling: true,\n            quantile_range: (25.0, 75.0),\n        }\n    }\n}\n\n/// Serializable parameters for a fitted RobustScaler.\n#[derive(Clone, Serialize, Deserialize)]\npub struct RobustScalerParams {\n    /// Configuration options.\n    pub config: RobustScalerConfig,\n    /// Center (median) for each feature.\n    pub center_: Vec\u003cf64\u003e,\n    /// Scale (IQR) for each feature.\n    pub scale_: Vec\u003cf64\u003e,\n    /// Number of features seen during fit.\n    pub n_features: usize,\n}\n\n/// RobustScaler transformer (unfitted).\n///\n/// Scales features using statistics that are robust to outliers.\n#[derive(Clone)]\npub struct RobustScaler\u003cB: Backend\u003e {\n    config: RobustScalerConfig,\n    _backend: PhantomData\u003cB\u003e,\n}\n\nimpl\u003cB: Backend\u003e Default for RobustScaler\u003cB\u003e {\n    fn default() -\u003e Self {\n        Self::new()\n    }\n}\n\nimpl\u003cB: Backend\u003e RobustScaler\u003cB\u003e {\n    /// Create a new RobustScaler with default configuration.\n    pub fn new() -\u003e Self {\n        Self {\n            config: RobustScalerConfig::default(),\n            _backend: PhantomData,\n        }\n    }\n\n    /// Set whether to center data by median.\n    pub fn with_centering(mut self, with_centering: bool) -\u003e Self {\n        self.config.with_centering = with_centering;\n        self\n    }\n\n    /// Set whether to scale data by IQR.\n    pub fn with_scaling(mut self, with_scaling: bool) -\u003e Self {\n        self.config.with_scaling = with_scaling;\n        self\n    }\n\n    /// Set the quantile range for IQR calculation.\n    pub fn with_quantile_range(mut self, min: f64, max: f64) -\u003e Self {\n        assert!(\n            (0.0..=100.0).contains(\u0026min) \u0026\u0026 (0.0..=100.0).contains(\u0026max) \u0026\u0026 min \u003c max,\n            \"Invalid quantile range: must be 0 \u003c= min \u003c max \u003c= 100\"\n        );\n        self.config.quantile_range = (min, max);\n        self\n    }\n}\n\n/// Compute quantiles for a column of data.\nfn compute_quantiles(data: \u0026[f64], q_low: f64, q_high: f64) -\u003e (f64, f64) {\n    let mut sorted: Vec\u003cf64\u003e = data.to_vec();\n    sorted.sort_by(|a, b| a.partial_cmp(b).unwrap_or(std::cmp::Ordering::Equal));\n\n    let n = sorted.len();\n    if n == 0 {\n        return (0.0, 1.0);\n    }\n\n    // Linear interpolation for quantiles\n    let low_idx = (q_low / 100.0 * (n - 1) as f64).min((n - 1) as f64);\n    let high_idx = (q_high / 100.0 * (n - 1) as f64).min((n - 1) as f64);\n\n    let low_val = interpolate(\u0026sorted, low_idx);\n    let high_val = interpolate(\u0026sorted, high_idx);\n\n    (low_val, high_val)\n}\n\n/// Linear interpolation at a fractional index.\nfn interpolate(sorted: \u0026[f64], idx: f64) -\u003e f64 {\n    let lower = idx.floor() as usize;\n    let upper = (lower + 1).min(sorted.len() - 1);\n    let frac = idx - lower as f64;\n\n    sorted[lower] * (1.0 - frac) + sorted[upper] * frac\n}\n\nimpl\u003cB: Backend\u003e Transformer\u003cB\u003e for RobustScaler\u003cB\u003e {\n    type Input = Tensor2D\u003cB\u003e;\n    type Output = Tensor2D\u003cB\u003e;\n    type Params = RobustScalerParams;\n    type Fitted = FittedRobustScaler\u003cB\u003e;\n\n    fn fit(\u0026self, data: \u0026Self::Input) -\u003e Result\u003cSelf::Fitted, PreprocessingError\u003e {\n        let (rows, cols) = data.shape();\n\n        if rows == 0 {\n            return Err(PreprocessingError::EmptyData(\n                \"Cannot fit RobustScaler on empty data\".to_string(),\n            ));\n        }\n\n        // Get the raw data\n        let flat_data = B::to_vec_1d(\u0026B::ravel_2d(\u0026data.data));\n\n        // Compute center (median) and scale (IQR) for each column\n        let mut center_vals = vec![0.0; cols];\n        let mut scale_vals = vec![1.0; cols];\n\n        let (q_low, q_high) = self.config.quantile_range;\n\n        for col in 0..cols {\n            // Extract column data\n            let column_data: Vec\u003cf64\u003e = (0..rows).map(|row| flat_data[row * cols + col]).collect();\n\n            // Compute quantiles\n            let (low, high) = compute_quantiles(\u0026column_data, q_low, q_high);\n\n            if self.config.with_centering {\n                // Median is the midpoint of the IQR\n                center_vals[col] = (low + high) / 2.0;\n            }\n\n            if self.config.with_scaling {\n                let iqr = high - low;\n                scale_vals[col] = if iqr == 0.0 { 1.0 } else { iqr };\n            }\n        }\n\n        let center_ = Tensor1D {\n            data: B::from_vec_1d(center_vals.iter().map(|\u0026x| x as f32).collect()),\n            backend: PhantomData::\u003cB\u003e,\n        };\n\n        let scale_ = Tensor1D {\n            data: B::from_vec_1d(scale_vals.iter().map(|\u0026x| x as f32).collect()),\n            backend: PhantomData::\u003cB\u003e,\n        };\n\n        Ok(FittedRobustScaler {\n            config: self.config.clone(),\n            center_,\n            scale_,\n            n_features: cols,\n            _backend: PhantomData,\n        })\n    }\n\n    fn fit_transform(\u0026self, data: \u0026Self::Input) -\u003e Result\u003cSelf::Output, PreprocessingError\u003e {\n        let fitted = self.fit(data)?;\n        fitted.transform(data)\n    }\n}\n\n/// Fitted RobustScaler ready for inference.\n#[derive(Clone)]\npub struct FittedRobustScaler\u003cB: Backend\u003e {\n    config: RobustScalerConfig,\n    center_: Tensor1D\u003cB\u003e,\n    scale_: Tensor1D\u003cB\u003e,\n    n_features: usize,\n    _backend: PhantomData\u003cB\u003e,\n}\n\nimpl\u003cB: Backend\u003e FittedRobustScaler\u003cB\u003e {\n    /// Get the center (median) values for each feature.\n    pub fn center(\u0026self) -\u003e \u0026Tensor1D\u003cB\u003e {\n        \u0026self.center_\n    }\n\n    /// Get the scale (IQR) values for each feature.\n    pub fn scale(\u0026self) -\u003e \u0026Tensor1D\u003cB\u003e {\n        \u0026self.scale_\n    }\n}\n\nimpl\u003cB: Backend\u003e FittedTransformer\u003cB\u003e for FittedRobustScaler\u003cB\u003e {\n    type Input = Tensor2D\u003cB\u003e;\n    type Output = Tensor2D\u003cB\u003e;\n    type Params = RobustScalerParams;\n\n    fn transform(\u0026self, data: \u0026Self::Input) -\u003e Result\u003cSelf::Output, PreprocessingError\u003e {\n        let (_, cols) = data.shape();\n\n        if cols != self.n_features {\n            return Err(PreprocessingError::FeatureMismatch {\n                expected_features: self.n_features,\n                got_features: cols,\n            });\n        }\n\n        let mut result_data = data.data.clone();\n\n        if self.config.with_centering {\n            result_data = B::broadcast_sub_1d_to_2d_rows(\u0026result_data, \u0026self.center_.data);\n        }\n\n        if self.config.with_scaling {\n            result_data = B::broadcast_div_1d_to_2d_rows(\u0026result_data, \u0026self.scale_.data);\n        }\n\n        Ok(Tensor2D {\n            data: result_data,\n            backend: PhantomData::\u003cB\u003e,\n        })\n    }\n\n    fn inverse_transform(\u0026self, data: \u0026Self::Output) -\u003e Result\u003cSelf::Input, PreprocessingError\u003e {\n        let (_, cols) = data.shape();\n\n        if cols != self.n_features {\n            return Err(PreprocessingError::FeatureMismatch {\n                expected_features: self.n_features,\n                got_features: cols,\n            });\n        }\n\n        let mut result_data = data.data.clone();\n\n        if self.config.with_scaling {\n            result_data = B::broadcast_mul_1d_to_2d_rows(\u0026result_data, \u0026self.scale_.data);\n        }\n\n        if self.config.with_centering {\n            result_data = B::broadcast_add_1d_to_2d_rows(\u0026result_data, \u0026self.center_.data);\n        }\n\n        Ok(Tensor2D {\n            data: result_data,\n            backend: PhantomData::\u003cB\u003e,\n        })\n    }\n\n    fn extract_params(\u0026self) -\u003e Self::Params {\n        RobustScalerParams {\n            config: self.config.clone(),\n            center_: self.center_.to_vec(),\n            scale_: self.scale_.to_vec(),\n            n_features: self.n_features,\n        }\n    }\n\n    fn from_params(params: Self::Params) -\u003e Result\u003cSelf, PreprocessingError\u003e {\n        let center_ = Tensor1D {\n            data: B::from_vec_1d(params.center_.iter().map(|\u0026x| x as f32).collect()),\n            backend: PhantomData::\u003cB\u003e,\n        };\n        let scale_ = Tensor1D {\n            data: B::from_vec_1d(params.scale_.iter().map(|\u0026x| x as f32).collect()),\n            backend: PhantomData::\u003cB\u003e,\n        };\n\n        Ok(Self {\n            config: params.config,\n            center_,\n            scale_,\n            n_features: params.n_features,\n            _backend: PhantomData,\n        })\n    }\n\n    fn n_features_in(\u0026self) -\u003e usize {\n        self.n_features\n    }\n\n    fn save_to_file\u003cP: AsRef\u003cstd::path::Path\u003e\u003e(\u0026self, path: P) -\u003e std::io::Result\u003c()\u003e {\n        let params = self.extract_params();\n        let bytes = bincode::serialize(\u0026params).map_err(std::io::Error::other)?;\n        std::fs::write(path, bytes)\n    }\n\n    fn load_from_file\u003cP: AsRef\u003cstd::path::Path\u003e\u003e(path: P) -\u003e Result\u003cSelf, PreprocessingError\u003e\n    where\n        Self: Sized,\n    {\n        let bytes = std::fs::read(path)?;\n        let params: RobustScalerParams = bincode::deserialize(\u0026bytes)\n            .map_err(|e| PreprocessingError::SerializationError(e.to_string()))?;\n        Self::from_params(params)\n    }\n}\n\n#[cfg(test)]\nmod tests {\n    use super::*;\n    use crate::backend::CpuBackend;\n\n    fn create_test_data() -\u003e Tensor2D\u003cCpuBackend\u003e {\n        // [[1, 2], [2, 4], [3, 6], [4, 8], [5, 100]]  -- second column has outlier\n        Tensor2D::new(\n            vec![1.0f32, 2.0, 2.0, 4.0, 3.0, 6.0, 4.0, 8.0, 5.0, 100.0],\n            5,\n            2,\n        )\n    }\n\n    #[test]\n    fn test_robust_scaler_fit() {\n        let data = create_test_data();\n        let scaler = RobustScaler::\u003cCpuBackend\u003e::new();\n        let fitted = scaler.fit(\u0026data).unwrap();\n\n        let center = fitted.center().to_vec();\n        let scale = fitted.scale().to_vec();\n\n        // First column: [1, 2, 3, 4, 5], median = 3, Q1 = 2, Q3 = 4, IQR = 2\n        assert!((center[0] - 3.0).abs() \u003c 0.1);\n        assert!((scale[0] - 2.0).abs() \u003c 0.1);\n\n        // Second column: [2, 4, 6, 8, 100], median = 6, Q1 = 4, Q3 = 8, IQR = 4\n        // (outlier 100 doesn't significantly affect median/IQR)\n        assert!((center[1] - 6.0).abs() \u003c 0.1);\n        assert!((scale[1] - 4.0).abs() \u003c 0.1);\n    }\n\n    #[test]\n    fn test_robust_scaler_transform() {\n        let data = create_test_data();\n        let scaler = RobustScaler::\u003cCpuBackend\u003e::new();\n        let fitted = scaler.fit(\u0026data).unwrap();\n\n        let transformed = fitted.transform(\u0026data).unwrap();\n        let values = transformed.ravel().to_vec();\n\n        // First column center (median) should be ~3, scale ~2\n        // Values: (1-3)/2 = -1, (2-3)/2 = -0.5, (3-3)/2 = 0, (4-3)/2 = 0.5, (5-3)/2 = 1\n        assert!((values[0] - (-1.0)).abs() \u003c 0.1);\n        assert!((values[2] - (-0.5)).abs() \u003c 0.1);\n        assert!((values[4] - 0.0).abs() \u003c 0.1);\n        assert!((values[6] - 0.5).abs() \u003c 0.1);\n        assert!((values[8] - 1.0).abs() \u003c 0.1);\n    }\n\n    #[test]\n    fn test_robust_scaler_inverse_transform() {\n        let data = create_test_data();\n        let scaler = RobustScaler::\u003cCpuBackend\u003e::new();\n        let fitted = scaler.fit(\u0026data).unwrap();\n\n        let transformed = fitted.transform(\u0026data).unwrap();\n        let recovered = fitted.inverse_transform(\u0026transformed).unwrap();\n\n        let original = data.ravel().to_vec();\n        let recovered_vec = recovered.ravel().to_vec();\n\n        for (o, r) in original.iter().zip(recovered_vec.iter()) {\n            assert!((o - r).abs() \u003c 1e-6, \"Expected {}, got {}\", o, r);\n        }\n    }\n\n    #[test]\n    fn test_robust_scaler_without_centering() {\n        let data = create_test_data();\n        let scaler = RobustScaler::\u003cCpuBackend\u003e::new().with_centering(false);\n        let fitted = scaler.fit(\u0026data).unwrap();\n\n        let center = fitted.center().to_vec();\n        assert!(center.iter().all(|\u0026c| c == 0.0));\n    }\n\n    #[test]\n    fn test_robust_scaler_without_scaling() {\n        let data = create_test_data();\n        let scaler = RobustScaler::\u003cCpuBackend\u003e::new().with_scaling(false);\n        let fitted = scaler.fit(\u0026data).unwrap();\n\n        let scale = fitted.scale().to_vec();\n        assert!(scale.iter().all(|\u0026s| s == 1.0));\n    }\n\n    #[test]\n    fn test_robust_scaler_serialization() {\n        let data = create_test_data();\n        let scaler = RobustScaler::\u003cCpuBackend\u003e::new();\n        let fitted = scaler.fit(\u0026data).unwrap();\n\n        let params = fitted.extract_params();\n        let restored = FittedRobustScaler::\u003cCpuBackend\u003e::from_params(params).unwrap();\n\n        let transformed1 = fitted.transform(\u0026data).unwrap();\n        let transformed2 = restored.transform(\u0026data).unwrap();\n\n        let t1 = transformed1.ravel().to_vec();\n        let t2 = transformed2.ravel().to_vec();\n\n        for (i, (a, b)) in t1.iter().zip(t2.iter()).enumerate() {\n            assert!(\n                (a - b).abs() \u003c 1e-6,\n                \"Mismatch at index {}: {} vs {}\",\n                i,\n                a,\n                b\n            );\n        }\n    }\n\n    #[test]\n    fn test_robust_scaler_feature_mismatch() {\n        let data = create_test_data(); // 2 features\n        let scaler = RobustScaler::\u003cCpuBackend\u003e::new();\n        let fitted = scaler.fit(\u0026data).unwrap();\n\n        let wrong_data = Tensor2D::\u003cCpuBackend\u003e::new(vec![1.0f32, 2.0, 3.0], 1, 3); // 3 features\n        let result = fitted.transform(\u0026wrong_data);\n\n        assert!(matches!(\n            result,\n            Err(PreprocessingError::FeatureMismatch {\n                expected_features: 2,\n                got_features: 3\n            })\n        ));\n    }\n}\n","traces":[{"line":43,"address":[2361792],"length":1,"stats":{"Line":3}},{"line":75,"address":[],"length":0,"stats":{"Line":0}},{"line":76,"address":[],"length":0,"stats":{"Line":0}},{"line":82,"address":[2966672],"length":1,"stats":{"Line":1}},{"line":84,"address":[2966685],"length":1,"stats":{"Line":2}},{"line":90,"address":[2966624],"length":1,"stats":{"Line":1}},{"line":91,"address":[2966638],"length":1,"stats":{"Line":1}},{"line":92,"address":[2966644],"length":1,"stats":{"Line":1}},{"line":96,"address":[2966576],"length":1,"stats":{"Line":1}},{"line":97,"address":[2966590],"length":1,"stats":{"Line":1}},{"line":98,"address":[2966596],"length":1,"stats":{"Line":1}},{"line":102,"address":[],"length":0,"stats":{"Line":0}},{"line":103,"address":[],"length":0,"stats":{"Line":0}},{"line":104,"address":[],"length":0,"stats":{"Line":0}},{"line":105,"address":[],"length":0,"stats":{"Line":0}},{"line":107,"address":[],"length":0,"stats":{"Line":0}},{"line":108,"address":[],"length":0,"stats":{"Line":0}},{"line":113,"address":[2363311,2363305,2362304],"length":1,"stats":{"Line":1}},{"line":114,"address":[2362385],"length":1,"stats":{"Line":6}},{"line":115,"address":[2960720,2960763],"length":1,"stats":{"Line":14}},{"line":117,"address":[2362492],"length":1,"stats":{"Line":1}},{"line":118,"address":[2362523],"length":1,"stats":{"Line":6}},{"line":119,"address":[2362529],"length":1,"stats":{"Line":0}},{"line":123,"address":[2362590,2362679],"length":1,"stats":{"Line":7}},{"line":124,"address":[2362881],"length":1,"stats":{"Line":1}},{"line":126,"address":[2363133],"length":1,"stats":{"Line":6}},{"line":127,"address":[2363202],"length":1,"stats":{"Line":1}},{"line":129,"address":[2363269],"length":1,"stats":{"Line":6}},{"line":133,"address":[2361840],"length":1,"stats":{"Line":1}},{"line":134,"address":[2361876],"length":1,"stats":{"Line":6}},{"line":135,"address":[2361968,2362140],"length":1,"stats":{"Line":1}},{"line":136,"address":[2362078],"length":1,"stats":{"Line":6}},{"line":138,"address":[2362179,2362133,2362280],"length":1,"stats":{"Line":7}},{"line":147,"address":[2957802,2956192,2958243],"length":1,"stats":{"Line":3}},{"line":148,"address":[2956258],"length":1,"stats":{"Line":3}},{"line":150,"address":[2956287],"length":1,"stats":{"Line":3}},{"line":151,"address":[2956331],"length":1,"stats":{"Line":0}},{"line":152,"address":[2956297],"length":1,"stats":{"Line":0}},{"line":157,"address":[2956466,2956518],"length":1,"stats":{"Line":1}},{"line":160,"address":[2956605],"length":1,"stats":{"Line":4}},{"line":161,"address":[2956638],"length":1,"stats":{"Line":1}},{"line":163,"address":[2956721],"length":1,"stats":{"Line":4}},{"line":165,"address":[2956766,2956861],"length":1,"stats":{"Line":5}},{"line":167,"address":[2958342,2957824,2956990,2958320],"length":1,"stats":{"Line":10}},{"line":170,"address":[2957924,2957839],"length":1,"stats":{"Line":7}},{"line":172,"address":[2957981,2958087],"length":1,"stats":{"Line":2}},{"line":174,"address":[2958015],"length":1,"stats":{"Line":5}},{"line":177,"address":[2957995,2958238],"length":1,"stats":{"Line":6}},{"line":178,"address":[2958119],"length":1,"stats":{"Line":5}},{"line":179,"address":[2958138],"length":1,"stats":{"Line":1}},{"line":184,"address":[2958256,2958266,2957068],"length":1,"stats":{"Line":3}},{"line":189,"address":[2958288,2957257,2957324,2958298],"length":1,"stats":{"Line":14}},{"line":193,"address":[2957583],"length":1,"stats":{"Line":1}},{"line":194,"address":[2957464],"length":1,"stats":{"Line":6}},{"line":195,"address":[2957519],"length":1,"stats":{"Line":1}},{"line":196,"address":[2957551],"length":1,"stats":{"Line":6}},{"line":197,"address":[],"length":0,"stats":{"Line":0}},{"line":198,"address":[],"length":0,"stats":{"Line":0}},{"line":202,"address":[],"length":0,"stats":{"Line":0}},{"line":203,"address":[],"length":0,"stats":{"Line":0}},{"line":204,"address":[],"length":0,"stats":{"Line":0}},{"line":220,"address":[2966752],"length":1,"stats":{"Line":2}},{"line":221,"address":[],"length":0,"stats":{"Line":0}},{"line":225,"address":[2966736],"length":1,"stats":{"Line":1}},{"line":226,"address":[2966744],"length":1,"stats":{"Line":2}},{"line":235,"address":[2960128,2960700,2960694],"length":1,"stats":{"Line":3}},{"line":236,"address":[2960177],"length":1,"stats":{"Line":3}},{"line":238,"address":[2960200],"length":1,"stats":{"Line":3}},{"line":239,"address":[2960252],"length":1,"stats":{"Line":1}},{"line":240,"address":[2960248],"length":1,"stats":{"Line":1}},{"line":241,"address":[],"length":0,"stats":{"Line":0}},{"line":245,"address":[2960210],"length":1,"stats":{"Line":2}},{"line":247,"address":[2960461,2960225],"length":1,"stats":{"Line":4}},{"line":248,"address":[2960391,2960335,2960384],"length":1,"stats":{"Line":4}},{"line":251,"address":[2960302,2960676],"length":1,"stats":{"Line":4}},{"line":252,"address":[2960603,2960567],"length":1,"stats":{"Line":2}},{"line":255,"address":[2960489],"length":1,"stats":{"Line":2}},{"line":256,"address":[2960466],"length":1,"stats":{"Line":2}},{"line":257,"address":[],"length":0,"stats":{"Line":0}},{"line":261,"address":[2960102,2960108,2959536],"length":1,"stats":{"Line":1}},{"line":262,"address":[2959585],"length":1,"stats":{"Line":1}},{"line":264,"address":[2959608],"length":1,"stats":{"Line":1}},{"line":265,"address":[2959660],"length":1,"stats":{"Line":0}},{"line":266,"address":[2959656],"length":1,"stats":{"Line":0}},{"line":267,"address":[],"length":0,"stats":{"Line":0}},{"line":271,"address":[2959618],"length":1,"stats":{"Line":1}},{"line":273,"address":[2959633,2959873],"length":1,"stats":{"Line":2}},{"line":274,"address":[2959796,2959730,2959803],"length":1,"stats":{"Line":2}},{"line":277,"address":[2960084,2959710],"length":1,"stats":{"Line":2}},{"line":278,"address":[2960011,2959992],"length":1,"stats":{"Line":1}},{"line":281,"address":[2959901],"length":1,"stats":{"Line":1}},{"line":282,"address":[2959878],"length":1,"stats":{"Line":1}},{"line":283,"address":[],"length":0,"stats":{"Line":0}},{"line":287,"address":[2959513,2959296,2959507],"length":1,"stats":{"Line":1}},{"line":289,"address":[2959319],"length":1,"stats":{"Line":1}},{"line":290,"address":[2959343],"length":1,"stats":{"Line":1}},{"line":291,"address":[2959353],"length":1,"stats":{"Line":1}},{"line":292,"address":[2959414],"length":1,"stats":{"Line":1}},{"line":296,"address":[2959189,2958464],"length":1,"stats":{"Line":1}},{"line":298,"address":[2959226,2958558,2958494,2959216],"length":1,"stats":{"Line":4}},{"line":302,"address":[2959248,2958758,2958687,2959258],"length":1,"stats":{"Line":4}},{"line":306,"address":[2958996],"length":1,"stats":{"Line":1}},{"line":307,"address":[2958908],"length":1,"stats":{"Line":1}},{"line":308,"address":[2958944],"length":1,"stats":{"Line":1}},{"line":309,"address":[],"length":0,"stats":{"Line":0}},{"line":310,"address":[2958992],"length":1,"stats":{"Line":1}},{"line":311,"address":[],"length":0,"stats":{"Line":0}},{"line":315,"address":[2959280],"length":1,"stats":{"Line":0}},{"line":316,"address":[2959285],"length":1,"stats":{"Line":0}},{"line":319,"address":[],"length":0,"stats":{"Line":0}},{"line":320,"address":[],"length":0,"stats":{"Line":0}},{"line":321,"address":[],"length":0,"stats":{"Line":0}},{"line":322,"address":[],"length":0,"stats":{"Line":0}},{"line":329,"address":[],"length":0,"stats":{"Line":0}},{"line":330,"address":[],"length":0,"stats":{"Line":0}},{"line":331,"address":[],"length":0,"stats":{"Line":0}},{"line":332,"address":[],"length":0,"stats":{"Line":0}}],"covered":82,"coverable":117},{"path":["/","home","victor","ML","machinelearne-rs","lib","src","preprocessing","scaling","standard.rs"],"content":"//! Standard Scaler (Z-score normalization).\n//!\n//! Transforms features by removing the mean and scaling to unit variance.\n//!\n//! The standard score of a sample `x` is calculated as:\n//! ```text\n//! z = (x - u) / s\n//! ```\n//! where `u` is the mean of the training samples, and `s` is the standard deviation.\n//!\n//! # Example\n//! ```ignore\n//! use machinelearne_rs::preprocessing::{Transformer, StandardScaler};\n//! use machinelearne_rs::backend::CpuBackend;\n//!\n//! let scaler = StandardScaler::\u003cCpuBackend\u003e::new()\n//!     .with_mean(true)\n//!     .with_std(true);\n//!\n//! let fitted = scaler.fit(\u0026data)?;\n//! let scaled = fitted.transform(\u0026data)?;\n//!\n//! // Later, for inference:\n//! let loaded = StandardScaler::load_from_file(\"scaler.bin\")?;\n//! let new_scaled = loaded.transform(\u0026new_data)?;\n//! ```\n\nuse crate::backend::{Backend, Tensor1D, Tensor2D};\nuse crate::preprocessing::error::PreprocessingError;\nuse crate::preprocessing::traits::{FittedTransformer, Transformer};\nuse serde::{Deserialize, Serialize};\nuse std::marker::PhantomData;\n\n/// Configuration for StandardScaler.\n#[derive(Clone, Debug, Serialize, Deserialize)]\npub struct StandardScalerConfig {\n    /// If True, center the data before scaling.\n    pub with_mean: bool,\n    /// If True, scale the data to unit variance.\n    pub with_std: bool,\n}\n\nimpl Default for StandardScalerConfig {\n    fn default() -\u003e Self {\n        Self {\n            with_mean: true,\n            with_std: true,\n        }\n    }\n}\n\n/// Serializable parameters for a fitted StandardScaler.\n#[derive(Clone, Debug, Serialize, Deserialize)]\npub struct StandardScalerParams {\n    /// Configuration options.\n    pub config: StandardScalerConfig,\n    /// Mean of each feature (None if with_mean=False).\n    pub mean: Vec\u003cf64\u003e,\n    /// Standard deviation of each feature (None if with_std=False).\n    pub std: Vec\u003cf64\u003e,\n    /// Number of features seen during fit.\n    pub n_features: usize,\n}\n\n// Note: SerializableParams is automatically implemented via blanket impl in serialization.rs\n// for types that implement Serialize + Deserialize\n\n/// StandardScaler transformer (unfitted).\n///\n/// Transforms features by removing the mean and scaling to unit variance.\n#[derive(Clone)]\npub struct StandardScaler\u003cB: Backend\u003e {\n    config: StandardScalerConfig,\n    _backend: PhantomData\u003cB\u003e,\n}\n\nimpl\u003cB: Backend\u003e Default for StandardScaler\u003cB\u003e {\n    fn default() -\u003e Self {\n        Self::new()\n    }\n}\n\nimpl\u003cB: Backend\u003e StandardScaler\u003cB\u003e {\n    /// Create a new StandardScaler with default configuration.\n    pub fn new() -\u003e Self {\n        Self {\n            config: StandardScalerConfig::default(),\n            _backend: PhantomData,\n        }\n    }\n\n    /// Set whether to center data by mean.\n    pub fn with_mean(mut self, with_mean: bool) -\u003e Self {\n        self.config.with_mean = with_mean;\n        self\n    }\n\n    /// Set whether to scale data to unit variance.\n    pub fn with_std(mut self, with_std: bool) -\u003e Self {\n        self.config.with_std = with_std;\n        self\n    }\n}\n\nimpl\u003cB: Backend\u003e Transformer\u003cB\u003e for StandardScaler\u003cB\u003e {\n    type Input = Tensor2D\u003cB\u003e;\n    type Output = Tensor2D\u003cB\u003e;\n    type Params = StandardScalerParams;\n    type Fitted = FittedStandardScaler\u003cB\u003e;\n\n    fn fit(\u0026self, data: \u0026Self::Input) -\u003e Result\u003cSelf::Fitted, PreprocessingError\u003e {\n        let (rows, cols) = data.shape();\n\n        if rows == 0 {\n            return Err(PreprocessingError::EmptyData(\n                \"Cannot fit StandardScaler on empty data\".to_string(),\n            ));\n        }\n\n        let mean = if self.config.with_mean {\n            Tensor1D {\n                data: B::col_mean_2d(\u0026data.data),\n                backend: PhantomData::\u003cB\u003e,\n            }\n        } else {\n            Tensor1D {\n                data: B::zeros_1d(cols),\n                backend: PhantomData::\u003cB\u003e,\n            }\n        };\n\n        let std = if self.config.with_std {\n            Tensor1D {\n                data: B::col_std_2d(\u0026data.data, 0), // population std (ddof=0)\n                backend: PhantomData::\u003cB\u003e,\n            }\n        } else {\n            let ones: Vec\u003cf64\u003e = vec![1.0; cols];\n            Tensor1D {\n                data: B::from_vec_1d(ones.iter().map(|\u0026x| x as f32).collect()),\n                backend: PhantomData::\u003cB\u003e,\n            }\n        };\n\n        // Handle zero std (constant features)\n        let std_vec = std.to_vec();\n        let std_adjusted: Vec\u003cf64\u003e = std_vec\n            .iter()\n            .map(|\u0026s| if s == 0.0 { 1.0 } else { s })\n            .collect();\n        let std_final = Tensor1D {\n            data: B::from_vec_1d(std_adjusted.iter().map(|\u0026x| x as f32).collect()),\n            backend: PhantomData::\u003cB\u003e,\n        };\n\n        Ok(FittedStandardScaler {\n            config: self.config.clone(),\n            mean,\n            std: std_final,\n            n_features: cols,\n            _backend: PhantomData,\n        })\n    }\n\n    fn fit_transform(\u0026self, data: \u0026Self::Input) -\u003e Result\u003cSelf::Output, PreprocessingError\u003e {\n        let fitted = self.fit(data)?;\n        fitted.transform(data)\n    }\n}\n\n/// Fitted StandardScaler ready for inference.\n#[derive(Clone)]\npub struct FittedStandardScaler\u003cB: Backend\u003e {\n    config: StandardScalerConfig,\n    mean: Tensor1D\u003cB\u003e,\n    std: Tensor1D\u003cB\u003e,\n    n_features: usize,\n    _backend: PhantomData\u003cB\u003e,\n}\n\nimpl\u003cB: Backend\u003e FittedStandardScaler\u003cB\u003e {\n    /// Get the mean values for each feature.\n    pub fn mean(\u0026self) -\u003e \u0026Tensor1D\u003cB\u003e {\n        \u0026self.mean\n    }\n\n    /// Get the standard deviation values for each feature.\n    pub fn std(\u0026self) -\u003e \u0026Tensor1D\u003cB\u003e {\n        \u0026self.std\n    }\n}\n\nimpl\u003cB: Backend\u003e FittedTransformer\u003cB\u003e for FittedStandardScaler\u003cB\u003e {\n    type Input = Tensor2D\u003cB\u003e;\n    type Output = Tensor2D\u003cB\u003e;\n    type Params = StandardScalerParams;\n\n    fn transform(\u0026self, data: \u0026Self::Input) -\u003e Result\u003cSelf::Output, PreprocessingError\u003e {\n        let (_, cols) = data.shape();\n\n        if cols != self.n_features {\n            return Err(PreprocessingError::FeatureMismatch {\n                expected_features: self.n_features,\n                got_features: cols,\n            });\n        }\n\n        let mut result_data = data.data.clone();\n\n        if self.config.with_mean {\n            result_data = B::broadcast_sub_1d_to_2d_rows(\u0026result_data, \u0026self.mean.data);\n        }\n\n        if self.config.with_std {\n            result_data = B::broadcast_div_1d_to_2d_rows(\u0026result_data, \u0026self.std.data);\n        }\n\n        Ok(Tensor2D {\n            data: result_data,\n            backend: PhantomData,\n        })\n    }\n\n    fn inverse_transform(\u0026self, data: \u0026Self::Output) -\u003e Result\u003cSelf::Input, PreprocessingError\u003e {\n        let (_, cols) = data.shape();\n\n        if cols != self.n_features {\n            return Err(PreprocessingError::FeatureMismatch {\n                expected_features: self.n_features,\n                got_features: cols,\n            });\n        }\n\n        let mut result_data = data.data.clone();\n\n        if self.config.with_std {\n            result_data = B::broadcast_mul_1d_to_2d_rows(\u0026result_data, \u0026self.std.data);\n        }\n\n        if self.config.with_mean {\n            result_data = B::broadcast_add_1d_to_2d_rows(\u0026result_data, \u0026self.mean.data);\n        }\n\n        Ok(Tensor2D {\n            data: result_data,\n            backend: PhantomData,\n        })\n    }\n\n    fn extract_params(\u0026self) -\u003e Self::Params {\n        StandardScalerParams {\n            config: self.config.clone(),\n            mean: self.mean.to_vec(),\n            std: self.std.to_vec(),\n            n_features: self.n_features,\n        }\n    }\n\n    fn from_params(params: Self::Params) -\u003e Result\u003cSelf, PreprocessingError\u003e {\n        let mean = Tensor1D {\n            data: B::from_vec_1d(params.mean.iter().map(|\u0026x| x as f32).collect()),\n            backend: PhantomData,\n        };\n        let std = Tensor1D {\n            data: B::from_vec_1d(params.std.iter().map(|\u0026x| x as f32).collect()),\n            backend: PhantomData,\n        };\n\n        Ok(Self {\n            config: params.config,\n            mean,\n            std,\n            n_features: params.n_features,\n            _backend: PhantomData,\n        })\n    }\n\n    fn n_features_in(\u0026self) -\u003e usize {\n        self.n_features\n    }\n\n    fn save_to_file\u003cP: AsRef\u003cstd::path::Path\u003e\u003e(\u0026self, path: P) -\u003e std::io::Result\u003c()\u003e {\n        let params = self.extract_params();\n        let bytes = bincode::serialize(\u0026params).map_err(std::io::Error::other)?;\n        std::fs::write(path, bytes)\n    }\n\n    fn load_from_file\u003cP: AsRef\u003cstd::path::Path\u003e\u003e(path: P) -\u003e Result\u003cSelf, PreprocessingError\u003e\n    where\n        Self: Sized,\n    {\n        let bytes = std::fs::read(path)?;\n        let params: StandardScalerParams = bincode::deserialize(\u0026bytes)\n            .map_err(|e| PreprocessingError::SerializationError(e.to_string()))?;\n        Self::from_params(params)\n    }\n}\n\n#[cfg(test)]\nmod tests {\n    use super::*;\n    use crate::backend::CpuBackend;\n\n    fn create_test_data() -\u003e Tensor2D\u003cCpuBackend\u003e {\n        // [[0, 1], [0, 1], [1, 3]]\n        Tensor2D::new(vec![0.0f32, 1.0, 0.0, 1.0, 1.0, 3.0], 3, 2)\n    }\n\n    #[test]\n    fn test_standard_scaler_fit() {\n        let data = create_test_data();\n        let scaler = StandardScaler::\u003cCpuBackend\u003e::new();\n        let fitted = scaler.fit(\u0026data).unwrap();\n\n        // Mean: [1/3, 5/3]\n        let mean = fitted.mean().to_vec();\n        assert!((mean[0] - 1.0 / 3.0).abs() \u003c 1e-10);\n        assert!((mean[1] - 5.0 / 3.0).abs() \u003c 1e-10);\n    }\n\n    #[test]\n    fn test_standard_scaler_transform() {\n        let data = create_test_data();\n        let scaler = StandardScaler::\u003cCpuBackend\u003e::new();\n        let fitted = scaler.fit(\u0026data).unwrap();\n\n        let transformed = fitted.transform(\u0026data).unwrap();\n\n        // After standardization, each column should have meanâ0 and stdâ1\n        let mean_vals: Vec\u003cf64\u003e =\n            CpuBackend::to_vec_1d(\u0026CpuBackend::col_mean_2d(\u0026transformed.data));\n        let std_vals: Vec\u003cf64\u003e =\n            CpuBackend::to_vec_1d(\u0026CpuBackend::col_std_2d(\u0026transformed.data, 0));\n\n        assert!(mean_vals[0].abs() \u003c 1e-10, \"mean[0] = {}\", mean_vals[0]);\n        assert!(mean_vals[1].abs() \u003c 1e-10, \"mean[1] = {}\", mean_vals[1]);\n        // Using a slightly larger tolerance for numerical stability\n        assert!((std_vals[0] - 1.0).abs() \u003c 1e-8, \"std[0] = {}\", std_vals[0]);\n        assert!((std_vals[1] - 1.0).abs() \u003c 1e-8, \"std[1] = {}\", std_vals[1]);\n    }\n\n    #[test]\n    fn test_standard_scaler_inverse_transform() {\n        let data = create_test_data();\n        let scaler = StandardScaler::\u003cCpuBackend\u003e::new();\n        let fitted = scaler.fit(\u0026data).unwrap();\n\n        let transformed = fitted.transform(\u0026data).unwrap();\n        let recovered = fitted.inverse_transform(\u0026transformed).unwrap();\n\n        let original = data.ravel().to_vec();\n        let recovered_vec = recovered.ravel().to_vec();\n\n        for (o, r) in original.iter().zip(recovered_vec.iter()) {\n            assert!((o - r).abs() \u003c 1e-10);\n        }\n    }\n\n    #[test]\n    fn test_standard_scaler_without_mean() {\n        let data = create_test_data();\n        let scaler = StandardScaler::\u003cCpuBackend\u003e::new().with_mean(false);\n        let fitted = scaler.fit(\u0026data).unwrap();\n\n        // Mean should be zeros\n        let mean = fitted.mean().to_vec();\n        assert!(mean.iter().all(|\u0026m| m == 0.0));\n    }\n\n    #[test]\n    fn test_standard_scaler_without_std() {\n        let data = create_test_data();\n        let scaler = StandardScaler::\u003cCpuBackend\u003e::new().with_std(false);\n        let fitted = scaler.fit(\u0026data).unwrap();\n\n        // Std should be ones\n        let std = fitted.std().to_vec();\n        assert!(std.iter().all(|\u0026s| s == 1.0));\n    }\n\n    #[test]\n    fn test_standard_scaler_serialization() {\n        let data = create_test_data();\n        let scaler = StandardScaler::\u003cCpuBackend\u003e::new();\n        let fitted = scaler.fit(\u0026data).unwrap();\n\n        let params = fitted.extract_params();\n        let restored = FittedStandardScaler::\u003cCpuBackend\u003e::from_params(params).unwrap();\n\n        let transformed1 = fitted.transform(\u0026data).unwrap();\n        let transformed2 = restored.transform(\u0026data).unwrap();\n\n        let t1 = transformed1.ravel().to_vec();\n        let t2 = transformed2.ravel().to_vec();\n\n        // Use 1e-6 tolerance due to f32-\u003ef64 conversion precision\n        for (i, (a, b)) in t1.iter().zip(t2.iter()).enumerate() {\n            assert!(\n                (a - b).abs() \u003c 1e-6,\n                \"Mismatch at index {}: {} vs {}\",\n                i,\n                a,\n                b\n            );\n        }\n    }\n\n    #[test]\n    fn test_standard_scaler_feature_mismatch() {\n        let data = create_test_data(); // 2 features\n        let scaler = StandardScaler::\u003cCpuBackend\u003e::new();\n        let fitted = scaler.fit(\u0026data).unwrap();\n\n        // Try to transform with wrong number of features\n        let wrong_data = Tensor2D::\u003cCpuBackend\u003e::new(vec![1.0f32, 2.0, 3.0], 1, 3); // 3 features\n        let result = fitted.transform(\u0026wrong_data);\n\n        assert!(matches!(\n            result,\n            Err(PreprocessingError::FeatureMismatch {\n                expected_features: 2,\n                got_features: 3\n            })\n        ));\n    }\n\n    #[test]\n    fn test_standard_scaler_fit_transform() {\n        let data = create_test_data();\n        let scaler = StandardScaler::\u003cCpuBackend\u003e::new();\n\n        let transformed = scaler.fit_transform(\u0026data).unwrap();\n\n        // After standardization, each column should have meanâ0 and stdâ1\n        let mean_vals: Vec\u003cf64\u003e =\n            CpuBackend::to_vec_1d(\u0026CpuBackend::col_mean_2d(\u0026transformed.data));\n        let std_vals: Vec\u003cf64\u003e =\n            CpuBackend::to_vec_1d(\u0026CpuBackend::col_std_2d(\u0026transformed.data, 0));\n\n        assert!(mean_vals[0].abs() \u003c 1e-10);\n        assert!(mean_vals[1].abs() \u003c 1e-10);\n    }\n}\n","traces":[{"line":44,"address":[3426928],"length":1,"stats":{"Line":6}},{"line":78,"address":[],"length":0,"stats":{"Line":0}},{"line":79,"address":[],"length":0,"stats":{"Line":0}},{"line":85,"address":[3438064],"length":1,"stats":{"Line":5}},{"line":87,"address":[3438065],"length":1,"stats":{"Line":5}},{"line":93,"address":[3438128],"length":1,"stats":{"Line":1}},{"line":94,"address":[3438156],"length":1,"stats":{"Line":1}},{"line":95,"address":[3438160],"length":1,"stats":{"Line":1}},{"line":99,"address":[3438080],"length":1,"stats":{"Line":1}},{"line":100,"address":[3438108],"length":1,"stats":{"Line":1}},{"line":101,"address":[3438112],"length":1,"stats":{"Line":1}},{"line":111,"address":[3430479,3429684,3428864],"length":1,"stats":{"Line":4}},{"line":112,"address":[3428930],"length":1,"stats":{"Line":4}},{"line":114,"address":[3428967],"length":1,"stats":{"Line":4}},{"line":115,"address":[3429011],"length":1,"stats":{"Line":0}},{"line":116,"address":[3428977],"length":1,"stats":{"Line":0}},{"line":120,"address":[3429130,3429230],"length":1,"stats":{"Line":5}},{"line":122,"address":[3429240],"length":1,"stats":{"Line":4}},{"line":127,"address":[3429161],"length":1,"stats":{"Line":1}},{"line":132,"address":[3429317,3429738],"length":1,"stats":{"Line":5}},{"line":134,"address":[3429381],"length":1,"stats":{"Line":4}},{"line":138,"address":[3429331],"length":1,"stats":{"Line":1}},{"line":140,"address":[3429440,3429516,3430554,3430544],"length":1,"stats":{"Line":4}},{"line":146,"address":[3429677],"length":1,"stats":{"Line":1}},{"line":147,"address":[3429788],"length":1,"stats":{"Line":4}},{"line":149,"address":[3430576,3429879,3430586],"length":1,"stats":{"Line":6}},{"line":152,"address":[3430522,3429929,3430512,3429996],"length":1,"stats":{"Line":10}},{"line":156,"address":[3430280],"length":1,"stats":{"Line":1}},{"line":157,"address":[3430128],"length":1,"stats":{"Line":4}},{"line":158,"address":[3430208],"length":1,"stats":{"Line":1}},{"line":159,"address":[3430248],"length":1,"stats":{"Line":4}},{"line":160,"address":[],"length":0,"stats":{"Line":0}},{"line":161,"address":[],"length":0,"stats":{"Line":0}},{"line":165,"address":[3428851,3428845,3428496],"length":1,"stats":{"Line":1}},{"line":166,"address":[3428534],"length":1,"stats":{"Line":1}},{"line":167,"address":[3428781],"length":1,"stats":{"Line":1}},{"line":183,"address":[3438192],"length":1,"stats":{"Line":1}},{"line":184,"address":[],"length":0,"stats":{"Line":0}},{"line":188,"address":[3438176],"length":1,"stats":{"Line":1}},{"line":189,"address":[3438184],"length":1,"stats":{"Line":1}},{"line":198,"address":[3432256,3432822,3432828],"length":1,"stats":{"Line":1}},{"line":199,"address":[3432305],"length":1,"stats":{"Line":3}},{"line":201,"address":[3432328],"length":1,"stats":{"Line":3}},{"line":202,"address":[3432380],"length":1,"stats":{"Line":1}},{"line":203,"address":[3432376],"length":1,"stats":{"Line":1}},{"line":204,"address":[],"length":0,"stats":{"Line":0}},{"line":208,"address":[3432338],"length":1,"stats":{"Line":3}},{"line":210,"address":[3432589,3432353],"length":1,"stats":{"Line":6}},{"line":211,"address":[3432519,3432512,3432463],"length":1,"stats":{"Line":6}},{"line":214,"address":[3432804,3432430],"length":1,"stats":{"Line":6}},{"line":215,"address":[3432695,3432731],"length":1,"stats":{"Line":3}},{"line":218,"address":[3432617],"length":1,"stats":{"Line":3}},{"line":219,"address":[3432594],"length":1,"stats":{"Line":3}},{"line":220,"address":[],"length":0,"stats":{"Line":0}},{"line":224,"address":[3432230,3432236,3431664],"length":1,"stats":{"Line":1}},{"line":225,"address":[3431713],"length":1,"stats":{"Line":1}},{"line":227,"address":[3431736],"length":1,"stats":{"Line":1}},{"line":228,"address":[3431788],"length":1,"stats":{"Line":0}},{"line":229,"address":[3431784],"length":1,"stats":{"Line":0}},{"line":230,"address":[],"length":0,"stats":{"Line":0}},{"line":234,"address":[3431746],"length":1,"stats":{"Line":1}},{"line":236,"address":[3432001,3431761],"length":1,"stats":{"Line":2}},{"line":237,"address":[3431931,3431924,3431858],"length":1,"stats":{"Line":2}},{"line":240,"address":[3431838,3432212],"length":1,"stats":{"Line":2}},{"line":241,"address":[3432120,3432139],"length":1,"stats":{"Line":1}},{"line":244,"address":[3432029],"length":1,"stats":{"Line":1}},{"line":245,"address":[3432006],"length":1,"stats":{"Line":2}},{"line":246,"address":[],"length":0,"stats":{"Line":0}},{"line":250,"address":[3431651,3431645,3431424],"length":1,"stats":{"Line":1}},{"line":252,"address":[3431455],"length":1,"stats":{"Line":1}},{"line":253,"address":[3431482],"length":1,"stats":{"Line":1}},{"line":254,"address":[3431492],"length":1,"stats":{"Line":1}},{"line":255,"address":[3431563],"length":1,"stats":{"Line":1}},{"line":259,"address":[3431323,3430656],"length":1,"stats":{"Line":1}},{"line":261,"address":[3430686,3430750,3431344,3431354],"length":1,"stats":{"Line":4}},{"line":265,"address":[3430950,3431386,3430879,3431376],"length":1,"stats":{"Line":4}},{"line":269,"address":[3431158],"length":1,"stats":{"Line":1}},{"line":270,"address":[3431100],"length":1,"stats":{"Line":1}},{"line":271,"address":[3431106],"length":1,"stats":{"Line":1}},{"line":272,"address":[],"length":0,"stats":{"Line":0}},{"line":273,"address":[3431154],"length":1,"stats":{"Line":1}},{"line":274,"address":[],"length":0,"stats":{"Line":0}},{"line":278,"address":[3431408],"length":1,"stats":{"Line":2}},{"line":279,"address":[3431413],"length":1,"stats":{"Line":2}},{"line":282,"address":[],"length":0,"stats":{"Line":0}},{"line":283,"address":[],"length":0,"stats":{"Line":0}},{"line":284,"address":[],"length":0,"stats":{"Line":0}},{"line":285,"address":[],"length":0,"stats":{"Line":0}},{"line":292,"address":[],"length":0,"stats":{"Line":0}},{"line":293,"address":[],"length":0,"stats":{"Line":0}},{"line":294,"address":[],"length":0,"stats":{"Line":0}},{"line":295,"address":[],"length":0,"stats":{"Line":0}}],"covered":69,"coverable":92},{"path":["/","home","victor","ML","machinelearne-rs","lib","src","preprocessing","traits.rs"],"content":"//! Core traits for preprocessing transformers.\n//!\n//! This module defines the two central traits:\n//! - [`Transformer`]: Used during fitting; has hyperparameters and can learn from data.\n//! - [`FittedTransformer`]: After fitting; ready for inference and serialization.\n\nuse crate::backend::Backend;\nuse crate::preprocessing::error::PreprocessingError;\nuse crate::serialization::SerializableParams;\n\n/// Trait for unfitted transformers with hyperparameters.\n///\n/// A transformer learns parameters from training data and can then transform\n/// new data using those learned parameters. This trait represents the\n/// configurable, unfitted state.\n///\n/// # Type Parameters\n/// - `B`: The backend (e.g., `CpuBackend`) used for computation.\n/// - `Input`: Input data type (typically `Tensor2D\u003cB\u003e`).\n/// - `Output`: Output data type (typically `Tensor2D\u003cB\u003e`).\n/// - `Params`: Serializable representation of learned parameters.\n/// - `Fitted`: The corresponding fitted transformer type.\n///\n/// # Example\n/// ```ignore\n/// use machinelearne_rs::preprocessing::{Transformer, StandardScaler};\n/// use machinelearne_rs::backend::CpuBackend;\n///\n/// let scaler = StandardScaler::\u003cCpuBackend\u003e::new();\n/// let fitted = scaler.fit(\u0026data)?;\n/// let transformed = fitted.transform(\u0026new_data)?;\n/// ```\npub trait Transformer\u003cB: Backend\u003e: Clone {\n    /// Input data type for transformation.\n    type Input;\n    /// Output data type after transformation.\n    type Output;\n    /// Serializable representation of learned parameters.\n    type Params: SerializableParams;\n    /// The fitted transformer type ready for inference.\n    type Fitted: FittedTransformer\u003c\n        B,\n        Params = Self::Params,\n        Input = Self::Input,\n        Output = Self::Output,\n    \u003e;\n\n    /// Fit the transformer to the training data.\n    ///\n    /// Learns parameters (e.g., mean and std for StandardScaler) from the data.\n    ///\n    /// # Errors\n    /// Returns [`PreprocessingError`] if:\n    /// - Data is empty\n    /// - Data contains invalid values (NaN, Inf)\n    /// - Shape is incompatible with the transformer\n    fn fit(\u0026self, data: \u0026Self::Input) -\u003e Result\u003cSelf::Fitted, PreprocessingError\u003e;\n\n    /// Fit the transformer and transform the data in one step.\n    ///\n    /// This is often more efficient than calling `fit` followed by `transform`,\n    /// especially for transformers that can compute the transform during fitting.\n    fn fit_transform(\u0026self, data: \u0026Self::Input) -\u003e Result\u003cSelf::Output, PreprocessingError\u003e;\n}\n\n/// Trait for fitted transformers ready for inference.\n///\n/// After fitting, a transformer contains learned parameters (e.g., mean_, std_\n/// for StandardScaler) and can transform new data. It can also be serialized\n/// and deserialized for deployment.\n///\n/// # Type Parameters\n/// - `B`: The backend used for computation.\n/// - `Params`: Serializable representation of learned parameters.\n///\n/// # Guarantees\n/// - `extract_params()` + `from_params()` is a round-trip.\n/// - `save_to_file` / `load_from_file` are cross-platform compatible.\npub trait FittedTransformer\u003cB: Backend\u003e: Clone {\n    /// Input data type for transformation.\n    type Input;\n    /// Output data type after transformation.\n    type Output;\n    /// Serializable representation of learned parameters.\n    type Params: SerializableParams;\n\n    /// Transform data using learned parameters.\n    ///\n    /// # Errors\n    /// Returns [`PreprocessingError`] if:\n    /// - Input shape doesn't match expected number of features\n    /// - Input contains invalid values\n    fn transform(\u0026self, data: \u0026Self::Input) -\u003e Result\u003cSelf::Output, PreprocessingError\u003e;\n\n    /// Reverse the transformation (if supported).\n    ///\n    /// Not all transformers support inverse transformation.\n    /// For example, OneHotEncoder cannot be inverted.\n    ///\n    /// # Errors\n    /// Returns [`PreprocessingError`] if inverse transform is not supported\n    /// or if the data cannot be inverse transformed.\n    fn inverse_transform(\u0026self, data: \u0026Self::Output) -\u003e Result\u003cSelf::Input, PreprocessingError\u003e;\n\n    /// Extract learned parameters as a serializable representation.\n    fn extract_params(\u0026self) -\u003e Self::Params;\n\n    /// Reconstruct a fitted transformer from parameters.\n    fn from_params(params: Self::Params) -\u003e Result\u003cSelf, PreprocessingError\u003e\n    where\n        Self: Sized;\n\n    /// Save the fitted transformer to a file.\n    fn save_to_file\u003cP: AsRef\u003cstd::path::Path\u003e\u003e(\u0026self, path: P) -\u003e std::io::Result\u003c()\u003e {\n        let params = self.extract_params();\n        let bytes = params.to_bytes().map_err(std::io::Error::other)?;\n        std::fs::write(path, bytes)\n    }\n\n    /// Load a fitted transformer from a file.\n    fn load_from_file\u003cP: AsRef\u003cstd::path::Path\u003e\u003e(path: P) -\u003e Result\u003cSelf, PreprocessingError\u003e\n    where\n        Self: Sized,\n    {\n        let bytes = std::fs::read(path)?;\n        let params = Self::Params::from_bytes(\u0026bytes)\n            .map_err(|e| PreprocessingError::SerializationError(e.to_string()))?;\n        Self::from_params(params)\n    }\n\n    /// Returns the number of features seen during fit.\n    fn n_features_in(\u0026self) -\u003e usize;\n}\n\n/// Marker trait for transformers that don't require fitting.\n///\n/// Stateless transformers (like Normalizer) can transform data without\n/// learning any parameters. They implement both `Transformer` and this trait.\npub trait StatelessTransformer\u003cB: Backend\u003e: Transformer\u003cB\u003e {\n    /// Transform data without fitting.\n    ///\n    /// For stateless transformers, this is equivalent to `fit_transform`\n    /// but communicates that no learning occurs.\n    fn transform_direct(data: \u0026Self::Input) -\u003e Result\u003cSelf::Output, PreprocessingError\u003e;\n}\n","traces":[{"line":114,"address":[3469840,3469312,3469878],"length":1,"stats":{"Line":4}},{"line":115,"address":[1999586],"length":1,"stats":{"Line":4}},{"line":116,"address":[3375222,3374885,3374828],"length":1,"stats":{"Line":8}},{"line":117,"address":[2817440,2817544],"length":1,"stats":{"Line":8}},{"line":121,"address":[3375982,3375264,3375955],"length":1,"stats":{"Line":4}},{"line":125,"address":[2000153],"length":1,"stats":{"Line":4}},{"line":126,"address":[2817966,2817920,2818063,2817837],"length":1,"stats":{"Line":12}},{"line":127,"address":[2000407,2000479],"length":1,"stats":{"Line":4}},{"line":128,"address":[2000693],"length":1,"stats":{"Line":4}}],"covered":9,"coverable":9},{"path":["/","home","victor","ML","machinelearne-rs","lib","src","regularizers","mod.rs"],"content":"use crate::backend::scalar::Scalar;\nuse crate::backend::Backend;\nuse crate::loss::{LinearParams, Tensor1D};\nuse crate::model::linear::LinearRegression;\nuse crate::model::TrainableModel;\n/// Computes the regularization penalty and its gradient w.r.t. model parameters.\n///\n/// This trait enables pluggable regularization strategies (e.g., L2, L1) without\n/// modifying the model or trainer logic.\n///\n/// # Returns\n/// A tuple `(penalty, grad)` where:\n/// - `penalty` is a scalar value added to the total loss (for logging/metrics),\n/// - `grad` is the gradient of the penalty w.r.t. model parameters, to be combined\n///   with the data-driven gradient during backpropagation.\npub trait Regularizer\u003cB: Backend, M: TrainableModel\u003cB\u003e\u003e {\n    fn regularizer_penalty_grad(\u0026self, model: \u0026M) -\u003e (Scalar\u003cB\u003e, M::Gradients);\n}\n\n/// L2 (ridge) regularization: penalty = Î» * ||w||Â².\n///\n/// Only applies to weights; bias is not regularized (standard practice).\n///\n/// Gradient w.r.t. weights: â/âw (Î» * wáµw) = 2Î»w.\npub struct L2\u003cB: Backend\u003e {\n    lambda: Scalar\u003cB\u003e,\n}\n\nimpl\u003cB: Backend\u003e L2\u003cB\u003e {\n    /// Creates an L2 regularizer with strength `lambda`.\n    ///\n    /// # Arguments\n    /// * `lambda` â non-negative regularization coefficient (Î» â¥ 0).\n    pub fn new(lambda: f64) -\u003e Self {\n        Self {\n            lambda: Scalar::\u003cB\u003e::new(lambda),\n        }\n    }\n}\n\nimpl\u003cB\u003e Regularizer\u003cB, LinearRegression\u003cB\u003e\u003e for L2\u003cB\u003e\nwhere\n    B: Backend,\n{\n    fn regularizer_penalty_grad(\n        \u0026self,\n        model: \u0026LinearRegression\u003cB\u003e,\n    ) -\u003e (\n        Scalar\u003cB\u003e,\n        \u003cLinearRegression\u003cB\u003e as TrainableModel\u003cB\u003e\u003e::Gradients,\n    ) {\n        let params = model.params();\n        let weight_grad = params.weights.scale(\u0026(self.lambda * Scalar::\u003cB\u003e::new(2.)));\n\n        let loss = params.weights.dot(\u0026params.weights);\n\n        let loss = self.lambda * loss;\n\n        (\n            loss,\n            LinearParams::\u003cB\u003e {\n                weights: weight_grad,\n                bias: Scalar::\u003cB\u003e::new(0.),\n            },\n        )\n    }\n}\n/// A no-op regularizer that adds zero penalty and zero gradient.\n///\n/// Useful as a default when no regularization is desired.\npub struct NoRegularizer;\n\nimpl\u003cB\u003e Regularizer\u003cB, LinearRegression\u003cB\u003e\u003e for NoRegularizer\nwhere\n    B: Backend,\n{\n    fn regularizer_penalty_grad(\n        \u0026self,\n        model: \u0026LinearRegression\u003cB\u003e,\n    ) -\u003e (\n        Scalar\u003cB\u003e,\n        \u003cLinearRegression\u003cB\u003e as TrainableModel\u003cB\u003e\u003e::Gradients,\n    ) {\n        let params = model.params();\n        let weight_grad = Tensor1D::\u003cB\u003e::zeros(params.weights.len());\n\n        (\n            Scalar::\u003cB\u003e::new(0.),\n            LinearParams::\u003cB\u003e {\n                weights: weight_grad,\n                bias: Scalar::\u003cB\u003e::new(0.),\n            },\n        )\n    }\n}\n\n#[cfg(test)]\nmod tests {\n    use super::*;\n    use crate::backend::CpuBackend;\n    use crate::model::linear::{LinearParams, LinearRegression};\n\n    #[test]\n    fn test_l2_regularizer() {\n        // Ð¡Ð¾Ð·Ð´Ð°ÑÐ¼ Ð¿Ð°ÑÐ°Ð¼ÐµÑÑÑ Ð½Ð°Ð¿ÑÑÐ¼ÑÑ\n        let weights = Tensor1D::\u003cCpuBackend\u003e::new(vec![3.0f32, 4.0]);\n        let bias = Scalar::\u003cCpuBackend\u003e::new(1.0);\n        let params = LinearParams { weights, bias };\n\n        let model = LinearRegression::\u003cCpuBackend\u003e::from_params(params);\n\n        let lambda = 0.5;\n        let l2 = L2::\u003cCpuBackend\u003e::new(lambda);\n\n        let (penalty, grad) = l2.regularizer_penalty_grad(\u0026model);\n\n        // ||w||Â² = 3Â² + 4Â² = 25\n        // penalty = Î» * ||w||Â² = 0.5 * 25 = 12.5\n        assert!((penalty.data - 12.5).abs() \u003c 1e-12);\n\n        // grad_w = 2 * Î» * w = 2 * 0.5 * [3, 4] = [3, 4]\n        assert_eq!(grad.weights.to_vec(), vec![3.0, 4.0]);\n        // grad_b = 0\n        assert_eq!(grad.bias.data, 0.0);\n    }\n\n    #[test]\n    fn test_no_regularizer() {\n        let weights = Tensor1D::\u003cCpuBackend\u003e::new(vec![1.0f32, 2.0, 3.0]);\n        let bias = Scalar::\u003cCpuBackend\u003e::new(5.0);\n        let params = LinearParams { weights, bias };\n        let model = LinearRegression::\u003cCpuBackend\u003e::from_params(params);\n\n        let no_reg = NoRegularizer;\n        let (penalty, grad) = no_reg.regularizer_penalty_grad(\u0026model);\n\n        assert_eq!(penalty.data, 0.0);\n        assert_eq!(grad.weights.to_vec(), vec![0.0, 0.0, 0.0]);\n        assert_eq!(grad.bias.data, 0.0);\n    }\n\n    #[test]\n    fn test_l2_zero_weights() {\n        let weights = Tensor1D::\u003cCpuBackend\u003e::zeros(2);\n        let bias = Scalar::\u003cCpuBackend\u003e::new(0.0);\n        let params = LinearParams { weights, bias };\n        let model = LinearRegression::\u003cCpuBackend\u003e::from_params(params);\n\n        let l2 = L2::\u003cCpuBackend\u003e::new(1.0);\n        let (penalty, grad) = l2.regularizer_penalty_grad(\u0026model);\n\n        assert_eq!(penalty.data, 0.0);\n        assert_eq!(grad.weights.to_vec(), vec![0.0, 0.0]);\n        assert_eq!(grad.bias.data, 0.0);\n    }\n}\n","traces":[{"line":34,"address":[2581392],"length":1,"stats":{"Line":1}},{"line":36,"address":[2581398],"length":1,"stats":{"Line":2}},{"line":45,"address":[2581900,2581408,2581906],"length":1,"stats":{"Line":2}},{"line":52,"address":[2581459],"length":1,"stats":{"Line":2}},{"line":53,"address":[2581490],"length":1,"stats":{"Line":2}},{"line":55,"address":[2581566,2581634],"length":1,"stats":{"Line":4}},{"line":57,"address":[2581643],"length":1,"stats":{"Line":2}},{"line":60,"address":[],"length":0,"stats":{"Line":0}},{"line":61,"address":[2581787],"length":1,"stats":{"Line":2}},{"line":62,"address":[2581675],"length":1,"stats":{"Line":2}},{"line":63,"address":[2581712],"length":1,"stats":{"Line":2}},{"line":77,"address":[2581936,2582264,2582270],"length":1,"stats":{"Line":1}},{"line":84,"address":[2581982],"length":1,"stats":{"Line":1}},{"line":85,"address":[2582003],"length":1,"stats":{"Line":1}},{"line":88,"address":[2582029],"length":1,"stats":{"Line":1}},{"line":89,"address":[2582175],"length":1,"stats":{"Line":1}},{"line":90,"address":[2582080],"length":1,"stats":{"Line":1}},{"line":91,"address":[2582108],"length":1,"stats":{"Line":1}}],"covered":17,"coverable":18},{"path":["/","home","victor","ML","machinelearne-rs","lib","src","serialization.rs"],"content":"//! Serialization of fitted model parameters.\n//!\n//! This module provides a backend-agnostic way to serialize and deserialize\n//! the numerical parameters of a fitted model, without coupling to specific\n//! serialization formats or backend resources (e.g., GPU buffers).\n\nuse std::error::Error;\n\n/// A trait for parameter representations that can be serialized to and from bytes.\n///\n/// Implementors should contain only plain numerical data (e.g., `Vec\u003cf32\u003e`, scalars),\n/// not backend-specific tensors or handles.\npub trait SerializableParams: Sized {\n    /// The error type returned during (de)serialization.\n    type Error: Error + Send + Sync + 'static;\n\n    /// Serialize the parameters into a byte buffer.\n    fn to_bytes(\u0026self) -\u003e Result\u003cVec\u003cu8\u003e, Self::Error\u003e;\n\n    /// Deserialize the parameters from a byte buffer.\n    fn from_bytes(bytes: \u0026[u8]) -\u003e Result\u003cSelf, Self::Error\u003e;\n}\n\n// Optional serde integration\n#[cfg(feature = \"serde\")]\nimpl\u003cT\u003e SerializableParams for T\nwhere\n    T: serde::Serialize + for\u003c'de\u003e serde::Deserialize\u003c'de\u003e,\n{\n    type Error = bincode::Error;\n\n    fn to_bytes(\u0026self) -\u003e Result\u003cVec\u003cu8\u003e, Self::Error\u003e {\n        bincode::serialize(self)\n    }\n\n    fn from_bytes(bytes: \u0026[u8]) -\u003e Result\u003cSelf, Self::Error\u003e {\n        bincode::deserialize(bytes)\n    }\n}\n","traces":[{"line":32,"address":[3407552],"length":1,"stats":{"Line":8}},{"line":33,"address":[3440513],"length":1,"stats":{"Line":8}},{"line":36,"address":[2823056],"length":1,"stats":{"Line":7}},{"line":37,"address":[2005861],"length":1,"stats":{"Line":7}}],"covered":4,"coverable":4},{"path":["/","home","victor","ML","machinelearne-rs","lib","src","trainer","mod.rs"],"content":"// trainer/mod.rs\nuse crate::{\n    backend::{Backend, Scalar, ScalarOps, Tensor1D, Tensor2D},\n    dataset::Dataset,\n    loss::Loss,\n    model::{ParamOps, TrainableModel},\n    optimizer::Optimizer,\n    regularizers::Regularizer,\n};\nuse std::fmt::{Debug, Display};\nuse std::marker::PhantomData;\n\n/// Orchestrates the training loop for a `TrainableModel`.\n///\n/// Combines a loss function, optimizer, and regularizer to fit a model on a dataset.\n/// Once built via `TrainerBuilder`, it is immutable and can be reused across multiple models\n/// (as long as types match).\n///\n/// The `fit` method returns a `FittedModel` (via `IntoFitted`), which contains only inference logic.\npub struct Trainer\u003cB, L, O, M, P, R\u003e\nwhere\n    B: Backend,\n    L: Loss\u003cB\u003e,\n    M: TrainableModel\u003cB, Params = P, Gradients = P\u003e,\n    O: Optimizer\u003cB, P\u003e,\n    R: Regularizer\u003cB, M\u003e,\n{\n    pub(crate) batch_size: usize,\n    pub(crate) max_epochs: usize,\n    pub(crate) verbose: bool,\n    pub(crate) loss_fn: L,\n    pub(crate) optimizer: O,\n    pub(crate) regularizer: R,\n    _phantom_backend: PhantomData\u003cB\u003e,\n    _phantom_model: PhantomData\u003cM\u003e,\n}\n\n/// Fluent builder for constructing a `Trainer` with custom hyperparameters.\n///\n/// Defaults:\n/// - `batch_size`: 32\n/// - `max_epochs`: 1000\n/// - `verbose`: true\npub struct TrainerBuilder\u003cB, L, O, M, P, R\u003e\nwhere\n    B: Backend,\n    L: Loss\u003cB\u003e,\n    M: TrainableModel\u003cB, Params = P, Gradients = P\u003e,\n    O: Optimizer\u003cB, P\u003e,\n    R: Regularizer\u003cB, M\u003e,\n{\n    batch_size: usize,\n    max_epochs: usize,\n    verbose: bool,\n    loss_fn: L,\n    optimizer: O,\n    regularizer: R,\n    _phantom_backend: PhantomData\u003cB\u003e,\n    _phantom_model: PhantomData\u003cM\u003e,\n}\n\nimpl\u003cB, L, O, M, P, R\u003e TrainerBuilder\u003cB, L, O, M, P, R\u003e\nwhere\n    B: Backend,\n    L: Loss\u003cB\u003e,\n    M: TrainableModel\u003cB, Params = P, Gradients = P\u003e,\n    O: Optimizer\u003cB, P\u003e,\n    R: Regularizer\u003cB, M\u003e,\n{\n    /// Creates a new `TrainerBuilder` with the given components.\n    ///\n    /// # Arguments\n    /// * `loss_fn` â differentiable loss (e.g., `MSELoss`)\n    /// * `optimizer` â parameter updater (e.g., `SGD`)\n    /// * `regularizer` â optional penalty term (e.g., `L2` or `NoRegularizer`)\n    pub fn new(loss_fn: L, optimizer: O, regularizer: R) -\u003e Self {\n        Self {\n            batch_size: 32,\n            max_epochs: 1000,\n            verbose: true,\n            loss_fn,\n            optimizer,\n            regularizer,\n            _phantom_backend: PhantomData,\n            _phantom_model: PhantomData,\n        }\n    }\n\n    pub fn batch_size(mut self, size: usize) -\u003e Self {\n        self.batch_size = size;\n        self\n    }\n\n    pub fn max_epochs(mut self, epochs: usize) -\u003e Self {\n        self.max_epochs = epochs;\n        self\n    }\n\n    /// Sets verbosity for training output.\n    ///\n    /// When `false`, suppresses epoch-by-epoch loss output.\n    /// Useful for benchmarking or production training.\n    pub fn verbose(mut self, verbose: bool) -\u003e Self {\n        self.verbose = verbose;\n        self\n    }\n\n    pub fn build(self) -\u003e Trainer\u003cB, L, O, M, P, R\u003e {\n        Trainer {\n            batch_size: self.batch_size,\n            max_epochs: self.max_epochs,\n            verbose: self.verbose,\n            loss_fn: self.loss_fn,\n            optimizer: self.optimizer,\n            regularizer: self.regularizer,\n            _phantom_backend: PhantomData,\n            _phantom_model: PhantomData,\n        }\n    }\n}\n\n// --- Ð ÐµÐ°Ð»Ð¸Ð·Ð°ÑÐ¸Ñ fit Ð¿ÐµÑÐµÐ½Ð¾ÑÐ¸ÑÑÑ Ð² Trainer ---\nimpl\u003cB, L, O, M, P, R\u003e Trainer\u003cB, L, O, M, P, R\u003e\nwhere\n    B: Backend,\n    B::Scalar: Debug + Display,\n    L: Loss\u003cB, Target = Tensor1D\u003cB\u003e, Prediction = Tensor1D\u003cB\u003e\u003e,\n    M: TrainableModel\u003c\n        B,\n        Input = Tensor2D\u003cB\u003e,\n        Prediction = L::Prediction,\n        Params = P,\n        Gradients = P,\n    \u003e,\n    O: Optimizer\u003cB, P\u003e,\n    R: Regularizer\u003cB, M\u003e,\n    P: ParamOps\u003cB\u003e,\n{\n    /// Trains the model on the provided dataset for up to `max_epochs`.\n    ///\n    /// # Returns\n    /// A fitted model ready for inference (`M::Output`), or an error if:\n    /// - The dataset is empty\n    /// - The dataset length is unknown (required for loss averaging)\n    /// - A batch fails to load\n    ///\n    /// # Notes\n    /// - Loss is averaged over the entire dataset per epoch.\n    /// - Gradients are averaged per batch before applying regularization.\n    pub fn fit\u003cD\u003e(\u0026self, mut model: M, dataset: \u0026D) -\u003e Result\u003cM::Output, String\u003e\n    where\n        D: Dataset,\n    {\n        let n_total = dataset.len().ok_or(\"Dataset length unknown\")?;\n        if n_total == 0 {\n            return Err(\"Dataset is empty\".into());\n        }\n\n        for epoch in 0..self.max_epochs {\n            let mut total_loss = Scalar::\u003cB\u003e::new(0.);\n            //let mut total_samples = 0;\n            for batch_result in dataset.batches::\u003cB\u003e(self.batch_size) {\n                let (batch_x, batch_y) =\n                    batch_result.map_err(|e| format!(\"Data error: {:?}\", e))?;\n                //let mut total_loss = Scalar::\u003cB\u003e::new(0.);\n                let preds = model.forward(\u0026batch_x);\n                total_loss = total_loss + self.loss_fn.loss(\u0026preds, \u0026batch_y);\n                let (reg_penalty, reg_grad) = self.regularizer.regularizer_penalty_grad(\u0026model);\n                total_loss = total_loss + reg_penalty;\n                let grad_preds = self.loss_fn.grad_wrt_prediction(\u0026preds, \u0026batch_y);\n                let grads = model.backward(\u0026batch_x, \u0026grad_preds);\n\n                let total_grads = grads.add(\u0026reg_grad);\n                let new_params = self.optimizer.step(model.params(), \u0026total_grads);\n                model.update_params(\u0026new_params);\n            }\n\n            let avg_loss = total_loss / Scalar::\u003cB\u003e::new(n_total as f64);\n            if self.verbose {\n                println!(\"Epoch {}: loss = {}\", epoch, avg_loss.data.to_f64());\n            }\n        }\n\n        Ok(model.into_fitted())\n    }\n}\n\n// --- Ð­ÐºÑÐ¿Ð¾ÑÑ ÑÐ´Ð¾Ð±Ð½Ð¾Ð³Ð¾ ÐºÐ¾Ð½ÑÑÑÑÐºÑÐ¾ÑÐ° ---\nimpl\u003cB, L, O, M, P, R\u003e Trainer\u003cB, L, O, M, P, R\u003e\nwhere\n    B: Backend,\n    L: Loss\u003cB\u003e,\n    M: TrainableModel\u003cB, Params = P, Gradients = P\u003e,\n    O: Optimizer\u003cB, P\u003e,\n    R: Regularizer\u003cB, M\u003e,\n{\n    /// Convenience constructor that starts the builder pattern.\n    ///\n    /// Equivalent to `TrainerBuilder::new(...)`.\n    pub fn builder(loss_fn: L, optimizer: O, regularizer: R) -\u003e TrainerBuilder\u003cB, L, O, M, P, R\u003e {\n        TrainerBuilder::new(loss_fn, optimizer, regularizer)\n    }\n}\n\n#[cfg(test)]\nmod tests {\n    use super::*;\n    use crate::{\n        backend::CpuBackend,\n        dataset::memory::InMemoryDataset,\n        loss::MSELoss,\n        model::linear::InferenceModel,\n        model::linear::LinearRegression,\n        optimizer::SGD,\n        regularizers::{NoRegularizer, L2},\n    };\n\n    // === TrainerBuilder Tests ===\n\n    #[test]\n    fn test_trainer_builder_default_values() {\n        let builder = TrainerBuilder::new(MSELoss, SGD::\u003cCpuBackend\u003e::new(0.01), NoRegularizer);\n\n        assert_eq!(builder.batch_size, 32);\n        assert_eq!(builder.max_epochs, 1000);\n        assert_eq!(builder.verbose, true);\n    }\n\n    #[test]\n    fn test_trainer_builder_custom_batch_size() {\n        let builder = TrainerBuilder::new(MSELoss, SGD::\u003cCpuBackend\u003e::new(0.01), NoRegularizer)\n            .batch_size(64);\n\n        assert_eq!(builder.batch_size, 64);\n        assert_eq!(builder.max_epochs, 1000);\n    }\n\n    #[test]\n    fn test_trainer_builder_custom_max_epochs() {\n        let builder = TrainerBuilder::new(MSELoss, SGD::\u003cCpuBackend\u003e::new(0.01), NoRegularizer)\n            .max_epochs(500);\n\n        assert_eq!(builder.batch_size, 32);\n        assert_eq!(builder.max_epochs, 500);\n    }\n\n    #[test]\n    fn test_trainer_builder_verbose() {\n        let builder = TrainerBuilder::new(MSELoss, SGD::\u003cCpuBackend\u003e::new(0.01), NoRegularizer)\n            .verbose(false);\n\n        assert_eq!(builder.verbose, false);\n\n        let builder_verbose =\n            TrainerBuilder::new(MSELoss, SGD::\u003cCpuBackend\u003e::new(0.01), NoRegularizer).verbose(true);\n\n        assert_eq!(builder_verbose.verbose, true);\n    }\n\n    #[test]\n    fn test_trainer_builder_chaining() {\n        let builder = TrainerBuilder::new(MSELoss, SGD::\u003cCpuBackend\u003e::new(0.01), NoRegularizer)\n            .batch_size(128)\n            .max_epochs(250)\n            .verbose(false);\n\n        assert_eq!(builder.batch_size, 128);\n        assert_eq!(builder.max_epochs, 250);\n        assert_eq!(builder.verbose, false);\n    }\n\n    #[test]\n    fn test_trainer_builder_chaining_order_independent() {\n        let builder1 = TrainerBuilder::new(MSELoss, SGD::\u003cCpuBackend\u003e::new(0.01), NoRegularizer)\n            .batch_size(16)\n            .max_epochs(100);\n\n        let builder2 = TrainerBuilder::new(MSELoss, SGD::\u003cCpuBackend\u003e::new(0.01), NoRegularizer)\n            .max_epochs(100)\n            .batch_size(16);\n\n        assert_eq!(builder1.batch_size, builder2.batch_size);\n        assert_eq!(builder1.max_epochs, builder2.max_epochs);\n    }\n\n    #[test]\n    fn test_trainer_builder_small_batch_size() {\n        let builder =\n            TrainerBuilder::new(MSELoss, SGD::\u003cCpuBackend\u003e::new(0.01), NoRegularizer).batch_size(1);\n\n        assert_eq!(builder.batch_size, 1);\n    }\n\n    #[test]\n    fn test_trainer_builder_large_batch_size() {\n        let builder = TrainerBuilder::new(MSELoss, SGD::\u003cCpuBackend\u003e::new(0.01), NoRegularizer)\n            .batch_size(10000);\n\n        assert_eq!(builder.batch_size, 10000);\n    }\n\n    #[test]\n    fn test_trainer_builder_zero_epochs() {\n        let builder =\n            TrainerBuilder::new(MSELoss, SGD::\u003cCpuBackend\u003e::new(0.01), NoRegularizer).max_epochs(0);\n\n        assert_eq!(builder.max_epochs, 0);\n    }\n\n    #[test]\n    fn test_trainer_builder_large_epochs() {\n        let builder = TrainerBuilder::new(MSELoss, SGD::\u003cCpuBackend\u003e::new(0.01), NoRegularizer)\n            .max_epochs(100000);\n\n        assert_eq!(builder.max_epochs, 100000);\n    }\n\n    #[test]\n    fn test_trainer_builder_creates_valid_trainer() {\n        let builder = TrainerBuilder::new(MSELoss, SGD::\u003cCpuBackend\u003e::new(0.01), NoRegularizer)\n            .batch_size(64)\n            .max_epochs(200)\n            .verbose(false);\n\n        let trainer = builder.build();\n\n        assert_eq!(trainer.batch_size, 64);\n        assert_eq!(trainer.max_epochs, 200);\n        assert_eq!(trainer.verbose, false);\n    }\n\n    #[test]\n    fn test_trainer_builder_does_not_consume_loss_fn() {\n        let _builder = TrainerBuilder::new(MSELoss, SGD::\u003cCpuBackend\u003e::new(0.01), NoRegularizer)\n            .batch_size(16)\n            .max_epochs(50);\n\n        // Components are reused via Clone for SGD, creating fresh instances for other builders\n        let _builder2 = TrainerBuilder::new(MSELoss, SGD::\u003cCpuBackend\u003e::new(0.01), NoRegularizer);\n    }\n\n    #[test]\n    fn test_trainer_builder_clone_components() {\n        let builder1 = TrainerBuilder::new(MSELoss, SGD::\u003cCpuBackend\u003e::new(0.01), NoRegularizer)\n            .batch_size(32)\n            .max_epochs(100);\n\n        // SGD implements Clone, so we can use it multiple times\n        let _builder2 = TrainerBuilder::new(MSELoss, builder1.optimizer.clone(), NoRegularizer);\n    }\n\n    #[test]\n    fn test_trainer_builder_zero_batch_size() {\n        // Builder allows 0 batch size - this is up to the user to validate\n        let builder =\n            TrainerBuilder::new(MSELoss, SGD::\u003cCpuBackend\u003e::new(0.01), NoRegularizer).batch_size(0);\n\n        assert_eq!(builder.batch_size, 0);\n\n        // Building should still work, but fit() will fail on batch iteration\n        let trainer = builder.build();\n        assert_eq!(trainer.batch_size, 0);\n    }\n\n    // === Trainer Tests (existing tests preserved) ===\n\n    #[test]\n    fn test_trainer_fit_linear_regression() {\n        // Ð¡Ð¾Ð·Ð´Ð°ÑÐ¼ ÑÐ¸Ð½ÑÐµÑÐ¸ÑÐµÑÐºÐ¸Ð¹ Ð´Ð°ÑÐ°ÑÐµÑ: y = 2*x1 + 3*x2 + 1\n        let x = vec![\n            vec![1.0, 0.0],\n            vec![0.0, 1.0],\n            vec![1.0, 1.0],\n            vec![2.0, 3.0],\n        ];\n        let y = vec![3.0, 4.0, 6.0, 14.0]; // 2*2 + 3*3 + 1 = 4+9+1=14\n        let dataset = InMemoryDataset::new(x, y).unwrap();\n\n        let model = LinearRegression::\u003cCpuBackend\u003e::new(2);\n        let loss = MSELoss;\n        let optimizer = SGD::new(0.1); // learning rate\n        let regularizer = NoRegularizer;\n\n        let trainer = Trainer::builder(loss, optimizer, regularizer)\n            .batch_size(4)\n            .max_epochs(100)\n            .verbose(false) // Suppress output in tests\n            .build();\n\n        let fitted_model = trainer.fit(model, \u0026dataset).unwrap();\n\n        // ÐÑÐ¾Ð²ÐµÑÐ¸Ð¼ Ð¿ÑÐµÐ´ÑÐºÐ°Ð·Ð°Ð½Ð¸Ñ\n        let test_input = Tensor2D::\u003cCpuBackend\u003e::new(vec![1.0, 0.0, 0.0, 1.0], 2, 2);\n        let preds = fitted_model.predict_batch(\u0026test_input);\n        let pred_vec = preds.to_vec();\n\n        // ÐÐ¶Ð¸Ð´Ð°ÐµÐ¼ Ð¿ÑÐ¸Ð±Ð»Ð¸Ð¶ÐµÐ½Ð¸Ðµ Ðº [3.0, 4.0]\n        assert!((pred_vec[0] - 3.0).abs() \u003c 0.5);\n        assert!((pred_vec[1] - 4.0).abs() \u003c 0.5);\n    }\n\n    #[test]\n    fn test_trainer_with_l2_regularization() {\n        let x = vec![vec![1.0], vec![2.0], vec![3.0]];\n        let y = vec![2.0, 4.0, 6.0]; // y = 2*x\n        let dataset = InMemoryDataset::new(x, y).unwrap();\n\n        let model = LinearRegression::\u003cCpuBackend\u003e::new(1);\n        let loss = MSELoss;\n        let optimizer = SGD::new(0.01);\n        let regularizer = L2::\u003cCpuBackend\u003e::new(1.0); // ÑÐ¸Ð»ÑÐ½Ð°Ñ ÑÐµÐ³ÑÐ»ÑÑÐ¸Ð·Ð°ÑÐ¸Ñ\n\n        let trainer = Trainer::builder(loss, optimizer, regularizer)\n            .batch_size(3)\n            .max_epochs(500)\n            .verbose(false) // Suppress output in tests\n            .build();\n\n        let fitted_model = trainer.fit(model, \u0026dataset).unwrap();\n        let weights = fitted_model.extract_params().weights;\n\n        // ÐÐµÐ· ÑÐµÐ³ÑÐ»ÑÑÐ¸Ð·Ð°ÑÐ¸Ð¸ Ð²ÐµÑ Ð±ÑÐ» Ð±Ñ ~2.0, Ñ L2 â Ð¼ÐµÐ½ÑÑÐµ\n        assert!(weights[0] \u003c 2.0);\n        assert!(weights[0] \u003e 0.0);\n    }\n\n    #[test]\n    fn test_trainer_empty_dataset() {\n        let x = vec![];\n        let y = vec![];\n        // Empty datasets should error at creation\n        let _dataset = InMemoryDataset::new(x, y).unwrap_err();\n    }\n\n    #[test]\n    fn test_trainer_unknown_dataset_length() {\n        // Ð¡Ð¾Ð·Ð´Ð°Ð´Ð¸Ð¼ mock-Ð´Ð°ÑÐ°ÑÐµÑ Ð±ÐµÐ· len()\n        struct MockDatasetWithoutLen {\n            x: Vec\u003cVec\u003cf32\u003e\u003e,\n            y: Vec\u003cf32\u003e,\n        }\n\n        impl Dataset for MockDatasetWithoutLen {\n            type Error = String; // â Ð¼ÐµÐ½ÑÐµÐ¼ Ð½Ð° String\n            type Item = ();\n\n            fn len(\u0026self) -\u003e Option\u003cusize\u003e {\n                None\n            }\n\n            fn get_batch\u003cB: Backend\u003e(\n                \u0026self,\n                range: std::ops::Range\u003cusize\u003e,\n            ) -\u003e Result\u003c(Tensor2D\u003cB\u003e, Tensor1D\u003cB\u003e), Self::Error\u003e {\n                let batch_x = \u0026self.x[range.clone()];\n                let batch_y = \u0026self.y[range];\n                let n = batch_x.len();\n                if n == 0 {\n                    return Err(\"Empty batch\".into());\n                }\n                let cols = batch_x[0].len();\n                let data = batch_x.iter().flat_map(|r| r.iter()).copied().collect();\n                Ok((\n                    Tensor2D::new(data, n, cols),\n                    Tensor1D::new(batch_y.to_vec()),\n                ))\n            }\n        }\n\n        let dataset = MockDatasetWithoutLen {\n            x: vec![vec![1.0]],\n            y: vec![1.0],\n        };\n\n        let model = LinearRegression::\u003cCpuBackend\u003e::new(1);\n        let loss = MSELoss;\n        let optimizer = SGD::\u003cCpuBackend\u003e::new(0.1);\n        let regularizer = NoRegularizer;\n\n        let trainer = Trainer::builder(loss, optimizer, regularizer)\n            .batch_size(1)\n            .max_epochs(1)\n            .build();\n\n        let result = trainer.fit(model, \u0026dataset);\n        assert!(result.is_err());\n    }\n}\n","traces":[{"line":76,"address":[2955920,2955872],"length":1,"stats":{"Line":3}},{"line":89,"address":[2955680,2955728],"length":1,"stats":{"Line":2}},{"line":90,"address":[2955688,2955745],"length":1,"stats":{"Line":4}},{"line":91,"address":[2955748,2955691],"length":1,"stats":{"Line":6}},{"line":94,"address":[2955776,2955824],"length":1,"stats":{"Line":5}},{"line":95,"address":[2955793,2955832],"length":1,"stats":{"Line":5}},{"line":96,"address":[2955797,2955836],"length":1,"stats":{"Line":5}},{"line":103,"address":[2956128,2956080],"length":1,"stats":{"Line":2}},{"line":104,"address":[2956103,2956142],"length":1,"stats":{"Line":2}},{"line":105,"address":[2956108,2956148],"length":1,"stats":{"Line":3}},{"line":108,"address":[2955968,2956032],"length":1,"stats":{"Line":2}},{"line":110,"address":[2956038,2955974],"length":1,"stats":{"Line":2}},{"line":111,"address":[2956041,2955977],"length":1,"stats":{"Line":2}},{"line":112,"address":[2955981,2956045],"length":1,"stats":{"Line":2}},{"line":113,"address":[],"length":0,"stats":{"Line":0}},{"line":114,"address":[2955985,2956049],"length":1,"stats":{"Line":3}},{"line":115,"address":[2955991],"length":1,"stats":{"Line":1}},{"line":150,"address":[2949952,2947392,2949900,2952512,2955020,2952476,2955036,2949916,2952460],"length":1,"stats":{"Line":3}},{"line":154,"address":[2950026,2955031,2952471,2949911,2947466,2952682,2952586,2947562,2950122],"length":1,"stats":{"Line":7}},{"line":155,"address":[2950285,2952845,2947725],"length":1,"stats":{"Line":2}},{"line":156,"address":[2947810,2952930,2947731,2952851,2950370,2950291],"length":1,"stats":{"Line":0}},{"line":159,"address":[2953007,2947887,2950333,2947773,2952893,2950447],"length":1,"stats":{"Line":4}},{"line":160,"address":[2953282,2950551,2950722,2947991,2953111,2948162],"length":1,"stats":{"Line":4}},{"line":162,"address":[2953291,2948171,2950731],"length":1,"stats":{"Line":2}},{"line":163,"address":[2953520,2955290,2951321,2950960,2948400,2955026,2949906,2955098,2952466,2955450,2955424,2955072,2948761,2955264,2955457,2955297,2953881],"length":1,"stats":{"Line":4}},{"line":164,"address":[],"length":0,"stats":{"Line":0}},{"line":166,"address":[2954158,2949038,2951598],"length":1,"stats":{"Line":2}},{"line":167,"address":[2949190,2949093,2954310,2951653,2951750,2954213],"length":1,"stats":{"Line":4}},{"line":168,"address":[2954354,2951794,2949234],"length":1,"stats":{"Line":2}},{"line":169,"address":[2954423,2951863,2954499,2951939,2949379,2949303],"length":1,"stats":{"Line":4}},{"line":170,"address":[2949388,2951948,2954508],"length":1,"stats":{"Line":2}},{"line":171,"address":[2952015,2949455,2954575],"length":1,"stats":{"Line":2}},{"line":173,"address":[2949526,2954646,2952086],"length":1,"stats":{"Line":2}},{"line":174,"address":[2954790,2949670,2952230,2949586,2952146,2954706],"length":1,"stats":{"Line":4}},{"line":175,"address":[2952253,2954813,2949693],"length":1,"stats":{"Line":2}},{"line":178,"address":[2950978,2953538,2948418],"length":1,"stats":{"Line":2}},{"line":179,"address":[2951093,2948533,2953653],"length":1,"stats":{"Line":2}},{"line":180,"address":[2953669,2948549,2951109],"length":1,"stats":{"Line":0}},{"line":184,"address":[2948015,2950575,2953135],"length":1,"stats":{"Line":2}},{"line":200,"address":[2955632,2955584],"length":1,"stats":{"Line":2}},{"line":201,"address":[2955602,2955656],"length":1,"stats":{"Line":2}}],"covered":37,"coverable":41}]};
    </script>
    <script crossorigin>/** @license React v16.13.1
 * react.production.min.js
 *
 * Copyright (c) Facebook, Inc. and its affiliates.
 *
 * This source code is licensed under the MIT license found in the
 * LICENSE file in the root directory of this source tree.
 */
'use strict';(function(d,r){"object"===typeof exports&&"undefined"!==typeof module?r(exports):"function"===typeof define&&define.amd?define(["exports"],r):(d=d||self,r(d.React={}))})(this,function(d){function r(a){for(var b="https://reactjs.org/docs/error-decoder.html?invariant="+a,c=1;c<arguments.length;c++)b+="&args[]="+encodeURIComponent(arguments[c]);return"Minified React error #"+a+"; visit "+b+" for the full message or use the non-minified dev environment for full errors and additional helpful warnings."}
function w(a,b,c){this.props=a;this.context=b;this.refs=ba;this.updater=c||ca}function da(){}function L(a,b,c){this.props=a;this.context=b;this.refs=ba;this.updater=c||ca}function ea(a,b,c){var g,e={},fa=null,d=null;if(null!=b)for(g in void 0!==b.ref&&(d=b.ref),void 0!==b.key&&(fa=""+b.key),b)ha.call(b,g)&&!ia.hasOwnProperty(g)&&(e[g]=b[g]);var h=arguments.length-2;if(1===h)e.children=c;else if(1<h){for(var k=Array(h),f=0;f<h;f++)k[f]=arguments[f+2];e.children=k}if(a&&a.defaultProps)for(g in h=a.defaultProps,
h)void 0===e[g]&&(e[g]=h[g]);return{$$typeof:x,type:a,key:fa,ref:d,props:e,_owner:M.current}}function va(a,b){return{$$typeof:x,type:a.type,key:b,ref:a.ref,props:a.props,_owner:a._owner}}function N(a){return"object"===typeof a&&null!==a&&a.$$typeof===x}function wa(a){var b={"=":"=0",":":"=2"};return"$"+(""+a).replace(/[=:]/g,function(a){return b[a]})}function ja(a,b,c,g){if(C.length){var e=C.pop();e.result=a;e.keyPrefix=b;e.func=c;e.context=g;e.count=0;return e}return{result:a,keyPrefix:b,func:c,
context:g,count:0}}function ka(a){a.result=null;a.keyPrefix=null;a.func=null;a.context=null;a.count=0;10>C.length&&C.push(a)}function O(a,b,c,g){var e=typeof a;if("undefined"===e||"boolean"===e)a=null;var d=!1;if(null===a)d=!0;else switch(e){case "string":case "number":d=!0;break;case "object":switch(a.$$typeof){case x:case xa:d=!0}}if(d)return c(g,a,""===b?"."+P(a,0):b),1;d=0;b=""===b?".":b+":";if(Array.isArray(a))for(var f=0;f<a.length;f++){e=a[f];var h=b+P(e,f);d+=O(e,h,c,g)}else if(null===a||
"object"!==typeof a?h=null:(h=la&&a[la]||a["@@iterator"],h="function"===typeof h?h:null),"function"===typeof h)for(a=h.call(a),f=0;!(e=a.next()).done;)e=e.value,h=b+P(e,f++),d+=O(e,h,c,g);else if("object"===e)throw c=""+a,Error(r(31,"[object Object]"===c?"object with keys {"+Object.keys(a).join(", ")+"}":c,""));return d}function Q(a,b,c){return null==a?0:O(a,"",b,c)}function P(a,b){return"object"===typeof a&&null!==a&&null!=a.key?wa(a.key):b.toString(36)}function ya(a,b,c){a.func.call(a.context,b,
a.count++)}function za(a,b,c){var g=a.result,e=a.keyPrefix;a=a.func.call(a.context,b,a.count++);Array.isArray(a)?R(a,g,c,function(a){return a}):null!=a&&(N(a)&&(a=va(a,e+(!a.key||b&&b.key===a.key?"":(""+a.key).replace(ma,"$&/")+"/")+c)),g.push(a))}function R(a,b,c,g,e){var d="";null!=c&&(d=(""+c).replace(ma,"$&/")+"/");b=ja(b,d,g,e);Q(a,za,b);ka(b)}function t(){var a=na.current;if(null===a)throw Error(r(321));return a}function S(a,b){var c=a.length;a.push(b);a:for(;;){var g=c-1>>>1,e=a[g];if(void 0!==
e&&0<D(e,b))a[g]=b,a[c]=e,c=g;else break a}}function n(a){a=a[0];return void 0===a?null:a}function E(a){var b=a[0];if(void 0!==b){var c=a.pop();if(c!==b){a[0]=c;a:for(var g=0,e=a.length;g<e;){var d=2*(g+1)-1,f=a[d],h=d+1,k=a[h];if(void 0!==f&&0>D(f,c))void 0!==k&&0>D(k,f)?(a[g]=k,a[h]=c,g=h):(a[g]=f,a[d]=c,g=d);else if(void 0!==k&&0>D(k,c))a[g]=k,a[h]=c,g=h;else break a}}return b}return null}function D(a,b){var c=a.sortIndex-b.sortIndex;return 0!==c?c:a.id-b.id}function F(a){for(var b=n(u);null!==
b;){if(null===b.callback)E(u);else if(b.startTime<=a)E(u),b.sortIndex=b.expirationTime,S(p,b);else break;b=n(u)}}function T(a){y=!1;F(a);if(!v)if(null!==n(p))v=!0,z(U);else{var b=n(u);null!==b&&G(T,b.startTime-a)}}function U(a,b){v=!1;y&&(y=!1,V());H=!0;var c=m;try{F(b);for(l=n(p);null!==l&&(!(l.expirationTime>b)||a&&!W());){var g=l.callback;if(null!==g){l.callback=null;m=l.priorityLevel;var e=g(l.expirationTime<=b);b=q();"function"===typeof e?l.callback=e:l===n(p)&&E(p);F(b)}else E(p);l=n(p)}if(null!==
l)var d=!0;else{var f=n(u);null!==f&&G(T,f.startTime-b);d=!1}return d}finally{l=null,m=c,H=!1}}function oa(a){switch(a){case 1:return-1;case 2:return 250;case 5:return 1073741823;case 4:return 1E4;default:return 5E3}}var f="function"===typeof Symbol&&Symbol.for,x=f?Symbol.for("react.element"):60103,xa=f?Symbol.for("react.portal"):60106,Aa=f?Symbol.for("react.fragment"):60107,Ba=f?Symbol.for("react.strict_mode"):60108,Ca=f?Symbol.for("react.profiler"):60114,Da=f?Symbol.for("react.provider"):60109,
Ea=f?Symbol.for("react.context"):60110,Fa=f?Symbol.for("react.forward_ref"):60112,Ga=f?Symbol.for("react.suspense"):60113,Ha=f?Symbol.for("react.memo"):60115,Ia=f?Symbol.for("react.lazy"):60116,la="function"===typeof Symbol&&Symbol.iterator,pa=Object.getOwnPropertySymbols,Ja=Object.prototype.hasOwnProperty,Ka=Object.prototype.propertyIsEnumerable,I=function(){try{if(!Object.assign)return!1;var a=new String("abc");a[5]="de";if("5"===Object.getOwnPropertyNames(a)[0])return!1;var b={};for(a=0;10>a;a++)b["_"+
String.fromCharCode(a)]=a;if("0123456789"!==Object.getOwnPropertyNames(b).map(function(a){return b[a]}).join(""))return!1;var c={};"abcdefghijklmnopqrst".split("").forEach(function(a){c[a]=a});return"abcdefghijklmnopqrst"!==Object.keys(Object.assign({},c)).join("")?!1:!0}catch(g){return!1}}()?Object.assign:function(a,b){if(null===a||void 0===a)throw new TypeError("Object.assign cannot be called with null or undefined");var c=Object(a);for(var g,e=1;e<arguments.length;e++){var d=Object(arguments[e]);
for(var f in d)Ja.call(d,f)&&(c[f]=d[f]);if(pa){g=pa(d);for(var h=0;h<g.length;h++)Ka.call(d,g[h])&&(c[g[h]]=d[g[h]])}}return c},ca={isMounted:function(a){return!1},enqueueForceUpdate:function(a,b,c){},enqueueReplaceState:function(a,b,c,d){},enqueueSetState:function(a,b,c,d){}},ba={};w.prototype.isReactComponent={};w.prototype.setState=function(a,b){if("object"!==typeof a&&"function"!==typeof a&&null!=a)throw Error(r(85));this.updater.enqueueSetState(this,a,b,"setState")};w.prototype.forceUpdate=
function(a){this.updater.enqueueForceUpdate(this,a,"forceUpdate")};da.prototype=w.prototype;f=L.prototype=new da;f.constructor=L;I(f,w.prototype);f.isPureReactComponent=!0;var M={current:null},ha=Object.prototype.hasOwnProperty,ia={key:!0,ref:!0,__self:!0,__source:!0},ma=/\/+/g,C=[],na={current:null},X;if("undefined"===typeof window||"function"!==typeof MessageChannel){var A=null,qa=null,ra=function(){if(null!==A)try{var a=q();A(!0,a);A=null}catch(b){throw setTimeout(ra,0),b;}},La=Date.now();var q=
function(){return Date.now()-La};var z=function(a){null!==A?setTimeout(z,0,a):(A=a,setTimeout(ra,0))};var G=function(a,b){qa=setTimeout(a,b)};var V=function(){clearTimeout(qa)};var W=function(){return!1};f=X=function(){}}else{var Y=window.performance,sa=window.Date,Ma=window.setTimeout,Na=window.clearTimeout;"undefined"!==typeof console&&(f=window.cancelAnimationFrame,"function"!==typeof window.requestAnimationFrame&&console.error("This browser doesn't support requestAnimationFrame. Make sure that you load a polyfill in older browsers. https://fb.me/react-polyfills"),
"function"!==typeof f&&console.error("This browser doesn't support cancelAnimationFrame. Make sure that you load a polyfill in older browsers. https://fb.me/react-polyfills"));if("object"===typeof Y&&"function"===typeof Y.now)q=function(){return Y.now()};else{var Oa=sa.now();q=function(){return sa.now()-Oa}}var J=!1,K=null,Z=-1,ta=5,ua=0;W=function(){return q()>=ua};f=function(){};X=function(a){0>a||125<a?console.error("forceFrameRate takes a positive int between 0 and 125, forcing framerates higher than 125 fps is not unsupported"):
ta=0<a?Math.floor(1E3/a):5};var B=new MessageChannel,aa=B.port2;B.port1.onmessage=function(){if(null!==K){var a=q();ua=a+ta;try{K(!0,a)?aa.postMessage(null):(J=!1,K=null)}catch(b){throw aa.postMessage(null),b;}}else J=!1};z=function(a){K=a;J||(J=!0,aa.postMessage(null))};G=function(a,b){Z=Ma(function(){a(q())},b)};V=function(){Na(Z);Z=-1}}var p=[],u=[],Pa=1,l=null,m=3,H=!1,v=!1,y=!1,Qa=0;B={ReactCurrentDispatcher:na,ReactCurrentOwner:M,IsSomeRendererActing:{current:!1},assign:I};I(B,{Scheduler:{__proto__:null,
unstable_ImmediatePriority:1,unstable_UserBlockingPriority:2,unstable_NormalPriority:3,unstable_IdlePriority:5,unstable_LowPriority:4,unstable_runWithPriority:function(a,b){switch(a){case 1:case 2:case 3:case 4:case 5:break;default:a=3}var c=m;m=a;try{return b()}finally{m=c}},unstable_next:function(a){switch(m){case 1:case 2:case 3:var b=3;break;default:b=m}var c=m;m=b;try{return a()}finally{m=c}},unstable_scheduleCallback:function(a,b,c){var d=q();if("object"===typeof c&&null!==c){var e=c.delay;
e="number"===typeof e&&0<e?d+e:d;c="number"===typeof c.timeout?c.timeout:oa(a)}else c=oa(a),e=d;c=e+c;a={id:Pa++,callback:b,priorityLevel:a,startTime:e,expirationTime:c,sortIndex:-1};e>d?(a.sortIndex=e,S(u,a),null===n(p)&&a===n(u)&&(y?V():y=!0,G(T,e-d))):(a.sortIndex=c,S(p,a),v||H||(v=!0,z(U)));return a},unstable_cancelCallback:function(a){a.callback=null},unstable_wrapCallback:function(a){var b=m;return function(){var c=m;m=b;try{return a.apply(this,arguments)}finally{m=c}}},unstable_getCurrentPriorityLevel:function(){return m},
unstable_shouldYield:function(){var a=q();F(a);var b=n(p);return b!==l&&null!==l&&null!==b&&null!==b.callback&&b.startTime<=a&&b.expirationTime<l.expirationTime||W()},unstable_requestPaint:f,unstable_continueExecution:function(){v||H||(v=!0,z(U))},unstable_pauseExecution:function(){},unstable_getFirstCallbackNode:function(){return n(p)},get unstable_now(){return q},get unstable_forceFrameRate(){return X},unstable_Profiling:null},SchedulerTracing:{__proto__:null,__interactionsRef:null,__subscriberRef:null,
unstable_clear:function(a){return a()},unstable_getCurrent:function(){return null},unstable_getThreadID:function(){return++Qa},unstable_trace:function(a,b,c){return c()},unstable_wrap:function(a){return a},unstable_subscribe:function(a){},unstable_unsubscribe:function(a){}}});d.Children={map:function(a,b,c){if(null==a)return a;var d=[];R(a,d,null,b,c);return d},forEach:function(a,b,c){if(null==a)return a;b=ja(null,null,b,c);Q(a,ya,b);ka(b)},count:function(a){return Q(a,function(){return null},null)},
toArray:function(a){var b=[];R(a,b,null,function(a){return a});return b},only:function(a){if(!N(a))throw Error(r(143));return a}};d.Component=w;d.Fragment=Aa;d.Profiler=Ca;d.PureComponent=L;d.StrictMode=Ba;d.Suspense=Ga;d.__SECRET_INTERNALS_DO_NOT_USE_OR_YOU_WILL_BE_FIRED=B;d.cloneElement=function(a,b,c){if(null===a||void 0===a)throw Error(r(267,a));var d=I({},a.props),e=a.key,f=a.ref,m=a._owner;if(null!=b){void 0!==b.ref&&(f=b.ref,m=M.current);void 0!==b.key&&(e=""+b.key);if(a.type&&a.type.defaultProps)var h=
a.type.defaultProps;for(k in b)ha.call(b,k)&&!ia.hasOwnProperty(k)&&(d[k]=void 0===b[k]&&void 0!==h?h[k]:b[k])}var k=arguments.length-2;if(1===k)d.children=c;else if(1<k){h=Array(k);for(var l=0;l<k;l++)h[l]=arguments[l+2];d.children=h}return{$$typeof:x,type:a.type,key:e,ref:f,props:d,_owner:m}};d.createContext=function(a,b){void 0===b&&(b=null);a={$$typeof:Ea,_calculateChangedBits:b,_currentValue:a,_currentValue2:a,_threadCount:0,Provider:null,Consumer:null};a.Provider={$$typeof:Da,_context:a};return a.Consumer=
a};d.createElement=ea;d.createFactory=function(a){var b=ea.bind(null,a);b.type=a;return b};d.createRef=function(){return{current:null}};d.forwardRef=function(a){return{$$typeof:Fa,render:a}};d.isValidElement=N;d.lazy=function(a){return{$$typeof:Ia,_ctor:a,_status:-1,_result:null}};d.memo=function(a,b){return{$$typeof:Ha,type:a,compare:void 0===b?null:b}};d.useCallback=function(a,b){return t().useCallback(a,b)};d.useContext=function(a,b){return t().useContext(a,b)};d.useDebugValue=function(a,b){};
d.useEffect=function(a,b){return t().useEffect(a,b)};d.useImperativeHandle=function(a,b,c){return t().useImperativeHandle(a,b,c)};d.useLayoutEffect=function(a,b){return t().useLayoutEffect(a,b)};d.useMemo=function(a,b){return t().useMemo(a,b)};d.useReducer=function(a,b,c){return t().useReducer(a,b,c)};d.useRef=function(a){return t().useRef(a)};d.useState=function(a){return t().useState(a)};d.version="16.13.1"});
</script>
    <script crossorigin>/** @license React v16.13.1
 * react-dom.production.min.js
 *
 * Copyright (c) Facebook, Inc. and its affiliates.
 *
 * This source code is licensed under the MIT license found in the
 * LICENSE file in the root directory of this source tree.
 */
/*
 Modernizr 3.0.0pre (Custom Build) | MIT
*/
'use strict';(function(I,ea){"object"===typeof exports&&"undefined"!==typeof module?ea(exports,require("react")):"function"===typeof define&&define.amd?define(["exports","react"],ea):(I=I||self,ea(I.ReactDOM={},I.React))})(this,function(I,ea){function k(a){for(var b="https://reactjs.org/docs/error-decoder.html?invariant="+a,c=1;c<arguments.length;c++)b+="&args[]="+encodeURIComponent(arguments[c]);return"Minified React error #"+a+"; visit "+b+" for the full message or use the non-minified dev environment for full errors and additional helpful warnings."}
function ji(a,b,c,d,e,f,g,h,m){yb=!1;gc=null;ki.apply(li,arguments)}function mi(a,b,c,d,e,f,g,h,m){ji.apply(this,arguments);if(yb){if(yb){var n=gc;yb=!1;gc=null}else throw Error(k(198));hc||(hc=!0,pd=n)}}function lf(a,b,c){var d=a.type||"unknown-event";a.currentTarget=mf(c);mi(d,b,void 0,a);a.currentTarget=null}function nf(){if(ic)for(var a in cb){var b=cb[a],c=ic.indexOf(a);if(!(-1<c))throw Error(k(96,a));if(!jc[c]){if(!b.extractEvents)throw Error(k(97,a));jc[c]=b;c=b.eventTypes;for(var d in c){var e=
void 0;var f=c[d],g=b,h=d;if(qd.hasOwnProperty(h))throw Error(k(99,h));qd[h]=f;var m=f.phasedRegistrationNames;if(m){for(e in m)m.hasOwnProperty(e)&&of(m[e],g,h);e=!0}else f.registrationName?(of(f.registrationName,g,h),e=!0):e=!1;if(!e)throw Error(k(98,d,a));}}}}function of(a,b,c){if(db[a])throw Error(k(100,a));db[a]=b;rd[a]=b.eventTypes[c].dependencies}function pf(a){var b=!1,c;for(c in a)if(a.hasOwnProperty(c)){var d=a[c];if(!cb.hasOwnProperty(c)||cb[c]!==d){if(cb[c])throw Error(k(102,c));cb[c]=
d;b=!0}}b&&nf()}function qf(a){if(a=rf(a)){if("function"!==typeof sd)throw Error(k(280));var b=a.stateNode;b&&(b=td(b),sd(a.stateNode,a.type,b))}}function sf(a){eb?fb?fb.push(a):fb=[a]:eb=a}function tf(){if(eb){var a=eb,b=fb;fb=eb=null;qf(a);if(b)for(a=0;a<b.length;a++)qf(b[a])}}function ud(){if(null!==eb||null!==fb)vd(),tf()}function uf(a,b,c){if(wd)return a(b,c);wd=!0;try{return vf(a,b,c)}finally{wd=!1,ud()}}function ni(a){if(wf.call(xf,a))return!0;if(wf.call(yf,a))return!1;if(oi.test(a))return xf[a]=
!0;yf[a]=!0;return!1}function pi(a,b,c,d){if(null!==c&&0===c.type)return!1;switch(typeof b){case "function":case "symbol":return!0;case "boolean":if(d)return!1;if(null!==c)return!c.acceptsBooleans;a=a.toLowerCase().slice(0,5);return"data-"!==a&&"aria-"!==a;default:return!1}}function qi(a,b,c,d){if(null===b||"undefined"===typeof b||pi(a,b,c,d))return!0;if(d)return!1;if(null!==c)switch(c.type){case 3:return!b;case 4:return!1===b;case 5:return isNaN(b);case 6:return isNaN(b)||1>b}return!1}function L(a,
b,c,d,e,f){this.acceptsBooleans=2===b||3===b||4===b;this.attributeName=d;this.attributeNamespace=e;this.mustUseProperty=c;this.propertyName=a;this.type=b;this.sanitizeURL=f}function xd(a,b,c,d){var e=E.hasOwnProperty(b)?E[b]:null;var f=null!==e?0===e.type:d?!1:!(2<b.length)||"o"!==b[0]&&"O"!==b[0]||"n"!==b[1]&&"N"!==b[1]?!1:!0;f||(qi(b,c,e,d)&&(c=null),d||null===e?ni(b)&&(null===c?a.removeAttribute(b):a.setAttribute(b,""+c)):e.mustUseProperty?a[e.propertyName]=null===c?3===e.type?!1:"":c:(b=e.attributeName,
d=e.attributeNamespace,null===c?a.removeAttribute(b):(e=e.type,c=3===e||4===e&&!0===c?"":""+c,d?a.setAttributeNS(d,b,c):a.setAttribute(b,c))))}function zb(a){if(null===a||"object"!==typeof a)return null;a=zf&&a[zf]||a["@@iterator"];return"function"===typeof a?a:null}function ri(a){if(-1===a._status){a._status=0;var b=a._ctor;b=b();a._result=b;b.then(function(b){0===a._status&&(b=b.default,a._status=1,a._result=b)},function(b){0===a._status&&(a._status=2,a._result=b)})}}function na(a){if(null==a)return null;
if("function"===typeof a)return a.displayName||a.name||null;if("string"===typeof a)return a;switch(a){case Ma:return"Fragment";case gb:return"Portal";case kc:return"Profiler";case Af:return"StrictMode";case lc:return"Suspense";case yd:return"SuspenseList"}if("object"===typeof a)switch(a.$$typeof){case Bf:return"Context.Consumer";case Cf:return"Context.Provider";case zd:var b=a.render;b=b.displayName||b.name||"";return a.displayName||(""!==b?"ForwardRef("+b+")":"ForwardRef");case Ad:return na(a.type);
case Df:return na(a.render);case Ef:if(a=1===a._status?a._result:null)return na(a)}return null}function Bd(a){var b="";do{a:switch(a.tag){case 3:case 4:case 6:case 7:case 10:case 9:var c="";break a;default:var d=a._debugOwner,e=a._debugSource,f=na(a.type);c=null;d&&(c=na(d.type));d=f;f="";e?f=" (at "+e.fileName.replace(si,"")+":"+e.lineNumber+")":c&&(f=" (created by "+c+")");c="\n    in "+(d||"Unknown")+f}b+=c;a=a.return}while(a);return b}function va(a){switch(typeof a){case "boolean":case "number":case "object":case "string":case "undefined":return a;
default:return""}}function Ff(a){var b=a.type;return(a=a.nodeName)&&"input"===a.toLowerCase()&&("checkbox"===b||"radio"===b)}function ti(a){var b=Ff(a)?"checked":"value",c=Object.getOwnPropertyDescriptor(a.constructor.prototype,b),d=""+a[b];if(!a.hasOwnProperty(b)&&"undefined"!==typeof c&&"function"===typeof c.get&&"function"===typeof c.set){var e=c.get,f=c.set;Object.defineProperty(a,b,{configurable:!0,get:function(){return e.call(this)},set:function(a){d=""+a;f.call(this,a)}});Object.defineProperty(a,
b,{enumerable:c.enumerable});return{getValue:function(){return d},setValue:function(a){d=""+a},stopTracking:function(){a._valueTracker=null;delete a[b]}}}}function mc(a){a._valueTracker||(a._valueTracker=ti(a))}function Gf(a){if(!a)return!1;var b=a._valueTracker;if(!b)return!0;var c=b.getValue();var d="";a&&(d=Ff(a)?a.checked?"true":"false":a.value);a=d;return a!==c?(b.setValue(a),!0):!1}function Cd(a,b){var c=b.checked;return M({},b,{defaultChecked:void 0,defaultValue:void 0,value:void 0,checked:null!=
c?c:a._wrapperState.initialChecked})}function Hf(a,b){var c=null==b.defaultValue?"":b.defaultValue,d=null!=b.checked?b.checked:b.defaultChecked;c=va(null!=b.value?b.value:c);a._wrapperState={initialChecked:d,initialValue:c,controlled:"checkbox"===b.type||"radio"===b.type?null!=b.checked:null!=b.value}}function If(a,b){b=b.checked;null!=b&&xd(a,"checked",b,!1)}function Dd(a,b){If(a,b);var c=va(b.value),d=b.type;if(null!=c)if("number"===d){if(0===c&&""===a.value||a.value!=c)a.value=""+c}else a.value!==
""+c&&(a.value=""+c);else if("submit"===d||"reset"===d){a.removeAttribute("value");return}b.hasOwnProperty("value")?Ed(a,b.type,c):b.hasOwnProperty("defaultValue")&&Ed(a,b.type,va(b.defaultValue));null==b.checked&&null!=b.defaultChecked&&(a.defaultChecked=!!b.defaultChecked)}function Jf(a,b,c){if(b.hasOwnProperty("value")||b.hasOwnProperty("defaultValue")){var d=b.type;if(!("submit"!==d&&"reset"!==d||void 0!==b.value&&null!==b.value))return;b=""+a._wrapperState.initialValue;c||b===a.value||(a.value=
b);a.defaultValue=b}c=a.name;""!==c&&(a.name="");a.defaultChecked=!!a._wrapperState.initialChecked;""!==c&&(a.name=c)}function Ed(a,b,c){if("number"!==b||a.ownerDocument.activeElement!==a)null==c?a.defaultValue=""+a._wrapperState.initialValue:a.defaultValue!==""+c&&(a.defaultValue=""+c)}function ui(a){var b="";ea.Children.forEach(a,function(a){null!=a&&(b+=a)});return b}function Fd(a,b){a=M({children:void 0},b);if(b=ui(b.children))a.children=b;return a}function hb(a,b,c,d){a=a.options;if(b){b={};
for(var e=0;e<c.length;e++)b["$"+c[e]]=!0;for(c=0;c<a.length;c++)e=b.hasOwnProperty("$"+a[c].value),a[c].selected!==e&&(a[c].selected=e),e&&d&&(a[c].defaultSelected=!0)}else{c=""+va(c);b=null;for(e=0;e<a.length;e++){if(a[e].value===c){a[e].selected=!0;d&&(a[e].defaultSelected=!0);return}null!==b||a[e].disabled||(b=a[e])}null!==b&&(b.selected=!0)}}function Gd(a,b){if(null!=b.dangerouslySetInnerHTML)throw Error(k(91));return M({},b,{value:void 0,defaultValue:void 0,children:""+a._wrapperState.initialValue})}
function Kf(a,b){var c=b.value;if(null==c){c=b.children;b=b.defaultValue;if(null!=c){if(null!=b)throw Error(k(92));if(Array.isArray(c)){if(!(1>=c.length))throw Error(k(93));c=c[0]}b=c}null==b&&(b="");c=b}a._wrapperState={initialValue:va(c)}}function Lf(a,b){var c=va(b.value),d=va(b.defaultValue);null!=c&&(c=""+c,c!==a.value&&(a.value=c),null==b.defaultValue&&a.defaultValue!==c&&(a.defaultValue=c));null!=d&&(a.defaultValue=""+d)}function Mf(a,b){b=a.textContent;b===a._wrapperState.initialValue&&""!==
b&&null!==b&&(a.value=b)}function Nf(a){switch(a){case "svg":return"http://www.w3.org/2000/svg";case "math":return"http://www.w3.org/1998/Math/MathML";default:return"http://www.w3.org/1999/xhtml"}}function Hd(a,b){return null==a||"http://www.w3.org/1999/xhtml"===a?Nf(b):"http://www.w3.org/2000/svg"===a&&"foreignObject"===b?"http://www.w3.org/1999/xhtml":a}function nc(a,b){var c={};c[a.toLowerCase()]=b.toLowerCase();c["Webkit"+a]="webkit"+b;c["Moz"+a]="moz"+b;return c}function oc(a){if(Id[a])return Id[a];
if(!ib[a])return a;var b=ib[a],c;for(c in b)if(b.hasOwnProperty(c)&&c in Of)return Id[a]=b[c];return a}function Jd(a){var b=Pf.get(a);void 0===b&&(b=new Map,Pf.set(a,b));return b}function Na(a){var b=a,c=a;if(a.alternate)for(;b.return;)b=b.return;else{a=b;do b=a,0!==(b.effectTag&1026)&&(c=b.return),a=b.return;while(a)}return 3===b.tag?c:null}function Qf(a){if(13===a.tag){var b=a.memoizedState;null===b&&(a=a.alternate,null!==a&&(b=a.memoizedState));if(null!==b)return b.dehydrated}return null}function Rf(a){if(Na(a)!==
a)throw Error(k(188));}function vi(a){var b=a.alternate;if(!b){b=Na(a);if(null===b)throw Error(k(188));return b!==a?null:a}for(var c=a,d=b;;){var e=c.return;if(null===e)break;var f=e.alternate;if(null===f){d=e.return;if(null!==d){c=d;continue}break}if(e.child===f.child){for(f=e.child;f;){if(f===c)return Rf(e),a;if(f===d)return Rf(e),b;f=f.sibling}throw Error(k(188));}if(c.return!==d.return)c=e,d=f;else{for(var g=!1,h=e.child;h;){if(h===c){g=!0;c=e;d=f;break}if(h===d){g=!0;d=e;c=f;break}h=h.sibling}if(!g){for(h=
f.child;h;){if(h===c){g=!0;c=f;d=e;break}if(h===d){g=!0;d=f;c=e;break}h=h.sibling}if(!g)throw Error(k(189));}}if(c.alternate!==d)throw Error(k(190));}if(3!==c.tag)throw Error(k(188));return c.stateNode.current===c?a:b}function Sf(a){a=vi(a);if(!a)return null;for(var b=a;;){if(5===b.tag||6===b.tag)return b;if(b.child)b.child.return=b,b=b.child;else{if(b===a)break;for(;!b.sibling;){if(!b.return||b.return===a)return null;b=b.return}b.sibling.return=b.return;b=b.sibling}}return null}function jb(a,b){if(null==
b)throw Error(k(30));if(null==a)return b;if(Array.isArray(a)){if(Array.isArray(b))return a.push.apply(a,b),a;a.push(b);return a}return Array.isArray(b)?[a].concat(b):[a,b]}function Kd(a,b,c){Array.isArray(a)?a.forEach(b,c):a&&b.call(c,a)}function pc(a){null!==a&&(Ab=jb(Ab,a));a=Ab;Ab=null;if(a){Kd(a,wi);if(Ab)throw Error(k(95));if(hc)throw a=pd,hc=!1,pd=null,a;}}function Ld(a){a=a.target||a.srcElement||window;a.correspondingUseElement&&(a=a.correspondingUseElement);return 3===a.nodeType?a.parentNode:
a}function Tf(a){if(!wa)return!1;a="on"+a;var b=a in document;b||(b=document.createElement("div"),b.setAttribute(a,"return;"),b="function"===typeof b[a]);return b}function Uf(a){a.topLevelType=null;a.nativeEvent=null;a.targetInst=null;a.ancestors.length=0;10>qc.length&&qc.push(a)}function Vf(a,b,c,d){if(qc.length){var e=qc.pop();e.topLevelType=a;e.eventSystemFlags=d;e.nativeEvent=b;e.targetInst=c;return e}return{topLevelType:a,eventSystemFlags:d,nativeEvent:b,targetInst:c,ancestors:[]}}function Wf(a){var b=
a.targetInst,c=b;do{if(!c){a.ancestors.push(c);break}var d=c;if(3===d.tag)d=d.stateNode.containerInfo;else{for(;d.return;)d=d.return;d=3!==d.tag?null:d.stateNode.containerInfo}if(!d)break;b=c.tag;5!==b&&6!==b||a.ancestors.push(c);c=Bb(d)}while(c);for(c=0;c<a.ancestors.length;c++){b=a.ancestors[c];var e=Ld(a.nativeEvent);d=a.topLevelType;var f=a.nativeEvent,g=a.eventSystemFlags;0===c&&(g|=64);for(var h=null,m=0;m<jc.length;m++){var n=jc[m];n&&(n=n.extractEvents(d,b,f,e,g))&&(h=jb(h,n))}pc(h)}}function Md(a,
b,c){if(!c.has(a)){switch(a){case "scroll":Cb(b,"scroll",!0);break;case "focus":case "blur":Cb(b,"focus",!0);Cb(b,"blur",!0);c.set("blur",null);c.set("focus",null);break;case "cancel":case "close":Tf(a)&&Cb(b,a,!0);break;case "invalid":case "submit":case "reset":break;default:-1===Db.indexOf(a)&&w(a,b)}c.set(a,null)}}function xi(a,b){var c=Jd(b);Nd.forEach(function(a){Md(a,b,c)});yi.forEach(function(a){Md(a,b,c)})}function Od(a,b,c,d,e){return{blockedOn:a,topLevelType:b,eventSystemFlags:c|32,nativeEvent:e,
container:d}}function Xf(a,b){switch(a){case "focus":case "blur":xa=null;break;case "dragenter":case "dragleave":ya=null;break;case "mouseover":case "mouseout":za=null;break;case "pointerover":case "pointerout":Eb.delete(b.pointerId);break;case "gotpointercapture":case "lostpointercapture":Fb.delete(b.pointerId)}}function Gb(a,b,c,d,e,f){if(null===a||a.nativeEvent!==f)return a=Od(b,c,d,e,f),null!==b&&(b=Hb(b),null!==b&&Yf(b)),a;a.eventSystemFlags|=d;return a}function zi(a,b,c,d,e){switch(b){case "focus":return xa=
Gb(xa,a,b,c,d,e),!0;case "dragenter":return ya=Gb(ya,a,b,c,d,e),!0;case "mouseover":return za=Gb(za,a,b,c,d,e),!0;case "pointerover":var f=e.pointerId;Eb.set(f,Gb(Eb.get(f)||null,a,b,c,d,e));return!0;case "gotpointercapture":return f=e.pointerId,Fb.set(f,Gb(Fb.get(f)||null,a,b,c,d,e)),!0}return!1}function Ai(a){var b=Bb(a.target);if(null!==b){var c=Na(b);if(null!==c)if(b=c.tag,13===b){if(b=Qf(c),null!==b){a.blockedOn=b;Pd(a.priority,function(){Bi(c)});return}}else if(3===b&&c.stateNode.hydrate){a.blockedOn=
3===c.tag?c.stateNode.containerInfo:null;return}}a.blockedOn=null}function rc(a){if(null!==a.blockedOn)return!1;var b=Qd(a.topLevelType,a.eventSystemFlags,a.container,a.nativeEvent);if(null!==b){var c=Hb(b);null!==c&&Yf(c);a.blockedOn=b;return!1}return!0}function Zf(a,b,c){rc(a)&&c.delete(b)}function Ci(){for(Rd=!1;0<fa.length;){var a=fa[0];if(null!==a.blockedOn){a=Hb(a.blockedOn);null!==a&&Di(a);break}var b=Qd(a.topLevelType,a.eventSystemFlags,a.container,a.nativeEvent);null!==b?a.blockedOn=b:fa.shift()}null!==
xa&&rc(xa)&&(xa=null);null!==ya&&rc(ya)&&(ya=null);null!==za&&rc(za)&&(za=null);Eb.forEach(Zf);Fb.forEach(Zf)}function Ib(a,b){a.blockedOn===b&&(a.blockedOn=null,Rd||(Rd=!0,$f(ag,Ci)))}function bg(a){if(0<fa.length){Ib(fa[0],a);for(var b=1;b<fa.length;b++){var c=fa[b];c.blockedOn===a&&(c.blockedOn=null)}}null!==xa&&Ib(xa,a);null!==ya&&Ib(ya,a);null!==za&&Ib(za,a);b=function(b){return Ib(b,a)};Eb.forEach(b);Fb.forEach(b);for(b=0;b<Jb.length;b++)c=Jb[b],c.blockedOn===a&&(c.blockedOn=null);for(;0<Jb.length&&
(b=Jb[0],null===b.blockedOn);)Ai(b),null===b.blockedOn&&Jb.shift()}function Sd(a,b){for(var c=0;c<a.length;c+=2){var d=a[c],e=a[c+1],f="on"+(e[0].toUpperCase()+e.slice(1));f={phasedRegistrationNames:{bubbled:f,captured:f+"Capture"},dependencies:[d],eventPriority:b};Td.set(d,b);cg.set(d,f);dg[e]=f}}function w(a,b){Cb(b,a,!1)}function Cb(a,b,c){var d=Td.get(b);switch(void 0===d?2:d){case 0:d=Ei.bind(null,b,1,a);break;case 1:d=Fi.bind(null,b,1,a);break;default:d=sc.bind(null,b,1,a)}c?a.addEventListener(b,
d,!0):a.addEventListener(b,d,!1)}function Ei(a,b,c,d){Oa||vd();var e=sc,f=Oa;Oa=!0;try{eg(e,a,b,c,d)}finally{(Oa=f)||ud()}}function Fi(a,b,c,d){Gi(Hi,sc.bind(null,a,b,c,d))}function sc(a,b,c,d){if(tc)if(0<fa.length&&-1<Nd.indexOf(a))a=Od(null,a,b,c,d),fa.push(a);else{var e=Qd(a,b,c,d);if(null===e)Xf(a,d);else if(-1<Nd.indexOf(a))a=Od(e,a,b,c,d),fa.push(a);else if(!zi(e,a,b,c,d)){Xf(a,d);a=Vf(a,d,null,b);try{uf(Wf,a)}finally{Uf(a)}}}}function Qd(a,b,c,d){c=Ld(d);c=Bb(c);if(null!==c){var e=Na(c);if(null===
e)c=null;else{var f=e.tag;if(13===f){c=Qf(e);if(null!==c)return c;c=null}else if(3===f){if(e.stateNode.hydrate)return 3===e.tag?e.stateNode.containerInfo:null;c=null}else e!==c&&(c=null)}}a=Vf(a,d,c,b);try{uf(Wf,a)}finally{Uf(a)}return null}function fg(a,b,c){return null==b||"boolean"===typeof b||""===b?"":c||"number"!==typeof b||0===b||Kb.hasOwnProperty(a)&&Kb[a]?(""+b).trim():b+"px"}function gg(a,b){a=a.style;for(var c in b)if(b.hasOwnProperty(c)){var d=0===c.indexOf("--"),e=fg(c,b[c],d);"float"===
c&&(c="cssFloat");d?a.setProperty(c,e):a[c]=e}}function Ud(a,b){if(b){if(Ii[a]&&(null!=b.children||null!=b.dangerouslySetInnerHTML))throw Error(k(137,a,""));if(null!=b.dangerouslySetInnerHTML){if(null!=b.children)throw Error(k(60));if(!("object"===typeof b.dangerouslySetInnerHTML&&"__html"in b.dangerouslySetInnerHTML))throw Error(k(61));}if(null!=b.style&&"object"!==typeof b.style)throw Error(k(62,""));}}function Vd(a,b){if(-1===a.indexOf("-"))return"string"===typeof b.is;switch(a){case "annotation-xml":case "color-profile":case "font-face":case "font-face-src":case "font-face-uri":case "font-face-format":case "font-face-name":case "missing-glyph":return!1;
default:return!0}}function oa(a,b){a=9===a.nodeType||11===a.nodeType?a:a.ownerDocument;var c=Jd(a);b=rd[b];for(var d=0;d<b.length;d++)Md(b[d],a,c)}function uc(){}function Wd(a){a=a||("undefined"!==typeof document?document:void 0);if("undefined"===typeof a)return null;try{return a.activeElement||a.body}catch(b){return a.body}}function hg(a){for(;a&&a.firstChild;)a=a.firstChild;return a}function ig(a,b){var c=hg(a);a=0;for(var d;c;){if(3===c.nodeType){d=a+c.textContent.length;if(a<=b&&d>=b)return{node:c,
offset:b-a};a=d}a:{for(;c;){if(c.nextSibling){c=c.nextSibling;break a}c=c.parentNode}c=void 0}c=hg(c)}}function jg(a,b){return a&&b?a===b?!0:a&&3===a.nodeType?!1:b&&3===b.nodeType?jg(a,b.parentNode):"contains"in a?a.contains(b):a.compareDocumentPosition?!!(a.compareDocumentPosition(b)&16):!1:!1}function kg(){for(var a=window,b=Wd();b instanceof a.HTMLIFrameElement;){try{var c="string"===typeof b.contentWindow.location.href}catch(d){c=!1}if(c)a=b.contentWindow;else break;b=Wd(a.document)}return b}
function Xd(a){var b=a&&a.nodeName&&a.nodeName.toLowerCase();return b&&("input"===b&&("text"===a.type||"search"===a.type||"tel"===a.type||"url"===a.type||"password"===a.type)||"textarea"===b||"true"===a.contentEditable)}function lg(a,b){switch(a){case "button":case "input":case "select":case "textarea":return!!b.autoFocus}return!1}function Yd(a,b){return"textarea"===a||"option"===a||"noscript"===a||"string"===typeof b.children||"number"===typeof b.children||"object"===typeof b.dangerouslySetInnerHTML&&
null!==b.dangerouslySetInnerHTML&&null!=b.dangerouslySetInnerHTML.__html}function kb(a){for(;null!=a;a=a.nextSibling){var b=a.nodeType;if(1===b||3===b)break}return a}function mg(a){a=a.previousSibling;for(var b=0;a;){if(8===a.nodeType){var c=a.data;if(c===ng||c===Zd||c===$d){if(0===b)return a;b--}else c===og&&b++}a=a.previousSibling}return null}function Bb(a){var b=a[Aa];if(b)return b;for(var c=a.parentNode;c;){if(b=c[Lb]||c[Aa]){c=b.alternate;if(null!==b.child||null!==c&&null!==c.child)for(a=mg(a);null!==
a;){if(c=a[Aa])return c;a=mg(a)}return b}a=c;c=a.parentNode}return null}function Hb(a){a=a[Aa]||a[Lb];return!a||5!==a.tag&&6!==a.tag&&13!==a.tag&&3!==a.tag?null:a}function Pa(a){if(5===a.tag||6===a.tag)return a.stateNode;throw Error(k(33));}function ae(a){return a[vc]||null}function pa(a){do a=a.return;while(a&&5!==a.tag);return a?a:null}function pg(a,b){var c=a.stateNode;if(!c)return null;var d=td(c);if(!d)return null;c=d[b];a:switch(b){case "onClick":case "onClickCapture":case "onDoubleClick":case "onDoubleClickCapture":case "onMouseDown":case "onMouseDownCapture":case "onMouseMove":case "onMouseMoveCapture":case "onMouseUp":case "onMouseUpCapture":case "onMouseEnter":(d=
!d.disabled)||(a=a.type,d=!("button"===a||"input"===a||"select"===a||"textarea"===a));a=!d;break a;default:a=!1}if(a)return null;if(c&&"function"!==typeof c)throw Error(k(231,b,typeof c));return c}function qg(a,b,c){if(b=pg(a,c.dispatchConfig.phasedRegistrationNames[b]))c._dispatchListeners=jb(c._dispatchListeners,b),c._dispatchInstances=jb(c._dispatchInstances,a)}function Ji(a){if(a&&a.dispatchConfig.phasedRegistrationNames){for(var b=a._targetInst,c=[];b;)c.push(b),b=pa(b);for(b=c.length;0<b--;)qg(c[b],
"captured",a);for(b=0;b<c.length;b++)qg(c[b],"bubbled",a)}}function be(a,b,c){a&&c&&c.dispatchConfig.registrationName&&(b=pg(a,c.dispatchConfig.registrationName))&&(c._dispatchListeners=jb(c._dispatchListeners,b),c._dispatchInstances=jb(c._dispatchInstances,a))}function Ki(a){a&&a.dispatchConfig.registrationName&&be(a._targetInst,null,a)}function lb(a){Kd(a,Ji)}function rg(){if(wc)return wc;var a,b=ce,c=b.length,d,e="value"in Ba?Ba.value:Ba.textContent,f=e.length;for(a=0;a<c&&b[a]===e[a];a++);var g=
c-a;for(d=1;d<=g&&b[c-d]===e[f-d];d++);return wc=e.slice(a,1<d?1-d:void 0)}function xc(){return!0}function yc(){return!1}function R(a,b,c,d){this.dispatchConfig=a;this._targetInst=b;this.nativeEvent=c;a=this.constructor.Interface;for(var e in a)a.hasOwnProperty(e)&&((b=a[e])?this[e]=b(c):"target"===e?this.target=d:this[e]=c[e]);this.isDefaultPrevented=(null!=c.defaultPrevented?c.defaultPrevented:!1===c.returnValue)?xc:yc;this.isPropagationStopped=yc;return this}function Li(a,b,c,d){if(this.eventPool.length){var e=
this.eventPool.pop();this.call(e,a,b,c,d);return e}return new this(a,b,c,d)}function Mi(a){if(!(a instanceof this))throw Error(k(279));a.destructor();10>this.eventPool.length&&this.eventPool.push(a)}function sg(a){a.eventPool=[];a.getPooled=Li;a.release=Mi}function tg(a,b){switch(a){case "keyup":return-1!==Ni.indexOf(b.keyCode);case "keydown":return 229!==b.keyCode;case "keypress":case "mousedown":case "blur":return!0;default:return!1}}function ug(a){a=a.detail;return"object"===typeof a&&"data"in
a?a.data:null}function Oi(a,b){switch(a){case "compositionend":return ug(b);case "keypress":if(32!==b.which)return null;vg=!0;return wg;case "textInput":return a=b.data,a===wg&&vg?null:a;default:return null}}function Pi(a,b){if(mb)return"compositionend"===a||!de&&tg(a,b)?(a=rg(),wc=ce=Ba=null,mb=!1,a):null;switch(a){case "paste":return null;case "keypress":if(!(b.ctrlKey||b.altKey||b.metaKey)||b.ctrlKey&&b.altKey){if(b.char&&1<b.char.length)return b.char;if(b.which)return String.fromCharCode(b.which)}return null;
case "compositionend":return xg&&"ko"!==b.locale?null:b.data;default:return null}}function yg(a){var b=a&&a.nodeName&&a.nodeName.toLowerCase();return"input"===b?!!Qi[a.type]:"textarea"===b?!0:!1}function zg(a,b,c){a=R.getPooled(Ag.change,a,b,c);a.type="change";sf(c);lb(a);return a}function Ri(a){pc(a)}function zc(a){var b=Pa(a);if(Gf(b))return a}function Si(a,b){if("change"===a)return b}function Bg(){Mb&&(Mb.detachEvent("onpropertychange",Cg),Nb=Mb=null)}function Cg(a){if("value"===a.propertyName&&
zc(Nb))if(a=zg(Nb,a,Ld(a)),Oa)pc(a);else{Oa=!0;try{ee(Ri,a)}finally{Oa=!1,ud()}}}function Ti(a,b,c){"focus"===a?(Bg(),Mb=b,Nb=c,Mb.attachEvent("onpropertychange",Cg)):"blur"===a&&Bg()}function Ui(a,b){if("selectionchange"===a||"keyup"===a||"keydown"===a)return zc(Nb)}function Vi(a,b){if("click"===a)return zc(b)}function Wi(a,b){if("input"===a||"change"===a)return zc(b)}function Xi(a){var b=this.nativeEvent;return b.getModifierState?b.getModifierState(a):(a=Yi[a])?!!b[a]:!1}function fe(a){return Xi}
function Zi(a,b){return a===b&&(0!==a||1/a===1/b)||a!==a&&b!==b}function Ob(a,b){if(Qa(a,b))return!0;if("object"!==typeof a||null===a||"object"!==typeof b||null===b)return!1;var c=Object.keys(a),d=Object.keys(b);if(c.length!==d.length)return!1;for(d=0;d<c.length;d++)if(!$i.call(b,c[d])||!Qa(a[c[d]],b[c[d]]))return!1;return!0}function Dg(a,b){var c=b.window===b?b.document:9===b.nodeType?b:b.ownerDocument;if(ge||null==nb||nb!==Wd(c))return null;c=nb;"selectionStart"in c&&Xd(c)?c={start:c.selectionStart,
end:c.selectionEnd}:(c=(c.ownerDocument&&c.ownerDocument.defaultView||window).getSelection(),c={anchorNode:c.anchorNode,anchorOffset:c.anchorOffset,focusNode:c.focusNode,focusOffset:c.focusOffset});return Pb&&Ob(Pb,c)?null:(Pb=c,a=R.getPooled(Eg.select,he,a,b),a.type="select",a.target=nb,lb(a),a)}function Ac(a){var b=a.keyCode;"charCode"in a?(a=a.charCode,0===a&&13===b&&(a=13)):a=b;10===a&&(a=13);return 32<=a||13===a?a:0}function q(a,b){0>ob||(a.current=ie[ob],ie[ob]=null,ob--)}function y(a,b,c){ob++;
ie[ob]=a.current;a.current=b}function pb(a,b){var c=a.type.contextTypes;if(!c)return Ca;var d=a.stateNode;if(d&&d.__reactInternalMemoizedUnmaskedChildContext===b)return d.__reactInternalMemoizedMaskedChildContext;var e={},f;for(f in c)e[f]=b[f];d&&(a=a.stateNode,a.__reactInternalMemoizedUnmaskedChildContext=b,a.__reactInternalMemoizedMaskedChildContext=e);return e}function N(a){a=a.childContextTypes;return null!==a&&void 0!==a}function Fg(a,b,c){if(B.current!==Ca)throw Error(k(168));y(B,b);y(G,c)}
function Gg(a,b,c){var d=a.stateNode;a=b.childContextTypes;if("function"!==typeof d.getChildContext)return c;d=d.getChildContext();for(var e in d)if(!(e in a))throw Error(k(108,na(b)||"Unknown",e));return M({},c,{},d)}function Bc(a){a=(a=a.stateNode)&&a.__reactInternalMemoizedMergedChildContext||Ca;Ra=B.current;y(B,a);y(G,G.current);return!0}function Hg(a,b,c){var d=a.stateNode;if(!d)throw Error(k(169));c?(a=Gg(a,b,Ra),d.__reactInternalMemoizedMergedChildContext=a,q(G),q(B),y(B,a)):q(G);y(G,c)}function Cc(){switch(aj()){case Dc:return 99;
case Ig:return 98;case Jg:return 97;case Kg:return 96;case Lg:return 95;default:throw Error(k(332));}}function Mg(a){switch(a){case 99:return Dc;case 98:return Ig;case 97:return Jg;case 96:return Kg;case 95:return Lg;default:throw Error(k(332));}}function Da(a,b){a=Mg(a);return bj(a,b)}function Ng(a,b,c){a=Mg(a);return je(a,b,c)}function Og(a){null===qa?(qa=[a],Ec=je(Dc,Pg)):qa.push(a);return Qg}function ha(){if(null!==Ec){var a=Ec;Ec=null;Rg(a)}Pg()}function Pg(){if(!ke&&null!==qa){ke=!0;var a=0;
try{var b=qa;Da(99,function(){for(;a<b.length;a++){var c=b[a];do c=c(!0);while(null!==c)}});qa=null}catch(c){throw null!==qa&&(qa=qa.slice(a+1)),je(Dc,ha),c;}finally{ke=!1}}}function Fc(a,b,c){c/=10;return 1073741821-(((1073741821-a+b/10)/c|0)+1)*c}function aa(a,b){if(a&&a.defaultProps){b=M({},b);a=a.defaultProps;for(var c in a)void 0===b[c]&&(b[c]=a[c])}return b}function le(){Gc=qb=Hc=null}function me(a){var b=Ic.current;q(Ic);a.type._context._currentValue=b}function Sg(a,b){for(;null!==a;){var c=
a.alternate;if(a.childExpirationTime<b)a.childExpirationTime=b,null!==c&&c.childExpirationTime<b&&(c.childExpirationTime=b);else if(null!==c&&c.childExpirationTime<b)c.childExpirationTime=b;else break;a=a.return}}function rb(a,b){Hc=a;Gc=qb=null;a=a.dependencies;null!==a&&null!==a.firstContext&&(a.expirationTime>=b&&(ia=!0),a.firstContext=null)}function W(a,b){if(Gc!==a&&!1!==b&&0!==b){if("number"!==typeof b||1073741823===b)Gc=a,b=1073741823;b={context:a,observedBits:b,next:null};if(null===qb){if(null===
Hc)throw Error(k(308));qb=b;Hc.dependencies={expirationTime:0,firstContext:b,responders:null}}else qb=qb.next=b}return a._currentValue}function ne(a){a.updateQueue={baseState:a.memoizedState,baseQueue:null,shared:{pending:null},effects:null}}function oe(a,b){a=a.updateQueue;b.updateQueue===a&&(b.updateQueue={baseState:a.baseState,baseQueue:a.baseQueue,shared:a.shared,effects:a.effects})}function Ea(a,b){a={expirationTime:a,suspenseConfig:b,tag:Tg,payload:null,callback:null,next:null};return a.next=
a}function Fa(a,b){a=a.updateQueue;if(null!==a){a=a.shared;var c=a.pending;null===c?b.next=b:(b.next=c.next,c.next=b);a.pending=b}}function Ug(a,b){var c=a.alternate;null!==c&&oe(c,a);a=a.updateQueue;c=a.baseQueue;null===c?(a.baseQueue=b.next=b,b.next=b):(b.next=c.next,c.next=b)}function Qb(a,b,c,d){var e=a.updateQueue;Ga=!1;var f=e.baseQueue,g=e.shared.pending;if(null!==g){if(null!==f){var h=f.next;f.next=g.next;g.next=h}f=g;e.shared.pending=null;h=a.alternate;null!==h&&(h=h.updateQueue,null!==h&&
(h.baseQueue=g))}if(null!==f){h=f.next;var m=e.baseState,n=0,k=null,ba=null,l=null;if(null!==h){var p=h;do{g=p.expirationTime;if(g<d){var t={expirationTime:p.expirationTime,suspenseConfig:p.suspenseConfig,tag:p.tag,payload:p.payload,callback:p.callback,next:null};null===l?(ba=l=t,k=m):l=l.next=t;g>n&&(n=g)}else{null!==l&&(l=l.next={expirationTime:1073741823,suspenseConfig:p.suspenseConfig,tag:p.tag,payload:p.payload,callback:p.callback,next:null});Vg(g,p.suspenseConfig);a:{var q=a,r=p;g=b;t=c;switch(r.tag){case 1:q=
r.payload;if("function"===typeof q){m=q.call(t,m,g);break a}m=q;break a;case 3:q.effectTag=q.effectTag&-4097|64;case Tg:q=r.payload;g="function"===typeof q?q.call(t,m,g):q;if(null===g||void 0===g)break a;m=M({},m,g);break a;case Jc:Ga=!0}}null!==p.callback&&(a.effectTag|=32,g=e.effects,null===g?e.effects=[p]:g.push(p))}p=p.next;if(null===p||p===h)if(g=e.shared.pending,null===g)break;else p=f.next=g.next,g.next=h,e.baseQueue=f=g,e.shared.pending=null}while(1)}null===l?k=m:l.next=ba;e.baseState=k;e.baseQueue=
l;Kc(n);a.expirationTime=n;a.memoizedState=m}}function Wg(a,b,c){a=b.effects;b.effects=null;if(null!==a)for(b=0;b<a.length;b++){var d=a[b],e=d.callback;if(null!==e){d.callback=null;d=e;e=c;if("function"!==typeof d)throw Error(k(191,d));d.call(e)}}}function Lc(a,b,c,d){b=a.memoizedState;c=c(d,b);c=null===c||void 0===c?b:M({},b,c);a.memoizedState=c;0===a.expirationTime&&(a.updateQueue.baseState=c)}function Xg(a,b,c,d,e,f,g){a=a.stateNode;return"function"===typeof a.shouldComponentUpdate?a.shouldComponentUpdate(d,
f,g):b.prototype&&b.prototype.isPureReactComponent?!Ob(c,d)||!Ob(e,f):!0}function Yg(a,b,c){var d=!1,e=Ca;var f=b.contextType;"object"===typeof f&&null!==f?f=W(f):(e=N(b)?Ra:B.current,d=b.contextTypes,f=(d=null!==d&&void 0!==d)?pb(a,e):Ca);b=new b(c,f);a.memoizedState=null!==b.state&&void 0!==b.state?b.state:null;b.updater=Mc;a.stateNode=b;b._reactInternalFiber=a;d&&(a=a.stateNode,a.__reactInternalMemoizedUnmaskedChildContext=e,a.__reactInternalMemoizedMaskedChildContext=f);return b}function Zg(a,
b,c,d){a=b.state;"function"===typeof b.componentWillReceiveProps&&b.componentWillReceiveProps(c,d);"function"===typeof b.UNSAFE_componentWillReceiveProps&&b.UNSAFE_componentWillReceiveProps(c,d);b.state!==a&&Mc.enqueueReplaceState(b,b.state,null)}function pe(a,b,c,d){var e=a.stateNode;e.props=c;e.state=a.memoizedState;e.refs=$g;ne(a);var f=b.contextType;"object"===typeof f&&null!==f?e.context=W(f):(f=N(b)?Ra:B.current,e.context=pb(a,f));Qb(a,c,e,d);e.state=a.memoizedState;f=b.getDerivedStateFromProps;
"function"===typeof f&&(Lc(a,b,f,c),e.state=a.memoizedState);"function"===typeof b.getDerivedStateFromProps||"function"===typeof e.getSnapshotBeforeUpdate||"function"!==typeof e.UNSAFE_componentWillMount&&"function"!==typeof e.componentWillMount||(b=e.state,"function"===typeof e.componentWillMount&&e.componentWillMount(),"function"===typeof e.UNSAFE_componentWillMount&&e.UNSAFE_componentWillMount(),b!==e.state&&Mc.enqueueReplaceState(e,e.state,null),Qb(a,c,e,d),e.state=a.memoizedState);"function"===
typeof e.componentDidMount&&(a.effectTag|=4)}function Rb(a,b,c){a=c.ref;if(null!==a&&"function"!==typeof a&&"object"!==typeof a){if(c._owner){c=c._owner;if(c){if(1!==c.tag)throw Error(k(309));var d=c.stateNode}if(!d)throw Error(k(147,a));var e=""+a;if(null!==b&&null!==b.ref&&"function"===typeof b.ref&&b.ref._stringRef===e)return b.ref;b=function(a){var b=d.refs;b===$g&&(b=d.refs={});null===a?delete b[e]:b[e]=a};b._stringRef=e;return b}if("string"!==typeof a)throw Error(k(284));if(!c._owner)throw Error(k(290,
a));}return a}function Nc(a,b){if("textarea"!==a.type)throw Error(k(31,"[object Object]"===Object.prototype.toString.call(b)?"object with keys {"+Object.keys(b).join(", ")+"}":b,""));}function ah(a){function b(b,c){if(a){var d=b.lastEffect;null!==d?(d.nextEffect=c,b.lastEffect=c):b.firstEffect=b.lastEffect=c;c.nextEffect=null;c.effectTag=8}}function c(c,d){if(!a)return null;for(;null!==d;)b(c,d),d=d.sibling;return null}function d(a,b){for(a=new Map;null!==b;)null!==b.key?a.set(b.key,b):a.set(b.index,
b),b=b.sibling;return a}function e(a,b){a=Sa(a,b);a.index=0;a.sibling=null;return a}function f(b,c,d){b.index=d;if(!a)return c;d=b.alternate;if(null!==d)return d=d.index,d<c?(b.effectTag=2,c):d;b.effectTag=2;return c}function g(b){a&&null===b.alternate&&(b.effectTag=2);return b}function h(a,b,c,d){if(null===b||6!==b.tag)return b=qe(c,a.mode,d),b.return=a,b;b=e(b,c);b.return=a;return b}function m(a,b,c,d){if(null!==b&&b.elementType===c.type)return d=e(b,c.props),d.ref=Rb(a,b,c),d.return=a,d;d=Oc(c.type,
c.key,c.props,null,a.mode,d);d.ref=Rb(a,b,c);d.return=a;return d}function n(a,b,c,d){if(null===b||4!==b.tag||b.stateNode.containerInfo!==c.containerInfo||b.stateNode.implementation!==c.implementation)return b=re(c,a.mode,d),b.return=a,b;b=e(b,c.children||[]);b.return=a;return b}function l(a,b,c,d,f){if(null===b||7!==b.tag)return b=Ha(c,a.mode,d,f),b.return=a,b;b=e(b,c);b.return=a;return b}function ba(a,b,c){if("string"===typeof b||"number"===typeof b)return b=qe(""+b,a.mode,c),b.return=a,b;if("object"===
typeof b&&null!==b){switch(b.$$typeof){case Pc:return c=Oc(b.type,b.key,b.props,null,a.mode,c),c.ref=Rb(a,null,b),c.return=a,c;case gb:return b=re(b,a.mode,c),b.return=a,b}if(Qc(b)||zb(b))return b=Ha(b,a.mode,c,null),b.return=a,b;Nc(a,b)}return null}function p(a,b,c,d){var e=null!==b?b.key:null;if("string"===typeof c||"number"===typeof c)return null!==e?null:h(a,b,""+c,d);if("object"===typeof c&&null!==c){switch(c.$$typeof){case Pc:return c.key===e?c.type===Ma?l(a,b,c.props.children,d,e):m(a,b,c,
d):null;case gb:return c.key===e?n(a,b,c,d):null}if(Qc(c)||zb(c))return null!==e?null:l(a,b,c,d,null);Nc(a,c)}return null}function t(a,b,c,d,e){if("string"===typeof d||"number"===typeof d)return a=a.get(c)||null,h(b,a,""+d,e);if("object"===typeof d&&null!==d){switch(d.$$typeof){case Pc:return a=a.get(null===d.key?c:d.key)||null,d.type===Ma?l(b,a,d.props.children,e,d.key):m(b,a,d,e);case gb:return a=a.get(null===d.key?c:d.key)||null,n(b,a,d,e)}if(Qc(d)||zb(d))return a=a.get(c)||null,l(b,a,d,e,null);
Nc(b,d)}return null}function q(e,g,h,m){for(var n=null,k=null,l=g,r=g=0,C=null;null!==l&&r<h.length;r++){l.index>r?(C=l,l=null):C=l.sibling;var O=p(e,l,h[r],m);if(null===O){null===l&&(l=C);break}a&&l&&null===O.alternate&&b(e,l);g=f(O,g,r);null===k?n=O:k.sibling=O;k=O;l=C}if(r===h.length)return c(e,l),n;if(null===l){for(;r<h.length;r++)l=ba(e,h[r],m),null!==l&&(g=f(l,g,r),null===k?n=l:k.sibling=l,k=l);return n}for(l=d(e,l);r<h.length;r++)C=t(l,e,r,h[r],m),null!==C&&(a&&null!==C.alternate&&l.delete(null===
C.key?r:C.key),g=f(C,g,r),null===k?n=C:k.sibling=C,k=C);a&&l.forEach(function(a){return b(e,a)});return n}function w(e,g,h,n){var m=zb(h);if("function"!==typeof m)throw Error(k(150));h=m.call(h);if(null==h)throw Error(k(151));for(var l=m=null,r=g,C=g=0,O=null,v=h.next();null!==r&&!v.done;C++,v=h.next()){r.index>C?(O=r,r=null):O=r.sibling;var q=p(e,r,v.value,n);if(null===q){null===r&&(r=O);break}a&&r&&null===q.alternate&&b(e,r);g=f(q,g,C);null===l?m=q:l.sibling=q;l=q;r=O}if(v.done)return c(e,r),m;
if(null===r){for(;!v.done;C++,v=h.next())v=ba(e,v.value,n),null!==v&&(g=f(v,g,C),null===l?m=v:l.sibling=v,l=v);return m}for(r=d(e,r);!v.done;C++,v=h.next())v=t(r,e,C,v.value,n),null!==v&&(a&&null!==v.alternate&&r.delete(null===v.key?C:v.key),g=f(v,g,C),null===l?m=v:l.sibling=v,l=v);a&&r.forEach(function(a){return b(e,a)});return m}return function(a,d,f,h){var m="object"===typeof f&&null!==f&&f.type===Ma&&null===f.key;m&&(f=f.props.children);var n="object"===typeof f&&null!==f;if(n)switch(f.$$typeof){case Pc:a:{n=
f.key;for(m=d;null!==m;){if(m.key===n){switch(m.tag){case 7:if(f.type===Ma){c(a,m.sibling);d=e(m,f.props.children);d.return=a;a=d;break a}break;default:if(m.elementType===f.type){c(a,m.sibling);d=e(m,f.props);d.ref=Rb(a,m,f);d.return=a;a=d;break a}}c(a,m);break}else b(a,m);m=m.sibling}f.type===Ma?(d=Ha(f.props.children,a.mode,h,f.key),d.return=a,a=d):(h=Oc(f.type,f.key,f.props,null,a.mode,h),h.ref=Rb(a,d,f),h.return=a,a=h)}return g(a);case gb:a:{for(m=f.key;null!==d;){if(d.key===m)if(4===d.tag&&d.stateNode.containerInfo===
f.containerInfo&&d.stateNode.implementation===f.implementation){c(a,d.sibling);d=e(d,f.children||[]);d.return=a;a=d;break a}else{c(a,d);break}else b(a,d);d=d.sibling}d=re(f,a.mode,h);d.return=a;a=d}return g(a)}if("string"===typeof f||"number"===typeof f)return f=""+f,null!==d&&6===d.tag?(c(a,d.sibling),d=e(d,f),d.return=a,a=d):(c(a,d),d=qe(f,a.mode,h),d.return=a,a=d),g(a);if(Qc(f))return q(a,d,f,h);if(zb(f))return w(a,d,f,h);n&&Nc(a,f);if("undefined"===typeof f&&!m)switch(a.tag){case 1:case 0:throw a=
a.type,Error(k(152,a.displayName||a.name||"Component"));}return c(a,d)}}function Ta(a){if(a===Sb)throw Error(k(174));return a}function se(a,b){y(Tb,b);y(Ub,a);y(ja,Sb);a=b.nodeType;switch(a){case 9:case 11:b=(b=b.documentElement)?b.namespaceURI:Hd(null,"");break;default:a=8===a?b.parentNode:b,b=a.namespaceURI||null,a=a.tagName,b=Hd(b,a)}q(ja);y(ja,b)}function tb(a){q(ja);q(Ub);q(Tb)}function bh(a){Ta(Tb.current);var b=Ta(ja.current);var c=Hd(b,a.type);b!==c&&(y(Ub,a),y(ja,c))}function te(a){Ub.current===
a&&(q(ja),q(Ub))}function Rc(a){for(var b=a;null!==b;){if(13===b.tag){var c=b.memoizedState;if(null!==c&&(c=c.dehydrated,null===c||c.data===$d||c.data===Zd))return b}else if(19===b.tag&&void 0!==b.memoizedProps.revealOrder){if(0!==(b.effectTag&64))return b}else if(null!==b.child){b.child.return=b;b=b.child;continue}if(b===a)break;for(;null===b.sibling;){if(null===b.return||b.return===a)return null;b=b.return}b.sibling.return=b.return;b=b.sibling}return null}function ue(a,b){return{responder:a,props:b}}
function S(){throw Error(k(321));}function ve(a,b){if(null===b)return!1;for(var c=0;c<b.length&&c<a.length;c++)if(!Qa(a[c],b[c]))return!1;return!0}function we(a,b,c,d,e,f){Ia=f;z=b;b.memoizedState=null;b.updateQueue=null;b.expirationTime=0;Sc.current=null===a||null===a.memoizedState?dj:ej;a=c(d,e);if(b.expirationTime===Ia){f=0;do{b.expirationTime=0;if(!(25>f))throw Error(k(301));f+=1;J=K=null;b.updateQueue=null;Sc.current=fj;a=c(d,e)}while(b.expirationTime===Ia)}Sc.current=Tc;b=null!==K&&null!==K.next;
Ia=0;J=K=z=null;Uc=!1;if(b)throw Error(k(300));return a}function ub(){var a={memoizedState:null,baseState:null,baseQueue:null,queue:null,next:null};null===J?z.memoizedState=J=a:J=J.next=a;return J}function vb(){if(null===K){var a=z.alternate;a=null!==a?a.memoizedState:null}else a=K.next;var b=null===J?z.memoizedState:J.next;if(null!==b)J=b,K=a;else{if(null===a)throw Error(k(310));K=a;a={memoizedState:K.memoizedState,baseState:K.baseState,baseQueue:K.baseQueue,queue:K.queue,next:null};null===J?z.memoizedState=
J=a:J=J.next=a}return J}function Ua(a,b){return"function"===typeof b?b(a):b}function Vc(a,b,c){b=vb();c=b.queue;if(null===c)throw Error(k(311));c.lastRenderedReducer=a;var d=K,e=d.baseQueue,f=c.pending;if(null!==f){if(null!==e){var g=e.next;e.next=f.next;f.next=g}d.baseQueue=e=f;c.pending=null}if(null!==e){e=e.next;d=d.baseState;var h=g=f=null,m=e;do{var n=m.expirationTime;if(n<Ia){var l={expirationTime:m.expirationTime,suspenseConfig:m.suspenseConfig,action:m.action,eagerReducer:m.eagerReducer,eagerState:m.eagerState,
next:null};null===h?(g=h=l,f=d):h=h.next=l;n>z.expirationTime&&(z.expirationTime=n,Kc(n))}else null!==h&&(h=h.next={expirationTime:1073741823,suspenseConfig:m.suspenseConfig,action:m.action,eagerReducer:m.eagerReducer,eagerState:m.eagerState,next:null}),Vg(n,m.suspenseConfig),d=m.eagerReducer===a?m.eagerState:a(d,m.action);m=m.next}while(null!==m&&m!==e);null===h?f=d:h.next=g;Qa(d,b.memoizedState)||(ia=!0);b.memoizedState=d;b.baseState=f;b.baseQueue=h;c.lastRenderedState=d}return[b.memoizedState,
c.dispatch]}function Wc(a,b,c){b=vb();c=b.queue;if(null===c)throw Error(k(311));c.lastRenderedReducer=a;var d=c.dispatch,e=c.pending,f=b.memoizedState;if(null!==e){c.pending=null;var g=e=e.next;do f=a(f,g.action),g=g.next;while(g!==e);Qa(f,b.memoizedState)||(ia=!0);b.memoizedState=f;null===b.baseQueue&&(b.baseState=f);c.lastRenderedState=f}return[f,d]}function xe(a){var b=ub();"function"===typeof a&&(a=a());b.memoizedState=b.baseState=a;a=b.queue={pending:null,dispatch:null,lastRenderedReducer:Ua,
lastRenderedState:a};a=a.dispatch=ch.bind(null,z,a);return[b.memoizedState,a]}function ye(a,b,c,d){a={tag:a,create:b,destroy:c,deps:d,next:null};b=z.updateQueue;null===b?(b={lastEffect:null},z.updateQueue=b,b.lastEffect=a.next=a):(c=b.lastEffect,null===c?b.lastEffect=a.next=a:(d=c.next,c.next=a,a.next=d,b.lastEffect=a));return a}function dh(a){return vb().memoizedState}function ze(a,b,c,d){var e=ub();z.effectTag|=a;e.memoizedState=ye(1|b,c,void 0,void 0===d?null:d)}function Ae(a,b,c,d){var e=vb();
d=void 0===d?null:d;var f=void 0;if(null!==K){var g=K.memoizedState;f=g.destroy;if(null!==d&&ve(d,g.deps)){ye(b,c,f,d);return}}z.effectTag|=a;e.memoizedState=ye(1|b,c,f,d)}function eh(a,b){return ze(516,4,a,b)}function Xc(a,b){return Ae(516,4,a,b)}function fh(a,b){return Ae(4,2,a,b)}function gh(a,b){if("function"===typeof b)return a=a(),b(a),function(){b(null)};if(null!==b&&void 0!==b)return a=a(),b.current=a,function(){b.current=null}}function hh(a,b,c){c=null!==c&&void 0!==c?c.concat([a]):null;
return Ae(4,2,gh.bind(null,b,a),c)}function Be(a,b){}function ih(a,b){ub().memoizedState=[a,void 0===b?null:b];return a}function Yc(a,b){var c=vb();b=void 0===b?null:b;var d=c.memoizedState;if(null!==d&&null!==b&&ve(b,d[1]))return d[0];c.memoizedState=[a,b];return a}function jh(a,b){var c=vb();b=void 0===b?null:b;var d=c.memoizedState;if(null!==d&&null!==b&&ve(b,d[1]))return d[0];a=a();c.memoizedState=[a,b];return a}function Ce(a,b,c){var d=Cc();Da(98>d?98:d,function(){a(!0)});Da(97<d?97:d,function(){var d=
X.suspense;X.suspense=void 0===b?null:b;try{a(!1),c()}finally{X.suspense=d}})}function ch(a,b,c){var d=ka(),e=Vb.suspense;d=Va(d,a,e);e={expirationTime:d,suspenseConfig:e,action:c,eagerReducer:null,eagerState:null,next:null};var f=b.pending;null===f?e.next=e:(e.next=f.next,f.next=e);b.pending=e;f=a.alternate;if(a===z||null!==f&&f===z)Uc=!0,e.expirationTime=Ia,z.expirationTime=Ia;else{if(0===a.expirationTime&&(null===f||0===f.expirationTime)&&(f=b.lastRenderedReducer,null!==f))try{var g=b.lastRenderedState,
h=f(g,c);e.eagerReducer=f;e.eagerState=h;if(Qa(h,g))return}catch(m){}finally{}Ja(a,d)}}function kh(a,b){var c=la(5,null,null,0);c.elementType="DELETED";c.type="DELETED";c.stateNode=b;c.return=a;c.effectTag=8;null!==a.lastEffect?(a.lastEffect.nextEffect=c,a.lastEffect=c):a.firstEffect=a.lastEffect=c}function lh(a,b){switch(a.tag){case 5:var c=a.type;b=1!==b.nodeType||c.toLowerCase()!==b.nodeName.toLowerCase()?null:b;return null!==b?(a.stateNode=b,!0):!1;case 6:return b=""===a.pendingProps||3!==b.nodeType?
null:b,null!==b?(a.stateNode=b,!0):!1;case 13:return!1;default:return!1}}function De(a){if(Wa){var b=Ka;if(b){var c=b;if(!lh(a,b)){b=kb(c.nextSibling);if(!b||!lh(a,b)){a.effectTag=a.effectTag&-1025|2;Wa=!1;ra=a;return}kh(ra,c)}ra=a;Ka=kb(b.firstChild)}else a.effectTag=a.effectTag&-1025|2,Wa=!1,ra=a}}function mh(a){for(a=a.return;null!==a&&5!==a.tag&&3!==a.tag&&13!==a.tag;)a=a.return;ra=a}function Zc(a){if(a!==ra)return!1;if(!Wa)return mh(a),Wa=!0,!1;var b=a.type;if(5!==a.tag||"head"!==b&&"body"!==
b&&!Yd(b,a.memoizedProps))for(b=Ka;b;)kh(a,b),b=kb(b.nextSibling);mh(a);if(13===a.tag){a=a.memoizedState;a=null!==a?a.dehydrated:null;if(!a)throw Error(k(317));a:{a=a.nextSibling;for(b=0;a;){if(8===a.nodeType){var c=a.data;if(c===og){if(0===b){Ka=kb(a.nextSibling);break a}b--}else c!==ng&&c!==Zd&&c!==$d||b++}a=a.nextSibling}Ka=null}}else Ka=ra?kb(a.stateNode.nextSibling):null;return!0}function Ee(){Ka=ra=null;Wa=!1}function T(a,b,c,d){b.child=null===a?Fe(b,null,c,d):wb(b,a.child,c,d)}function nh(a,
b,c,d,e){c=c.render;var f=b.ref;rb(b,e);d=we(a,b,c,d,f,e);if(null!==a&&!ia)return b.updateQueue=a.updateQueue,b.effectTag&=-517,a.expirationTime<=e&&(a.expirationTime=0),sa(a,b,e);b.effectTag|=1;T(a,b,d,e);return b.child}function oh(a,b,c,d,e,f){if(null===a){var g=c.type;if("function"===typeof g&&!Ge(g)&&void 0===g.defaultProps&&null===c.compare&&void 0===c.defaultProps)return b.tag=15,b.type=g,ph(a,b,g,d,e,f);a=Oc(c.type,null,d,null,b.mode,f);a.ref=b.ref;a.return=b;return b.child=a}g=a.child;if(e<
f&&(e=g.memoizedProps,c=c.compare,c=null!==c?c:Ob,c(e,d)&&a.ref===b.ref))return sa(a,b,f);b.effectTag|=1;a=Sa(g,d);a.ref=b.ref;a.return=b;return b.child=a}function ph(a,b,c,d,e,f){return null!==a&&Ob(a.memoizedProps,d)&&a.ref===b.ref&&(ia=!1,e<f)?(b.expirationTime=a.expirationTime,sa(a,b,f)):He(a,b,c,d,f)}function qh(a,b){var c=b.ref;if(null===a&&null!==c||null!==a&&a.ref!==c)b.effectTag|=128}function He(a,b,c,d,e){var f=N(c)?Ra:B.current;f=pb(b,f);rb(b,e);c=we(a,b,c,d,f,e);if(null!==a&&!ia)return b.updateQueue=
a.updateQueue,b.effectTag&=-517,a.expirationTime<=e&&(a.expirationTime=0),sa(a,b,e);b.effectTag|=1;T(a,b,c,e);return b.child}function rh(a,b,c,d,e){if(N(c)){var f=!0;Bc(b)}else f=!1;rb(b,e);if(null===b.stateNode)null!==a&&(a.alternate=null,b.alternate=null,b.effectTag|=2),Yg(b,c,d),pe(b,c,d,e),d=!0;else if(null===a){var g=b.stateNode,h=b.memoizedProps;g.props=h;var m=g.context,n=c.contextType;"object"===typeof n&&null!==n?n=W(n):(n=N(c)?Ra:B.current,n=pb(b,n));var l=c.getDerivedStateFromProps,k="function"===
typeof l||"function"===typeof g.getSnapshotBeforeUpdate;k||"function"!==typeof g.UNSAFE_componentWillReceiveProps&&"function"!==typeof g.componentWillReceiveProps||(h!==d||m!==n)&&Zg(b,g,d,n);Ga=!1;var p=b.memoizedState;g.state=p;Qb(b,d,g,e);m=b.memoizedState;h!==d||p!==m||G.current||Ga?("function"===typeof l&&(Lc(b,c,l,d),m=b.memoizedState),(h=Ga||Xg(b,c,h,d,p,m,n))?(k||"function"!==typeof g.UNSAFE_componentWillMount&&"function"!==typeof g.componentWillMount||("function"===typeof g.componentWillMount&&
g.componentWillMount(),"function"===typeof g.UNSAFE_componentWillMount&&g.UNSAFE_componentWillMount()),"function"===typeof g.componentDidMount&&(b.effectTag|=4)):("function"===typeof g.componentDidMount&&(b.effectTag|=4),b.memoizedProps=d,b.memoizedState=m),g.props=d,g.state=m,g.context=n,d=h):("function"===typeof g.componentDidMount&&(b.effectTag|=4),d=!1)}else g=b.stateNode,oe(a,b),h=b.memoizedProps,g.props=b.type===b.elementType?h:aa(b.type,h),m=g.context,n=c.contextType,"object"===typeof n&&null!==
n?n=W(n):(n=N(c)?Ra:B.current,n=pb(b,n)),l=c.getDerivedStateFromProps,(k="function"===typeof l||"function"===typeof g.getSnapshotBeforeUpdate)||"function"!==typeof g.UNSAFE_componentWillReceiveProps&&"function"!==typeof g.componentWillReceiveProps||(h!==d||m!==n)&&Zg(b,g,d,n),Ga=!1,m=b.memoizedState,g.state=m,Qb(b,d,g,e),p=b.memoizedState,h!==d||m!==p||G.current||Ga?("function"===typeof l&&(Lc(b,c,l,d),p=b.memoizedState),(l=Ga||Xg(b,c,h,d,m,p,n))?(k||"function"!==typeof g.UNSAFE_componentWillUpdate&&
"function"!==typeof g.componentWillUpdate||("function"===typeof g.componentWillUpdate&&g.componentWillUpdate(d,p,n),"function"===typeof g.UNSAFE_componentWillUpdate&&g.UNSAFE_componentWillUpdate(d,p,n)),"function"===typeof g.componentDidUpdate&&(b.effectTag|=4),"function"===typeof g.getSnapshotBeforeUpdate&&(b.effectTag|=256)):("function"!==typeof g.componentDidUpdate||h===a.memoizedProps&&m===a.memoizedState||(b.effectTag|=4),"function"!==typeof g.getSnapshotBeforeUpdate||h===a.memoizedProps&&m===
a.memoizedState||(b.effectTag|=256),b.memoizedProps=d,b.memoizedState=p),g.props=d,g.state=p,g.context=n,d=l):("function"!==typeof g.componentDidUpdate||h===a.memoizedProps&&m===a.memoizedState||(b.effectTag|=4),"function"!==typeof g.getSnapshotBeforeUpdate||h===a.memoizedProps&&m===a.memoizedState||(b.effectTag|=256),d=!1);return Ie(a,b,c,d,f,e)}function Ie(a,b,c,d,e,f){qh(a,b);var g=0!==(b.effectTag&64);if(!d&&!g)return e&&Hg(b,c,!1),sa(a,b,f);d=b.stateNode;gj.current=b;var h=g&&"function"!==typeof c.getDerivedStateFromError?
null:d.render();b.effectTag|=1;null!==a&&g?(b.child=wb(b,a.child,null,f),b.child=wb(b,null,h,f)):T(a,b,h,f);b.memoizedState=d.state;e&&Hg(b,c,!0);return b.child}function sh(a){var b=a.stateNode;b.pendingContext?Fg(a,b.pendingContext,b.pendingContext!==b.context):b.context&&Fg(a,b.context,!1);se(a,b.containerInfo)}function th(a,b,c){var d=b.mode,e=b.pendingProps,f=D.current,g=!1,h;(h=0!==(b.effectTag&64))||(h=0!==(f&2)&&(null===a||null!==a.memoizedState));h?(g=!0,b.effectTag&=-65):null!==a&&null===
a.memoizedState||void 0===e.fallback||!0===e.unstable_avoidThisFallback||(f|=1);y(D,f&1);if(null===a){void 0!==e.fallback&&De(b);if(g){g=e.fallback;e=Ha(null,d,0,null);e.return=b;if(0===(b.mode&2))for(a=null!==b.memoizedState?b.child.child:b.child,e.child=a;null!==a;)a.return=e,a=a.sibling;c=Ha(g,d,c,null);c.return=b;e.sibling=c;b.memoizedState=Je;b.child=e;return c}d=e.children;b.memoizedState=null;return b.child=Fe(b,null,d,c)}if(null!==a.memoizedState){a=a.child;d=a.sibling;if(g){e=e.fallback;
c=Sa(a,a.pendingProps);c.return=b;if(0===(b.mode&2)&&(g=null!==b.memoizedState?b.child.child:b.child,g!==a.child))for(c.child=g;null!==g;)g.return=c,g=g.sibling;d=Sa(d,e);d.return=b;c.sibling=d;c.childExpirationTime=0;b.memoizedState=Je;b.child=c;return d}c=wb(b,a.child,e.children,c);b.memoizedState=null;return b.child=c}a=a.child;if(g){g=e.fallback;e=Ha(null,d,0,null);e.return=b;e.child=a;null!==a&&(a.return=e);if(0===(b.mode&2))for(a=null!==b.memoizedState?b.child.child:b.child,e.child=a;null!==
a;)a.return=e,a=a.sibling;c=Ha(g,d,c,null);c.return=b;e.sibling=c;c.effectTag|=2;e.childExpirationTime=0;b.memoizedState=Je;b.child=e;return c}b.memoizedState=null;return b.child=wb(b,a,e.children,c)}function uh(a,b){a.expirationTime<b&&(a.expirationTime=b);var c=a.alternate;null!==c&&c.expirationTime<b&&(c.expirationTime=b);Sg(a.return,b)}function Ke(a,b,c,d,e,f){var g=a.memoizedState;null===g?a.memoizedState={isBackwards:b,rendering:null,renderingStartTime:0,last:d,tail:c,tailExpiration:0,tailMode:e,
lastEffect:f}:(g.isBackwards=b,g.rendering=null,g.renderingStartTime=0,g.last=d,g.tail=c,g.tailExpiration=0,g.tailMode=e,g.lastEffect=f)}function vh(a,b,c){var d=b.pendingProps,e=d.revealOrder,f=d.tail;T(a,b,d.children,c);d=D.current;if(0!==(d&2))d=d&1|2,b.effectTag|=64;else{if(null!==a&&0!==(a.effectTag&64))a:for(a=b.child;null!==a;){if(13===a.tag)null!==a.memoizedState&&uh(a,c);else if(19===a.tag)uh(a,c);else if(null!==a.child){a.child.return=a;a=a.child;continue}if(a===b)break a;for(;null===a.sibling;){if(null===
a.return||a.return===b)break a;a=a.return}a.sibling.return=a.return;a=a.sibling}d&=1}y(D,d);if(0===(b.mode&2))b.memoizedState=null;else switch(e){case "forwards":c=b.child;for(e=null;null!==c;)a=c.alternate,null!==a&&null===Rc(a)&&(e=c),c=c.sibling;c=e;null===c?(e=b.child,b.child=null):(e=c.sibling,c.sibling=null);Ke(b,!1,e,c,f,b.lastEffect);break;case "backwards":c=null;e=b.child;for(b.child=null;null!==e;){a=e.alternate;if(null!==a&&null===Rc(a)){b.child=e;break}a=e.sibling;e.sibling=c;c=e;e=a}Ke(b,
!0,c,null,f,b.lastEffect);break;case "together":Ke(b,!1,null,null,void 0,b.lastEffect);break;default:b.memoizedState=null}return b.child}function sa(a,b,c){null!==a&&(b.dependencies=a.dependencies);var d=b.expirationTime;0!==d&&Kc(d);if(b.childExpirationTime<c)return null;if(null!==a&&b.child!==a.child)throw Error(k(153));if(null!==b.child){a=b.child;c=Sa(a,a.pendingProps);b.child=c;for(c.return=b;null!==a.sibling;)a=a.sibling,c=c.sibling=Sa(a,a.pendingProps),c.return=b;c.sibling=null}return b.child}
function $c(a,b){switch(a.tailMode){case "hidden":b=a.tail;for(var c=null;null!==b;)null!==b.alternate&&(c=b),b=b.sibling;null===c?a.tail=null:c.sibling=null;break;case "collapsed":c=a.tail;for(var d=null;null!==c;)null!==c.alternate&&(d=c),c=c.sibling;null===d?b||null===a.tail?a.tail=null:a.tail.sibling=null:d.sibling=null}}function hj(a,b,c){var d=b.pendingProps;switch(b.tag){case 2:case 16:case 15:case 0:case 11:case 7:case 8:case 12:case 9:case 14:return null;case 1:return N(b.type)&&(q(G),q(B)),
null;case 3:return tb(),q(G),q(B),c=b.stateNode,c.pendingContext&&(c.context=c.pendingContext,c.pendingContext=null),null!==a&&null!==a.child||!Zc(b)||(b.effectTag|=4),wh(b),null;case 5:te(b);c=Ta(Tb.current);var e=b.type;if(null!==a&&null!=b.stateNode)ij(a,b,e,d,c),a.ref!==b.ref&&(b.effectTag|=128);else{if(!d){if(null===b.stateNode)throw Error(k(166));return null}a=Ta(ja.current);if(Zc(b)){d=b.stateNode;e=b.type;var f=b.memoizedProps;d[Aa]=b;d[vc]=f;switch(e){case "iframe":case "object":case "embed":w("load",
d);break;case "video":case "audio":for(a=0;a<Db.length;a++)w(Db[a],d);break;case "source":w("error",d);break;case "img":case "image":case "link":w("error",d);w("load",d);break;case "form":w("reset",d);w("submit",d);break;case "details":w("toggle",d);break;case "input":Hf(d,f);w("invalid",d);oa(c,"onChange");break;case "select":d._wrapperState={wasMultiple:!!f.multiple};w("invalid",d);oa(c,"onChange");break;case "textarea":Kf(d,f),w("invalid",d),oa(c,"onChange")}Ud(e,f);a=null;for(var g in f)if(f.hasOwnProperty(g)){var h=
f[g];"children"===g?"string"===typeof h?d.textContent!==h&&(a=["children",h]):"number"===typeof h&&d.textContent!==""+h&&(a=["children",""+h]):db.hasOwnProperty(g)&&null!=h&&oa(c,g)}switch(e){case "input":mc(d);Jf(d,f,!0);break;case "textarea":mc(d);Mf(d);break;case "select":case "option":break;default:"function"===typeof f.onClick&&(d.onclick=uc)}c=a;b.updateQueue=c;null!==c&&(b.effectTag|=4)}else{g=9===c.nodeType?c:c.ownerDocument;"http://www.w3.org/1999/xhtml"===a&&(a=Nf(e));"http://www.w3.org/1999/xhtml"===
a?"script"===e?(a=g.createElement("div"),a.innerHTML="<script>\x3c/script>",a=a.removeChild(a.firstChild)):"string"===typeof d.is?a=g.createElement(e,{is:d.is}):(a=g.createElement(e),"select"===e&&(g=a,d.multiple?g.multiple=!0:d.size&&(g.size=d.size))):a=g.createElementNS(a,e);a[Aa]=b;a[vc]=d;jj(a,b,!1,!1);b.stateNode=a;g=Vd(e,d);switch(e){case "iframe":case "object":case "embed":w("load",a);h=d;break;case "video":case "audio":for(h=0;h<Db.length;h++)w(Db[h],a);h=d;break;case "source":w("error",a);
h=d;break;case "img":case "image":case "link":w("error",a);w("load",a);h=d;break;case "form":w("reset",a);w("submit",a);h=d;break;case "details":w("toggle",a);h=d;break;case "input":Hf(a,d);h=Cd(a,d);w("invalid",a);oa(c,"onChange");break;case "option":h=Fd(a,d);break;case "select":a._wrapperState={wasMultiple:!!d.multiple};h=M({},d,{value:void 0});w("invalid",a);oa(c,"onChange");break;case "textarea":Kf(a,d);h=Gd(a,d);w("invalid",a);oa(c,"onChange");break;default:h=d}Ud(e,h);var m=h;for(f in m)if(m.hasOwnProperty(f)){var n=
m[f];"style"===f?gg(a,n):"dangerouslySetInnerHTML"===f?(n=n?n.__html:void 0,null!=n&&xh(a,n)):"children"===f?"string"===typeof n?("textarea"!==e||""!==n)&&Wb(a,n):"number"===typeof n&&Wb(a,""+n):"suppressContentEditableWarning"!==f&&"suppressHydrationWarning"!==f&&"autoFocus"!==f&&(db.hasOwnProperty(f)?null!=n&&oa(c,f):null!=n&&xd(a,f,n,g))}switch(e){case "input":mc(a);Jf(a,d,!1);break;case "textarea":mc(a);Mf(a);break;case "option":null!=d.value&&a.setAttribute("value",""+va(d.value));break;case "select":a.multiple=
!!d.multiple;c=d.value;null!=c?hb(a,!!d.multiple,c,!1):null!=d.defaultValue&&hb(a,!!d.multiple,d.defaultValue,!0);break;default:"function"===typeof h.onClick&&(a.onclick=uc)}lg(e,d)&&(b.effectTag|=4)}null!==b.ref&&(b.effectTag|=128)}return null;case 6:if(a&&null!=b.stateNode)kj(a,b,a.memoizedProps,d);else{if("string"!==typeof d&&null===b.stateNode)throw Error(k(166));c=Ta(Tb.current);Ta(ja.current);Zc(b)?(c=b.stateNode,d=b.memoizedProps,c[Aa]=b,c.nodeValue!==d&&(b.effectTag|=4)):(c=(9===c.nodeType?
c:c.ownerDocument).createTextNode(d),c[Aa]=b,b.stateNode=c)}return null;case 13:q(D);d=b.memoizedState;if(0!==(b.effectTag&64))return b.expirationTime=c,b;c=null!==d;d=!1;null===a?void 0!==b.memoizedProps.fallback&&Zc(b):(e=a.memoizedState,d=null!==e,c||null===e||(e=a.child.sibling,null!==e&&(f=b.firstEffect,null!==f?(b.firstEffect=e,e.nextEffect=f):(b.firstEffect=b.lastEffect=e,e.nextEffect=null),e.effectTag=8)));if(c&&!d&&0!==(b.mode&2))if(null===a&&!0!==b.memoizedProps.unstable_avoidThisFallback||
0!==(D.current&1))F===Xa&&(F=ad);else{if(F===Xa||F===ad)F=bd;0!==Xb&&null!==U&&(Ya(U,P),yh(U,Xb))}if(c||d)b.effectTag|=4;return null;case 4:return tb(),wh(b),null;case 10:return me(b),null;case 17:return N(b.type)&&(q(G),q(B)),null;case 19:q(D);d=b.memoizedState;if(null===d)return null;e=0!==(b.effectTag&64);f=d.rendering;if(null===f)if(e)$c(d,!1);else{if(F!==Xa||null!==a&&0!==(a.effectTag&64))for(f=b.child;null!==f;){a=Rc(f);if(null!==a){b.effectTag|=64;$c(d,!1);e=a.updateQueue;null!==e&&(b.updateQueue=
e,b.effectTag|=4);null===d.lastEffect&&(b.firstEffect=null);b.lastEffect=d.lastEffect;for(d=b.child;null!==d;)e=d,f=c,e.effectTag&=2,e.nextEffect=null,e.firstEffect=null,e.lastEffect=null,a=e.alternate,null===a?(e.childExpirationTime=0,e.expirationTime=f,e.child=null,e.memoizedProps=null,e.memoizedState=null,e.updateQueue=null,e.dependencies=null):(e.childExpirationTime=a.childExpirationTime,e.expirationTime=a.expirationTime,e.child=a.child,e.memoizedProps=a.memoizedProps,e.memoizedState=a.memoizedState,
e.updateQueue=a.updateQueue,f=a.dependencies,e.dependencies=null===f?null:{expirationTime:f.expirationTime,firstContext:f.firstContext,responders:f.responders}),d=d.sibling;y(D,D.current&1|2);return b.child}f=f.sibling}}else{if(!e)if(a=Rc(f),null!==a){if(b.effectTag|=64,e=!0,c=a.updateQueue,null!==c&&(b.updateQueue=c,b.effectTag|=4),$c(d,!0),null===d.tail&&"hidden"===d.tailMode&&!f.alternate)return b=b.lastEffect=d.lastEffect,null!==b&&(b.nextEffect=null),null}else 2*Y()-d.renderingStartTime>d.tailExpiration&&
1<c&&(b.effectTag|=64,e=!0,$c(d,!1),b.expirationTime=b.childExpirationTime=c-1);d.isBackwards?(f.sibling=b.child,b.child=f):(c=d.last,null!==c?c.sibling=f:b.child=f,d.last=f)}return null!==d.tail?(0===d.tailExpiration&&(d.tailExpiration=Y()+500),c=d.tail,d.rendering=c,d.tail=c.sibling,d.lastEffect=b.lastEffect,d.renderingStartTime=Y(),c.sibling=null,b=D.current,y(D,e?b&1|2:b&1),c):null}throw Error(k(156,b.tag));}function lj(a,b){switch(a.tag){case 1:return N(a.type)&&(q(G),q(B)),b=a.effectTag,b&4096?
(a.effectTag=b&-4097|64,a):null;case 3:tb();q(G);q(B);b=a.effectTag;if(0!==(b&64))throw Error(k(285));a.effectTag=b&-4097|64;return a;case 5:return te(a),null;case 13:return q(D),b=a.effectTag,b&4096?(a.effectTag=b&-4097|64,a):null;case 19:return q(D),null;case 4:return tb(),null;case 10:return me(a),null;default:return null}}function Le(a,b){return{value:a,source:b,stack:Bd(b)}}function Me(a,b){var c=b.source,d=b.stack;null===d&&null!==c&&(d=Bd(c));null!==c&&na(c.type);b=b.value;null!==a&&1===a.tag&&
na(a.type);try{console.error(b)}catch(e){setTimeout(function(){throw e;})}}function mj(a,b){try{b.props=a.memoizedProps,b.state=a.memoizedState,b.componentWillUnmount()}catch(c){Za(a,c)}}function zh(a){var b=a.ref;if(null!==b)if("function"===typeof b)try{b(null)}catch(c){Za(a,c)}else b.current=null}function nj(a,b){switch(b.tag){case 0:case 11:case 15:case 22:return;case 1:if(b.effectTag&256&&null!==a){var c=a.memoizedProps,d=a.memoizedState;a=b.stateNode;b=a.getSnapshotBeforeUpdate(b.elementType===
b.type?c:aa(b.type,c),d);a.__reactInternalSnapshotBeforeUpdate=b}return;case 3:case 5:case 6:case 4:case 17:return}throw Error(k(163));}function Ah(a,b){b=b.updateQueue;b=null!==b?b.lastEffect:null;if(null!==b){var c=b=b.next;do{if((c.tag&a)===a){var d=c.destroy;c.destroy=void 0;void 0!==d&&d()}c=c.next}while(c!==b)}}function Bh(a,b){b=b.updateQueue;b=null!==b?b.lastEffect:null;if(null!==b){var c=b=b.next;do{if((c.tag&a)===a){var d=c.create;c.destroy=d()}c=c.next}while(c!==b)}}function oj(a,b,c,d){switch(c.tag){case 0:case 11:case 15:case 22:Bh(3,
c);return;case 1:a=c.stateNode;c.effectTag&4&&(null===b?a.componentDidMount():(d=c.elementType===c.type?b.memoizedProps:aa(c.type,b.memoizedProps),a.componentDidUpdate(d,b.memoizedState,a.__reactInternalSnapshotBeforeUpdate)));b=c.updateQueue;null!==b&&Wg(c,b,a);return;case 3:b=c.updateQueue;if(null!==b){a=null;if(null!==c.child)switch(c.child.tag){case 5:a=c.child.stateNode;break;case 1:a=c.child.stateNode}Wg(c,b,a)}return;case 5:a=c.stateNode;null===b&&c.effectTag&4&&lg(c.type,c.memoizedProps)&&
a.focus();return;case 6:return;case 4:return;case 12:return;case 13:null===c.memoizedState&&(c=c.alternate,null!==c&&(c=c.memoizedState,null!==c&&(c=c.dehydrated,null!==c&&bg(c))));return;case 19:case 17:case 20:case 21:return}throw Error(k(163));}function Ch(a,b,c){"function"===typeof Ne&&Ne(b);switch(b.tag){case 0:case 11:case 14:case 15:case 22:a=b.updateQueue;if(null!==a&&(a=a.lastEffect,null!==a)){var d=a.next;Da(97<c?97:c,function(){var a=d;do{var c=a.destroy;if(void 0!==c){var g=b;try{c()}catch(h){Za(g,
h)}}a=a.next}while(a!==d)})}break;case 1:zh(b);c=b.stateNode;"function"===typeof c.componentWillUnmount&&mj(b,c);break;case 5:zh(b);break;case 4:Dh(a,b,c)}}function Eh(a){var b=a.alternate;a.return=null;a.child=null;a.memoizedState=null;a.updateQueue=null;a.dependencies=null;a.alternate=null;a.firstEffect=null;a.lastEffect=null;a.pendingProps=null;a.memoizedProps=null;a.stateNode=null;null!==b&&Eh(b)}function Fh(a){return 5===a.tag||3===a.tag||4===a.tag}function Gh(a){a:{for(var b=a.return;null!==
b;){if(Fh(b)){var c=b;break a}b=b.return}throw Error(k(160));}b=c.stateNode;switch(c.tag){case 5:var d=!1;break;case 3:b=b.containerInfo;d=!0;break;case 4:b=b.containerInfo;d=!0;break;default:throw Error(k(161));}c.effectTag&16&&(Wb(b,""),c.effectTag&=-17);a:b:for(c=a;;){for(;null===c.sibling;){if(null===c.return||Fh(c.return)){c=null;break a}c=c.return}c.sibling.return=c.return;for(c=c.sibling;5!==c.tag&&6!==c.tag&&18!==c.tag;){if(c.effectTag&2)continue b;if(null===c.child||4===c.tag)continue b;
else c.child.return=c,c=c.child}if(!(c.effectTag&2)){c=c.stateNode;break a}}d?Oe(a,c,b):Pe(a,c,b)}function Oe(a,b,c){var d=a.tag,e=5===d||6===d;if(e)a=e?a.stateNode:a.stateNode.instance,b?8===c.nodeType?c.parentNode.insertBefore(a,b):c.insertBefore(a,b):(8===c.nodeType?(b=c.parentNode,b.insertBefore(a,c)):(b=c,b.appendChild(a)),c=c._reactRootContainer,null!==c&&void 0!==c||null!==b.onclick||(b.onclick=uc));else if(4!==d&&(a=a.child,null!==a))for(Oe(a,b,c),a=a.sibling;null!==a;)Oe(a,b,c),a=a.sibling}
function Pe(a,b,c){var d=a.tag,e=5===d||6===d;if(e)a=e?a.stateNode:a.stateNode.instance,b?c.insertBefore(a,b):c.appendChild(a);else if(4!==d&&(a=a.child,null!==a))for(Pe(a,b,c),a=a.sibling;null!==a;)Pe(a,b,c),a=a.sibling}function Dh(a,b,c){for(var d=b,e=!1,f,g;;){if(!e){e=d.return;a:for(;;){if(null===e)throw Error(k(160));f=e.stateNode;switch(e.tag){case 5:g=!1;break a;case 3:f=f.containerInfo;g=!0;break a;case 4:f=f.containerInfo;g=!0;break a}e=e.return}e=!0}if(5===d.tag||6===d.tag){a:for(var h=
a,m=d,n=c,l=m;;)if(Ch(h,l,n),null!==l.child&&4!==l.tag)l.child.return=l,l=l.child;else{if(l===m)break a;for(;null===l.sibling;){if(null===l.return||l.return===m)break a;l=l.return}l.sibling.return=l.return;l=l.sibling}g?(h=f,m=d.stateNode,8===h.nodeType?h.parentNode.removeChild(m):h.removeChild(m)):f.removeChild(d.stateNode)}else if(4===d.tag){if(null!==d.child){f=d.stateNode.containerInfo;g=!0;d.child.return=d;d=d.child;continue}}else if(Ch(a,d,c),null!==d.child){d.child.return=d;d=d.child;continue}if(d===
b)break;for(;null===d.sibling;){if(null===d.return||d.return===b)return;d=d.return;4===d.tag&&(e=!1)}d.sibling.return=d.return;d=d.sibling}}function Qe(a,b){switch(b.tag){case 0:case 11:case 14:case 15:case 22:Ah(3,b);return;case 1:return;case 5:var c=b.stateNode;if(null!=c){var d=b.memoizedProps,e=null!==a?a.memoizedProps:d;a=b.type;var f=b.updateQueue;b.updateQueue=null;if(null!==f){c[vc]=d;"input"===a&&"radio"===d.type&&null!=d.name&&If(c,d);Vd(a,e);b=Vd(a,d);for(e=0;e<f.length;e+=2){var g=f[e],
h=f[e+1];"style"===g?gg(c,h):"dangerouslySetInnerHTML"===g?xh(c,h):"children"===g?Wb(c,h):xd(c,g,h,b)}switch(a){case "input":Dd(c,d);break;case "textarea":Lf(c,d);break;case "select":b=c._wrapperState.wasMultiple,c._wrapperState.wasMultiple=!!d.multiple,a=d.value,null!=a?hb(c,!!d.multiple,a,!1):b!==!!d.multiple&&(null!=d.defaultValue?hb(c,!!d.multiple,d.defaultValue,!0):hb(c,!!d.multiple,d.multiple?[]:"",!1))}}}return;case 6:if(null===b.stateNode)throw Error(k(162));b.stateNode.nodeValue=b.memoizedProps;
return;case 3:b=b.stateNode;b.hydrate&&(b.hydrate=!1,bg(b.containerInfo));return;case 12:return;case 13:c=b;null===b.memoizedState?d=!1:(d=!0,c=b.child,Re=Y());if(null!==c)a:for(a=c;;){if(5===a.tag)f=a.stateNode,d?(f=f.style,"function"===typeof f.setProperty?f.setProperty("display","none","important"):f.display="none"):(f=a.stateNode,e=a.memoizedProps.style,e=void 0!==e&&null!==e&&e.hasOwnProperty("display")?e.display:null,f.style.display=fg("display",e));else if(6===a.tag)a.stateNode.nodeValue=d?
"":a.memoizedProps;else if(13===a.tag&&null!==a.memoizedState&&null===a.memoizedState.dehydrated){f=a.child.sibling;f.return=a;a=f;continue}else if(null!==a.child){a.child.return=a;a=a.child;continue}if(a===c)break;for(;null===a.sibling;){if(null===a.return||a.return===c)break a;a=a.return}a.sibling.return=a.return;a=a.sibling}Hh(b);return;case 19:Hh(b);return;case 17:return}throw Error(k(163));}function Hh(a){var b=a.updateQueue;if(null!==b){a.updateQueue=null;var c=a.stateNode;null===c&&(c=a.stateNode=
new pj);b.forEach(function(b){var d=qj.bind(null,a,b);c.has(b)||(c.add(b),b.then(d,d))})}}function Ih(a,b,c){c=Ea(c,null);c.tag=3;c.payload={element:null};var d=b.value;c.callback=function(){cd||(cd=!0,Se=d);Me(a,b)};return c}function Jh(a,b,c){c=Ea(c,null);c.tag=3;var d=a.type.getDerivedStateFromError;if("function"===typeof d){var e=b.value;c.payload=function(){Me(a,b);return d(e)}}var f=a.stateNode;null!==f&&"function"===typeof f.componentDidCatch&&(c.callback=function(){"function"!==typeof d&&
(null===La?La=new Set([this]):La.add(this),Me(a,b));var c=b.stack;this.componentDidCatch(b.value,{componentStack:null!==c?c:""})});return c}function ka(){return(p&(ca|ma))!==H?1073741821-(Y()/10|0):0!==dd?dd:dd=1073741821-(Y()/10|0)}function Va(a,b,c){b=b.mode;if(0===(b&2))return 1073741823;var d=Cc();if(0===(b&4))return 99===d?1073741823:1073741822;if((p&ca)!==H)return P;if(null!==c)a=Fc(a,c.timeoutMs|0||5E3,250);else switch(d){case 99:a=1073741823;break;case 98:a=Fc(a,150,100);break;case 97:case 96:a=
Fc(a,5E3,250);break;case 95:a=2;break;default:throw Error(k(326));}null!==U&&a===P&&--a;return a}function ed(a,b){a.expirationTime<b&&(a.expirationTime=b);var c=a.alternate;null!==c&&c.expirationTime<b&&(c.expirationTime=b);var d=a.return,e=null;if(null===d&&3===a.tag)e=a.stateNode;else for(;null!==d;){c=d.alternate;d.childExpirationTime<b&&(d.childExpirationTime=b);null!==c&&c.childExpirationTime<b&&(c.childExpirationTime=b);if(null===d.return&&3===d.tag){e=d.stateNode;break}d=d.return}null!==e&&
(U===e&&(Kc(b),F===bd&&Ya(e,P)),yh(e,b));return e}function fd(a){var b=a.lastExpiredTime;if(0!==b)return b;b=a.firstPendingTime;if(!Kh(a,b))return b;var c=a.lastPingedTime;a=a.nextKnownPendingLevel;a=c>a?c:a;return 2>=a&&b!==a?0:a}function V(a){if(0!==a.lastExpiredTime)a.callbackExpirationTime=1073741823,a.callbackPriority=99,a.callbackNode=Og(Te.bind(null,a));else{var b=fd(a),c=a.callbackNode;if(0===b)null!==c&&(a.callbackNode=null,a.callbackExpirationTime=0,a.callbackPriority=90);else{var d=ka();
1073741823===b?d=99:1===b||2===b?d=95:(d=10*(1073741821-b)-10*(1073741821-d),d=0>=d?99:250>=d?98:5250>=d?97:95);if(null!==c){var e=a.callbackPriority;if(a.callbackExpirationTime===b&&e>=d)return;c!==Qg&&Rg(c)}a.callbackExpirationTime=b;a.callbackPriority=d;b=1073741823===b?Og(Te.bind(null,a)):Ng(d,Lh.bind(null,a),{timeout:10*(1073741821-b)-Y()});a.callbackNode=b}}}function Lh(a,b){dd=0;if(b)return b=ka(),Ue(a,b),V(a),null;var c=fd(a);if(0!==c){b=a.callbackNode;if((p&(ca|ma))!==H)throw Error(k(327));
xb();a===U&&c===P||$a(a,c);if(null!==t){var d=p;p|=ca;var e=Mh();do try{rj();break}catch(h){Nh(a,h)}while(1);le();p=d;gd.current=e;if(F===hd)throw b=id,$a(a,c),Ya(a,c),V(a),b;if(null===t)switch(e=a.finishedWork=a.current.alternate,a.finishedExpirationTime=c,d=F,U=null,d){case Xa:case hd:throw Error(k(345));case Oh:Ue(a,2<c?2:c);break;case ad:Ya(a,c);d=a.lastSuspendedTime;c===d&&(a.nextKnownPendingLevel=Ve(e));if(1073741823===ta&&(e=Re+Ph-Y(),10<e)){if(jd){var f=a.lastPingedTime;if(0===f||f>=c){a.lastPingedTime=
c;$a(a,c);break}}f=fd(a);if(0!==f&&f!==c)break;if(0!==d&&d!==c){a.lastPingedTime=d;break}a.timeoutHandle=We(ab.bind(null,a),e);break}ab(a);break;case bd:Ya(a,c);d=a.lastSuspendedTime;c===d&&(a.nextKnownPendingLevel=Ve(e));if(jd&&(e=a.lastPingedTime,0===e||e>=c)){a.lastPingedTime=c;$a(a,c);break}e=fd(a);if(0!==e&&e!==c)break;if(0!==d&&d!==c){a.lastPingedTime=d;break}1073741823!==Yb?d=10*(1073741821-Yb)-Y():1073741823===ta?d=0:(d=10*(1073741821-ta)-5E3,e=Y(),c=10*(1073741821-c)-e,d=e-d,0>d&&(d=0),d=
(120>d?120:480>d?480:1080>d?1080:1920>d?1920:3E3>d?3E3:4320>d?4320:1960*sj(d/1960))-d,c<d&&(d=c));if(10<d){a.timeoutHandle=We(ab.bind(null,a),d);break}ab(a);break;case Xe:if(1073741823!==ta&&null!==kd){f=ta;var g=kd;d=g.busyMinDurationMs|0;0>=d?d=0:(e=g.busyDelayMs|0,f=Y()-(10*(1073741821-f)-(g.timeoutMs|0||5E3)),d=f<=e?0:e+d-f);if(10<d){Ya(a,c);a.timeoutHandle=We(ab.bind(null,a),d);break}}ab(a);break;default:throw Error(k(329));}V(a);if(a.callbackNode===b)return Lh.bind(null,a)}}return null}function Te(a){var b=
a.lastExpiredTime;b=0!==b?b:1073741823;if((p&(ca|ma))!==H)throw Error(k(327));xb();a===U&&b===P||$a(a,b);if(null!==t){var c=p;p|=ca;var d=Mh();do try{tj();break}catch(e){Nh(a,e)}while(1);le();p=c;gd.current=d;if(F===hd)throw c=id,$a(a,b),Ya(a,b),V(a),c;if(null!==t)throw Error(k(261));a.finishedWork=a.current.alternate;a.finishedExpirationTime=b;U=null;ab(a);V(a)}return null}function uj(){if(null!==bb){var a=bb;bb=null;a.forEach(function(a,c){Ue(c,a);V(c)});ha()}}function Qh(a,b){var c=p;p|=1;try{return a(b)}finally{p=
c,p===H&&ha()}}function Rh(a,b){var c=p;p&=-2;p|=Ye;try{return a(b)}finally{p=c,p===H&&ha()}}function $a(a,b){a.finishedWork=null;a.finishedExpirationTime=0;var c=a.timeoutHandle;-1!==c&&(a.timeoutHandle=-1,vj(c));if(null!==t)for(c=t.return;null!==c;){var d=c;switch(d.tag){case 1:d=d.type.childContextTypes;null!==d&&void 0!==d&&(q(G),q(B));break;case 3:tb();q(G);q(B);break;case 5:te(d);break;case 4:tb();break;case 13:q(D);break;case 19:q(D);break;case 10:me(d)}c=c.return}U=a;t=Sa(a.current,null);
P=b;F=Xa;id=null;Yb=ta=1073741823;kd=null;Xb=0;jd=!1}function Nh(a,b){do{try{le();Sc.current=Tc;if(Uc)for(var c=z.memoizedState;null!==c;){var d=c.queue;null!==d&&(d.pending=null);c=c.next}Ia=0;J=K=z=null;Uc=!1;if(null===t||null===t.return)return F=hd,id=b,t=null;a:{var e=a,f=t.return,g=t,h=b;b=P;g.effectTag|=2048;g.firstEffect=g.lastEffect=null;if(null!==h&&"object"===typeof h&&"function"===typeof h.then){var m=h;if(0===(g.mode&2)){var n=g.alternate;n?(g.updateQueue=n.updateQueue,g.memoizedState=
n.memoizedState,g.expirationTime=n.expirationTime):(g.updateQueue=null,g.memoizedState=null)}var l=0!==(D.current&1),k=f;do{var p;if(p=13===k.tag){var q=k.memoizedState;if(null!==q)p=null!==q.dehydrated?!0:!1;else{var w=k.memoizedProps;p=void 0===w.fallback?!1:!0!==w.unstable_avoidThisFallback?!0:l?!1:!0}}if(p){var y=k.updateQueue;if(null===y){var r=new Set;r.add(m);k.updateQueue=r}else y.add(m);if(0===(k.mode&2)){k.effectTag|=64;g.effectTag&=-2981;if(1===g.tag)if(null===g.alternate)g.tag=17;else{var O=
Ea(1073741823,null);O.tag=Jc;Fa(g,O)}g.expirationTime=1073741823;break a}h=void 0;g=b;var v=e.pingCache;null===v?(v=e.pingCache=new wj,h=new Set,v.set(m,h)):(h=v.get(m),void 0===h&&(h=new Set,v.set(m,h)));if(!h.has(g)){h.add(g);var x=xj.bind(null,e,m,g);m.then(x,x)}k.effectTag|=4096;k.expirationTime=b;break a}k=k.return}while(null!==k);h=Error((na(g.type)||"A React component")+" suspended while rendering, but no fallback UI was specified.\n\nAdd a <Suspense fallback=...> component higher in the tree to provide a loading indicator or placeholder to display."+
Bd(g))}F!==Xe&&(F=Oh);h=Le(h,g);k=f;do{switch(k.tag){case 3:m=h;k.effectTag|=4096;k.expirationTime=b;var A=Ih(k,m,b);Ug(k,A);break a;case 1:m=h;var u=k.type,B=k.stateNode;if(0===(k.effectTag&64)&&("function"===typeof u.getDerivedStateFromError||null!==B&&"function"===typeof B.componentDidCatch&&(null===La||!La.has(B)))){k.effectTag|=4096;k.expirationTime=b;var H=Jh(k,m,b);Ug(k,H);break a}}k=k.return}while(null!==k)}t=Sh(t)}catch(cj){b=cj;continue}break}while(1)}function Mh(a){a=gd.current;gd.current=
Tc;return null===a?Tc:a}function Vg(a,b){a<ta&&2<a&&(ta=a);null!==b&&a<Yb&&2<a&&(Yb=a,kd=b)}function Kc(a){a>Xb&&(Xb=a)}function tj(){for(;null!==t;)t=Th(t)}function rj(){for(;null!==t&&!yj();)t=Th(t)}function Th(a){var b=zj(a.alternate,a,P);a.memoizedProps=a.pendingProps;null===b&&(b=Sh(a));Uh.current=null;return b}function Sh(a){t=a;do{var b=t.alternate;a=t.return;if(0===(t.effectTag&2048)){b=hj(b,t,P);if(1===P||1!==t.childExpirationTime){for(var c=0,d=t.child;null!==d;){var e=d.expirationTime,
f=d.childExpirationTime;e>c&&(c=e);f>c&&(c=f);d=d.sibling}t.childExpirationTime=c}if(null!==b)return b;null!==a&&0===(a.effectTag&2048)&&(null===a.firstEffect&&(a.firstEffect=t.firstEffect),null!==t.lastEffect&&(null!==a.lastEffect&&(a.lastEffect.nextEffect=t.firstEffect),a.lastEffect=t.lastEffect),1<t.effectTag&&(null!==a.lastEffect?a.lastEffect.nextEffect=t:a.firstEffect=t,a.lastEffect=t))}else{b=lj(t);if(null!==b)return b.effectTag&=2047,b;null!==a&&(a.firstEffect=a.lastEffect=null,a.effectTag|=
2048)}b=t.sibling;if(null!==b)return b;t=a}while(null!==t);F===Xa&&(F=Xe);return null}function Ve(a){var b=a.expirationTime;a=a.childExpirationTime;return b>a?b:a}function ab(a){var b=Cc();Da(99,Aj.bind(null,a,b));return null}function Aj(a,b){do xb();while(null!==Zb);if((p&(ca|ma))!==H)throw Error(k(327));var c=a.finishedWork,d=a.finishedExpirationTime;if(null===c)return null;a.finishedWork=null;a.finishedExpirationTime=0;if(c===a.current)throw Error(k(177));a.callbackNode=null;a.callbackExpirationTime=
0;a.callbackPriority=90;a.nextKnownPendingLevel=0;var e=Ve(c);a.firstPendingTime=e;d<=a.lastSuspendedTime?a.firstSuspendedTime=a.lastSuspendedTime=a.nextKnownPendingLevel=0:d<=a.firstSuspendedTime&&(a.firstSuspendedTime=d-1);d<=a.lastPingedTime&&(a.lastPingedTime=0);d<=a.lastExpiredTime&&(a.lastExpiredTime=0);a===U&&(t=U=null,P=0);1<c.effectTag?null!==c.lastEffect?(c.lastEffect.nextEffect=c,e=c.firstEffect):e=c:e=c.firstEffect;if(null!==e){var f=p;p|=ma;Uh.current=null;Ze=tc;var g=kg();if(Xd(g)){if("selectionStart"in
g)var h={start:g.selectionStart,end:g.selectionEnd};else a:{h=(h=g.ownerDocument)&&h.defaultView||window;var m=h.getSelection&&h.getSelection();if(m&&0!==m.rangeCount){h=m.anchorNode;var n=m.anchorOffset,q=m.focusNode;m=m.focusOffset;try{h.nodeType,q.nodeType}catch(sb){h=null;break a}var ba=0,w=-1,y=-1,B=0,D=0,r=g,z=null;b:for(;;){for(var v;;){r!==h||0!==n&&3!==r.nodeType||(w=ba+n);r!==q||0!==m&&3!==r.nodeType||(y=ba+m);3===r.nodeType&&(ba+=r.nodeValue.length);if(null===(v=r.firstChild))break;z=r;
r=v}for(;;){if(r===g)break b;z===h&&++B===n&&(w=ba);z===q&&++D===m&&(y=ba);if(null!==(v=r.nextSibling))break;r=z;z=r.parentNode}r=v}h=-1===w||-1===y?null:{start:w,end:y}}else h=null}h=h||{start:0,end:0}}else h=null;$e={activeElementDetached:null,focusedElem:g,selectionRange:h};tc=!1;l=e;do try{Bj()}catch(sb){if(null===l)throw Error(k(330));Za(l,sb);l=l.nextEffect}while(null!==l);l=e;do try{for(g=a,h=b;null!==l;){var x=l.effectTag;x&16&&Wb(l.stateNode,"");if(x&128){var A=l.alternate;if(null!==A){var u=
A.ref;null!==u&&("function"===typeof u?u(null):u.current=null)}}switch(x&1038){case 2:Gh(l);l.effectTag&=-3;break;case 6:Gh(l);l.effectTag&=-3;Qe(l.alternate,l);break;case 1024:l.effectTag&=-1025;break;case 1028:l.effectTag&=-1025;Qe(l.alternate,l);break;case 4:Qe(l.alternate,l);break;case 8:n=l,Dh(g,n,h),Eh(n)}l=l.nextEffect}}catch(sb){if(null===l)throw Error(k(330));Za(l,sb);l=l.nextEffect}while(null!==l);u=$e;A=kg();x=u.focusedElem;h=u.selectionRange;if(A!==x&&x&&x.ownerDocument&&jg(x.ownerDocument.documentElement,
x)){null!==h&&Xd(x)&&(A=h.start,u=h.end,void 0===u&&(u=A),"selectionStart"in x?(x.selectionStart=A,x.selectionEnd=Math.min(u,x.value.length)):(u=(A=x.ownerDocument||document)&&A.defaultView||window,u.getSelection&&(u=u.getSelection(),n=x.textContent.length,g=Math.min(h.start,n),h=void 0===h.end?g:Math.min(h.end,n),!u.extend&&g>h&&(n=h,h=g,g=n),n=ig(x,g),q=ig(x,h),n&&q&&(1!==u.rangeCount||u.anchorNode!==n.node||u.anchorOffset!==n.offset||u.focusNode!==q.node||u.focusOffset!==q.offset)&&(A=A.createRange(),
A.setStart(n.node,n.offset),u.removeAllRanges(),g>h?(u.addRange(A),u.extend(q.node,q.offset)):(A.setEnd(q.node,q.offset),u.addRange(A))))));A=[];for(u=x;u=u.parentNode;)1===u.nodeType&&A.push({element:u,left:u.scrollLeft,top:u.scrollTop});"function"===typeof x.focus&&x.focus();for(x=0;x<A.length;x++)u=A[x],u.element.scrollLeft=u.left,u.element.scrollTop=u.top}tc=!!Ze;$e=Ze=null;a.current=c;l=e;do try{for(x=a;null!==l;){var F=l.effectTag;F&36&&oj(x,l.alternate,l);if(F&128){A=void 0;var E=l.ref;if(null!==
E){var G=l.stateNode;switch(l.tag){case 5:A=G;break;default:A=G}"function"===typeof E?E(A):E.current=A}}l=l.nextEffect}}catch(sb){if(null===l)throw Error(k(330));Za(l,sb);l=l.nextEffect}while(null!==l);l=null;Cj();p=f}else a.current=c;if(ld)ld=!1,Zb=a,$b=b;else for(l=e;null!==l;)b=l.nextEffect,l.nextEffect=null,l=b;b=a.firstPendingTime;0===b&&(La=null);1073741823===b?a===af?ac++:(ac=0,af=a):ac=0;"function"===typeof bf&&bf(c.stateNode,d);V(a);if(cd)throw cd=!1,a=Se,Se=null,a;if((p&Ye)!==H)return null;
ha();return null}function Bj(){for(;null!==l;){var a=l.effectTag;0!==(a&256)&&nj(l.alternate,l);0===(a&512)||ld||(ld=!0,Ng(97,function(){xb();return null}));l=l.nextEffect}}function xb(){if(90!==$b){var a=97<$b?97:$b;$b=90;return Da(a,Dj)}}function Dj(){if(null===Zb)return!1;var a=Zb;Zb=null;if((p&(ca|ma))!==H)throw Error(k(331));var b=p;p|=ma;for(a=a.current.firstEffect;null!==a;){try{var c=a;if(0!==(c.effectTag&512))switch(c.tag){case 0:case 11:case 15:case 22:Ah(5,c),Bh(5,c)}}catch(d){if(null===
a)throw Error(k(330));Za(a,d)}c=a.nextEffect;a.nextEffect=null;a=c}p=b;ha();return!0}function Vh(a,b,c){b=Le(c,b);b=Ih(a,b,1073741823);Fa(a,b);a=ed(a,1073741823);null!==a&&V(a)}function Za(a,b){if(3===a.tag)Vh(a,a,b);else for(var c=a.return;null!==c;){if(3===c.tag){Vh(c,a,b);break}else if(1===c.tag){var d=c.stateNode;if("function"===typeof c.type.getDerivedStateFromError||"function"===typeof d.componentDidCatch&&(null===La||!La.has(d))){a=Le(b,a);a=Jh(c,a,1073741823);Fa(c,a);c=ed(c,1073741823);null!==
c&&V(c);break}}c=c.return}}function xj(a,b,c){var d=a.pingCache;null!==d&&d.delete(b);U===a&&P===c?F===bd||F===ad&&1073741823===ta&&Y()-Re<Ph?$a(a,P):jd=!0:Kh(a,c)&&(b=a.lastPingedTime,0!==b&&b<c||(a.lastPingedTime=c,V(a)))}function qj(a,b){var c=a.stateNode;null!==c&&c.delete(b);b=0;0===b&&(b=ka(),b=Va(b,a,null));a=ed(a,b);null!==a&&V(a)}function Ej(a){if("undefined"===typeof __REACT_DEVTOOLS_GLOBAL_HOOK__)return!1;var b=__REACT_DEVTOOLS_GLOBAL_HOOK__;if(b.isDisabled||!b.supportsFiber)return!0;try{var c=
b.inject(a);bf=function(a,e){try{b.onCommitFiberRoot(c,a,void 0,64===(a.current.effectTag&64))}catch(f){}};Ne=function(a){try{b.onCommitFiberUnmount(c,a)}catch(e){}}}catch(d){}return!0}function Fj(a,b,c,d){this.tag=a;this.key=c;this.sibling=this.child=this.return=this.stateNode=this.type=this.elementType=null;this.index=0;this.ref=null;this.pendingProps=b;this.dependencies=this.memoizedState=this.updateQueue=this.memoizedProps=null;this.mode=d;this.effectTag=0;this.lastEffect=this.firstEffect=this.nextEffect=
null;this.childExpirationTime=this.expirationTime=0;this.alternate=null}function Ge(a){a=a.prototype;return!(!a||!a.isReactComponent)}function Gj(a){if("function"===typeof a)return Ge(a)?1:0;if(void 0!==a&&null!==a){a=a.$$typeof;if(a===zd)return 11;if(a===Ad)return 14}return 2}function Sa(a,b){var c=a.alternate;null===c?(c=la(a.tag,b,a.key,a.mode),c.elementType=a.elementType,c.type=a.type,c.stateNode=a.stateNode,c.alternate=a,a.alternate=c):(c.pendingProps=b,c.effectTag=0,c.nextEffect=null,c.firstEffect=
null,c.lastEffect=null);c.childExpirationTime=a.childExpirationTime;c.expirationTime=a.expirationTime;c.child=a.child;c.memoizedProps=a.memoizedProps;c.memoizedState=a.memoizedState;c.updateQueue=a.updateQueue;b=a.dependencies;c.dependencies=null===b?null:{expirationTime:b.expirationTime,firstContext:b.firstContext,responders:b.responders};c.sibling=a.sibling;c.index=a.index;c.ref=a.ref;return c}function Oc(a,b,c,d,e,f){var g=2;d=a;if("function"===typeof a)Ge(a)&&(g=1);else if("string"===typeof a)g=
5;else a:switch(a){case Ma:return Ha(c.children,e,f,b);case Hj:g=8;e|=7;break;case Af:g=8;e|=1;break;case kc:return a=la(12,c,b,e|8),a.elementType=kc,a.type=kc,a.expirationTime=f,a;case lc:return a=la(13,c,b,e),a.type=lc,a.elementType=lc,a.expirationTime=f,a;case yd:return a=la(19,c,b,e),a.elementType=yd,a.expirationTime=f,a;default:if("object"===typeof a&&null!==a)switch(a.$$typeof){case Cf:g=10;break a;case Bf:g=9;break a;case zd:g=11;break a;case Ad:g=14;break a;case Ef:g=16;d=null;break a;case Df:g=
22;break a}throw Error(k(130,null==a?a:typeof a,""));}b=la(g,c,b,e);b.elementType=a;b.type=d;b.expirationTime=f;return b}function Ha(a,b,c,d){a=la(7,a,d,b);a.expirationTime=c;return a}function qe(a,b,c){a=la(6,a,null,b);a.expirationTime=c;return a}function re(a,b,c){b=la(4,null!==a.children?a.children:[],a.key,b);b.expirationTime=c;b.stateNode={containerInfo:a.containerInfo,pendingChildren:null,implementation:a.implementation};return b}function Ij(a,b,c){this.tag=b;this.current=null;this.containerInfo=
a;this.pingCache=this.pendingChildren=null;this.finishedExpirationTime=0;this.finishedWork=null;this.timeoutHandle=-1;this.pendingContext=this.context=null;this.hydrate=c;this.callbackNode=null;this.callbackPriority=90;this.lastExpiredTime=this.lastPingedTime=this.nextKnownPendingLevel=this.lastSuspendedTime=this.firstSuspendedTime=this.firstPendingTime=0}function Kh(a,b){var c=a.firstSuspendedTime;a=a.lastSuspendedTime;return 0!==c&&c>=b&&a<=b}function Ya(a,b){var c=a.firstSuspendedTime,d=a.lastSuspendedTime;
c<b&&(a.firstSuspendedTime=b);if(d>b||0===c)a.lastSuspendedTime=b;b<=a.lastPingedTime&&(a.lastPingedTime=0);b<=a.lastExpiredTime&&(a.lastExpiredTime=0)}function yh(a,b){b>a.firstPendingTime&&(a.firstPendingTime=b);var c=a.firstSuspendedTime;0!==c&&(b>=c?a.firstSuspendedTime=a.lastSuspendedTime=a.nextKnownPendingLevel=0:b>=a.lastSuspendedTime&&(a.lastSuspendedTime=b+1),b>a.nextKnownPendingLevel&&(a.nextKnownPendingLevel=b))}function Ue(a,b){var c=a.lastExpiredTime;if(0===c||c>b)a.lastExpiredTime=b}
function md(a,b,c,d){var e=b.current,f=ka(),g=Vb.suspense;f=Va(f,e,g);a:if(c){c=c._reactInternalFiber;b:{if(Na(c)!==c||1!==c.tag)throw Error(k(170));var h=c;do{switch(h.tag){case 3:h=h.stateNode.context;break b;case 1:if(N(h.type)){h=h.stateNode.__reactInternalMemoizedMergedChildContext;break b}}h=h.return}while(null!==h);throw Error(k(171));}if(1===c.tag){var m=c.type;if(N(m)){c=Gg(c,m,h);break a}}c=h}else c=Ca;null===b.context?b.context=c:b.pendingContext=c;b=Ea(f,g);b.payload={element:a};d=void 0===
d?null:d;null!==d&&(b.callback=d);Fa(e,b);Ja(e,f);return f}function cf(a){a=a.current;if(!a.child)return null;switch(a.child.tag){case 5:return a.child.stateNode;default:return a.child.stateNode}}function Wh(a,b){a=a.memoizedState;null!==a&&null!==a.dehydrated&&a.retryTime<b&&(a.retryTime=b)}function df(a,b){Wh(a,b);(a=a.alternate)&&Wh(a,b)}function ef(a,b,c){c=null!=c&&!0===c.hydrate;var d=new Ij(a,b,c),e=la(3,null,null,2===b?7:1===b?3:0);d.current=e;e.stateNode=d;ne(e);a[Lb]=d.current;c&&0!==b&&
xi(a,9===a.nodeType?a:a.ownerDocument);this._internalRoot=d}function bc(a){return!(!a||1!==a.nodeType&&9!==a.nodeType&&11!==a.nodeType&&(8!==a.nodeType||" react-mount-point-unstable "!==a.nodeValue))}function Jj(a,b){b||(b=a?9===a.nodeType?a.documentElement:a.firstChild:null,b=!(!b||1!==b.nodeType||!b.hasAttribute("data-reactroot")));if(!b)for(var c;c=a.lastChild;)a.removeChild(c);return new ef(a,0,b?{hydrate:!0}:void 0)}function nd(a,b,c,d,e){var f=c._reactRootContainer;if(f){var g=f._internalRoot;
if("function"===typeof e){var h=e;e=function(){var a=cf(g);h.call(a)}}md(b,g,a,e)}else{f=c._reactRootContainer=Jj(c,d);g=f._internalRoot;if("function"===typeof e){var m=e;e=function(){var a=cf(g);m.call(a)}}Rh(function(){md(b,g,a,e)})}return cf(g)}function Kj(a,b,c){var d=3<arguments.length&&void 0!==arguments[3]?arguments[3]:null;return{$$typeof:gb,key:null==d?null:""+d,children:a,containerInfo:b,implementation:c}}function Xh(a,b){var c=2<arguments.length&&void 0!==arguments[2]?arguments[2]:null;
if(!bc(b))throw Error(k(200));return Kj(a,b,null,c)}if(!ea)throw Error(k(227));var ki=function(a,b,c,d,e,f,g,h,m){var n=Array.prototype.slice.call(arguments,3);try{b.apply(c,n)}catch(C){this.onError(C)}},yb=!1,gc=null,hc=!1,pd=null,li={onError:function(a){yb=!0;gc=a}},td=null,rf=null,mf=null,ic=null,cb={},jc=[],qd={},db={},rd={},wa=!("undefined"===typeof window||"undefined"===typeof window.document||"undefined"===typeof window.document.createElement),M=ea.__SECRET_INTERNALS_DO_NOT_USE_OR_YOU_WILL_BE_FIRED.assign,
sd=null,eb=null,fb=null,ee=function(a,b){return a(b)},eg=function(a,b,c,d,e){return a(b,c,d,e)},vd=function(){},vf=ee,Oa=!1,wd=!1,Z=ea.__SECRET_INTERNALS_DO_NOT_USE_OR_YOU_WILL_BE_FIRED.Scheduler,Lj=Z.unstable_cancelCallback,ff=Z.unstable_now,$f=Z.unstable_scheduleCallback,Mj=Z.unstable_shouldYield,Yh=Z.unstable_requestPaint,Pd=Z.unstable_runWithPriority,Nj=Z.unstable_getCurrentPriorityLevel,Oj=Z.unstable_ImmediatePriority,Zh=Z.unstable_UserBlockingPriority,ag=Z.unstable_NormalPriority,Pj=Z.unstable_LowPriority,
Qj=Z.unstable_IdlePriority,oi=/^[:A-Z_a-z\u00C0-\u00D6\u00D8-\u00F6\u00F8-\u02FF\u0370-\u037D\u037F-\u1FFF\u200C-\u200D\u2070-\u218F\u2C00-\u2FEF\u3001-\uD7FF\uF900-\uFDCF\uFDF0-\uFFFD][:A-Z_a-z\u00C0-\u00D6\u00D8-\u00F6\u00F8-\u02FF\u0370-\u037D\u037F-\u1FFF\u200C-\u200D\u2070-\u218F\u2C00-\u2FEF\u3001-\uD7FF\uF900-\uFDCF\uFDF0-\uFFFD\-.0-9\u00B7\u0300-\u036F\u203F-\u2040]*$/,wf=Object.prototype.hasOwnProperty,yf={},xf={},E={};"children dangerouslySetInnerHTML defaultValue defaultChecked innerHTML suppressContentEditableWarning suppressHydrationWarning style".split(" ").forEach(function(a){E[a]=
new L(a,0,!1,a,null,!1)});[["acceptCharset","accept-charset"],["className","class"],["htmlFor","for"],["httpEquiv","http-equiv"]].forEach(function(a){var b=a[0];E[b]=new L(b,1,!1,a[1],null,!1)});["contentEditable","draggable","spellCheck","value"].forEach(function(a){E[a]=new L(a,2,!1,a.toLowerCase(),null,!1)});["autoReverse","externalResourcesRequired","focusable","preserveAlpha"].forEach(function(a){E[a]=new L(a,2,!1,a,null,!1)});"allowFullScreen async autoFocus autoPlay controls default defer disabled disablePictureInPicture formNoValidate hidden loop noModule noValidate open playsInline readOnly required reversed scoped seamless itemScope".split(" ").forEach(function(a){E[a]=
new L(a,3,!1,a.toLowerCase(),null,!1)});["checked","multiple","muted","selected"].forEach(function(a){E[a]=new L(a,3,!0,a,null,!1)});["capture","download"].forEach(function(a){E[a]=new L(a,4,!1,a,null,!1)});["cols","rows","size","span"].forEach(function(a){E[a]=new L(a,6,!1,a,null,!1)});["rowSpan","start"].forEach(function(a){E[a]=new L(a,5,!1,a.toLowerCase(),null,!1)});var gf=/[\-:]([a-z])/g,hf=function(a){return a[1].toUpperCase()};"accent-height alignment-baseline arabic-form baseline-shift cap-height clip-path clip-rule color-interpolation color-interpolation-filters color-profile color-rendering dominant-baseline enable-background fill-opacity fill-rule flood-color flood-opacity font-family font-size font-size-adjust font-stretch font-style font-variant font-weight glyph-name glyph-orientation-horizontal glyph-orientation-vertical horiz-adv-x horiz-origin-x image-rendering letter-spacing lighting-color marker-end marker-mid marker-start overline-position overline-thickness paint-order panose-1 pointer-events rendering-intent shape-rendering stop-color stop-opacity strikethrough-position strikethrough-thickness stroke-dasharray stroke-dashoffset stroke-linecap stroke-linejoin stroke-miterlimit stroke-opacity stroke-width text-anchor text-decoration text-rendering underline-position underline-thickness unicode-bidi unicode-range units-per-em v-alphabetic v-hanging v-ideographic v-mathematical vector-effect vert-adv-y vert-origin-x vert-origin-y word-spacing writing-mode xmlns:xlink x-height".split(" ").forEach(function(a){var b=
a.replace(gf,hf);E[b]=new L(b,1,!1,a,null,!1)});"xlink:actuate xlink:arcrole xlink:role xlink:show xlink:title xlink:type".split(" ").forEach(function(a){var b=a.replace(gf,hf);E[b]=new L(b,1,!1,a,"http://www.w3.org/1999/xlink",!1)});["xml:base","xml:lang","xml:space"].forEach(function(a){var b=a.replace(gf,hf);E[b]=new L(b,1,!1,a,"http://www.w3.org/XML/1998/namespace",!1)});["tabIndex","crossOrigin"].forEach(function(a){E[a]=new L(a,1,!1,a.toLowerCase(),null,!1)});E.xlinkHref=new L("xlinkHref",1,
!1,"xlink:href","http://www.w3.org/1999/xlink",!0);["src","href","action","formAction"].forEach(function(a){E[a]=new L(a,1,!1,a.toLowerCase(),null,!0)});var da=ea.__SECRET_INTERNALS_DO_NOT_USE_OR_YOU_WILL_BE_FIRED;da.hasOwnProperty("ReactCurrentDispatcher")||(da.ReactCurrentDispatcher={current:null});da.hasOwnProperty("ReactCurrentBatchConfig")||(da.ReactCurrentBatchConfig={suspense:null});var si=/^(.*)[\\\/]/,Q="function"===typeof Symbol&&Symbol.for,Pc=Q?Symbol.for("react.element"):60103,gb=Q?Symbol.for("react.portal"):
60106,Ma=Q?Symbol.for("react.fragment"):60107,Af=Q?Symbol.for("react.strict_mode"):60108,kc=Q?Symbol.for("react.profiler"):60114,Cf=Q?Symbol.for("react.provider"):60109,Bf=Q?Symbol.for("react.context"):60110,Hj=Q?Symbol.for("react.concurrent_mode"):60111,zd=Q?Symbol.for("react.forward_ref"):60112,lc=Q?Symbol.for("react.suspense"):60113,yd=Q?Symbol.for("react.suspense_list"):60120,Ad=Q?Symbol.for("react.memo"):60115,Ef=Q?Symbol.for("react.lazy"):60116,Df=Q?Symbol.for("react.block"):60121,zf="function"===
typeof Symbol&&Symbol.iterator,od,xh=function(a){return"undefined"!==typeof MSApp&&MSApp.execUnsafeLocalFunction?function(b,c,d,e){MSApp.execUnsafeLocalFunction(function(){return a(b,c,d,e)})}:a}(function(a,b){if("http://www.w3.org/2000/svg"!==a.namespaceURI||"innerHTML"in a)a.innerHTML=b;else{od=od||document.createElement("div");od.innerHTML="<svg>"+b.valueOf().toString()+"</svg>";for(b=od.firstChild;a.firstChild;)a.removeChild(a.firstChild);for(;b.firstChild;)a.appendChild(b.firstChild)}}),Wb=function(a,
b){if(b){var c=a.firstChild;if(c&&c===a.lastChild&&3===c.nodeType){c.nodeValue=b;return}}a.textContent=b},ib={animationend:nc("Animation","AnimationEnd"),animationiteration:nc("Animation","AnimationIteration"),animationstart:nc("Animation","AnimationStart"),transitionend:nc("Transition","TransitionEnd")},Id={},Of={};wa&&(Of=document.createElement("div").style,"AnimationEvent"in window||(delete ib.animationend.animation,delete ib.animationiteration.animation,delete ib.animationstart.animation),"TransitionEvent"in
window||delete ib.transitionend.transition);var $h=oc("animationend"),ai=oc("animationiteration"),bi=oc("animationstart"),ci=oc("transitionend"),Db="abort canplay canplaythrough durationchange emptied encrypted ended error loadeddata loadedmetadata loadstart pause play playing progress ratechange seeked seeking stalled suspend timeupdate volumechange waiting".split(" "),Pf=new ("function"===typeof WeakMap?WeakMap:Map),Ab=null,wi=function(a){if(a){var b=a._dispatchListeners,c=a._dispatchInstances;
if(Array.isArray(b))for(var d=0;d<b.length&&!a.isPropagationStopped();d++)lf(a,b[d],c[d]);else b&&lf(a,b,c);a._dispatchListeners=null;a._dispatchInstances=null;a.isPersistent()||a.constructor.release(a)}},qc=[],Rd=!1,fa=[],xa=null,ya=null,za=null,Eb=new Map,Fb=new Map,Jb=[],Nd="mousedown mouseup touchcancel touchend touchstart auxclick dblclick pointercancel pointerdown pointerup dragend dragstart drop compositionend compositionstart keydown keypress keyup input textInput close cancel copy cut paste click change contextmenu reset submit".split(" "),
yi="focus blur dragenter dragleave mouseover mouseout pointerover pointerout gotpointercapture lostpointercapture".split(" "),dg={},cg=new Map,Td=new Map,Rj=["abort","abort",$h,"animationEnd",ai,"animationIteration",bi,"animationStart","canplay","canPlay","canplaythrough","canPlayThrough","durationchange","durationChange","emptied","emptied","encrypted","encrypted","ended","ended","error","error","gotpointercapture","gotPointerCapture","load","load","loadeddata","loadedData","loadedmetadata","loadedMetadata",
"loadstart","loadStart","lostpointercapture","lostPointerCapture","playing","playing","progress","progress","seeking","seeking","stalled","stalled","suspend","suspend","timeupdate","timeUpdate",ci,"transitionEnd","waiting","waiting"];Sd("blur blur cancel cancel click click close close contextmenu contextMenu copy copy cut cut auxclick auxClick dblclick doubleClick dragend dragEnd dragstart dragStart drop drop focus focus input input invalid invalid keydown keyDown keypress keyPress keyup keyUp mousedown mouseDown mouseup mouseUp paste paste pause pause play play pointercancel pointerCancel pointerdown pointerDown pointerup pointerUp ratechange rateChange reset reset seeked seeked submit submit touchcancel touchCancel touchend touchEnd touchstart touchStart volumechange volumeChange".split(" "),
0);Sd("drag drag dragenter dragEnter dragexit dragExit dragleave dragLeave dragover dragOver mousemove mouseMove mouseout mouseOut mouseover mouseOver pointermove pointerMove pointerout pointerOut pointerover pointerOver scroll scroll toggle toggle touchmove touchMove wheel wheel".split(" "),1);Sd(Rj,2);(function(a,b){for(var c=0;c<a.length;c++)Td.set(a[c],b)})("change selectionchange textInput compositionstart compositionend compositionupdate".split(" "),0);var Hi=Zh,Gi=Pd,tc=!0,Kb={animationIterationCount:!0,
borderImageOutset:!0,borderImageSlice:!0,borderImageWidth:!0,boxFlex:!0,boxFlexGroup:!0,boxOrdinalGroup:!0,columnCount:!0,columns:!0,flex:!0,flexGrow:!0,flexPositive:!0,flexShrink:!0,flexNegative:!0,flexOrder:!0,gridArea:!0,gridRow:!0,gridRowEnd:!0,gridRowSpan:!0,gridRowStart:!0,gridColumn:!0,gridColumnEnd:!0,gridColumnSpan:!0,gridColumnStart:!0,fontWeight:!0,lineClamp:!0,lineHeight:!0,opacity:!0,order:!0,orphans:!0,tabSize:!0,widows:!0,zIndex:!0,zoom:!0,fillOpacity:!0,floodOpacity:!0,stopOpacity:!0,
strokeDasharray:!0,strokeDashoffset:!0,strokeMiterlimit:!0,strokeOpacity:!0,strokeWidth:!0},Sj=["Webkit","ms","Moz","O"];Object.keys(Kb).forEach(function(a){Sj.forEach(function(b){b=b+a.charAt(0).toUpperCase()+a.substring(1);Kb[b]=Kb[a]})});var Ii=M({menuitem:!0},{area:!0,base:!0,br:!0,col:!0,embed:!0,hr:!0,img:!0,input:!0,keygen:!0,link:!0,meta:!0,param:!0,source:!0,track:!0,wbr:!0}),ng="$",og="/$",$d="$?",Zd="$!",Ze=null,$e=null,We="function"===typeof setTimeout?setTimeout:void 0,vj="function"===
typeof clearTimeout?clearTimeout:void 0,jf=Math.random().toString(36).slice(2),Aa="__reactInternalInstance$"+jf,vc="__reactEventHandlers$"+jf,Lb="__reactContainere$"+jf,Ba=null,ce=null,wc=null;M(R.prototype,{preventDefault:function(){this.defaultPrevented=!0;var a=this.nativeEvent;a&&(a.preventDefault?a.preventDefault():"unknown"!==typeof a.returnValue&&(a.returnValue=!1),this.isDefaultPrevented=xc)},stopPropagation:function(){var a=this.nativeEvent;a&&(a.stopPropagation?a.stopPropagation():"unknown"!==
typeof a.cancelBubble&&(a.cancelBubble=!0),this.isPropagationStopped=xc)},persist:function(){this.isPersistent=xc},isPersistent:yc,destructor:function(){var a=this.constructor.Interface,b;for(b in a)this[b]=null;this.nativeEvent=this._targetInst=this.dispatchConfig=null;this.isPropagationStopped=this.isDefaultPrevented=yc;this._dispatchInstances=this._dispatchListeners=null}});R.Interface={type:null,target:null,currentTarget:function(){return null},eventPhase:null,bubbles:null,cancelable:null,timeStamp:function(a){return a.timeStamp||
Date.now()},defaultPrevented:null,isTrusted:null};R.extend=function(a){function b(){return c.apply(this,arguments)}var c=this,d=function(){};d.prototype=c.prototype;d=new d;M(d,b.prototype);b.prototype=d;b.prototype.constructor=b;b.Interface=M({},c.Interface,a);b.extend=c.extend;sg(b);return b};sg(R);var Tj=R.extend({data:null}),Uj=R.extend({data:null}),Ni=[9,13,27,32],de=wa&&"CompositionEvent"in window,cc=null;wa&&"documentMode"in document&&(cc=document.documentMode);var Vj=wa&&"TextEvent"in window&&
!cc,xg=wa&&(!de||cc&&8<cc&&11>=cc),wg=String.fromCharCode(32),ua={beforeInput:{phasedRegistrationNames:{bubbled:"onBeforeInput",captured:"onBeforeInputCapture"},dependencies:["compositionend","keypress","textInput","paste"]},compositionEnd:{phasedRegistrationNames:{bubbled:"onCompositionEnd",captured:"onCompositionEndCapture"},dependencies:"blur compositionend keydown keypress keyup mousedown".split(" ")},compositionStart:{phasedRegistrationNames:{bubbled:"onCompositionStart",captured:"onCompositionStartCapture"},
dependencies:"blur compositionstart keydown keypress keyup mousedown".split(" ")},compositionUpdate:{phasedRegistrationNames:{bubbled:"onCompositionUpdate",captured:"onCompositionUpdateCapture"},dependencies:"blur compositionupdate keydown keypress keyup mousedown".split(" ")}},vg=!1,mb=!1,Wj={eventTypes:ua,extractEvents:function(a,b,c,d,e){var f;if(de)b:{switch(a){case "compositionstart":var g=ua.compositionStart;break b;case "compositionend":g=ua.compositionEnd;break b;case "compositionupdate":g=
ua.compositionUpdate;break b}g=void 0}else mb?tg(a,c)&&(g=ua.compositionEnd):"keydown"===a&&229===c.keyCode&&(g=ua.compositionStart);g?(xg&&"ko"!==c.locale&&(mb||g!==ua.compositionStart?g===ua.compositionEnd&&mb&&(f=rg()):(Ba=d,ce="value"in Ba?Ba.value:Ba.textContent,mb=!0)),e=Tj.getPooled(g,b,c,d),f?e.data=f:(f=ug(c),null!==f&&(e.data=f)),lb(e),f=e):f=null;(a=Vj?Oi(a,c):Pi(a,c))?(b=Uj.getPooled(ua.beforeInput,b,c,d),b.data=a,lb(b)):b=null;return null===f?b:null===b?f:[f,b]}},Qi={color:!0,date:!0,
datetime:!0,"datetime-local":!0,email:!0,month:!0,number:!0,password:!0,range:!0,search:!0,tel:!0,text:!0,time:!0,url:!0,week:!0},Ag={change:{phasedRegistrationNames:{bubbled:"onChange",captured:"onChangeCapture"},dependencies:"blur change click focus input keydown keyup selectionchange".split(" ")}},Mb=null,Nb=null,kf=!1;wa&&(kf=Tf("input")&&(!document.documentMode||9<document.documentMode));var Xj={eventTypes:Ag,_isInputEventSupported:kf,extractEvents:function(a,b,c,d,e){e=b?Pa(b):window;var f=
e.nodeName&&e.nodeName.toLowerCase();if("select"===f||"input"===f&&"file"===e.type)var g=Si;else if(yg(e))if(kf)g=Wi;else{g=Ui;var h=Ti}else(f=e.nodeName)&&"input"===f.toLowerCase()&&("checkbox"===e.type||"radio"===e.type)&&(g=Vi);if(g&&(g=g(a,b)))return zg(g,c,d);h&&h(a,e,b);"blur"===a&&(a=e._wrapperState)&&a.controlled&&"number"===e.type&&Ed(e,"number",e.value)}},dc=R.extend({view:null,detail:null}),Yi={Alt:"altKey",Control:"ctrlKey",Meta:"metaKey",Shift:"shiftKey"},di=0,ei=0,fi=!1,gi=!1,ec=dc.extend({screenX:null,
screenY:null,clientX:null,clientY:null,pageX:null,pageY:null,ctrlKey:null,shiftKey:null,altKey:null,metaKey:null,getModifierState:fe,button:null,buttons:null,relatedTarget:function(a){return a.relatedTarget||(a.fromElement===a.srcElement?a.toElement:a.fromElement)},movementX:function(a){if("movementX"in a)return a.movementX;var b=di;di=a.screenX;return fi?"mousemove"===a.type?a.screenX-b:0:(fi=!0,0)},movementY:function(a){if("movementY"in a)return a.movementY;var b=ei;ei=a.screenY;return gi?"mousemove"===
a.type?a.screenY-b:0:(gi=!0,0)}}),hi=ec.extend({pointerId:null,width:null,height:null,pressure:null,tangentialPressure:null,tiltX:null,tiltY:null,twist:null,pointerType:null,isPrimary:null}),fc={mouseEnter:{registrationName:"onMouseEnter",dependencies:["mouseout","mouseover"]},mouseLeave:{registrationName:"onMouseLeave",dependencies:["mouseout","mouseover"]},pointerEnter:{registrationName:"onPointerEnter",dependencies:["pointerout","pointerover"]},pointerLeave:{registrationName:"onPointerLeave",dependencies:["pointerout",
"pointerover"]}},Yj={eventTypes:fc,extractEvents:function(a,b,c,d,e){var f="mouseover"===a||"pointerover"===a,g="mouseout"===a||"pointerout"===a;if(f&&0===(e&32)&&(c.relatedTarget||c.fromElement)||!g&&!f)return null;f=d.window===d?d:(f=d.ownerDocument)?f.defaultView||f.parentWindow:window;if(g){if(g=b,b=(b=c.relatedTarget||c.toElement)?Bb(b):null,null!==b){var h=Na(b);if(b!==h||5!==b.tag&&6!==b.tag)b=null}}else g=null;if(g===b)return null;if("mouseout"===a||"mouseover"===a){var m=ec;var n=fc.mouseLeave;
var l=fc.mouseEnter;var k="mouse"}else if("pointerout"===a||"pointerover"===a)m=hi,n=fc.pointerLeave,l=fc.pointerEnter,k="pointer";a=null==g?f:Pa(g);f=null==b?f:Pa(b);n=m.getPooled(n,g,c,d);n.type=k+"leave";n.target=a;n.relatedTarget=f;c=m.getPooled(l,b,c,d);c.type=k+"enter";c.target=f;c.relatedTarget=a;d=g;k=b;if(d&&k)a:{m=d;l=k;g=0;for(a=m;a;a=pa(a))g++;a=0;for(b=l;b;b=pa(b))a++;for(;0<g-a;)m=pa(m),g--;for(;0<a-g;)l=pa(l),a--;for(;g--;){if(m===l||m===l.alternate)break a;m=pa(m);l=pa(l)}m=null}else m=
null;l=m;for(m=[];d&&d!==l;){g=d.alternate;if(null!==g&&g===l)break;m.push(d);d=pa(d)}for(d=[];k&&k!==l;){g=k.alternate;if(null!==g&&g===l)break;d.push(k);k=pa(k)}for(k=0;k<m.length;k++)be(m[k],"bubbled",n);for(k=d.length;0<k--;)be(d[k],"captured",c);return 0===(e&64)?[n]:[n,c]}},Qa="function"===typeof Object.is?Object.is:Zi,$i=Object.prototype.hasOwnProperty,Zj=wa&&"documentMode"in document&&11>=document.documentMode,Eg={select:{phasedRegistrationNames:{bubbled:"onSelect",captured:"onSelectCapture"},
dependencies:"blur contextmenu dragend focus keydown keyup mousedown mouseup selectionchange".split(" ")}},nb=null,he=null,Pb=null,ge=!1,ak={eventTypes:Eg,extractEvents:function(a,b,c,d,e,f){e=f||(d.window===d?d.document:9===d.nodeType?d:d.ownerDocument);if(!(f=!e)){a:{e=Jd(e);f=rd.onSelect;for(var g=0;g<f.length;g++)if(!e.has(f[g])){e=!1;break a}e=!0}f=!e}if(f)return null;e=b?Pa(b):window;switch(a){case "focus":if(yg(e)||"true"===e.contentEditable)nb=e,he=b,Pb=null;break;case "blur":Pb=he=nb=null;
break;case "mousedown":ge=!0;break;case "contextmenu":case "mouseup":case "dragend":return ge=!1,Dg(c,d);case "selectionchange":if(Zj)break;case "keydown":case "keyup":return Dg(c,d)}return null}},bk=R.extend({animationName:null,elapsedTime:null,pseudoElement:null}),ck=R.extend({clipboardData:function(a){return"clipboardData"in a?a.clipboardData:window.clipboardData}}),dk=dc.extend({relatedTarget:null}),ek={Esc:"Escape",Spacebar:" ",Left:"ArrowLeft",Up:"ArrowUp",Right:"ArrowRight",Down:"ArrowDown",
Del:"Delete",Win:"OS",Menu:"ContextMenu",Apps:"ContextMenu",Scroll:"ScrollLock",MozPrintableKey:"Unidentified"},fk={8:"Backspace",9:"Tab",12:"Clear",13:"Enter",16:"Shift",17:"Control",18:"Alt",19:"Pause",20:"CapsLock",27:"Escape",32:" ",33:"PageUp",34:"PageDown",35:"End",36:"Home",37:"ArrowLeft",38:"ArrowUp",39:"ArrowRight",40:"ArrowDown",45:"Insert",46:"Delete",112:"F1",113:"F2",114:"F3",115:"F4",116:"F5",117:"F6",118:"F7",119:"F8",120:"F9",121:"F10",122:"F11",123:"F12",144:"NumLock",145:"ScrollLock",
224:"Meta"},gk=dc.extend({key:function(a){if(a.key){var b=ek[a.key]||a.key;if("Unidentified"!==b)return b}return"keypress"===a.type?(a=Ac(a),13===a?"Enter":String.fromCharCode(a)):"keydown"===a.type||"keyup"===a.type?fk[a.keyCode]||"Unidentified":""},location:null,ctrlKey:null,shiftKey:null,altKey:null,metaKey:null,repeat:null,locale:null,getModifierState:fe,charCode:function(a){return"keypress"===a.type?Ac(a):0},keyCode:function(a){return"keydown"===a.type||"keyup"===a.type?a.keyCode:0},which:function(a){return"keypress"===
a.type?Ac(a):"keydown"===a.type||"keyup"===a.type?a.keyCode:0}}),hk=ec.extend({dataTransfer:null}),ik=dc.extend({touches:null,targetTouches:null,changedTouches:null,altKey:null,metaKey:null,ctrlKey:null,shiftKey:null,getModifierState:fe}),jk=R.extend({propertyName:null,elapsedTime:null,pseudoElement:null}),kk=ec.extend({deltaX:function(a){return"deltaX"in a?a.deltaX:"wheelDeltaX"in a?-a.wheelDeltaX:0},deltaY:function(a){return"deltaY"in a?a.deltaY:"wheelDeltaY"in a?-a.wheelDeltaY:"wheelDelta"in a?
-a.wheelDelta:0},deltaZ:null,deltaMode:null}),lk={eventTypes:dg,extractEvents:function(a,b,c,d,e){e=cg.get(a);if(!e)return null;switch(a){case "keypress":if(0===Ac(c))return null;case "keydown":case "keyup":a=gk;break;case "blur":case "focus":a=dk;break;case "click":if(2===c.button)return null;case "auxclick":case "dblclick":case "mousedown":case "mousemove":case "mouseup":case "mouseout":case "mouseover":case "contextmenu":a=ec;break;case "drag":case "dragend":case "dragenter":case "dragexit":case "dragleave":case "dragover":case "dragstart":case "drop":a=
hk;break;case "touchcancel":case "touchend":case "touchmove":case "touchstart":a=ik;break;case $h:case ai:case bi:a=bk;break;case ci:a=jk;break;case "scroll":a=dc;break;case "wheel":a=kk;break;case "copy":case "cut":case "paste":a=ck;break;case "gotpointercapture":case "lostpointercapture":case "pointercancel":case "pointerdown":case "pointermove":case "pointerout":case "pointerover":case "pointerup":a=hi;break;default:a=R}b=a.getPooled(e,b,c,d);lb(b);return b}};(function(a){if(ic)throw Error(k(101));
ic=Array.prototype.slice.call(a);nf()})("ResponderEventPlugin SimpleEventPlugin EnterLeaveEventPlugin ChangeEventPlugin SelectEventPlugin BeforeInputEventPlugin".split(" "));(function(a,b,c){td=a;rf=b;mf=c})(ae,Hb,Pa);pf({SimpleEventPlugin:lk,EnterLeaveEventPlugin:Yj,ChangeEventPlugin:Xj,SelectEventPlugin:ak,BeforeInputEventPlugin:Wj});var ie=[],ob=-1,Ca={},B={current:Ca},G={current:!1},Ra=Ca,bj=Pd,je=$f,Rg=Lj,aj=Nj,Dc=Oj,Ig=Zh,Jg=ag,Kg=Pj,Lg=Qj,Qg={},yj=Mj,Cj=void 0!==Yh?Yh:function(){},qa=null,
Ec=null,ke=!1,ii=ff(),Y=1E4>ii?ff:function(){return ff()-ii},Ic={current:null},Hc=null,qb=null,Gc=null,Tg=0,Jc=2,Ga=!1,Vb=da.ReactCurrentBatchConfig,$g=(new ea.Component).refs,Mc={isMounted:function(a){return(a=a._reactInternalFiber)?Na(a)===a:!1},enqueueSetState:function(a,b,c){a=a._reactInternalFiber;var d=ka(),e=Vb.suspense;d=Va(d,a,e);e=Ea(d,e);e.payload=b;void 0!==c&&null!==c&&(e.callback=c);Fa(a,e);Ja(a,d)},enqueueReplaceState:function(a,b,c){a=a._reactInternalFiber;var d=ka(),e=Vb.suspense;
d=Va(d,a,e);e=Ea(d,e);e.tag=1;e.payload=b;void 0!==c&&null!==c&&(e.callback=c);Fa(a,e);Ja(a,d)},enqueueForceUpdate:function(a,b){a=a._reactInternalFiber;var c=ka(),d=Vb.suspense;c=Va(c,a,d);d=Ea(c,d);d.tag=Jc;void 0!==b&&null!==b&&(d.callback=b);Fa(a,d);Ja(a,c)}},Qc=Array.isArray,wb=ah(!0),Fe=ah(!1),Sb={},ja={current:Sb},Ub={current:Sb},Tb={current:Sb},D={current:0},Sc=da.ReactCurrentDispatcher,X=da.ReactCurrentBatchConfig,Ia=0,z=null,K=null,J=null,Uc=!1,Tc={readContext:W,useCallback:S,useContext:S,
useEffect:S,useImperativeHandle:S,useLayoutEffect:S,useMemo:S,useReducer:S,useRef:S,useState:S,useDebugValue:S,useResponder:S,useDeferredValue:S,useTransition:S},dj={readContext:W,useCallback:ih,useContext:W,useEffect:eh,useImperativeHandle:function(a,b,c){c=null!==c&&void 0!==c?c.concat([a]):null;return ze(4,2,gh.bind(null,b,a),c)},useLayoutEffect:function(a,b){return ze(4,2,a,b)},useMemo:function(a,b){var c=ub();b=void 0===b?null:b;a=a();c.memoizedState=[a,b];return a},useReducer:function(a,b,c){var d=
ub();b=void 0!==c?c(b):b;d.memoizedState=d.baseState=b;a=d.queue={pending:null,dispatch:null,lastRenderedReducer:a,lastRenderedState:b};a=a.dispatch=ch.bind(null,z,a);return[d.memoizedState,a]},useRef:function(a){var b=ub();a={current:a};return b.memoizedState=a},useState:xe,useDebugValue:Be,useResponder:ue,useDeferredValue:function(a,b){var c=xe(a),d=c[0],e=c[1];eh(function(){var c=X.suspense;X.suspense=void 0===b?null:b;try{e(a)}finally{X.suspense=c}},[a,b]);return d},useTransition:function(a){var b=
xe(!1),c=b[0];b=b[1];return[ih(Ce.bind(null,b,a),[b,a]),c]}},ej={readContext:W,useCallback:Yc,useContext:W,useEffect:Xc,useImperativeHandle:hh,useLayoutEffect:fh,useMemo:jh,useReducer:Vc,useRef:dh,useState:function(a){return Vc(Ua)},useDebugValue:Be,useResponder:ue,useDeferredValue:function(a,b){var c=Vc(Ua),d=c[0],e=c[1];Xc(function(){var c=X.suspense;X.suspense=void 0===b?null:b;try{e(a)}finally{X.suspense=c}},[a,b]);return d},useTransition:function(a){var b=Vc(Ua),c=b[0];b=b[1];return[Yc(Ce.bind(null,
b,a),[b,a]),c]}},fj={readContext:W,useCallback:Yc,useContext:W,useEffect:Xc,useImperativeHandle:hh,useLayoutEffect:fh,useMemo:jh,useReducer:Wc,useRef:dh,useState:function(a){return Wc(Ua)},useDebugValue:Be,useResponder:ue,useDeferredValue:function(a,b){var c=Wc(Ua),d=c[0],e=c[1];Xc(function(){var c=X.suspense;X.suspense=void 0===b?null:b;try{e(a)}finally{X.suspense=c}},[a,b]);return d},useTransition:function(a){var b=Wc(Ua),c=b[0];b=b[1];return[Yc(Ce.bind(null,b,a),[b,a]),c]}},ra=null,Ka=null,Wa=
!1,gj=da.ReactCurrentOwner,ia=!1,Je={dehydrated:null,retryTime:0};var jj=function(a,b,c,d){for(c=b.child;null!==c;){if(5===c.tag||6===c.tag)a.appendChild(c.stateNode);else if(4!==c.tag&&null!==c.child){c.child.return=c;c=c.child;continue}if(c===b)break;for(;null===c.sibling;){if(null===c.return||c.return===b)return;c=c.return}c.sibling.return=c.return;c=c.sibling}};var wh=function(a){};var ij=function(a,b,c,d,e){var f=a.memoizedProps;if(f!==d){var g=b.stateNode;Ta(ja.current);a=null;switch(c){case "input":f=
Cd(g,f);d=Cd(g,d);a=[];break;case "option":f=Fd(g,f);d=Fd(g,d);a=[];break;case "select":f=M({},f,{value:void 0});d=M({},d,{value:void 0});a=[];break;case "textarea":f=Gd(g,f);d=Gd(g,d);a=[];break;default:"function"!==typeof f.onClick&&"function"===typeof d.onClick&&(g.onclick=uc)}Ud(c,d);var h,m;c=null;for(h in f)if(!d.hasOwnProperty(h)&&f.hasOwnProperty(h)&&null!=f[h])if("style"===h)for(m in g=f[h],g)g.hasOwnProperty(m)&&(c||(c={}),c[m]="");else"dangerouslySetInnerHTML"!==h&&"children"!==h&&"suppressContentEditableWarning"!==
h&&"suppressHydrationWarning"!==h&&"autoFocus"!==h&&(db.hasOwnProperty(h)?a||(a=[]):(a=a||[]).push(h,null));for(h in d){var k=d[h];g=null!=f?f[h]:void 0;if(d.hasOwnProperty(h)&&k!==g&&(null!=k||null!=g))if("style"===h)if(g){for(m in g)!g.hasOwnProperty(m)||k&&k.hasOwnProperty(m)||(c||(c={}),c[m]="");for(m in k)k.hasOwnProperty(m)&&g[m]!==k[m]&&(c||(c={}),c[m]=k[m])}else c||(a||(a=[]),a.push(h,c)),c=k;else"dangerouslySetInnerHTML"===h?(k=k?k.__html:void 0,g=g?g.__html:void 0,null!=k&&g!==k&&(a=a||
[]).push(h,k)):"children"===h?g===k||"string"!==typeof k&&"number"!==typeof k||(a=a||[]).push(h,""+k):"suppressContentEditableWarning"!==h&&"suppressHydrationWarning"!==h&&(db.hasOwnProperty(h)?(null!=k&&oa(e,h),a||g===k||(a=[])):(a=a||[]).push(h,k))}c&&(a=a||[]).push("style",c);e=a;if(b.updateQueue=e)b.effectTag|=4}};var kj=function(a,b,c,d){c!==d&&(b.effectTag|=4)};var pj="function"===typeof WeakSet?WeakSet:Set,wj="function"===typeof WeakMap?WeakMap:Map,sj=Math.ceil,gd=da.ReactCurrentDispatcher,
Uh=da.ReactCurrentOwner,H=0,Ye=8,ca=16,ma=32,Xa=0,hd=1,Oh=2,ad=3,bd=4,Xe=5,p=H,U=null,t=null,P=0,F=Xa,id=null,ta=1073741823,Yb=1073741823,kd=null,Xb=0,jd=!1,Re=0,Ph=500,l=null,cd=!1,Se=null,La=null,ld=!1,Zb=null,$b=90,bb=null,ac=0,af=null,dd=0,Ja=function(a,b){if(50<ac)throw ac=0,af=null,Error(k(185));a=ed(a,b);if(null!==a){var c=Cc();1073741823===b?(p&Ye)!==H&&(p&(ca|ma))===H?Te(a):(V(a),p===H&&ha()):V(a);(p&4)===H||98!==c&&99!==c||(null===bb?bb=new Map([[a,b]]):(c=bb.get(a),(void 0===c||c>b)&&bb.set(a,
b)))}};var zj=function(a,b,c){var d=b.expirationTime;if(null!==a){var e=b.pendingProps;if(a.memoizedProps!==e||G.current)ia=!0;else{if(d<c){ia=!1;switch(b.tag){case 3:sh(b);Ee();break;case 5:bh(b);if(b.mode&4&&1!==c&&e.hidden)return b.expirationTime=b.childExpirationTime=1,null;break;case 1:N(b.type)&&Bc(b);break;case 4:se(b,b.stateNode.containerInfo);break;case 10:d=b.memoizedProps.value;e=b.type._context;y(Ic,e._currentValue);e._currentValue=d;break;case 13:if(null!==b.memoizedState){d=b.child.childExpirationTime;
if(0!==d&&d>=c)return th(a,b,c);y(D,D.current&1);b=sa(a,b,c);return null!==b?b.sibling:null}y(D,D.current&1);break;case 19:d=b.childExpirationTime>=c;if(0!==(a.effectTag&64)){if(d)return vh(a,b,c);b.effectTag|=64}e=b.memoizedState;null!==e&&(e.rendering=null,e.tail=null);y(D,D.current);if(!d)return null}return sa(a,b,c)}ia=!1}}else ia=!1;b.expirationTime=0;switch(b.tag){case 2:d=b.type;null!==a&&(a.alternate=null,b.alternate=null,b.effectTag|=2);a=b.pendingProps;e=pb(b,B.current);rb(b,c);e=we(null,
b,d,a,e,c);b.effectTag|=1;if("object"===typeof e&&null!==e&&"function"===typeof e.render&&void 0===e.$$typeof){b.tag=1;b.memoizedState=null;b.updateQueue=null;if(N(d)){var f=!0;Bc(b)}else f=!1;b.memoizedState=null!==e.state&&void 0!==e.state?e.state:null;ne(b);var g=d.getDerivedStateFromProps;"function"===typeof g&&Lc(b,d,g,a);e.updater=Mc;b.stateNode=e;e._reactInternalFiber=b;pe(b,d,a,c);b=Ie(null,b,d,!0,f,c)}else b.tag=0,T(null,b,e,c),b=b.child;return b;case 16:a:{e=b.elementType;null!==a&&(a.alternate=
null,b.alternate=null,b.effectTag|=2);a=b.pendingProps;ri(e);if(1!==e._status)throw e._result;e=e._result;b.type=e;f=b.tag=Gj(e);a=aa(e,a);switch(f){case 0:b=He(null,b,e,a,c);break a;case 1:b=rh(null,b,e,a,c);break a;case 11:b=nh(null,b,e,a,c);break a;case 14:b=oh(null,b,e,aa(e.type,a),d,c);break a}throw Error(k(306,e,""));}return b;case 0:return d=b.type,e=b.pendingProps,e=b.elementType===d?e:aa(d,e),He(a,b,d,e,c);case 1:return d=b.type,e=b.pendingProps,e=b.elementType===d?e:aa(d,e),rh(a,b,d,e,c);
case 3:sh(b);d=b.updateQueue;if(null===a||null===d)throw Error(k(282));d=b.pendingProps;e=b.memoizedState;e=null!==e?e.element:null;oe(a,b);Qb(b,d,null,c);d=b.memoizedState.element;if(d===e)Ee(),b=sa(a,b,c);else{if(e=b.stateNode.hydrate)Ka=kb(b.stateNode.containerInfo.firstChild),ra=b,e=Wa=!0;if(e)for(c=Fe(b,null,d,c),b.child=c;c;)c.effectTag=c.effectTag&-3|1024,c=c.sibling;else T(a,b,d,c),Ee();b=b.child}return b;case 5:return bh(b),null===a&&De(b),d=b.type,e=b.pendingProps,f=null!==a?a.memoizedProps:
null,g=e.children,Yd(d,e)?g=null:null!==f&&Yd(d,f)&&(b.effectTag|=16),qh(a,b),b.mode&4&&1!==c&&e.hidden?(b.expirationTime=b.childExpirationTime=1,b=null):(T(a,b,g,c),b=b.child),b;case 6:return null===a&&De(b),null;case 13:return th(a,b,c);case 4:return se(b,b.stateNode.containerInfo),d=b.pendingProps,null===a?b.child=wb(b,null,d,c):T(a,b,d,c),b.child;case 11:return d=b.type,e=b.pendingProps,e=b.elementType===d?e:aa(d,e),nh(a,b,d,e,c);case 7:return T(a,b,b.pendingProps,c),b.child;case 8:return T(a,
b,b.pendingProps.children,c),b.child;case 12:return T(a,b,b.pendingProps.children,c),b.child;case 10:a:{d=b.type._context;e=b.pendingProps;g=b.memoizedProps;f=e.value;var h=b.type._context;y(Ic,h._currentValue);h._currentValue=f;if(null!==g)if(h=g.value,f=Qa(h,f)?0:("function"===typeof d._calculateChangedBits?d._calculateChangedBits(h,f):1073741823)|0,0===f){if(g.children===e.children&&!G.current){b=sa(a,b,c);break a}}else for(h=b.child,null!==h&&(h.return=b);null!==h;){var m=h.dependencies;if(null!==
m){g=h.child;for(var l=m.firstContext;null!==l;){if(l.context===d&&0!==(l.observedBits&f)){1===h.tag&&(l=Ea(c,null),l.tag=Jc,Fa(h,l));h.expirationTime<c&&(h.expirationTime=c);l=h.alternate;null!==l&&l.expirationTime<c&&(l.expirationTime=c);Sg(h.return,c);m.expirationTime<c&&(m.expirationTime=c);break}l=l.next}}else g=10===h.tag?h.type===b.type?null:h.child:h.child;if(null!==g)g.return=h;else for(g=h;null!==g;){if(g===b){g=null;break}h=g.sibling;if(null!==h){h.return=g.return;g=h;break}g=g.return}h=
g}T(a,b,e.children,c);b=b.child}return b;case 9:return e=b.type,f=b.pendingProps,d=f.children,rb(b,c),e=W(e,f.unstable_observedBits),d=d(e),b.effectTag|=1,T(a,b,d,c),b.child;case 14:return e=b.type,f=aa(e,b.pendingProps),f=aa(e.type,f),oh(a,b,e,f,d,c);case 15:return ph(a,b,b.type,b.pendingProps,d,c);case 17:return d=b.type,e=b.pendingProps,e=b.elementType===d?e:aa(d,e),null!==a&&(a.alternate=null,b.alternate=null,b.effectTag|=2),b.tag=1,N(d)?(a=!0,Bc(b)):a=!1,rb(b,c),Yg(b,d,e),pe(b,d,e,c),Ie(null,
b,d,!0,a,c);case 19:return vh(a,b,c)}throw Error(k(156,b.tag));};var bf=null,Ne=null,la=function(a,b,c,d){return new Fj(a,b,c,d)};ef.prototype.render=function(a){md(a,this._internalRoot,null,null)};ef.prototype.unmount=function(){var a=this._internalRoot,b=a.containerInfo;md(null,a,null,function(){b[Lb]=null})};var Di=function(a){if(13===a.tag){var b=Fc(ka(),150,100);Ja(a,b);df(a,b)}};var Yf=function(a){13===a.tag&&(Ja(a,3),df(a,3))};var Bi=function(a){if(13===a.tag){var b=ka();b=Va(b,a,null);Ja(a,
b);df(a,b)}};sd=function(a,b,c){switch(b){case "input":Dd(a,c);b=c.name;if("radio"===c.type&&null!=b){for(c=a;c.parentNode;)c=c.parentNode;c=c.querySelectorAll("input[name="+JSON.stringify(""+b)+'][type="radio"]');for(b=0;b<c.length;b++){var d=c[b];if(d!==a&&d.form===a.form){var e=ae(d);if(!e)throw Error(k(90));Gf(d);Dd(d,e)}}}break;case "textarea":Lf(a,c);break;case "select":b=c.value,null!=b&&hb(a,!!c.multiple,b,!1)}};(function(a,b,c,d){ee=a;eg=b;vd=c;vf=d})(Qh,function(a,b,c,d,e){var f=p;p|=4;
try{return Da(98,a.bind(null,b,c,d,e))}finally{p=f,p===H&&ha()}},function(){(p&(1|ca|ma))===H&&(uj(),xb())},function(a,b){var c=p;p|=2;try{return a(b)}finally{p=c,p===H&&ha()}});var mk={Events:[Hb,Pa,ae,pf,qd,lb,function(a){Kd(a,Ki)},sf,tf,sc,pc,xb,{current:!1}]};(function(a){var b=a.findFiberByHostInstance;return Ej(M({},a,{overrideHookState:null,overrideProps:null,setSuspenseHandler:null,scheduleUpdate:null,currentDispatcherRef:da.ReactCurrentDispatcher,findHostInstanceByFiber:function(a){a=Sf(a);
return null===a?null:a.stateNode},findFiberByHostInstance:function(a){return b?b(a):null},findHostInstancesForRefresh:null,scheduleRefresh:null,scheduleRoot:null,setRefreshHandler:null,getCurrentFiber:null}))})({findFiberByHostInstance:Bb,bundleType:0,version:"16.13.1",rendererPackageName:"react-dom"});I.__SECRET_INTERNALS_DO_NOT_USE_OR_YOU_WILL_BE_FIRED=mk;I.createPortal=Xh;I.findDOMNode=function(a){if(null==a)return null;if(1===a.nodeType)return a;var b=a._reactInternalFiber;if(void 0===
b){if("function"===typeof a.render)throw Error(k(188));throw Error(k(268,Object.keys(a)));}a=Sf(b);a=null===a?null:a.stateNode;return a};I.flushSync=function(a,b){if((p&(ca|ma))!==H)throw Error(k(187));var c=p;p|=1;try{return Da(99,a.bind(null,b))}finally{p=c,ha()}};I.hydrate=function(a,b,c){if(!bc(b))throw Error(k(200));return nd(null,a,b,!0,c)};I.render=function(a,b,c){if(!bc(b))throw Error(k(200));return nd(null,a,b,!1,c)};I.unmountComponentAtNode=function(a){if(!bc(a))throw Error(k(40));return a._reactRootContainer?
(Rh(function(){nd(null,null,a,!1,function(){a._reactRootContainer=null;a[Lb]=null})}),!0):!1};I.unstable_batchedUpdates=Qh;I.unstable_createPortal=function(a,b){return Xh(a,b,2<arguments.length&&void 0!==arguments[2]?arguments[2]:null)};I.unstable_renderSubtreeIntoContainer=function(a,b,c,d){if(!bc(c))throw Error(k(200));if(null==a||void 0===a._reactInternalFiber)throw Error(k(38));return nd(a,b,c,!1,d)};I.version="16.13.1"});
</script>
    <script>const e = React.createElement;

function pathToString(path) {
  if (path[0] === '/') {
    return '/' + path.slice(1).join('/');
  } else {
    return path.join('/');
  }
}

function findCommonPath(files) {
  if (!files || !files.length) {
    return [];
  }

  function isPrefix(arr, prefix) {
    if (arr.length < prefix.length) {
      return false;
    }
    for (let i = prefix.length - 1; i >= 0; --i) {
      if (arr[i] !== prefix[i]) {
        return false;
      }
    }
    return true;
  }

  let commonPath = files[0].path.slice(0, -1);
  while (commonPath.length) {
    if (files.every(file => isPrefix(file.path, commonPath))) {
      break;
    }
    commonPath.pop();
  }
  return commonPath;
}

function findFolders(files) {
  if (!files || !files.length) {
    return [];
  }

  let folders = files.filter(file => file.path.length > 1).map(file => file.path[0]);
  folders = [...new Set(folders)]; // unique
  folders.sort();

  folders = folders.map(folder => {
    let filesInFolder = files
      .filter(file => file.path[0] === folder)
      .map(file => ({
        ...file,
        path: file.path.slice(1),
        parent: [...file.parent, file.path[0]],
      }));

    const children = findFolders(filesInFolder); // recursion

    return {
      is_folder: true,
      path: [folder],
      parent: files[0].parent,
      children,
      covered: children.reduce((sum, file) => sum + file.covered, 0),
      coverable: children.reduce((sum, file) => sum + file.coverable, 0),
      prevRun: {
        covered: children.reduce((sum, file) => sum + file.prevRun.covered, 0),
        coverable: children.reduce((sum, file) => sum + file.prevRun.coverable, 0),
      },
    };
  });

  return [...folders, ...files.filter(file => file.path.length === 1)];
}

class App extends React.Component {
  constructor(...args) {
    super(...args);

    this.state = {
      current: [],
    };
  }

  componentDidMount() {
    this.updateStateFromLocation();
    window.addEventListener('hashchange', () => this.updateStateFromLocation(), false);
  }

  updateStateFromLocation() {
    if (window.location.hash.length > 1) {
      const current = window.location.hash.slice(1).split('/');
      this.setState({current});
    } else {
      this.setState({current: []});
    }
  }

  getCurrentPath() {
    let file = this.props.root;
    let path = [file];
    for (let p of this.state.current) {
      file = file.children.find(file => file.path[0] === p);
      if (!file) {
        return path;
      }
      path.push(file);
    }
    return path;
  }

  render() {
    const path = this.getCurrentPath();
    const file = path[path.length - 1];

    let w = null;
    if (file.is_folder) {
      w = e(FilesList, {
        folder: file,
        onSelectFile: this.selectFile.bind(this),
        onBack: path.length > 1 ? this.back.bind(this) : null,
      });
    } else {
      w = e(DisplayFile, {
        file,
        onBack: this.back.bind(this),
      });
    }

    return e('div', {className: 'app'}, w);
  }

  selectFile(file) {
    this.setState(
      ({current}) => {
        return {current: [...current, file.path[0]]};
      },
      () => this.updateHash(),
    );
  }

  back(file) {
    this.setState(
      ({current}) => {
        return {current: current.slice(0, current.length - 1)};
      },
      () => this.updateHash(),
    );
  }

  updateHash() {
    if (!this.state.current || !this.state.current.length) {
      window.location = '#';
    } else {
      window.location = '#' + this.state.current.join('/');
    }
  }
}

function FilesList({folder, onSelectFile, onBack}) {
  let files = folder.children;
  return e(
    'div',
    {className: 'display-folder'},
    e(FileHeader, {file: folder, onBack}),
    e(
      'table',
      {className: 'files-list'},
      e('thead', {className: 'files-list__head'}, e('tr', null, e('th', null, 'Path'), e('th', null, 'Coverage'))),
      e(
        'tbody',
        {className: 'files-list__body'},
        files.map(file => e(File, {file, onClick: onSelectFile})),
      ),
    ),
  );
}

function File({file, onClick}) {
  const coverage = file.coverable ? (file.covered / file.coverable) * 100 : -1;
  const coverageDelta =
    file.prevRun && (file.covered / file.coverable) * 100 - (file.prevRun.covered / file.prevRun.coverable) * 100;

  return e(
    'tr',
    {
      className:
        'files-list__file' +
        (coverage >= 0 && coverage < 50 ? ' files-list__file_low' : '') +
        (coverage >= 50 && coverage < 80 ? ' files-list__file_medium' : '') +
        (coverage >= 80 ? ' files-list__file_high' : '') +
        (file.is_folder ? ' files-list__file_folder' : ''),
      onClick: () => onClick(file),
    },
    e('td', null, e('a', null, pathToString(file.path))),
    e(
      'td',
      null,
      file.covered + ' / ' + file.coverable + (coverage >= 0 ? ' (' + coverage.toFixed(2) + '%)' : ''),
      e(
        'span',
        {title: 'Change from the previous run'},
        coverageDelta ? ` (${coverageDelta > 0 ? '+' : ''}${coverageDelta.toFixed(2)}%)` : '',
      ),
    ),
  );
}

function DisplayFile({file, onBack}) {
  return e('div', {className: 'display-file'}, e(FileHeader, {file, onBack}), e(FileContent, {file}));
}

function FileHeader({file, onBack}) {
  const coverage = (file.covered / file.coverable) * 100;
  const coverageDelta = file.prevRun && coverage - (file.prevRun.covered / file.prevRun.coverable) * 100;

  return e(
    'div',
    {className: 'file-header'},
    onBack ? e('a', {className: 'file-header__back', onClick: onBack}, 'Back') : null,
    e('div', {className: 'file-header__name'}, pathToString([...file.parent, ...file.path])),
    e(
      'div',
      {className: 'file-header__stat'},
      'Covered: ' + file.covered + ' of ' + file.coverable + (file.coverable ? ' (' + coverage.toFixed(2) + '%)' : ''),
      e(
        'span',
        {title: 'Change from the previous run'},
        coverageDelta ? ` (${coverageDelta > 0 ? '+' : ''}${coverageDelta.toFixed(2)}%)` : '',
      ),
      e('input', {id: 'theme-toggle', type: 'checkbox', hidden: true}),
      e('label', {for: 'theme-toggle', id: 'theme-toggle-label'}, 'ð'),
    ),
  );
}

function FileContent({file}) {
  return e(
    'pre',
    {className: 'file-content'},
    file.content.split(/\r?\n/).map((line, index) => {
      const trace = file.traces.find(trace => trace.line === index + 1);
      const covered = trace && trace.stats.Line;
      const uncovered = trace && !trace.stats.Line;
      const nbHit = covered? trace.stats.Line: 0;
      return e(
        'div',
        { className: 'code-text-container' },
        e(
          'code',
          {
            className: 'code-line' + (covered ? ' code-line_covered' : '') + (uncovered ? ' code-line_uncovered' : ''),
          },
          line
        ),
        e(
          'div',
          { className: 'cover-indicator' + (covered? ' check-cover': '') + (uncovered? ' no-cover': '')},
          e(
            'div',
            { className: (covered? 'stat-line-hit': '')},
            covered? nbHit: ""
          )
        )
      );
    }),
  );
}

(function () {
  const commonPath = findCommonPath(data.files);
  const prevFilesMap = new Map();

  previousData &&
    previousData.files.forEach(file => {
      const path = file.path.slice(commonPath.length).join('/');
      prevFilesMap.set(path, file);
    });

  const files = data.files.map(file => {
    const path = file.path.slice(commonPath.length);
    const {covered = 0, coverable = 0} = prevFilesMap.get(path.join('/')) || {};
    return {
      ...file,
      path,
      parent: commonPath,
      prevRun: {covered, coverable},
    };
  });

  const children = findFolders(files);

  const root = {
    is_folder: true,
    children,
    path: commonPath,
    parent: [],
    covered: children.reduce((sum, file) => sum + file.covered, 0),
    coverable: children.reduce((sum, file) => sum + file.coverable, 0),
    prevRun: {
      covered: children.reduce((sum, file) => sum + file.prevRun.covered, 0),
      coverable: children.reduce((sum, file) => sum + file.prevRun.coverable, 0),
    },
  };

  ReactDOM.render(e(App, {root, prevFilesMap}), document.getElementById('root'));

  const toggle = document.getElementById('theme-toggle');
  const label = document.getElementById('theme-toggle-label');
  label.textContent = 'ð';

  toggle.addEventListener('change', () => {
    if (toggle.checked) {
      document.documentElement.setAttribute('data-theme', 'dark');
      label.textContent = 'âï¸';
    } else {
      document.documentElement.removeAttribute('data-theme');
      label.textContent = 'ð';
    }
  });
})();
</script>
</body>
</html>